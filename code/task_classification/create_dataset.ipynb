{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "Aim to create the dataset of abstract for task classification.\n",
    "From OpenAlex inverted_abstract will create a reconstructed string version of the abstract and classify this task in regard of the dataset the paper references"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using list of paper that references a dataset with OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#Dictionnary with dataset as key and organ as value (e.g ACDC:Cardiac)\n",
    "datasets_context = {}\n",
    "ds_reader = csv.DictReader(open('../../data/datasets.csv'))\n",
    "for ds in ds_reader:\n",
    "    datasets_context[ds[\"name\"]] = ds[\"context\"]\n",
    "    \n",
    "df = pd.read_csv(\"../../results/extracted_csv/paper_openalex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = \"https://api.openalex.org/works/doi\"\n",
    "\n",
    "#Dictionnary with doi as key and tuple (abstract,task) as value\n",
    "paper_abstract = {}\n",
    "with open(\"../../results/extracted_csv/paper_openalex.csv\",\"r\") as paper_csv:\n",
    "    with open(\"../../data/abstract_dataset.csv\",\"w\") as abstract_file:\n",
    "        abstract_file.write(f'title,doi,publication_year,abstract,task')\n",
    "        text = csv.reader(paper_csv,)\n",
    "        header=next(text)\n",
    "        for i,l in enumerate(text):\n",
    "            organ = datasets_context[l[-1]]\n",
    "            #Check if the paper were already seen before to not compute 2 times the abstract\n",
    "            if not paper_abstract.get(l[1],None):\n",
    "                r = requests.get(query_url+l[1])\n",
    "                if r.status_code == 200:\n",
    "                    r_json = r.json()\n",
    "                    if r_json[\"abstract_inverted_index\"]:\n",
    "                        abstract = np.full(1000,\"\",dtype=object)\n",
    "                        for w in r_json[\"abstract_inverted_index\"]:\n",
    "                            for indices in r_json[\"abstract_inverted_index\"][w]:\n",
    "                                abstract[indices] = ''.join(filter(str.isalnum, w)).lower()\n",
    "                        abstract = abstract[abstract != \"\"]\n",
    "                        str_abstract = ' '.join(abstract)\n",
    "                        paper_abstract[l[1]] = (str_abstract,organ)\n",
    "                        abstract_file.write(f'\\n{l[0]},{l[1]},\"{paper_abstract[l[1]][0]}\",{organ}')\n",
    "            #If the paper were already seen, we check if the organ were the same to not have a duplicate in the dataset\n",
    "            elif organ != paper_abstract[l[1]][1]:\n",
    "                abstract_file.write(f'\\n{l[0]},{l[1]},{l[2]},\"{paper_abstract[l[1]][0]}\",{organ}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using list of papers that cite a dataset or an organ in the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnary with keyword as key and organ as value\n",
    "keyword_dict = {}\n",
    "with open(\"../../data/keywords.csv\",'r') as f:\n",
    "    for l in csv.DictReader(f):\n",
    "        keyword_dict[l[\"keyword\"]] = l[\"organ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = \"https://api.openalex.org/works\"\n",
    "with open('../../results/classification/raw_abstract.csv',\"w\") as abstract_file:\n",
    "    abstract_file.write(f'title,doi,publication_year,abstract,task')\n",
    "    for year in range(2014,2024):\n",
    "        print(year)\n",
    "        #OpenAlex return 25 result per page so we have to iterate to get all of them\n",
    "        next_page = True\n",
    "        page_number = 1\n",
    "        while next_page:\n",
    "            #Query for papers tagged with \"Machine Learning\" and \"Segmentation\" concept, have an abstract and is published on the current iteration year\n",
    "            query_param = {\n",
    "                'page':page_number,\n",
    "                'filter':f\"concepts.id:C89600930,concepts.id:C154945302,has_abstract:true,publication_year:{year}\"\n",
    "            }\n",
    "            r_year = requests.get(query_url,params=query_param)\n",
    "            if r_year.status_code == 200:\n",
    "                r_json = r_year.json()\n",
    "                #If \"results\" field is empty we have reached the end for the current year\n",
    "                if not r_json[\"results\"]:\n",
    "                    next_page = False\n",
    "                else:\n",
    "                    # For each paper in the current page\n",
    "                    for paper in r_json[\"results\"]:\n",
    "                        # Maximum size of the abstract, if the paper abstarct is longer it will be truncated\n",
    "                        abstract = np.full(2500,\"\",dtype=object)\n",
    "                        # The \"abstract_inverted_index\" field is a dictionnary with word as key and locations of this word in the abstract\n",
    "                        # So we fill the abstarct variable above at the index of the word to reconstruct the abstract\n",
    "                        for w in paper[\"abstract_inverted_index\"]:\n",
    "                            for indices in paper[\"abstract_inverted_index\"][w]:\n",
    "                                if indices < 2500:\n",
    "                                    abstract[indices] = ''.join(filter(str.isalnum, w)).lower()\n",
    "                        # Remove empty location mostly due to a shorter abstract \n",
    "                        abstract = abstract[abstract != \"\"]\n",
    "                        #Convert array to string\n",
    "                        str_abstract = ' '.join(abstract)\n",
    "\n",
    "                        #Get title and remove \",\" and \"\\n\" that will create problem in the result csv \n",
    "                        title = paper[\"title\"]\n",
    "                        if title:\n",
    "                            title = title.replace(\",\",\"\")\n",
    "                            title = title.replace(\"\\n\",\"\")\n",
    "\n",
    "                        # Search for keyword in the abstract\n",
    "                        for k in keyword_dict:\n",
    "                            if k in str_abstract:\n",
    "                                abstract_file.write(f'\\n{title},{paper[\"doi\"]},{paper[\"publication_year\"]},\"{str_abstract}\",{keyword_dict[k]}')\n",
    "                    page_number += 1\n",
    "            else:\n",
    "                next_page=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates in the previous generated csv due to the detection of multiple keywords for the same organ\n",
    "df = pd.read_csv(\"../../results/classification/raw_abstract.csv\")\n",
    "df = df.drop_duplicates(subset=['title','task'])\n",
    "df.to_csv(\"../../data/abstract_dataset.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of non-relevant papers selected manually to add to the dataset to make the model able to detect non related papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = \"https://api.openalex.org/works/doi\"\n",
    "\n",
    "with open(\"../../data/paper_other.csv\",\"r\") as paper_csv:\n",
    "    with open(\"../../data/abstract_dataset_nonrelevant.csv\",\"w\") as abstract_file:\n",
    "        abstract_file.write(\"title,doi,publication_year,abstract,task\")\n",
    "        text = csv.reader(paper_csv,)\n",
    "        for i,l in enumerate(text):\n",
    "            r = requests.get(query_url+l[0])\n",
    "            if r.status_code == 200:\n",
    "                r_json = r.json()\n",
    "                if r_json[\"abstract_inverted_index\"]:\n",
    "                    abstract = np.full(1000,\"\",dtype=object)\n",
    "                    for w in r_json[\"abstract_inverted_index\"]:\n",
    "                        for indices in r_json[\"abstract_inverted_index\"][w]:\n",
    "                            abstract[indices] = ''.join(filter(str.isalnum, w)).lower()\n",
    "                    abstract = abstract[abstract != \"\"]\n",
    "                    str_abstract = ' '.join(abstract)\n",
    "                    title = r_json[\"title\"]\n",
    "                    title = title.replace(\",\",\"\")\n",
    "                    title = title.replace(\"\\n\",\"\")\n",
    "                    abstract_file.write(f'\\n{title},{l[0]},\"{str_abstract}\",Other')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
