{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract dataset used by a paper with OpenAlex API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple of request for the paper \"Exploring Soothness...\":  https://api.openalex.org/works/https://doi.org/10.1007/978-3-031-16443-9_4?select=referenced_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACDC': 'Cardiac',\n",
      " 'BRATS': 'Brain',\n",
      " 'I2CVB': 'Prostate',\n",
      " 'LA': 'Cardiac',\n",
      " 'M&Ms': 'Prostate',\n",
      " 'MSCMRSeg': 'Cardiac',\n",
      " 'Medical Decathlon': 'Prostate',\n",
      " 'PROMISE12': 'Cardiac',\n",
      " 'Synapse': 'Multi-organ'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import csv\n",
    "\n",
    "#Dictionnary of papers with names as key and DOIs as value. Loaded from papers.csv file\n",
    "papers = {}\n",
    "pap_reader = csv.DictReader(open('../data/papers.csv'))\n",
    "for p in pap_reader:\n",
    "    papers[p[\"name\"]] = p[\"DOI\"]\n",
    "\n",
    "\n",
    "#Dictionnary with dataset names as key and openalex id as value. We associate an openalex ID because it's the value in the \"referenced_works\" field given by the API.\n",
    "#To save a request, I use directly the OpenAlex ID otherwise you need to make a request with the ID to get the DOI or the name.\n",
    "datasets_id = {}\n",
    "\n",
    "#Dictionnary with dataset's name as key and context/segmented organ as value\n",
    "datasets_context = {}\n",
    "\n",
    "#The 2 informations are in datasets.csv file\n",
    "ds_reader = csv.DictReader(open('../data/datasets.csv'))\n",
    "for ds in ds_reader:\n",
    "    datasets_id[ds[\"name\"]] = ds[\"openalex_id\"]\n",
    "    datasets_context[ds[\"name\"]] = ds[\"context\"]\n",
    "\n",
    "pprint(datasets_context)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the OpenAlex API with paper's DOI and check in the \"referenced_works\" field which contains a list of OpenAlex ID if one of them match a dataset we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation': ['ACDC',\n",
      "                                                                                              'LA'],\n",
      " 'MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation': ['PROMISE12',\n",
      "                                                                                   'I2CVB'],\n",
      " 'Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation': [],\n",
      " 'TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation': [],\n",
      " 'nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer': ['ACDC']}\n"
     ]
    }
   ],
   "source": [
    "#Base url for the request\n",
    "base_url = \"https://api.openalex.org/works/https://doi.org/\"\n",
    "\n",
    "#To only get the field of referenced works in the response\n",
    "query_param = {\n",
    "    \"select\":\"referenced_works\"\n",
    "}\n",
    "\n",
    "\n",
    "#Will be filled with datasets found in the \"referenced works\" response of each paper\n",
    "datasets_used = {p:[] for p in papers}\n",
    "\n",
    "for paper_name in papers:\n",
    "    request_url = base_url + papers[paper_name]\n",
    "    request = requests.get(request_url, params=query_param)\n",
    "    json_response = request.json()\n",
    "    for ds in datasets_id:\n",
    "        if datasets_id[ds] in json_response['referenced_works']:\n",
    "            datasets_used[paper_name].append(ds)\n",
    "pprint(datasets_used)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Paper Task with the abstract contains in API Response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple of request for the paper \"Exploring Soothness...\":  https://api.openalex.org/works/https://doi.org/10.1007/978-3-031-16443-9_4?select=abstract_inverted_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're searching for keywords in the abstract of the papers by using the same query as before but looking in the \"abstract_inverted_index\" field which contains the list of word in the abstract. The goal here is not to find the dataset but the task of the paper (brain segmentation, heart segmentation, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation': [],\n",
       " 'MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation': ['Cardiac',\n",
       "  'Prostate'],\n",
       " 'nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer': [],\n",
       " 'Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation': ['Cardiac',\n",
       "  'Multi-organ'],\n",
       " 'TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation': ['Cardiac',\n",
       "  'Multi-organ']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Base url for the request\n",
    "base_url = \"https://api.openalex.org/works/https://doi.org/\"\n",
    "\n",
    "#To only get the field of referenced works in the response\n",
    "query_param = {\n",
    "    \"select\":\"abstract_inverted_index\"\n",
    "}\n",
    "\n",
    "key_words = [\"Heart\",\"Cardiac\",\"Prostate\",\"Multi-organ\",\"Brain\",\"Liver\",\"Aorta\",\"Gallblader\",\"Spleen\",\"Kidney\",\"Pancreas\",\"Stomach\",\"Lung\",\"Breast\",\"Chest\",\"Skin\",\"Eye\",\"Retina\"]\n",
    "paper_theme = {p:[] for p in papers}\n",
    "\n",
    "for paper_name in papers:\n",
    "    request_url = base_url + papers[paper_name]\n",
    "    request = requests.get(request_url, params=query_param)\n",
    "    json_response = request.json()\n",
    "    abstract_words = list(json_response[\"abstract_inverted_index\"])\n",
    "    for kw in key_words:\n",
    "        if np.array([kw.upper() in w.upper() for w in abstract_words]).any():\n",
    "            paper_theme[paper_name].append(kw)\n",
    "\n",
    "paper_theme"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the information between context obtained with abstract and context obtained with referenced datasets "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can combine the 2 previous informations (referenced datasets and task in the abstract) to be more accurate on the task of a paper. As we can get the context/task of a dataset in datasets.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================Before fusion using only abstract==============================\n",
      "{'Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation': [],\n",
      " 'MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation': ['Cardiac',\n",
      "                                                                                   'Prostate'],\n",
      " 'Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation': ['Cardiac',\n",
      "                                                                          'Multi-organ'],\n",
      " 'TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation': ['Cardiac',\n",
      "                                                                                 'Multi-organ'],\n",
      " 'nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer': []}\n",
      "\n",
      "==============================After fusion using both abstract and referenced papers==============================\n",
      "{'Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation': ['Cardiac'],\n",
      " 'MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation': ['Cardiac',\n",
      "                                                                                   'Prostate'],\n",
      " 'Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation': ['Cardiac',\n",
      "                                                                          'Multi-organ'],\n",
      " 'TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation': ['Cardiac',\n",
      "                                                                                 'Multi-organ'],\n",
      " 'nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer': ['Cardiac']}\n"
     ]
    }
   ],
   "source": [
    "print(\"==============================Before fusion using only abstract==============================\")\n",
    "pprint(paper_theme)\n",
    "for paper_name in papers:\n",
    "    datasets = datasets_used[paper_name]\n",
    "    for ds in datasets:\n",
    "        if datasets_context[ds] not in paper_theme[paper_name]:\n",
    "            paper_theme[paper_name].append(datasets_context[ds])\n",
    "print(\"\\n==============================After fusion using both abstract and referenced papers==============================\")\n",
    "pprint(paper_theme)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "extract with dataset"
    ]
   },
   "source": [
    "## Extract papers that use a dataset with OpenAlex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple of request for ACDC: https://api.openalex.org/works?page=2&filter=cites:W2804047627"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather papers that cite one of the dataset we want to monitor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it proceed ?\n",
    "\n",
    "2. For each dataset, make a get request to https://api.openalex.org/works/filter=cites:{openalexID_of_the_dataset}\n",
    "3. From the API response, we iterate through the pages (a page contains a limited number of references) to get the DOI and year of the paper that cites the dataset\n",
    "4. Export the result in paper_OpenAlex.csv \n",
    "Each line contains the title of the paper,the doi,the publication year, a dataset name. So we have a line per dataset a paper is referencing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionnary containing with dataset as key and a list of papers\n",
    "paper_using = {ds:[] for ds in datasets_id}\n",
    "\n",
    "\n",
    "for ds in datasets_id:\n",
    "    #The list of paper referencing the dataset is decomposed in multiple pages so we have to iterate with the query parameter \"page\" to get them all.\n",
    "    next_page = True\n",
    "    page_number = 1\n",
    "    while next_page:\n",
    "        #Definition of the request\n",
    "        base_url = \"https://api.openalex.org/works\"\n",
    "        query_param = {\n",
    "            \"filter\":f\"cites:{datasets_id[ds]}\",\n",
    "            \"page\":page_number\n",
    "        }\n",
    "        request = requests.get(base_url,params=query_param)\n",
    "\n",
    "        if request.status_code == 200:\n",
    "            request_json = request.json()\n",
    "            \n",
    "            #For each paper referencing the dataset we get the title (with a little transformation to remove \",\" and \"\\n\" inside of them), doi and publication year\n",
    "            for res in request_json[\"results\"]:\n",
    "                title = res[\"title\"]\n",
    "                title = title.replace(\",\",\"\")\n",
    "                title = title.replace(\"\\n\",\"\")\n",
    "                doi = res[\"doi\"]\n",
    "                if doi is not None:\n",
    "                    doi = doi[16:] #Remove the https://doi.org/\n",
    "                    paper_using[ds].append((title,doi,res[\"publication_year\"]))\n",
    "\n",
    "            #If the results field is empty that mean we are at the last page so we can continue to the next dataset\n",
    "            #otherwise we need to go to next page of the current dataset\n",
    "            if not request_json[\"results\"]:\n",
    "                next_page = False\n",
    "            else:\n",
    "                page_number += 1\n",
    "        else:\n",
    "            next_page = False\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the result in papers_openalex.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../extracted_csv/paper_openalex.csv\",\"w\") as file:\n",
    "    file.write(f\"name,DOI,publication_year,dataset_used\")\n",
    "    for ds in datasets_id:\n",
    "        lst_papers = paper_using[ds]\n",
    "        for pap in lst_papers:\n",
    "            file.write(f\"\\n{pap[0]},{pap[1]},{pap[2]},{ds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
