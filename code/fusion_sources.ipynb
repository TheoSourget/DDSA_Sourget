{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion sources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this file is to merge the information extracted with the different sources such as OpenAlex or OpenCitation together to generate the final output of the process which contains all the association paper-dataset-task to allow further analysis after."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this proceed ?\n",
    "1. Load every generated csv present in extracted_csv folder\n",
    "2. For each line in a csv:\n",
    "    1. Test if the association DOI-Dataset has already been seen\n",
    "    2. If not create a new line in merged.csv with the addition of the information about the task find in dataset.csv \"context\" field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from itertools import product\n",
    "import random\n",
    "random.seed(1907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../extracted_csv/paper_poci.csv',\n",
       " '../extracted_csv/paper_openalex.csv',\n",
       " '../extracted_csv/paper_coci.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_csv = glob.glob(\"../extracted_csv/*.csv\")\n",
    "\n",
    "extracted_csv.remove('../extracted_csv/paper_dimensions.csv')\n",
    "extracted_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../extracted_csv/paper_poci.csv\n",
      "../extracted_csv/paper_openalex.csv\n"
     ]
    }
   ],
   "source": [
    "#Dictionnary with dataset's name as key and task as value (e.g ACDC:cardiac)\n",
    "datasets_context = {}\n",
    "ds_reader = csv.DictReader(open('../data/datasets.csv'))\n",
    "for ds in ds_reader:\n",
    "    datasets_context[ds[\"name\"]] = ds[\"context\"]\n",
    "\n",
    "\n",
    "#Dictionnaries to keep track of which DOI/Paper we've already encountered to not add duplicates\n",
    "doi_using_dataset = {ds:[] for ds in datasets_context}\n",
    "title_using_dataset = {ds:[] for ds in datasets_context}\n",
    "\n",
    "#Get the list of csv from different sources\n",
    "extracted_csv = glob.glob(\"../extracted_csv/*.csv\")\n",
    "\n",
    "extracted_csv.remove('../extracted_csv/paper_dimensions.csv')\n",
    "\n",
    "#Dictionnary with paper DOI as key and list of sources that give back that paper as value\n",
    "sources_coverage = {}\n",
    "\n",
    "#title of papers as keys and DOI as value, use for papers with multiple DOI to do the coverage analysis\n",
    "name_doi = {}\n",
    "\n",
    "#Url use to request a DOI in order to get the venue\n",
    "base_url_venue = \"https://api.openalex.org/works/https://doi.org/\"\n",
    "\n",
    "# /!\\ WARNING /!\\: If merged.csv already exist, the previous content will be removed !\n",
    "with open(\"../processed_csv/merged2.csv\",\"w\") as merged :\n",
    "    merged.write(f\"name,DOI,publication_year,dataset_used,task\")\n",
    "    for i,csv_path in enumerate(extracted_csv):\n",
    "        print(csv_path)\n",
    "        csv_reader = csv.DictReader(open(csv_path))\n",
    "        for line in csv_reader:\n",
    "            \n",
    "            #Check if we've already seen this paper before with another source\n",
    "            doi_encountered = line['DOI'].lower() not in doi_using_dataset[line['dataset_used']]\n",
    "            title_encountered = line['name'].lower() not in title_using_dataset[line['dataset_used']]\n",
    "\n",
    "            if doi_encountered and title_encountered:\n",
    "\n",
    "                #Get the paper's venue\n",
    "                request_url = base_url_venue + line['DOI']\n",
    "                request = requests.get(request_url)\n",
    "                venue = None\n",
    "                if request.status_code == 200:\n",
    "                    json_response = request.json()\n",
    "                    venue = json_response[\"host_venue\"][\"display_name\"]\n",
    "\n",
    "                doi_using_dataset[line['dataset_used']].append(line['DOI'].lower())\n",
    "                title_using_dataset[line['dataset_used']].append(line['name'].lower())\n",
    "                \n",
    "                merged.write(f\"\\n{line['name'].lower()},{line['DOI'].lower()},{line['publication_year'].lower()},{line['dataset_used']},{datasets_context[line['dataset_used']]}\")\n",
    "                sources_coverage[line['DOI'].lower()] = [False for _ in range(len(extracted_csv))] + [venue,line['publication_year']]\n",
    "                sources_coverage[line['DOI'].lower()][i] = True\n",
    "                name_doi[line[\"name\"].lower()] = line['DOI'].lower()\n",
    "                \n",
    "            elif line['name'].lower() in name_doi:\n",
    "                sources_coverage[name_doi[line['name'].lower()]][i] = True\n",
    "            elif line['DOI'].lower() in sources_coverage:\n",
    "                sources_coverage[line['DOI'].lower()][i] = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of coverage\n",
    "Analysis the result from the different sources and compare them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = []\n",
    "for e in extracted_csv:\n",
    "    columns_name.append(e[23:-4])\n",
    "columns_name.append(\"venue\")\n",
    "columns_name.append(\"publication_year\")\n",
    "df = pd.DataFrame.from_dict(sources_coverage).transpose()\n",
    "df.index.name = 'DOI'\n",
    "df.columns = columns_name\n",
    "df.to_csv(\"../processed_csv/coverage.csv\")\n",
    "\n",
    "possibleCombination = sorted(np.array([ele for ele in product([True,False], repeat = df.shape[1]-2)]),key=sum)\n",
    "possibleCombination_code = {str(e):i for i,e in enumerate(possibleCombination)}\n",
    "\n",
    "\n",
    "def compute_combination(row):\n",
    "    combination = np.array([row[i] for i in range(df.shape[1]-2)])\n",
    "    return possibleCombination_code[str(combination)]\n",
    "\n",
    "\n",
    "df[\"combination\"] =df.apply(compute_combination,axis=1)\n",
    "\n",
    "df = df.sort_values('combination',ascending=False)\n",
    "\n",
    "def color_background(row):\n",
    "    return [\n",
    "        'background-color: green; color: green' if cell == True\n",
    "        else 'background-color: red; color: red'\n",
    "        for cell in row\n",
    "    ]\n",
    "df_html = df.iloc[:,:df.shape[1]-1].style.apply(color_background,subset=columns_name[:-2])\n",
    "df_html.to_html(\"../res/coverage.html\",col_space='100px')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
