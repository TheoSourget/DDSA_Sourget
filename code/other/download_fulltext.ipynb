{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of papers from venue that cites a dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To request OpenAlex\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Better print of dict\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "#To load data\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#To filter invalid pdf\n",
    "from pypdf import PdfReader\n",
    "from pypdf.errors import PdfReadError\n",
    "\n",
    "#To handle files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#To extract images from pdf\n",
    "import fitz\n",
    "\n",
    "#To extract tables from pdf\n",
    "import camelot\n",
    "import ghostscript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of datasets and list of venues\n",
    "We will search for paper from venue contained in [venues.csv](../../data/venues.csv) referencing one or more of the datasets contained in [datasets.csv](../../data/datasets.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionnary with dataset's name as key and DOI as value\n",
    "datasets_doi = {}\n",
    "ds_reader = csv.DictReader(open('../../data/datasets.csv'))\n",
    "for ds in ds_reader:\n",
    "    datasets_doi[ds[\"name\"]] = ds[\"DOI\"]\n",
    "\n",
    "#Dictionnary with venues name as key and openalex id as value loaded from venues.csv\n",
    "venue_id = {}\n",
    "ds_reader = csv.DictReader(open('../../data/venues.csv'))\n",
    "for ds in ds_reader:\n",
    "    venue_id[ds[\"name\"]] = ds[\"openalex_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "{'ACDC': '10.1109/TMI.2018.2837502',\n",
      " 'BRATS': '10.1109/tmi.2014.2377694',\n",
      " 'I2CVB': '10.1016/j.compbiomed.2015.02.009',\n",
      " 'LA': '10.1016/j.media.2020.101832',\n",
      " 'M&Ms': '10.1109/tmi.2021.3090082',\n",
      " 'MSCMRSeg': '10.48550/arxiv.2006.12434',\n",
      " 'Medical Decathlon': '10.1038/s41467-022-30695-9',\n",
      " 'PROMISE12': '10.1016/j.media.2013.12.002',\n",
      " 'Synapse': '10.7303/syn3193805'}\n",
      "\n",
      "Venues:\n",
      "{'LNCS': 'S106296714'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Datasets:\")\n",
    "pprint(datasets_doi)\n",
    "print(\"\\nVenues:\")\n",
    "pprint(venue_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to convert DOI to OpenAlex ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't convert DOI for Synapse into OpenAlex ID\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert a DOI to OpenAlex ID used as value in some API field such as \"referenced_works\"\n",
    "@param\n",
    "    - DOI: the doi we want to convert\n",
    "@return\n",
    "    The OpenAlex ID if the DOI is in OpenAlex database, None otherwise\n",
    "\"\"\"\n",
    "def doi_to_OpenAlexId(doi):\n",
    "    base_url = f\"https://api.openalex.org/works/doi:{doi}\"\n",
    "    r = requests.get(base_url)\n",
    "    if r.status_code == 200:\n",
    "        r_json = r.json()\n",
    "        return r_json[\"id\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Dictionnary with dataset names as key and openalex id as value. We associate an openalex ID because it's the value in the \"referenced_works\" field given by the API.\n",
    "datasets_id = {}\n",
    "\n",
    "#Convert DOI to OpenAlexID\n",
    "for ds  in datasets_doi:\n",
    "    openalex_id = doi_to_OpenAlexId(datasets_doi[ds])\n",
    "    if not openalex_id:\n",
    "        print(f\"Couldn't convert DOI for {ds} into OpenAlex ID\")\n",
    "    datasets_id[ds]=openalex_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACDC': 'https://openalex.org/W2804047627',\n",
      " 'BRATS': 'https://openalex.org/W1641498739',\n",
      " 'I2CVB': 'https://openalex.org/W2049522781',\n",
      " 'LA': 'https://openalex.org/W3093394156',\n",
      " 'M&Ms': 'https://openalex.org/W4226199676',\n",
      " 'MSCMRSeg': 'https://openalex.org/W4312016581',\n",
      " 'Medical Decathlon': 'https://openalex.org/W3172681723',\n",
      " 'PROMISE12': 'https://openalex.org/W2106033751',\n",
      " 'Synapse': None}\n"
     ]
    }
   ],
   "source": [
    "pprint(datasets_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: Synapse value is None meaning it's note in OpenAlex database, therefore we won't be able to find references to it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of papers from each venues citing at least one of the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query OpenAlex filtering on the dataset and the venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionnary containing with dataset as key and a list of papers referencing the dataset as value\n",
    "paper_referencing = {ds:[] for ds in datasets_id}\n",
    "\n",
    "\n",
    "for ds in datasets_id:\n",
    "    for venue in venue_id:\n",
    "        #The list of paper referencing the dataset is decomposed in multiple pages so we have to iterate with the query parameter \"page\" to get them all.\n",
    "        next_page = True\n",
    "        page_number = 1\n",
    "        while next_page:\n",
    "            #Definition of the request\n",
    "            base_url = \"https://api.openalex.org/works\"\n",
    "            query_param = {\n",
    "                \"filter\":f\"cites:{datasets_id[ds]},locations.source.id:{venue_id[venue]}\",\n",
    "                \"page\":page_number\n",
    "            }\n",
    "            request = requests.get(base_url,params=query_param)\n",
    "\n",
    "            if request.status_code == 200:\n",
    "                request_json = request.json()\n",
    "                \n",
    "                #For each paper referencing the dataset we get the title (with a little transformation to remove \",\" and \"\\n\" inside of them), doi and publication year\n",
    "                for res in request_json[\"results\"]:\n",
    "                    title = res[\"title\"]\n",
    "                    title = title.replace(\",\",\"\")\n",
    "                    title = title.replace(\"\\n\",\"\")\n",
    "                    \n",
    "                    fulltext_url = res[\"open_access\"][\"oa_url\"]\n",
    "                    #Remove review paper\n",
    "                    if \"review\" in title.lower():\n",
    "                        continue\n",
    "\n",
    "                    doi = res[\"doi\"]\n",
    "                    if doi is not None:\n",
    "                        doi = doi[16:] #Remove the https://doi.org/\n",
    "                        paper_referencing[ds].append((title,doi,res[\"publication_year\"],res[\"abstract_inverted_index\"],fulltext_url))\n",
    "\n",
    "                #If the results field is empty that mean we are at the last page so we can continue to the next dataset\n",
    "                #otherwise we need to go to next page of the current dataset\n",
    "                if not request_json[\"results\"]:\n",
    "                    next_page = False\n",
    "                else:\n",
    "                    page_number += 1\n",
    "            else:\n",
    "                next_page = False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of citations for ACDC: 120\n",
      "Number of citations for LA: 10\n",
      "Number of citations for MSCMRSeg: 0\n",
      "Number of citations for M&Ms: 34\n",
      "Number of citations for PROMISE12: 29\n",
      "Number of citations for Medical Decathlon: 4\n",
      "Number of citations for I2CVB: 13\n",
      "Number of citations for BRATS: 494\n",
      "Number of citations for Synapse: 0\n"
     ]
    }
   ],
   "source": [
    "for d in paper_referencing:\n",
    "    print(f\"Number of citations for {d}: {len(paper_referencing[d])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to filter out wrong references\n",
    "This part of the notebook is to obtained a more accurate list of paper to only keep paper that are actually using the dataset and not only referencing the paper for another reason.\n",
    "\n",
    "To do that we will:\n",
    "1. Search for the dataset name in the abstract, making the hypothesis that if authors put the name of a dataset inside an abstract they must be using it.\n",
    "2. Look for the name of the dataset in Table of the fulltext, again if the dataset name is in a Table it should be used (especially if we removed review papers)\n",
    "3. Classify the figure in the fulltext to detect the organ of the cited dataset. (does not confirm at 100% that the actual dataset is really used and not another one focusing on the same organ)\n",
    "\n",
    "If none of the above operation validate the citations, the paper is removed from the list. Otherwise, we can keep it.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Check abstract for dataset's name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now query OpenAlex again for everypaper and search for the dataset name inside the \"abstract_inverted_index\" field of the API if it's present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reconstruct and transform the abstract of a paper using abstract_inverted_index field. We will removed non alpha numeric caracters and lower every word.\n",
    "\"\"\"\n",
    "def reconstruct_abstract(paper):\n",
    "    # Maximum size of the abstract, if the paper abstarct is longer it will be truncated\n",
    "    abstract = np.full(2500,\"\",dtype=object)\n",
    "    # The \"abstract_inverted_index\" field is a dictionnary with word as key and locations of this word in the abstract\n",
    "    # So we fill the abstarct variable above at the index of the word to reconstruct the abstract\n",
    "    if paper[3]:\n",
    "        for w in paper[3]:\n",
    "            for indices in paper[3][w]:\n",
    "                if indices < 2500:\n",
    "                    abstract[indices] = ''.join(filter(str.isalnum, w)).lower()\n",
    "        # Remove empty location mostly due to a shorter abstract \n",
    "        abstract = abstract[abstract != \"\"]\n",
    "        #Convert array to string\n",
    "    str_abstract = ' '.join(abstract)\n",
    "    return str_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_using_abstract = {dataset:[] for dataset in paper_referencing}\n",
    "for dataset in paper_referencing:\n",
    "    for paper in paper_referencing[dataset]:\n",
    "        abstract = reconstruct_abstract(paper)\n",
    "        if dataset.lower() in abstract:\n",
    "            paper_using_abstract[dataset].append(paper[:3])\n",
    "            confirmed_papers.append(paper[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of citations for ACDC: 21\n",
      "Number of citations for LA: 10\n",
      "Number of citations for MSCMRSeg: 0\n",
      "Number of citations for M&Ms: 0\n",
      "Number of citations for PROMISE12: 3\n",
      "Number of citations for Medical Decathlon: 0\n",
      "Number of citations for I2CVB: 1\n",
      "Number of citations for BRATS: 266\n",
      "Number of citations for Synapse: 0\n"
     ]
    }
   ],
   "source": [
    "for d in paper_using_abstract:\n",
    "    print(f\"Number of citations for {d}: {len(paper_using_abstract[d])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download paper's fulltexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A Persistent Homology-Based Topological Loss Function for Multi-class CNN Segmentation of Cardiac MRI', '10.1007/978-3-030-68107-4_1', 2020, {'With': [0], 'respect': [1, 39], 'to': [2, 40, 119, 130], 'spatial': [3, 52], 'overlap,': [4], 'CNN-based': [5], 'segmentation': [6, 105], 'of': [7, 19, 68, 93, 104, 122, 138], 'short': [8, 144], 'axis': [9, 145], 'cardiovascular': [10], 'magnetic': [11], 'resonance': [12], '(CMR)': [13], 'images': [14], 'has': [15, 82], 'achieved': [16], 'a': [17, 46, 75, 101, 136], 'level': [18], 'performance': [20], 'consistent': [21], 'with': [22, 38], 'inter': [23], 'observer': [24], 'variation.': [25], 'However,': [26], 'conventional': [27], 'training': [28, 147], 'procedures': [29], 'frequently': [30, 73], 'depend': [31], 'on': [32], 'pixel-wise': [33], 'loss': [34, 88], 'functions,': [35], 'limiting': [36], 'optimisation': [37], 'extended': [41], 'or': [42, 58], 'global': [43], 'features.': [44], 'As': [45], 'result,': [47], 'inferred': [48], 'segmentations': [49], 'can': [50], 'lack': [51], 'coherence,': [53], 'including': [54], 'spurious': [55], 'connected': [56], 'components': [57], 'holes.': [59], 'Such': [60], 'results': [61], 'are': [62], 'implausible,': [63], 'violating': [64], 'the': [65, 91, 120, 142], 'anticipated': [66], 'topology': [67, 106], 'image': [69, 94], 'segments,': [70], 'which': [71], 'is': [72], 'known': [74], 'priori.': [76], 'Addressing': [77], 'this': [78], 'challenge,': [79], 'published': [80], 'work': [81], 'employed': [83], 'persistent': [84], 'homology,': [85], 'constructing': [86], 'topological': [87, 126, 133], 'functions': [89], 'for': [90], 'evaluation': [92], 'segments': [95], 'against': [96], 'an': [97], 'explicit': [98], 'prior.': [99], 'Building': [100], 'richer': [102], 'description': [103], 'by': [107], 'considering': [108], 'all': [109, 132], 'possible': [110], 'labels': [111], 'and': [112], 'label': [113], 'pairs,': [114], 'we': [115], 'extend': [116], 'these': [117], 'losses': [118], 'task': [121], 'multi-class': [123], 'segmentation.': [124], 'These': [125], 'priors': [127], 'allow': [128], 'us': [129], 'resolve': [131], 'errors': [134], 'in': [135], 'subset': [137], '150': [139], 'examples': [140], 'from': [141], 'ACDC': [143], 'CMR': [146], 'data': [148], 'set,': [149], 'without': [150], 'sacrificing': [151], 'overlap': [152], 'performance.': [153]}, 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7610940')\n"
     ]
    }
   ],
   "source": [
    "print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'10.1007/978-3-030-68107-4_1' in downloaded_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.1007/978-3-030-20351-1_3', '10.1007/978-3-030-32245-8_60', '10.1007/978-3-030-59719-1_77', '10.1007/978-3-030-00889-5_27', '10.1007/978-3-030-32245-8_68', '10.1007/978-3-030-87193-2_8', '10.1007/978-3-030-32251-9_72', '10.1007/978-3-030-59725-2_42', '10.1007/978-3-030-68107-4_24', '10.1007/978-3-030-87199-4_39', '10.1007/978-3-030-59719-1_8', '10.1007/978-3-030-68107-4_18', '10.1007/978-3-030-12029-0_26', '10.1007/978-3-030-32245-8_83', '10.1007/978-3-030-33843-5_1', '10.1007/978-3-030-39074-7_19', '10.1007/978-3-030-59719-1_13', '10.1007/978-3-030-87199-4_35', '10.1007/978-3-030-87196-3_3', '10.1007/978-3-030-59719-1_9', '10.1007/978-3-030-59719-1_10']\n"
     ]
    }
   ],
   "source": [
    "print(downloaded_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_doi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACDC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 22/120 [00:00<00:03, 29.78it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../results/papers_fulltext/downloaded_pdf/A Persistent Homology-Based Topological Loss Function for Multi-class CNN Segmentation of Cardiac MRI.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/theo/M2/DDSA_Sourget/code/other/download_fulltext.ipynb Cell 26\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theo/M2/DDSA_Sourget/code/other/download_fulltext.ipynb#X54sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theo/M2/DDSA_Sourget/code/other/download_fulltext.ipynb#X54sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theo/M2/DDSA_Sourget/code/other/download_fulltext.ipynb#X54sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m#Try to read the pdf (Raise an error if the file is an invalid pdf)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/theo/M2/DDSA_Sourget/code/other/download_fulltext.ipynb#X54sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     PdfReader(file_path,strict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theo/M2/DDSA_Sourget/code/other/download_fulltext.ipynb#X54sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mexcept\u001b[39;00m PdfReadError:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theo/M2/DDSA_Sourget/code/other/download_fulltext.ipynb#X54sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m#If a PdfReadError is raised, the pdf is invalid and therefore removed from downloaded list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theo/M2/DDSA_Sourget/code/other/download_fulltext.ipynb#X54sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     downloaded_doi\u001b[39m.\u001b[39mremove(doi)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pypdf/_reader.py:320\u001b[0m, in \u001b[0;36mPdfReader.__init__\u001b[0;34m(self, stream, strict, password)\u001b[0m\n\u001b[1;32m    314\u001b[0m     logger_warning(\n\u001b[1;32m    315\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPdfReader stream/file object is not in binary mode. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt may not be read correctly.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    317\u001b[0m         \u001b[39m__name__\u001b[39m,\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(stream, (\u001b[39mstr\u001b[39m, Path)):\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(stream, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m fh:\n\u001b[1;32m    321\u001b[0m         stream \u001b[39m=\u001b[39m BytesIO(fh\u001b[39m.\u001b[39mread())\n\u001b[1;32m    322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(stream)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../results/papers_fulltext/downloaded_pdf/A Persistent Homology-Based Topological Loss Function for Multi-class CNN Segmentation of Cardiac MRI.pdf'"
     ]
    }
   ],
   "source": [
    "for dataset in paper_referencing:\n",
    "    print(dataset)\n",
    "    for paper in tqdm(paper_referencing[dataset]):\n",
    "        doi = paper[1]\n",
    "        fulltext_url = paper[4]\n",
    "        if not fulltext_url:\n",
    "            continue\n",
    "        if not paper in paper_using_abstract[dataset]:\n",
    "            #Check that paper hasn't been downloaded before (for another dataset), otherwise try to download it\n",
    "            file_path = f\"../../results/papers_fulltext/downloaded_pdf/{paper[0].replace('/','')}.pdf\"\n",
    "            if doi not in downloaded_doi:\n",
    "                try:\n",
    "                    r_fulltext = requests.get(fulltext_url,allow_redirects=True,timeout=10)\n",
    "                    pdf_content = r_fulltext.content\n",
    "                    if r_fulltext.status_code == 200:\n",
    "                        open(file_path,\"wb\").write(r_fulltext.content)\n",
    "                        downloaded_doi.append(doi)\n",
    "                    else:\n",
    "                        continue\n",
    "                except requests.exceptions.RequestException as ce:\n",
    "                    continue\n",
    "            try:\n",
    "                #Try to read the pdf (Raise an error if the file is an invalid pdf)\n",
    "                PdfReader(file_path,strict=True)\n",
    "            except PdfReadError:\n",
    "                #If a PdfReadError is raised, the pdf is invalid and therefore removed from downloaded list\n",
    "                downloaded_doi.remove(doi)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of downloaded fulltext: 214\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of downloaded fulltext: {len(downloaded_doi)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check tables in fulltext for dataset's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO using code below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check for dataset's organ in figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO using code below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check fulltext of paper for either figures or tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To download fulltext\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#To filter invalid pdf\n",
    "from pypdf import PdfReader\n",
    "from pypdf.errors import PdfReadError\n",
    "\n",
    "#To handle files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#To extract images from pdf\n",
    "import fitz\n",
    "\n",
    "#To extract tables from pdf\n",
    "import camelot\n",
    "import ghostscript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download papers full text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter paper to get for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only get paper from 2023 referencing ACDC paper\n",
    "df = pd.read_csv(\"../../results/extracted_csv/paper_openalex.csv\")\n",
    "df_2023 = df[df[\"publication_year\"] == 2023]\n",
    "df_acdc = df_2023[df_2023[\"dataset_used\"] == \"ACDC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1016/b978-0-32-385773-4.00023-x\n",
      "10.1016/b978-0-32-385773-4.00025-3\n",
      "10.1016/b978-0-32-385773-4.00009-5\n",
      "https://doi.org/10.1016/b978-0-32-385773-4.00009-5\n",
      "10.1016/j.compbiomed.2022.106439\n",
      "10.1371/journal.pdig.0000159\n",
      "https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000159&type=printable\n",
      "10.1109/access.2023.3234241\n",
      "10.1016/j.compmedimag.2022.102174\n",
      "10.1088/1361-6560/acb19a\n",
      "https://iopscience.iop.org/article/10.1088/1361-6560/acb19a/pdf\n",
      "10.1007/s00740-022-00474-9\n",
      "10.1016/j.patcog.2023.109318\n",
      "https://doi.org/10.1016/j.patcog.2023.109318\n",
      "10.1109/access.2023.3238058\n",
      "10.1038/s41598-023-28348-y\n",
      "https://www.nature.com/articles/s41598-023-28348-y.pdf\n",
      "10.3389/fphys.2023.1027076\n",
      "https://www.frontiersin.org/articles/10.3389/fphys.2023.1027076/pdf\n",
      "10.1016/b978-0-12-821983-6.00008-4\n",
      "10.3390/bioengineering10020166\n",
      "https://www.mdpi.com/2306-5354/10/2/166/pdf?version=1674889337\n",
      "10.1016/j.media.2023.102762\n",
      "http://arxiv.org/pdf/2206.01136\n",
      "10.1016/j.bspc.2023.104631\n",
      "10.1109/wacv56688.2023.00365\n",
      "https://eprints.gla.ac.uk/282738/2/282738.pdf\n"
     ]
    }
   ],
   "source": [
    "url_base = \"https://api.openalex.org/works/https://doi.org/\"\n",
    "paper_id = 1\n",
    "for doi in df_acdc[\"DOI\"]:\n",
    "    url = url_base + doi \n",
    "    r_paper = requests.get(url)\n",
    "    if r_paper.status_code == 200:\n",
    "        r_paper_json = r_paper.json()\n",
    "        fulltext_url = r_paper_json[\"open_access\"][\"oa_url\"]\n",
    "        if fulltext_url:\n",
    "            r_fulltext = requests.get(fulltext_url,allow_redirects=True)\n",
    "            if r_fulltext.status_code == 200:\n",
    "                print(fulltext_url)\n",
    "                open(f\"../../results/papers_fulltext/{paper_id}.pdf\",\"wb\").write(r_fulltext.content)\n",
    "                paper_id += 1\n",
    "        #Stop after 10th download, only to test and maybe not get block by some site\n",
    "        if paper_id == 10:\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Error {r_paper.status_code} for {doi}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract images from valid pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get every valid pdf path\n",
    "valid_pdf_path = glob.glob(\"../../results/papers_fulltext/valid_pdf/*.pdf\")\n",
    "\n",
    "#path of folder where images will be stored\n",
    "images_path = \"../../results/papers_fulltext/images\"\n",
    "#Create this folder if it does not exist\n",
    "if not os.path.exists(images_path):\n",
    "    os.makedirs(images_path)\n",
    "\n",
    "#For each pdf file\n",
    "for file in valid_pdf_path:\n",
    "    #Open the file\n",
    "    pdf_file = fitz.open(file)\n",
    "\n",
    "    #Get the number of pages in PDF file\n",
    "    page_nums = len(pdf_file)\n",
    "\n",
    "    #Create empty list to store images information\n",
    "    images_list = []\n",
    "\n",
    "    #Extract all images information from each page\n",
    "    for page_num in range(page_nums):\n",
    "        page_content = pdf_file[page_num]\n",
    "        images_list.extend(page_content.get_images())\n",
    "\n",
    "    #If there is at least one image in the pdf\n",
    "    if len(images_list)!=0:\n",
    "        #Create a subfolder for the article, this way we easily know from which paper the images is coming from\n",
    "        if not os.path.exists(os.path.join(images_path, os.path.basename(file.replace(\".pdf\",\"\")))):\n",
    "            os.makedirs(os.path.join(images_path, os.path.basename(file.replace(\".pdf\",\"\"))))\n",
    "\n",
    "        #Save all the extracted images\n",
    "        for i, img in enumerate(images_list, start=1):\n",
    "            #Extract the image object number\n",
    "            xref = img[0]\n",
    "            #Extract image\n",
    "            base_image = pdf_file.extract_image(xref)\n",
    "            #Store image bytes\n",
    "            image_bytes = base_image['image']\n",
    "            #Store image extension\n",
    "            image_ext = base_image['ext']\n",
    "            #Generate image file name\n",
    "            image_name = str(i) + '.' + image_ext\n",
    "            #Save image\n",
    "            with open(os.path.join(images_path, os.path.basename(file.replace(\".pdf\",\"\")),image_name) , 'wb') as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "                image_file.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract tables from valid PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pdf_path = glob.glob(\"../../results/papers_fulltext/valid_pdf/*.pdf\")\n",
    "for pdf in valid_pdf_path:\n",
    "    tables = camelot.read_pdf(pdf,\"all\",flavor=\"stream\",suppress_stdout=True)\n",
    "    for t in tables:\n",
    "        t_str = t.df.to_string()\n",
    "        if \"ACDC\" in t_str:\n",
    "            print(\"ACDC IN\",pdf)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACDC IN ../../results/papers_fulltext/valid_pdf/5.pdf\n",
      "ACDC IN ../../results/papers_fulltext/valid_pdf/9.pdf\n",
      "ACDC IN ../../results/papers_fulltext/valid_pdf/6.pdf\n",
      "ACDC IN ../../results/papers_fulltext/valid_pdf/7.pdf\n",
      "ACDC IN ../../results/papers_fulltext/valid_pdf/8.pdf\n"
     ]
    }
   ],
   "source": [
    "for pdf in valid_pdf_path:\n",
    "    tables = camelot.read_pdf(pdf,\"all\",flavor=\"stream\",suppress_stdout=True)\n",
    "    for t in tables:\n",
    "        t_str = t.df.to_string()\n",
    "        if \"ACDC\" in t_str:\n",
    "            print(\"ACDC IN\",pdf)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
