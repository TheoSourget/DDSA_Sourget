{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion sources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this file is to merge the information extracted with the different sources such as OpenAlex or OpenCitation together to generate the final output of the process which contains all the association paper-dataset-task to allow further analysis after.\n",
    "It also compute the \"coverage\" for a paper which tells if a source has return a paper or not, this information will be used to investigate why some references are missing and compare sources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this proceed ?\n",
    "1. Load every generated csv present in extracted_csv folder\n",
    "2. For each line in a csv:\n",
    "    1. Test if the association DOI-Dataset has already been seen\n",
    "    2. If not create a new line in merged.csv with the addition of the information about the task find in dataset.csv \"context\" field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../extracted_csv/paper_poci.csv\n",
      "../extracted_csv/paper_openalex.csv\n",
      "../extracted_csv/paper_coci.csv\n"
     ]
    }
   ],
   "source": [
    "#Dictionnary with dataset's name as key and task as value (e.g ACDC:cardiac)\n",
    "datasets_context = {}\n",
    "ds_reader = csv.DictReader(open('../../data/datasets.csv'))\n",
    "for ds in ds_reader:\n",
    "    datasets_context[ds[\"name\"]] = ds[\"context\"]\n",
    "\n",
    "\n",
    "#Dictionnaries to keep track of which DOI/Paper we've already encountered to not add duplicates\n",
    "doi_using_dataset = {ds:[] for ds in datasets_context}\n",
    "title_using_dataset = {ds:[] for ds in datasets_context}\n",
    "\n",
    "#Get the list of csv from different sources except dimensions for time reason\n",
    "extracted_csv = glob.glob(\"../../results/extracted_csv/*.csv\")\n",
    "extracted_csv.remove('../../results/extracted_csv/paper_dimensions.csv')\n",
    "\n",
    "#Dictionnary with paper DOI as key and a list of boolean as value. The boolean is True if the sources return that paper and false otherwise\n",
    "sources_coverage = {}\n",
    "\n",
    "#title of papers as keys and DOI as value, use for papers with multiple DOI to do the coverage analysis\n",
    "name_doi = {}\n",
    "\n",
    "#Url use to request a DOI in order to get the venue\n",
    "base_url_venue = \"https://api.openalex.org/works/https://doi.org/\"\n",
    "\n",
    "# /!\\ WARNING /!\\: If merged.csv already exist, the previous content will be removed !\n",
    "with open(\"../../results/processed_csv/merged.csv\",\"w\") as merged :\n",
    "    merged.write(f\"name,DOI,publication_year,dataset_used,task\")\n",
    "    for i,csv_path in enumerate(extracted_csv):\n",
    "        print(csv_path)\n",
    "        csv_reader = csv.DictReader(open(csv_path))\n",
    "        for line in csv_reader:\n",
    "            \n",
    "            #Check if we've already seen this paper before with another source for the same dataset\n",
    "            doi_not_encountered = line['DOI'].lower() not in doi_using_dataset[line['dataset_used']]\n",
    "            title_not_encountered = line['name'].lower() not in title_using_dataset[line['dataset_used']]\n",
    "\n",
    "            #If not we add a a line in the output csv and create the line in coverage\n",
    "            if doi_not_encountered and title_not_encountered:\n",
    "\n",
    "                #Get the paper's venue and if it's accessible in open access or not\n",
    "                request_url = base_url_venue + line['DOI']\n",
    "                request = requests.get(request_url)\n",
    "                venue = None\n",
    "                if request.status_code == 200:\n",
    "                    json_response = request.json()\n",
    "                    venue = json_response[\"host_venue\"][\"display_name\"]\n",
    "                    is_oa = json_response[\"open_access\"][\"is_oa\"]\n",
    "                doi_using_dataset[line['dataset_used']].append(line['DOI'].lower())\n",
    "                title_using_dataset[line['dataset_used']].append(line['name'].lower())\n",
    "                \n",
    "                merged.write(f\"\\n{line['name'].lower()},{line['DOI'].lower()},{line['publication_year'].lower()},{line['dataset_used']},{datasets_context[line['dataset_used']]}\")\n",
    "                sources_coverage[line['DOI'].lower()] = [False for _ in range(len(extracted_csv))] + [venue,line['publication_year'],is_oa]\n",
    "                sources_coverage[line['DOI'].lower()][i] = True\n",
    "                name_doi[line[\"name\"].lower()] = line['DOI'].lower()\n",
    "\n",
    "            #Otherwise we add the sources into the coverage for this paper    \n",
    "            elif line['name'].lower() in name_doi:\n",
    "                sources_coverage[name_doi[line['name'].lower()]][i] = True\n",
    "            elif line['DOI'].lower() in sources_coverage:\n",
    "                sources_coverage[line['DOI'].lower()][i] = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of coverage.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = []\n",
    "for e in extracted_csv:\n",
    "    columns_name.append(e[23:-4])\n",
    "columns_name.append(\"venue\")\n",
    "columns_name.append(\"publication_year\")\n",
    "columns_name.append(\"is_oa\")\n",
    "df = pd.DataFrame.from_dict(sources_coverage).transpose()\n",
    "df.index.name = 'DOI'\n",
    "df.columns = columns_name\n",
    "df.to_csv(\"../../results/processed_csv/coverage.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
