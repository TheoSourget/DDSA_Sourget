"About the data: Exported on Mar 06, 2023. Criteria: '""Multi-Atlas Labeling Beyond the Cranial Vault - Workshop and Challenge""' in full data. © 2023 Digital Science &amp; Research Solutions Inc. All rights reserved. Parts of this work may also be protected by copyright of content providers and other third parties, which together with all rights of Digital Science, user agrees not to violate. Redistribution / external use of this work (or parts thereof) is prohibited without prior written approval. Please contact info@dimensions.ai for further information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rank,Publication ID,DOI,PMID,PMCID,Title,Abstract,Acknowledgements,Funding,Source title,Anthology title,MeSH terms,Publication Date,PubYear,Publication Date (online),Publication Date (print),Volume,Issue,Pagination,Open Access,Publication Type,Authors,Authors (Raw Affiliation),Corresponding Authors,Authors Affiliations,Times cited,Recent citations,RCR,FCR,Source Linkout,Dimensions URL,Fields of Research (ANZSRC 2020),Sustainable Development Goals
9308,pub.1133127339,10.1109/tbme.2020.3042640,33275574,,Convolutional Neural Network Ensemble Segmentation With Ratio-Based Sampling for the Arteries and Veins in Abdominal CT Scans,"OBJECTIVE: Three-dimensional (3D) blood vessel structure information is important for diagnosis and treatment in various clinical scenarios. We present a fully automatic method for the extraction and differentiation of the arterial and venous vessel trees from abdominal contrast enhanced computed tomography (CE-CT) volumes using convolutional neural networks (CNNs).
METHODS: We used a novel ratio-based sampling method to train 2D and 3D versions of the U-Net, the V-Net and the DeepVesselNet. Networks were trained with a combination of the Dice and cross entropy loss. Performance was evaluated on 20 IRCAD subjects. Best performing networks were combined into an ensemble. We investigated seven different weighting schemes. Trained networks were additionally applied to 26 BTCV cases to validate the generalizability.
RESULTS: Based on our experiments, the optimal configuration is an equally weighted ensemble of 2D and 3D U- and V-Nets. Our method achieved Dice similarity coefficients of 0.758 ± 0.050 (veins) and 0.838 ± 0.074 (arteries) on the IRCAD data set. Application to the BTCV data set showed a high transfer ability.
CONCLUSION: Abdominal vascular structures can be segmented more accurately using ensembles than individual CNNs. 2D and 3D networks have complementary strengths and weaknesses. Our ensemble of 2D and 3D U-Nets and V-Nets in combination with ratio-based sampling achieves a high agreement with manual annotations for both artery and vein segmentation. Our results surpass other state-of-the-art methods.
SIGNIFICANCE: Our segmentation pipeline can provide valuable information for the planning of living donor organ transplantations.",This work was supported in part by the Research Campus M2OLIE and in part by the German Federal Ministry of Education and Research (BMBF) within the framework Forschungscampus: Public-Private Partnership for Innovations under Grant 13GW0388A. The authors would like to acknowledge the support of NVIDIA Corporation with the donation of the NVIDIA Titan Xp used for this research.,,IEEE Transactions on Biomedical Engineering,,"Abdomen; Arteries; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Tomography, X-Ray Computed",2021-04-21,2021,2021-04-21,2021-05,68,5,1518-1526,Closed,Article,"Golla, Alena-Kathrin; Bauer, Dominik F.; Schmidt, Ralf; Russ, Tom; Nrenberg, Dominik; Chung, Khanlian; Tnnes, Christian; Schad, Lothar R.; Zllner, Frank G.","Golla, Alena-Kathrin (Chair of Computer Assisted Clinical Medicine, Mannheim Institute for Intelligent Systems in Medicine, Medical Faculty Mannheim, Heidelberg University, 68167, Mannheim, Germany); Bauer, Dominik F. (Chair of Computer Assisted Clinical Medicine, Mannheim Institute for Intelligent Systems in Medicine, Medical Faculty Mannheim, Heidelberg University); Schmidt, Ralf (Chair of Computer Assisted Clinical Medicine, Mannheim Institute for Intelligent Systems in Medicine, Medical Faculty Mannheim, Heidelberg University); Russ, Tom (Chair of Computer Assisted Clinical Medicine, Mannheim Institute for Intelligent Systems in Medicine, Medical Faculty Mannheim, Heidelberg University); Nrenberg, Dominik (Department of Clinical Radiology and Nuclear Medicine, University Medical Centre Mannheim, Heidelberg University); Chung, Khanlian (Chair of Computer Assisted Clinical Medicine, Mannheim Institute for Intelligent Systems in Medicine, Medical Faculty Mannheim, Heidelberg University); Tnnes, Christian (Chair of Computer Assisted Clinical Medicine, Mannheim Institute for Intelligent Systems in Medicine, Medical Faculty Mannheim, Heidelberg University); Schad, Lothar R. (Chair of Computer Assisted Clinical Medicine, Mannheim Institute for Intelligent Systems in Medicine, Medical Faculty Mannheim, Heidelberg University); Zllner, Frank G. (Chair of Computer Assisted Clinical Medicine, Mannheim Institute for Intelligent Systems in Medicine, Medical Faculty Mannheim, Heidelberg University)","Golla, Alena-Kathrin (Heidelberg University)","Golla, Alena-Kathrin (Heidelberg University); Bauer, Dominik F. (); Schmidt, Ralf (); Russ, Tom (); Nrenberg, Dominik (); Chung, Khanlian (); Tnnes, Christian (); Schad, Lothar R. (); Zllner, Frank G. ()",13,13,2.67,10.07,,https://app.dimensions.ai/details/publication/pub.1133127339,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
8173,pub.1150859651,10.1109/embc48229.2022.9870911,36086344,,Self-Supervised Pretext Tasks in Model Robustness & Generalizability: A Revisit from Medical Imaging Perspective,"Self-supervised pretext tasks have been introduced as an effective strategy when learning target tasks on small annotated data sets. However, while current research focuses on exploring novel pretext tasks for meaningful and reusable representation learning for the target task, the study of its robustness and generalizability has remained relatively under-explored. Specifically, it is crucial in medical imaging to proactively investigate performance under different perturbations for reliable deployment of clinical applications. In this work, we revisit medical imaging networks pre-trained with self-supervised learnings and categorically evaluate robustness and generalizability compared to vanilla supervised learning. Our experiments on pneumonia detection in X-rays and multi-organ segmentation in CT yield conclusive results exposing the hidden benefits of self-supervision pre-training for learning robust feature representations.",,,Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),Diagnostic Imaging; Radiography,2022-07,2022,,2022-07,00,,5074-5079,Closed,Proceeding,"Navarro, Fernando; Watanabe, Christopher; Shit, Suprosanna; Sekuboyina, Anjany; Peeken, Jan C.; Combs, Stephanie E.; Menze, Bjoern H.","Navarro, Fernando (Department of Informatics, Technical University of Munich, Germany; Center for Translational Cancer Research (TranslaTUM), Klinikum rechts der Isar, Germany; Department of Radio Oncology and Radiation Therapy, Klinikum rechts der Isar, Munich, Germany; Department of Quantitative Biomedicine, University of Zurich, Zurich, Switzerland); Watanabe, Christopher (Department of Informatics, Technical University of Munich, Germany); Shit, Suprosanna (Department of Informatics, Technical University of Munich, Germany; Center for Translational Cancer Research (TranslaTUM), Klinikum rechts der Isar, Germany); Sekuboyina, Anjany (Department of Informatics, Technical University of Munich, Germany; Department of Quantitative Biomedicine, University of Zurich, Zurich, Switzerland); Peeken, Jan C. (Department of Radio Oncology and Radiation Therapy, Klinikum rechts der Isar, Munich, Germany); Combs, Stephanie E. (Department of Radio Oncology and Radiation Therapy, Klinikum rechts der Isar, Munich, Germany); Menze, Bjoern H. (Department of Quantitative Biomedicine, University of Zurich, Zurich, Switzerland)",,"Navarro, Fernando (Technical University of Munich; Rechts der Isar Hospital; Rechts der Isar Hospital; University of Zurich); Watanabe, Christopher (Technical University of Munich); Shit, Suprosanna (Technical University of Munich; Rechts der Isar Hospital); Sekuboyina, Anjany (Technical University of Munich; University of Zurich); Peeken, Jan C. (Rechts der Isar Hospital); Combs, Stephanie E. (Rechts der Isar Hospital); Menze, Bjoern H. (University of Zurich)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1150859651,46 Information and Computing Sciences; 4611 Machine Learning,
7643,pub.1135074379,10.1016/j.media.2021.101979,33636451,,Marginal loss and exclusion loss for partially supervised multi-organ segmentation,"Annotating multiple organs in medical images is both costly and time-consuming; therefore, existing multi-organ datasets with labels are often low in sample size and mostly partially labeled, that is, a dataset has a few organs labeled but not all organs. In this paper, we investigate how to learn a single multi-organ segmentation network from a union of such datasets. To this end, we propose two types of novel loss function, particularly designed for this scenario: (i) marginal loss and (ii) exclusion loss. Because the background label for a partially labeled image is, in fact, a 'merged' label of all unlabelled organs and 'true' background (in the sense of full labels), the probability of this 'merged' background label is a marginal probability, summing the relevant probabilities before merging. This marginal probability can be plugged into any existing loss function (such as cross entropy loss, Dice loss, etc.) to form a marginal loss. Leveraging the fact that the organs are non-overlapping, we propose the exclusion loss to gauge the dissimilarity between labeled organs and the estimated segmentation of unlabelled organs. Experiments on a union of five benchmark datasets in multi-organ segmentation of liver, spleen, left and right kidneys, and pancreas demonstrate that using our newly proposed loss functions brings a conspicuous performance improvement for state-of-the-art methods without introducing any extra computation.","This work was supported by the State’s Key Project of Research and Development Plan (No. 2017YFA0104302, 2017YFC0109202 and 2017YFC0107900); the National Natural Science Foundation (No. 61871117); and the Science and Technology Program of Guangdong (No. 2018B030333001). Li Xiao thanks CCF-Tencent Open Fund for support.",,Medical Image Analysis,,"Humans; Kidney; Liver; Probability; Spleen; Tomography, X-Ray Computed",2021-02-03,2021,2021-02-03,2021-05,70,,101979,All OA; Green,Article,"Shi, Gonglei; Xiao, Li; Chen, Yang; Zhou, S Kevin","Shi, Gonglei (Medical Imaging, Robotics, Analytic Computing Laboratory & Engineering (MIRACLE), Key Lab of Intelligent Information Processing of Chinese Academy of Sciences, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China; School of Computer Science and Engineering, Southeast University, Nanjing, 210000, China.); Xiao, Li (Medical Imaging, Robotics, Analytic Computing Laboratory & Engineering (MIRACLE), Key Lab of Intelligent Information Processing of Chinese Academy of Sciences, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China. Electronic address: xiaoli@ict.ac.cn.); Chen, Yang (School of Computer Science and Engineering, Southeast University, Nanjing, 210000, China.); Zhou, S Kevin (Medical Imaging, Robotics, Analytic Computing Laboratory & Engineering (MIRACLE), Key Lab of Intelligent Information Processing of Chinese Academy of Sciences, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China; School of Biomedical Engineering & Suzhou Institute For Advanced Research, University of Science and Technology, Suzhou, 215123, China. Electronic address: zhoushaohua@ict.ac.cn.)","Xiao, Li (Institute of Computing Technology); Zhou, S Kevin (Institute of Computing Technology; University of Science and Technology of China)","Shi, Gonglei (Institute of Computing Technology; Southeast University); Xiao, Li (Institute of Computing Technology); Chen, Yang (Southeast University); Zhou, S Kevin (Institute of Computing Technology; University of Science and Technology of China)",32,32,2.76,,http://arxiv.org/pdf/2007.03868,https://app.dimensions.ai/details/publication/pub.1135074379,32 Biomedical and Clinical Sciences; 40 Engineering,
7449,pub.1111907267,10.1117/1.jmi.6.1.014003,30746392,PMC6361550,Adaptive Bayesian label fusion using kernel-based similarity metrics in hippocampus segmentation,"The effectiveness of brain magnetic resonance imaging (MRI) as a useful evaluation tool strongly depends on the performed segmentation of associated tissues or anatomical structures. We introduce an enhanced brain segmentation approach of Bayesian label fusion that includes the construction of adaptive target-specific probabilistic priors using atlases ranked by kernel-based similarity metrics to deal with the anatomical variability of collected MRI data. In particular, the developed segmentation approach appraises patch-based voxel representation to enhance the voxel embedding in spaces with increased tissue discrimination, as well as the construction of a neighborhood-dependent model that addresses the label assignment of each region with a different patch complexity. To measure the similarity between the target and training atlases, we propose a tensor-based kernel metric that also includes the training labeling set. We evaluate the proposed approach, adaptive Bayesian label fusion using kernel-based similarity metrics, in the specific case of hippocampus segmentation of five benchmark MRI collections, including ADNI dataset, resulting in an increased performance (assessed through the Dice index) as compared to other recent works.","This work was developed within the framework of the research project 1110-744-55778, funded by COLCIENCIAS. Part of the data used for this project was funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012).",,Journal of Medical Imaging,,,2019-01,2019,2019-02-04,2019-01,6,1,014003-014003,All OA; Green,Article,"Cárdenas-Peña, David; Tobar-Rodríguez, Andres; Castellanos-Dominguez, German; Initiative, Alzheimer’s Disease Neuroimaging","Cárdenas-Peña, David (Universidad Nacional de Colombia, Signal Processing and Recognition Group, Manizales, Colombia); Tobar-Rodríguez, Andres (Universidad Nacional de Colombia, Signal Processing and Recognition Group, Manizales, Colombia); Castellanos-Dominguez, German (Universidad Nacional de Colombia, Signal Processing and Recognition Group, Manizales, Colombia); Initiative, Alzheimer’s Disease Neuroimaging (Universidad Nacional de Colombia, Signal Processing and Recognition Group, Manizales, Colombia)","Cárdenas-Peña, David (National University of Colombia)","Cárdenas-Peña, David (National University of Colombia); Tobar-Rodríguez, Andres (National University of Colombia); Castellanos-Dominguez, German (National University of Colombia); Initiative, Alzheimer’s Disease Neuroimaging (National University of Colombia)",3,2,0.33,0.84,https://europepmc.org/articles/pmc6361550?pdf=render,https://app.dimensions.ai/details/publication/pub.1111907267,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences; 40 Engineering; 4003 Biomedical Engineering,
7143,pub.1137258386,10.1007/s11548-021-02363-8,33864189,,Deep learning to segment pelvic bones: large-scale CT datasets and baseline models,"Purpose:Pelvic bone segmentation in CT has always been an essential step in clinical diagnosis and surgery planning of pelvic bone diseases. Existing methods for pelvic bone segmentation are either hand-crafted or semi-automatic and achieve limited accuracy when dealing with image appearance variations due to the multi-site domain shift, the presence of contrasted vessels, coprolith and chyme, bone fractures, low dose, metal artifacts, etc. Due to the lack of a large-scale pelvic CT dataset with annotations, deep learning methods are not fully explored.Methods:In this paper, we aim to bridge the data gap by curating a large pelvic CT dataset pooled from multiple sources, including 1184 CT volumes with a variety of appearance variations. Then, we propose for the first time, to the best of our knowledge, to learn a deep multi-class network for segmenting lumbar spine, sacrum, left hip, and right hip, from multiple-domain images simultaneously to obtain more effective and robust feature representations. Finally, we introduce a post-processor based on the signed distance function (SDF).Results:Extensive experiments on our dataset demonstrate the effectiveness of our automatic method, achieving an average Dice of 0.987 for a metal-free volume. SDF post-processor yields a decrease of 15.1% in Hausdorff distance compared with traditional post-processor.Conclusion:We believe this large-scale dataset will promote the development of the whole community and open source the images, annotations, codes, and trained baseline models at https://github.com/ICT-MIRACLE-lab/CTPelvic1K.",,,International Journal of Computer Assisted Radiology and Surgery,,"Algorithms; Deep Learning; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Pelvic Bones; Pelvis; Reproducibility of Results; Tomography, X-Ray Computed",2021-04-16,2021,2021-04-16,2021-05,16,5,749-756,All OA; Green,Article,"Liu, Pengbo; Han, Hu; Du, Yuanqi; Zhu, Heqin; Li, Yinhao; Gu, Feng; Xiao, Honghu; Li, Jun; Zhao, Chunpeng; Xiao, Li; Wu, Xinbao; Zhou, S. Kevin","Liu, Pengbo (Institute of Computing Technology, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China); Han, Hu (Institute of Computing Technology, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China); Du, Yuanqi (George Mason University, Virginia, USA); Zhu, Heqin (Institute of Computing Technology, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China); Li, Yinhao (Institute of Computing Technology, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China); Gu, Feng (Institute of Computing Technology, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China; Beijing Electronic Science and Technology Institute, Beijing, China); Xiao, Honghu (Beijing Jishuitan Hospital, Beijing, China); Li, Jun (Institute of Computing Technology, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China); Zhao, Chunpeng (Beijing Jishuitan Hospital, Beijing, China); Xiao, Li (Institute of Computing Technology, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China); Wu, Xinbao (Beijing Jishuitan Hospital, Beijing, China); Zhou, S. Kevin (Institute of Computing Technology, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China; School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China)","Zhou, S. Kevin (Institute of Computing Technology; University of Science and Technology of China)","Liu, Pengbo (Institute of Computing Technology); Han, Hu (Institute of Computing Technology); Du, Yuanqi (George Mason University); Zhu, Heqin (Institute of Computing Technology); Li, Yinhao (Institute of Computing Technology); Gu, Feng (Institute of Computing Technology; Beijing Electronic Science and Technology Institute); Xiao, Honghu (Beijing Jishuitan Hospital); Li, Jun (Institute of Computing Technology); Zhao, Chunpeng (Beijing Jishuitan Hospital); Xiao, Li (Institute of Computing Technology); Wu, Xinbao (Beijing Jishuitan Hospital); Zhou, S. Kevin (Institute of Computing Technology; University of Science and Technology of China)",21,21,2.77,18.41,http://arxiv.org/pdf/2012.08721,https://app.dimensions.ai/details/publication/pub.1137258386,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
6954,pub.1091888385,10.1002/mp.12593,28940372,,Esophagus segmentation in CT via 3D fully convolutional neural network and random walk,"PURPOSE: Precise delineation of organs at risk is a crucial task in radiotherapy treatment planning for delivering high doses to the tumor while sparing healthy tissues. In recent years, automated segmentation methods have shown an increasingly high performance for the delineation of various anatomical structures. However, this task remains challenging for organs like the esophagus, which have a versatile shape and poor contrast to neighboring tissues. For human experts, segmenting the esophagus from CT images is a time-consuming and error-prone process. To tackle these issues, we propose a random walker approach driven by a 3D fully convolutional neural network (CNN) to automatically segment the esophagus from CT images.
METHODS: First, a soft probability map is generated by the CNN. Then, an active contour model (ACM) is fitted to the CNN soft probability map to get a first estimation of the esophagus location. The outputs of the CNN and ACM are then used in conjunction with a probability model based on CT Hounsfield (HU) values to drive the random walker. Training and evaluation were done on 50 CTs from two different datasets, with clinically used peer-reviewed esophagus contours. Results were assessed regarding spatial overlap and shape similarity.
RESULTS: The esophagus contours generated by the proposed algorithm showed a mean Dice coefficient of 0.76 ± 0.11, an average symmetric square distance of 1.36 ± 0.90 mm, and an average Hausdorff distance of 11.68 ± 6.80, compared to the reference contours. These results translate to a very good agreement with reference contours and an increase in accuracy compared to existing methods. Furthermore, when considering the results reported in the literature for the publicly available Synapse dataset, our method outperformed all existing approaches, which suggests that the proposed method represents the current state-of-the-art for automatic esophagus segmentation.
CONCLUSION: We show that a CNN can yield accurate estimations of esophagus location, and that the results of this model can be refined by a random walk step taking pixel intensities and neighborhood relationships into account. One of the main advantages of our network over previous methods is that it performs 3D convolutions, thus fully exploiting the 3D spatial context and performing an efficient volume-wise prediction. The whole segmentation process is fully automatic and yields esophagus delineations in very good agreement with the gold standard, showing that it can compete with previously published methods.","This work is supported by the National Science and Engineering Research Council of Canada (NSERC), discovery grant program, and by the ETS Research Chair on Artificial Intelligence in Medical Imaging.",,Medical Physics,,"Esophagus; Imaging, Three-Dimensional; Machine Learning; Neural Networks, Computer; Tomography, X-Ray Computed",2017-10-23,2017,2017-10-23,2017-12,44,12,6341-6352,Closed,Article,"Fechter, Tobias; Adebahr, Sonja; Baltas, Dimos; Ayed, Ismail Ben; Desrosiers, Christian; Dolz, Jose","Fechter, Tobias (Division of Medical Physics, Department of Radiation Oncology, Medical Center, Faculty of Medicine, University of Freiburg, German Cancer Consortium (DKTK) Partner Site Freiburg, German Cancer Research Center (DKFZ), Heidelberg, Germany); Adebahr, Sonja (Department of Radiation Oncology, Medical Center, Faculty of Medicine, University of Freiburg, German Cancer Consortium (DKTK) Partner Site Freiburg, German Cancer Research Center (DKFZ), Heidelberg, Germany); Baltas, Dimos (Division of Medical Physics, Department of Radiation Oncology, Medical Center, Faculty of Medicine, University of Freiburg, German Cancer Consortium (DKTK) Partner Site Freiburg, German Cancer Research Center (DKFZ), Heidelberg, Germany); Ayed, Ismail Ben (Laboratory for Imagery, Vision and Artificial Intelligence (LIVIA), École de technologie supérieure, Montréal, Canada); Desrosiers, Christian (Laboratory for Imagery, Vision and Artificial Intelligence (LIVIA), École de technologie supérieure, Montréal, Canada); Dolz, Jose (Laboratory for Imagery, Vision and Artificial Intelligence (LIVIA), École de technologie supérieure, Montréal, Canada)","Fechter, Tobias (German Cancer Research Center)","Fechter, Tobias (German Cancer Research Center); Adebahr, Sonja (German Cancer Research Center); Baltas, Dimos (German Cancer Research Center); Ayed, Ismail Ben (École de Technologie Supérieure); Desrosiers, Christian (École de Technologie Supérieure); Dolz, Jose (École de Technologie Supérieure)",58,19,2.63,27.11,,https://app.dimensions.ai/details/publication/pub.1091888385,51 Physical Sciences; 5105 Medical and Biological Physics,
6633,pub.1154058718,10.1007/s11517-022-02723-9,36580181,,Dual encoder network with transformer-CNN for multi-organ segmentation,"Medical image segmentation is a critical step in many imaging applications. Automatic segmentation has gained extensive concern using a convolutional neural network (CNN). However, the traditional CNN-based methods fail to extract global and long-range contextual information due to local convolution operation. Transformer overcomes the limitation of CNN-based models. Inspired by the success of transformers in computer vision (CV), many researchers focus on designing the transformer-based U-shaped method in medical image segmentation. The transformer-based approach cannot effectively capture the fine-grained details. This paper proposes a dual encoder network with transformer-CNN for multi-organ segmentation. The new segmentation framework takes full advantage of CNN and transformer to enhance the segmentation accuracy. The Swin-transformer encoder extracts global information, and the CNN encoder captures local information. We introduce fusion modules to fuse convolutional features and the sequence of features from the transformer. Feature fusion is concatenated through the skip connection to smooth the decision boundary effectively. We extensively evaluate our method on the synapse multi-organ CT dataset and the automated cardiac diagnosis challenge (ACDC) dataset. The results demonstrate that the proposed method achieves Dice similarity coefficient (DSC) metrics of 80.68% and 91.12% on the synapse multi-organ CT and ACDC datasets, respectively. We perform the ablation studies on the ACDC dataset, demonstrating the effectiveness of critical components of our method. Our results match the ground-truth boundary more consistently than the existing models. Our approach gains more accurate results on challenging 2D images for multi-organ segmentation. Compared with the state-of-the-art methods, our proposed method achieves superior performance in multi-organ segmentation tasks.Graphical AbstractThe key process in medical image segmentation.",,"This study was funded by the National Natural Science Foundation of China (grant numbers 61504055 and 61701218), the Natural Science Foundation of Hunan Province of China (grant numbers 2020JJ4514 and 2020JJ4519), and the Postgraduate Research Innovation Project of Hunan Province of China (grant numbers CX20200934). This study was also funded by Hunan provincial base for scientific and technological innovation cooperation.",Medical & Biological Engineering & Computing,,"Benchmarking; Electric Power Supplies; Heart; Neural Networks, Computer; Synapses; Image Processing, Computer-Assisted",2022-12-29,2022,2022-12-29,2023-03,61,3,661-671,Closed,Article,"Hong, Zhifang; Chen, Mingzhi; Hu, Weijie; Yan, Shiyu; Qu, Aiping; Chen, Lingna; Chen, Junxi","Hong, Zhifang (Computer School, University of South China, 421001, Hengyang, China); Chen, Mingzhi (College of Mechanical and Vehicle Engineering, Hunan University, 410082, Hengyang, China); Hu, Weijie (School of Economics and Management, Beijing University of Chemical Technology, 100029, Beijing, China); Yan, Shiyu (Computer School, University of South China, 421001, Hengyang, China); Qu, Aiping (Computer School, University of South China, 421001, Hengyang, China); Chen, Lingna (Computer School, University of South China, 421001, Hengyang, China); Chen, Junxi (Affiliated Nanhua Hospital, University of South China, 421001, Hengyang, China)","Chen, Lingna (University of South China)","Hong, Zhifang (University of South China); Chen, Mingzhi (Hunan University); Hu, Weijie (Beijing University of Chemical Technology); Yan, Shiyu (University of South China); Qu, Aiping (University of South China); Chen, Lingna (University of South China); Chen, Junxi (University of South China)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154058718,46 Information and Computing Sciences; 4605 Data Management and Data Science,
6197,pub.1153876329,10.1117/1.jmi.9.6.064003,36569410,PMC9768435,Contour interpolation by deep learning approach,"Purpose: Contour interpolation is an important tool for expediting manual segmentation of anatomical structures. The process allows users to manually contour on discontinuous slices and then automatically fill in the gaps, therefore saving time and efforts. The most used conventional shape-based interpolation (SBI) algorithm, which operates on shape information, often performs suboptimally near the superior and inferior borders of organs and for the gastrointestinal structures. In this study, we present a generic deep learning solution to improve the robustness and accuracy for contour interpolation, especially for these historically difficult cases.
Approach: A generic deep contour interpolation model was developed and trained using 16,796 publicly available cases from 5 different data libraries, covering 15 organs. The network inputs were a 128 × 128 × 5  image patch and the two-dimensional contour masks for the top and bottom slices of the patch. The outputs were the organ masks for the three middle slices. The performance was evaluated on both dice scores and distance-to-agreement (DTA) values.
Results: The deep contour interpolation model achieved a dice score of 0.95 ± 0.05  and a mean DTA value of 1.09 ± 2.30    mm  , averaged on 3167 testing cases of all 15 organs. In a comparison, the results by the conventional SBI method were 0.94 ± 0.08  and 1.50 ± 3.63    mm  , respectively. For the difficult cases, the dice score and DTA value were 0.91 ± 0.09  and 1.68 ± 2.28    mm  by the deep interpolator, compared with 0.86 ± 0.13  and 3.43 ± 5.89    mm  by SBI. The t-test results confirmed that the performance improvements were statistically significant ( p < 0.05  ) for all cases in dice scores and for small organs and difficult cases in DTA values. Ablation studies were also performed.
Conclusions: A deep learning method was developed to enhance the process of contour interpolation. It could be useful for expediting the tasks of manual segmentation of organs and structures in the medical images.",This study was supported by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) (Grant No. R03-EB028427).,,Journal of Medical Imaging,,,2022-11,2022,2022-12-21,2022-11,9,6,064003-064003,Closed,Article,"Zhao, Chenxi; Duan, Ye; Yang, Deshan","Zhao, Chenxi (University of Missouri, Electrical Engineering and Computer Science Department, Columbia, Missouri, United States); Duan, Ye (University of Missouri, Electrical Engineering and Computer Science Department, Columbia, Missouri, United States); Yang, Deshan (Duke University, Department of Radiation Oncology, Durham, North Carolina, United States)","Zhao, Chenxi (University of Missouri)","Zhao, Chenxi (University of Missouri); Duan, Ye (University of Missouri); Yang, Deshan (Duke University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153876329,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences; 40 Engineering; 4003 Biomedical Engineering,
6191,pub.1129822130,10.1002/mp.14422,32740931,,Complete abdomen and pelvis segmentation using U‐net variant architecture,"PURPOSE: Organ segmentation of computed tomography (CT) imaging is essential for radiotherapy treatment planning. Treatment planning requires segmentation not only of the affected tissue, but nearby healthy organs-at-risk, which is laborious and time-consuming. We present a fully automated segmentation method based on the three-dimensional (3D) U-Net convolutional neural network (CNN) capable of whole abdomen and pelvis segmentation into 33 unique organ and tissue structures, including tissues that may be overlooked by other automated segmentation approaches such as adipose tissue, skeletal muscle, and connective tissue and vessels. Whole abdomen segmentation is capable of quantifying exposure beyond a handful of organs-at-risk to all tissues within the abdomen.
METHODS: Sixty-six (66) CT examinations of 64 individuals were included in the training and validation sets and 18 CT examinations from 16 individuals were included in the test set. All pixels in each examination were segmented by image analysts (with physician correction) and assigned one of 33 labels. Segmentation was performed with a 3D U-Net variant architecture which included residual blocks, and model performance was quantified on 18 test cases. Human interobserver variability (using semiautomated segmentation) was also reported on two scans, and manual interobserver variability of three individuals was reported on one scan. Model performance was also compared to several of the best models reported in the literature for multiple organ segmentation.
RESULTS: The accuracy of the 3D U-Net model ranges from a Dice coefficient of 0.95 in the liver, 0.93 in the kidneys, 0.79 in the pancreas, 0.69 in the adrenals, and 0.51 in the renal arteries. Model accuracy is within 5% of human segmentation in eight of 19 organs and within 10% accuracy in 13 of 19 organs.
CONCLUSIONS: The CNN approaches the accuracy of human tracers and on certain complex organs displays more consistent prediction than human tracers. Fully automated deep learning-based segmentation of CT abdomen has the potential to improve both the speed and accuracy of radiotherapy dose prediction for organs-at-risk.","Dr. Weston is supported by Mayo Clinic Graduate School of Biomedical Sciences. Funding support was also provided by the Department of Radiology, Mayo Clinic. The authors would like to thank Zeynettin Akkus, Jason Cai, Scott Squires, Bill Ryan, Dan Hertzfeldt, Rachel Marks, Clare Buntrock, Isabel Bazley, Cailin Austin, Nicholas DeBlois, Lucas Betts, Megan Berry, Margaret Cantlon, Angela Weiler, Bailey Ullom, and Lindsey Anding.",,Medical Physics,,"Abdomen; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Organs at Risk; Pelvis; Tomography, X-Ray Computed",2020-10-07,2020,2020-10-07,2020-11,47,11,5609-5618,Closed,Article,"Weston, Alexander D.; Korfiatis, Panagiotis; Philbrick, Kenneth A.; Conte, Gian Marco; Kostandy, Petro; Sakinis, Thomas; Zeinoddini, Atefeh; Boonrod, Arunnit; Moynagh, Michael; Takahashi, Naoki; Erickson, Bradley J.","Weston, Alexander D. (Health Sciences Research, Mayo Clinic, 4500 San Pablo Road S, Jacksonville, FL, 32250, USA); Korfiatis, Panagiotis (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Philbrick, Kenneth A. (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Conte, Gian Marco (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Kostandy, Petro (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Sakinis, Thomas (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Zeinoddini, Atefeh (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Boonrod, Arunnit (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Moynagh, Michael (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Takahashi, Naoki (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA); Erickson, Bradley J. (Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN, 55905, USA)","Erickson, Bradley J. (Mayo Clinic)","Weston, Alexander D. (Mayo Clinic); Korfiatis, Panagiotis (Mayo Clinic); Philbrick, Kenneth A. (Mayo Clinic); Conte, Gian Marco (Mayo Clinic); Kostandy, Petro (Mayo Clinic); Sakinis, Thomas (Mayo Clinic); Zeinoddini, Atefeh (Mayo Clinic); Boonrod, Arunnit (Mayo Clinic); Moynagh, Michael (Mayo Clinic); Takahashi, Naoki (Mayo Clinic); Erickson, Bradley J. (Mayo Clinic)",12,12,1.1,9.1,,https://app.dimensions.ai/details/publication/pub.1129822130,51 Physical Sciences; 5105 Medical and Biological Physics,
6179,pub.1146889506,10.1148/radiol.211914,35380492,PMC9270681,Fully Automated Abdominal CT Biomarkers for Type 2 Diabetes Using Deep Learning,"Background CT biomarkers both inside and outside the pancreas can potentially be used to diagnose type 2 diabetes mellitus. Previous studies on this topic have shown significant results but were limited by manual methods and small study samples. Purpose To investigate abdominal CT biomarkers for type 2 diabetes mellitus in a large clinical data set using fully automated deep learning. Materials and Methods For external validation, noncontrast abdominal CT images were retrospectively collected from consecutive patients who underwent routine colorectal cancer screening with CT colonography from 2004 to 2016. The pancreas was segmented using a deep learning method that outputs measurements of interest, including CT attenuation, volume, fat content, and pancreas fractal dimension. Additional biomarkers assessed included visceral fat, atherosclerotic plaque, liver and muscle CT attenuation, and muscle volume. Univariable and multivariable analyses were performed, separating patients into groups based on time between type 2 diabetes diagnosis and CT date and including clinical factors such as sex, age, body mass index (BMI), BMI greater than 30 kg/m2, and height. The best set of predictors for type 2 diabetes were determined using multinomial logistic regression. Results A total of 8992 patients (mean age, 57 years ± 8 [SD]; 5009 women) were evaluated in the test set, of whom 572 had type 2 diabetes mellitus. The deep learning model had a mean Dice similarity coefficient for the pancreas of 0.69 ± 0.17, similar to the interobserver Dice similarity coefficient of 0.69 ± 0.09 (P = .92). The univariable analysis showed that patients with diabetes had, on average, lower pancreatic CT attenuation (mean, 18.74 HU ± 16.54 vs 29.99 HU ± 13.41; P < .0001) and greater visceral fat volume (mean, 235.0 mL ± 108.6 vs 130.9 mL ± 96.3; P < .0001) than those without diabetes. Patients with diabetes also showed a progressive decrease in pancreatic attenuation with greater duration of disease. The final multivariable model showed pairwise areas under the receiver operating characteristic curve (AUCs) of 0.81 and 0.85 between patients without and patients with diabetes who were diagnosed 0-2499 days before and after undergoing CT, respectively. In the multivariable analysis, adding clinical data did not improve upon CT-based AUC performance (AUC = 0.67 for the CT-only model vs 0.68 for the CT and clinical model). The best predictors of type 2 diabetes mellitus included intrapancreatic fat percentage, pancreatic fractal dimension, plaque severity between the L1 and L4 vertebra levels, average liver CT attenuation, and BMI. Conclusion The diagnosis of type 2 diabetes mellitus was associated with abdominal CT biomarkers, especially measures of pancreatic CT attenuation and visceral fat. © RSNA, 2022 Online supplemental material is available for this article.",This work utilized the computational resources of the National Institutes of Health high-performance computing Biowulf cluster.,,Radiology,,"Biomarkers; Deep Learning; Diabetes Mellitus, Type 2; Female; Humans; Middle Aged; Retrospective Studies; Tomography, X-Ray Computed",2022-04-05,2022,2022-04-05,2022-07,304,1,85-95,Closed,Article,"Tallam, Hima; Elton, Daniel C; Lee, Sungwon; Wakim, Paul; Pickhardt, Perry J; Summers, Ronald M","Tallam, Hima (From the Department of Radiology and Imaging Sciences (H.T., D.C.E., S.L., R.M.S.) and Department of Biostatistics and Clinical Epidemiology Service (P.W.), Clinical Center, National Institutes of Health, 10 Center Dr, Bldg 10, Room 1C224D, MSC 1182, Bethesda, MD 20892-1182; and Department of Radiology, University of Wisconsin School of Medicine and Public Health, Madison, Wis (P.J.P.).); Elton, Daniel C (From the Department of Radiology and Imaging Sciences (H.T., D.C.E., S.L., R.M.S.) and Department of Biostatistics and Clinical Epidemiology Service (P.W.), Clinical Center, National Institutes of Health, 10 Center Dr, Bldg 10, Room 1C224D, MSC 1182, Bethesda, MD 20892-1182; and Department of Radiology, University of Wisconsin School of Medicine and Public Health, Madison, Wis (P.J.P.).); Lee, Sungwon (From the Department of Radiology and Imaging Sciences (H.T., D.C.E., S.L., R.M.S.) and Department of Biostatistics and Clinical Epidemiology Service (P.W.), Clinical Center, National Institutes of Health, 10 Center Dr, Bldg 10, Room 1C224D, MSC 1182, Bethesda, MD 20892-1182; and Department of Radiology, University of Wisconsin School of Medicine and Public Health, Madison, Wis (P.J.P.).); Wakim, Paul (From the Department of Radiology and Imaging Sciences (H.T., D.C.E., S.L., R.M.S.) and Department of Biostatistics and Clinical Epidemiology Service (P.W.), Clinical Center, National Institutes of Health, 10 Center Dr, Bldg 10, Room 1C224D, MSC 1182, Bethesda, MD 20892-1182; and Department of Radiology, University of Wisconsin School of Medicine and Public Health, Madison, Wis (P.J.P.).); Pickhardt, Perry J (From the Department of Radiology and Imaging Sciences (H.T., D.C.E., S.L., R.M.S.) and Department of Biostatistics and Clinical Epidemiology Service (P.W.), Clinical Center, National Institutes of Health, 10 Center Dr, Bldg 10, Room 1C224D, MSC 1182, Bethesda, MD 20892-1182; and Department of Radiology, University of Wisconsin School of Medicine and Public Health, Madison, Wis (P.J.P.).); Summers, Ronald M (From the Department of Radiology and Imaging Sciences (H.T., D.C.E., S.L., R.M.S.) and Department of Biostatistics and Clinical Epidemiology Service (P.W.), Clinical Center, National Institutes of Health, 10 Center Dr, Bldg 10, Room 1C224D, MSC 1182, Bethesda, MD 20892-1182; and Department of Radiology, University of Wisconsin School of Medicine and Public Health, Madison, Wis (P.J.P.).)",,"Tallam, Hima (National Institutes of Health Clinical Center); Elton, Daniel C (National Institutes of Health Clinical Center); Lee, Sungwon (National Institutes of Health Clinical Center); Wakim, Paul (National Institutes of Health Clinical Center); Pickhardt, Perry J (National Institutes of Health Clinical Center); Summers, Ronald M (National Institutes of Health Clinical Center)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1146889506,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
6157,pub.1131399260,10.1007/978-3-030-59713-9_34,33364627,PMC7757792,Unified Cross-Modality Feature Disentangler for Unsupervised Multi-domain MRI Abdomen Organs Segmentation,"Our contribution is a unified cross-modality feature disentagling approach for multi-domain image translation and multiple organ segmentation. Using CT as the labeled source domain, our approach learns to segment multi-modal (T1-weighted and T2-weighted) MRI having no labeled data. Our approach uses a variational auto-encoder (VAE) to disentangle the image content from style. The VAE constrains the style feature encoding to match a universal prior (Gaussian) that is assumed to span the styles of all the source and target modalities. The extracted image style is converted into a latent style scaling code, which modulates the generator to produce multi-modality images according to the target domain code from the image content features. Finally, we introduce a joint distribution matching discriminator that combines the translated images with task-relevant segmentation probability maps to further constrain and regularize image-to-image (I2I) translations. We performed extensive comparisons to multiple state-of-the-art I2I translation and segmentation methods. Our approach resulted in the lowest average multi-domain image reconstruction error of 1.34 ± 0.04. Our approach produced an average Dice similarity coefficient (DSC) of 0.85 for T1w and 0.90 for T2w MRI for multi-organ segmentation, which was highly comparable to a fully supervised MRI multi-organ segmentation network (DSC of 0.86 for T1w and 0.90 for T2w MRI)
.",This work was supported by the MSK Cancer Center support grant/core grant P30 CA008748.,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12262,,347-358,All OA; Green,Chapter,"Jiang, Jue; Veeraraghavan, Harini","Jiang, Jue (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA); Veeraraghavan, Harini (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA)","Veeraraghavan, Harini (Memorial Sloan Kettering Cancer Center)","Jiang, Jue (Memorial Sloan Kettering Cancer Center); Veeraraghavan, Harini (Memorial Sloan Kettering Cancer Center)",18,18,0.56,8.57,http://arxiv.org/pdf/2007.09669,https://app.dimensions.ai/details/publication/pub.1131399260,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
6011,pub.1153356675,10.1002/mp.16135,36463516,,Abdomen CT multi‐organ segmentation using token‐based MLP‐Mixer,"BACKGROUND: Manual contouring is very labor-intensive, time-consuming, and subject to intra- and inter-observer variability. An automated deep learning approach to fast and accurate contouring and segmentation is desirable during radiotherapy treatment planning.
PURPOSE: This work investigates an efficient deep-learning-based segmentation algorithm in abdomen computed tomography (CT) to facilitate radiation treatment planning.
METHODS: In this work, we propose a novel deep-learning model utilizing U-shaped multi-layer perceptron mixer (MLP-Mixer) and convolutional neural network (CNN) for multi-organ segmentation in abdomen CT images. The proposed model has a similar structure to V-net, while a proposed MLP-Convolutional block replaces each convolutional block. The MLP-Convolutional block consists of three components: an early convolutional block for local features extraction and feature resampling, a token-based MLP-Mixer layer for capturing global features with high efficiency, and a token projector for pixel-level detail recovery. We evaluate our proposed network using: (1) an institutional dataset with 60 patient cases and (2) a public dataset (BCTV) with 30 patient cases. The network performance was quantitatively evaluated in three domains: (1) volume similarity between the ground truth contours and the network predictions using the Dice score coefficient (DSC), sensitivity, and precision; (2) surface similarity using Hausdorff distance (HD), mean surface distance (MSD) and residual mean square distance (RMS); and (3) the computational complexity reported by the number of network parameters, training time, and inference time. The performance of the proposed network is compared with other state-of-the-art networks.
RESULTS: In the institutional dataset, the proposed network achieved the following volume similarity measures when averaged over all organs: DSC = 0.912, sensitivity = 0.917, precision = 0.917, average surface similarities were HD = 11.95 mm, MSD = 1.90 mm, RMS = 3.86 mm. The proposed network achieved DSC = 0.786 and HD = 9.04 mm on the public dataset. The network also shows statistically significant improvement, which is evaluated by a two-tailed Wilcoxon Mann-Whitney U test, on right lung (MSD where the maximum p-value is 0.001), spinal cord (sensitivity, precision, HD, RMSD where p-value ranges from 0.001 to 0.039), and stomach (DSC where the maximum p-value is 0.01) over all other competing networks. On the public dataset, the network report statistically significant improvement, which is shown by the Wilcoxon Mann-Whitney test, on pancreas (HD where the maximum p-value is 0.006), left (HD where the maximum p-value is 0.022) and right adrenal glands (DSC where the maximum p-value is 0.026). In both datasets, the proposed method can generate contours in less than 5 s. Overall, the proposed MLP-Vnet demonstrates comparable or better performance than competing methods with much lower memory complexity and higher speed.
CONCLUSIONS: The proposed MLP-Vnet demonstrates superior segmentation performance, in terms of accuracy and efficiency, relative to state-of-the-art methods. This reliable and efficient method demonstrates potential to streamline clinical workflows in abdominal radiotherapy, which may be especially important for online adaptive treatments.","This research is supported in part by the National Cancer Institute of the National Institute of Health (NIH) under Award Number R01CA215718, R01EB032680 and R01EB028324.",,Medical Physics,,,2022-12-04,2022,2022-12-20,2022-12-04,,,,Closed,Article,"Pan, Shaoyan; Chang, Chih‐Wei; Wang, Tonghe; Wynne, Jacob; Hu, Mingzhe; Lei, Yang; Liu, Tian; Patel, Pretesh; Roper, Justin; Yang, Xiaofeng","Pan, Shaoyan (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA; Department of Biomedical Informatics, Emory University, Atlanta, Georgia, USA); Chang, Chih‐Wei (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA); Wang, Tonghe (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA); Wynne, Jacob (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA); Hu, Mingzhe (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA; Department of Biomedical Informatics, Emory University, Atlanta, Georgia, USA); Lei, Yang (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA); Liu, Tian (Department of Radiation Oncology, Mount Sinai Medical Center, New York, New York, USA); Patel, Pretesh (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA); Roper, Justin (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA); Yang, Xiaofeng (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, Georgia, USA; Department of Biomedical Informatics, Emory University, Atlanta, Georgia, USA)","Yang, Xiaofeng (Emory University; Emory University)","Pan, Shaoyan (Emory University; Emory University); Chang, Chih‐Wei (Emory University); Wang, Tonghe (Emory University); Wynne, Jacob (Emory University); Hu, Mingzhe (Emory University; Emory University); Lei, Yang (Emory University); Liu, Tian (Mount Sinai Hospital); Patel, Pretesh (Emory University); Roper, Justin (Emory University); Yang, Xiaofeng (Emory University; Emory University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153356675,51 Physical Sciences; 5105 Medical and Biological Physics,
6011,pub.1124557779,10.1109/tmi.2019.2963882,32012001,,Unpaired Multi-Modal Segmentation via Knowledge Distillation,"Multi-modal learning is typically performed with network architectures containing modality-specific layers and shared layers, utilizing co-registered images of different modalities. We propose a novel learning scheme for unpaired cross-modality image segmentation, with a highly compact architecture achieving superior segmentation accuracy. In our method, we heavily reuse network parameters, by sharing all convolutional kernels across CT and MRI, and only employ modality-specific internal normalization layers which compute respective statistics. To effectively train such a highly compact model, we introduce a novel loss term inspired by knowledge distillation, by explicitly constraining the KL-divergence of our derived prediction distributions between modalities. We have extensively validated our approach on two multi-class segmentation problems: i) cardiac structure segmentation, and ii) abdominal organ segmentation. Different network settings, i.e., 2D dilated network and 3D U-net, are utilized to investigate our method's general efficacy. Experimental results on both tasks demonstrate that our novel multi-modal learning scheme consistently outperforms single-modal training and previous multi-modal approaches.","This work was supported in part by the European Research Council (ERC) through the European Union’s Horizon 2020 Research and Innovation Programme, under Grant 757173, in part by the Project MIRA, under Grant ERC-2017-STG, and in part by the Hong Kong Innovation and Technology Commission through ITSP Scheme under Grant ITS/426/17FP and Grant ITS/311/18FP.",,IEEE Transactions on Medical Imaging,,"Algorithms; Magnetic Resonance Imaging; Neural Networks, Computer",2020-02-03,2020,2020-02-03,2020-07,39,7,2415-2425,All OA; Hybrid,Article,"Dou, Qi; Liu, Quande; Heng, Pheng Ann; Glocker, Ben","Dou, Qi (Biomedical Image Analysis Group, Imperial College London, London, SW7 2AZ, U.K.); Liu, Quande (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Heng, Pheng Ann (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Glocker, Ben (Biomedical Image Analysis Group, Imperial College London, London, SW7 2AZ, U.K.)","Dou, Qi (Imperial College London)","Dou, Qi (Imperial College London); Liu, Quande (Chinese University of Hong Kong); Heng, Pheng Ann (Chinese University of Hong Kong); Glocker, Ben (Imperial College London)",85,75,5.75,43.81,https://ieeexplore.ieee.org/ielx7/42/9130176/08979396.pdf,https://app.dimensions.ai/details/publication/pub.1124557779,46 Information and Computing Sciences; 4611 Machine Learning,
6008,pub.1134079153,10.1016/j.media.2020.101950,33421920,,CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation,"Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative analysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Previous abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were designed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 ± 0.00 / 0.95 ± 0.01), but the best MSSD performance remains limited (21.89 ± 13.94 / 20.85 ± 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 ± 0.15 MSSD: 36.33 ± 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon.","The organizers would like to thank Ivana Isgum and Tom Vercauteren in the challenge committee of ISBI 2019 for their guidance and support. We express our gratitude to supporting organizations of the grand-challenge.org platform. We thank Esranur Kazaz, Umut Baran Ekinci, Ece Köse, Fabian Isensee, David Völgyes, and Javier Coronel for their contributions. Last but not least, our special thanks go to Ludmila I. Kuncheva for her valuable contributions. This work is supported by Scientific and Technological Research Council of Turkey (TUBITAK) ARDEB-EEEAG under grant number 116E133 and TUBITAK BIDEB-2214 International Doctoral Research Fellowship Programme. The work of P. Ernst, S. Chatterjee, O. Speck and, A. Nürnberger was conducted within the context of the International Graduate School MEMoRIAL at OvGU Magdeburg, Germany, supported by ESF (project no. ZS/2016/08/80646). The work of S. Aslan within the context of Ca’ Foscari University of Venice is supported by under TUBITAK BIDEB-2219 grant no 1059B191701102.",,Medical Image Analysis,,"Abdomen; Algorithms; Humans; Liver; Tomography, X-Ray Computed",2020-12-25,2020,2020-12-25,2021-04,69,,101950,All OA; Green,Article,"Kavur, A Emre; Gezer, N Sinem; Barış, Mustafa; Aslan, Sinem; Conze, Pierre-Henri; Groza, Vladimir; Pham, Duc Duy; Chatterjee, Soumick; Ernst, Philipp; Özkan, Savaş; Baydar, Bora; Lachinov, Dmitry; Han, Shuo; Pauli, Josef; Isensee, Fabian; Perkonigg, Matthias; Sathish, Rachana; Rajan, Ronnie; Sheet, Debdoot; Dovletov, Gurbandurdy; Speck, Oliver; Nürnberger, Andreas; Maier-Hein, Klaus H; Bozdağı Akar, Gözde; Ünal, Gözde; Dicle, Oğuz; Selver, M Alper","Kavur, A Emre (Graduate School of Natural and Applied Sciences, Dokuz Eylul University, Izmir, Turkey.); Gezer, N Sinem (Department of Radiology, Faculty Of Medicine, Dokuz Eylul University, Izmir, Turkey.); Barış, Mustafa (Department of Radiology, Faculty Of Medicine, Dokuz Eylul University, Izmir, Turkey.); Aslan, Sinem (Ca' Foscari University of Venice, ECLT and DAIS, Venice, Italy; Ege University, International Computer Institute, Izmir, Turkey.); Conze, Pierre-Henri (IMT Atlantique, LaTIM UMR 1101, Brest, France.); Groza, Vladimir (Median Technologies, Valbonne, France.); Pham, Duc Duy (Intelligent Systems, Faculty of Engineering, University of Duisburg-Essen, Germany.); Chatterjee, Soumick (Data and Knowledge Engineering Group, Otto von Guericke University, Magdeburg, Germany; Biomedical Magnetic Resonance, Otto von Guericke University Magdeburg, Germany.); Ernst, Philipp (Data and Knowledge Engineering Group, Otto von Guericke University, Magdeburg, Germany.); Özkan, Savaş (Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey.); Baydar, Bora (Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey.); Lachinov, Dmitry (Department of Ophthalmology and Optometry, Medical Uni. of Vienna, Austria.); Han, Shuo (Johns Hopkins University, Baltimore, USA.); Pauli, Josef (Intelligent Systems, Faculty of Engineering, University of Duisburg-Essen, Germany.); Isensee, Fabian (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany.); Perkonigg, Matthias (CIR Lab Dept of Biomedical Imaging and Image-guided Therapy Medical Uni. of Vienna, Austria.); Sathish, Rachana (Department of Electrical Engineering, Indian Institute of Technology, Kharagpur, India.); Rajan, Ronnie (School of Medical Science and Technology, Indian Institute of Technology, Kharagpur, India.); Sheet, Debdoot (Department of Electrical Engineering, Indian Institute of Technology, Kharagpur, India.); Dovletov, Gurbandurdy (Intelligent Systems, Faculty of Engineering, University of Duisburg-Essen, Germany.); Speck, Oliver (Biomedical Magnetic Resonance, Otto von Guericke University Magdeburg, Germany.); Nürnberger, Andreas (Data and Knowledge Engineering Group, Otto von Guericke University, Magdeburg, Germany.); Maier-Hein, Klaus H (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany.); Bozdağı Akar, Gözde (Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey.); Ünal, Gözde (Faculty of Computer and Informatics Engineering, İstanbul Technical University, İstanbul, Turkey.); Dicle, Oğuz (Department of Radiology, Faculty Of Medicine, Dokuz Eylul University, Izmir, Turkey.); Selver, M Alper (Department of Electrical and Electronics Engineering, Dokuz Eylul University, Izmir, Turkey. Electronic address: alper.selver@deu.edu.tr.)","Kavur, A Emre (Dokuz Eylül University)","Kavur, A Emre (Dokuz Eylül University); Gezer, N Sinem (Dokuz Eylül University); Barış, Mustafa (Dokuz Eylül University); Aslan, Sinem (Ca' Foscari University of Venice; Ege University); Conze, Pierre-Henri (IMT Atlantique); Groza, Vladimir (); Pham, Duc Duy (University of Duisburg-Essen); Chatterjee, Soumick (Otto-von-Guericke University Magdeburg); Ernst, Philipp (Otto-von-Guericke University Magdeburg); Özkan, Savaş (Middle East Technical University); Baydar, Bora (Middle East Technical University); Lachinov, Dmitry (Medical University of Vienna); Han, Shuo (Johns Hopkins University); Pauli, Josef (University of Duisburg-Essen); Isensee, Fabian (German Cancer Research Center); Perkonigg, Matthias (); Sathish, Rachana (Indian Institute of Technology Kharagpur); Rajan, Ronnie (Indian Institute of Technology Kharagpur); Sheet, Debdoot (Indian Institute of Technology Kharagpur); Dovletov, Gurbandurdy (University of Duisburg-Essen); Speck, Oliver (Otto-von-Guericke University Magdeburg); Nürnberger, Andreas (Otto-von-Guericke University Magdeburg); Maier-Hein, Klaus H (German Cancer Research Center); Bozdağı Akar, Gözde (Middle East Technical University); Ünal, Gözde (Istanbul Technical University); Dicle, Oğuz (Dokuz Eylül University); Selver, M Alper (Dokuz Eylül University)",181,171,18.06,,http://arxiv.org/pdf/2001.06535,https://app.dimensions.ai/details/publication/pub.1134079153,32 Biomedical and Clinical Sciences,
6004,pub.1143319585,10.1016/j.compbiomed.2021.105067,34920364,,Few-shot medical image segmentation using a global correlation network with discriminative embedding,"Despite impressive developments in deep convolutional neural networks for medical imaging, the paradigm of supervised learning requires numerous annotations in training to avoid overfitting. In clinical cases, massive semantic annotations are difficult to acquire where biomedical expert knowledge is required. Moreover, it is common when only a few annotated classes are available. In this study, we proposed a new approach to few-shot medical image segmentation, which enables a segmentation model to quickly generalize to an unseen class with few training images. We constructed a few-shot image segmentation mechanism using a deep convolutional network trained episodically. Motivated by the spatial consistency and regularity in medical images, we developed an efficient global correlation module to model the correlation between a support and query image and incorporate it into the deep network. We enhanced the discrimination ability of the deep embedding scheme to encourage clustering of feature domains belonging to the same class while keeping feature domains of different organs far apart. We experimented using anatomical abdomen images from both CT and MRI modalities.","The study is supported partly by the National Key Research and Development Program of China (No. 2019YFC0118100), ZheJiang Province Key Research Development Program (No. 2020C03073), China Postdoctoral Science Foundation (No. 2021M702726), National Natural Science Foundation of China under Grants 82172033, 61971369, U19B2031, Science and Technology Key Project of Fujian Province (No. 2019HZ020009), Fundamental Research Funds for the Central Universities 20720200003, and Tencent Open Fund.",,Computers in Biology and Medicine,,,2021-11-27,2021,2021-11-27,2022-01,140,,105067,All OA; Green,Article,"Sun, Liyan; Li, Chenxin; Ding, Xinghao; Huang, Yue; Chen, Zhong; Wang, Guisheng; Yu, Yizhou; Paisley, John","Sun, Liyan (School of Informatics, Xiamen University, Xiamen, 361 005, Fujian, China; School of Electronic Science and Engineering, Xiamen University, Xiamen, 361 005, Fujian, China.); Li, Chenxin (School of Informatics, Xiamen University, Xiamen, 361 005, Fujian, China.); Ding, Xinghao (School of Informatics, Xiamen University, Xiamen, 361 005, Fujian, China. Electronic address: dxh@xmu.edu.cn.); Huang, Yue (School of Informatics, Xiamen University, Xiamen, 361 005, Fujian, China.); Chen, Zhong (School of Electronic Science and Engineering, Xiamen University, Xiamen, 361 005, Fujian, China.); Wang, Guisheng (Department of Radiology, Third Medical Centre, Chinese PLA General Hospital, Beijing, 100 036, China.); Yu, Yizhou (Deepwise AI Laboratory, Beijing, 100 125, China.); Paisley, John (Department of Electrical Engineering and the Data Science Institute, Columbia University, New York, 10 027, NY, USA.)","Ding, Xinghao (Xiamen University)","Sun, Liyan (Xiamen University); Li, Chenxin (Xiamen University); Ding, Xinghao (Xiamen University); Huang, Yue (Xiamen University); Chen, Zhong (Xiamen University); Wang, Guisheng (Chinese PLA General Hospital); Yu, Yizhou (); Paisley, John (Columbia University)",14,14,0.93,11.08,http://arxiv.org/pdf/2012.05440,https://app.dimensions.ai/details/publication/pub.1143319585,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4611 Machine Learning,
6004,pub.1139833089,10.1016/j.compbiomed.2021.104658,34311262,,Multiorgan segmentation from partially labeled datasets with conditional nnU-Net,"Accurate and robust multiorgan abdominal CT segmentation plays a significant role in numerous clinical applications, such as therapy treatment planning and treatment delivery. Almost all existing segmentation networks rely on fully annotated data with strong supervision. However, annotating fully annotated multiorgan data in CT images is both laborious and time-consuming. In comparison, massive partially labeled datasets are usually easily accessible. In this paper, we propose conditional nnU-Net trained on the union of partially labeled datasets for multiorgan segmentation. The deep model employs the state-of-the-art nnU-Net as the backbone and introduces a conditioning strategy by feeding auxiliary information into the decoder architecture as an additional input layer. This model leverages the prior conditional information to identify the organ class at the pixel-wise level and encourages organs' spatial information recovery. Furthermore, we adopt a deep supervision mechanism to refine the outputs at different scales and apply the combination of Dice loss and Focal loss to optimize the training model. Our proposed method is evaluated on seven publicly available datasets of the liver, pancreas, spleen and kidney, in which promising segmentation performance has been achieved. The proposed conditional nnU-Net breaks down the barriers between nonoverlapping labeled datasets and further alleviates the problem of data hunger in multiorgan segmentation.","This research was supported by the National Natural Science Foundation of China (Grant No. 81871457), the National Natural Science Foundation of China (Grant No. 51775368) and the National Natural Science Foundation of China (Grant No. 51811530310).",,Computers in Biology and Medicine,,,2021-07-21,2021,2021-07-21,2021-09,136,,104658,Closed,Article,"Zhang, Guobin; Yang, Zhiyong; Huo, Bin; Chai, Shude; Jiang, Shan","Zhang, Guobin (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China.); Yang, Zhiyong (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China.); Huo, Bin (Department of Oncology, Tianjin Medical University Second Hospital, Tianjin, 300211, China.); Chai, Shude (Department of Oncology, Tianjin Medical University Second Hospital, Tianjin, 300211, China.); Jiang, Shan (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China. Electronic address: shanjmri@tju.edu.cn.)","Jiang, Shan (Tianjin University)","Zhang, Guobin (Tianjin University); Yang, Zhiyong (Tianjin University); Huo, Bin (Tianjin Medical University); Chai, Shude (Tianjin Medical University); Jiang, Shan (Tianjin University)",6,6,0.7,4.23,,https://app.dimensions.ai/details/publication/pub.1139833089,31 Biological Sciences; 3102 Bioinformatics and Computational Biology; 42 Health Sciences; 4203 Health Services and Systems; 46 Information and Computing Sciences; 4601 Applied Computing,
6002,pub.1128370375,10.1109/tmi.2020.3001036,32746108,PMC7665851,Multi-Organ Segmentation Over Partially Labeled Datasets With Multi-Scale Feature Abstraction,"Shortage of fully annotated datasets has been a limiting factor in developing deep learning based image segmentation algorithms and the problem becomes more pronounced in multi-organ segmentation. In this paper, we propose a unified training strategy that enables a novel multi-scale deep neural network to be trained on multiple partially labeled datasets for multi-organ segmentation. In addition, a new network architecture for multi-scale feature abstraction is proposed to integrate pyramid input and feature analysis into a U-shape pyramid structure. To bridge the semantic gap caused by directly merging features from different scales, an equal convolutional depth mechanism is introduced. Furthermore, we employ a deep supervision mechanism to refine the outputs in different scales. To fully leverage the segmentation features from all the scales, we design an adaptive weighting layer to fuse the outputs in an automatic fashion. All these mechanisms together are integrated into a Pyramid Input Pyramid Output Feature Abstraction Network (PIPO-FAN). Our proposed method was evaluated on four publicly available datasets, including BTCV, LiTS, KiTS and Spleen, where very promising performance has been achieved. The source code of this work is publicly shared at https://github.com/DIAL-RPI/PIPO-FAN to facilitate others to reproduce the work and build their own models using the introduced mechanisms.","This work was supported in part by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health (NIH) under Award R21EB028001 and Award R01EB027898, and by the National Cancer Institute (NCI) under the NIH Bench-to-Bedside Award. The authors would like to thank NVIDIA Corporation for the donation of two Titan Xp GPUs used for this research. They would also like to thank Prof. George Xu (RPI), Mr. Zhao Peng (USTC), and Dr. Sheng Xu (NIH) for the insightful discussions.",,IEEE Transactions on Medical Imaging,,"Algorithms; Neural Networks, Computer; Spleen",2020-10-28,2020,2020-10-28,2020-11,39,11,3619-3629,All OA; Green,Article,"Fang, Xi; Yan, Pingkun","Fang, Xi (Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, 12180, USA; Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechnic Institute, Troy, NY, 12180, USA); Yan, Pingkun (Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, 12180, USA; Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechnic Institute, Troy, NY, 12180, USA)","Yan, Pingkun (Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute)","Fang, Xi (Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute); Yan, Pingkun (Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute)",62,56,2.8,31.95,https://europepmc.org/articles/pmc7665851?pdf=render,https://app.dimensions.ai/details/publication/pub.1128370375,46 Information and Computing Sciences; 4611 Machine Learning,
5997,pub.1150769701,10.1109/tmi.2022.3204551,36063521,,MsVRL: Self-Supervised Multiscale Visual Representation Learning via Cross-Level Consistency for Medical Image Segmentation,"Automated medical image segmentation for organs or lesions plays an essential role in clinical diagnoses and treatment plannings. However, training an accurate and robust segmentation model is still a long-standing challenge due to the time-consuming and expertise-intensive annotations for training data, especially 3-D medical images. Recently, self-supervised learning emerges as a promising approach for unsupervised visual representation learning, showing great potential to alleviate the expertise annotations for medical images. Although global representation learning has attained remarkable results on iconic datasets, such as ImageNet, it can not be applied directly to medical image segmentation, because the segmentation task is non-iconic, and the targets always vary in physical scales. To address these problems, we propose a Multi-scale Visual Representation self-supervised Learning (MsVRL) model, to perform finer-grained representation and deal with different target scales. Specifically, a multi-scale representation conception, a canvas matching method, an embedding pre-sampling module, a center-ness branch, and a cross-level consistent loss are introduced to improve the performance. After pre-trained on unlabeled datasets (RibFrac and part of MSD), MsVRL performs downstream segmentation tasks on labeled datasets (BCV, spleen of MSD, and KiTS). Results of the experiments show that MsVRL outperforms other state-of-the-art works on these medical image segmentation tasks.",,This work was supported by the National Key Research and Development Program of China under Grant 2020AAA0109002.,IEEE Transactions on Medical Imaging,,,2022-09-05,2022,2022-09-05,2022-09-05,42,1,91-102,Closed,Article,"Zheng, Ruifeng; Zhong, Ying; Yan, Senxiang; Sun, Hongcheng; Shen, Haibin; Huang, Kejie","Zheng, Ruifeng (College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, 310027, China); Zhong, Ying (College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, 310027, China); Yan, Senxiang (Department of Radiation Oncology, the First Affiliated Hospital, College of Medicine, Zhejiang University, Hangzhou, Zhejiang, 310003, China); Sun, Hongcheng (Department of General Surgery, Shanghai General Hospital, Shanghai Jiao Tong University, Shanghai, 200080, China); Shen, Haibin (College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, 310027, China); Huang, Kejie (College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, 310027, China)","Huang, Kejie (Zhejiang University)","Zheng, Ruifeng (Zhejiang University); Zhong, Ying (Zhejiang University); Yan, Senxiang (Zhejiang University); Sun, Hongcheng (Shanghai First People's Hospital; Shanghai Jiao Tong University); Shen, Haibin (Zhejiang University); Huang, Kejie (Zhejiang University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1150769701,46 Information and Computing Sciences; 4611 Machine Learning,
5997,pub.1144163062,10.1016/j.compbiomed.2021.105144,34971982,,Domain generalization on medical imaging classification using episodic training with task augmentation,"Medical imaging datasets usually exhibit domain shift due to the variations of scanner vendors, imaging protocols, etc. This raises the concern about the generalization capacity of machine learning models. Domain generalization (DG), which aims to learn a model from multiple source domains such that it can be directly generalized to unseen test domains, seems particularly promising to medical imaging community. To address DG, recent model-agnostic meta-learning (MAML) has been introduced, which transfers the knowledge from previous training tasks to facilitate the learning of novel testing tasks. However, in clinical practice, there are usually only a few annotated source domains available, which decreases the capacity of training task generation and thus increases the risk of overfitting to training tasks in the paradigm. In this paper, we propose a novel DG scheme of episodic training with task augmentation on medical imaging classification. Based on meta-learning, we develop the paradigm of episodic training to construct the knowledge transfer from episodic training-task simulation to the real testing task of DG. Motivated by the limited number of source domains in real-world medical deployment, we consider the unique task-level overfitting and we propose task augmentation to enhance the variety during training task generation to alleviate it. With the established learning framework, we further exploit a novel meta-objective to regularize the deep embedding of training domains. To validate the effectiveness of the proposed method, we perform experiments on histopathological images and abdominal CT images.","The study is supported partly by the National Key Research and Development Program of China (No. 2019YFC0118100), National Natural Science Foundation of China under Grants 82 172 033, 61 971 369, U19B2031, Science and Technology Key Project of Fujian Province (No.2019HZ020009), Fundamental Research Funds for the Central Universities 20 720 200 003, and Tencent Open Fund.",,Computers in Biology and Medicine,,Computer Simulation; Diagnostic Imaging; Machine Learning; Radiography,2021-12-24,2021,2021-12-24,2022-02,141,,105144,All OA; Green,Article,"Li, Chenxin; Lin, Xin; Mao, Yijin; Lin, Wei; Qi, Qi; Ding, Xinghao; Huang, Yue; Liang, Dong; Yu, Yizhou","Li, Chenxin (School of Informatics, Xiamen University, Xiamen, 361005, China.); Lin, Xin (School of Informatics, Xiamen University, Xiamen, 361005, China.); Mao, Yijin (School of Informatics, Xiamen University, Xiamen, 361005, China.); Lin, Wei (School of Informatics, Xiamen University, Xiamen, 361005, China.); Qi, Qi (School of Informatics, Xiamen University, Xiamen, 361005, China.); Ding, Xinghao (School of Informatics, Xiamen University, Xiamen, 361005, China.); Huang, Yue (School of Informatics, Xiamen University, Xiamen, 361005, China. Electronic address: huangyue05@gmail.com.); Liang, Dong (Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China.); Yu, Yizhou (Deepwise AI Laboratory, Beijing, 100125, China.)","Huang, Yue (Xiamen University)","Li, Chenxin (Xiamen University); Lin, Xin (Xiamen University); Mao, Yijin (Xiamen University); Lin, Wei (Xiamen University); Qi, Qi (Xiamen University); Ding, Xinghao (Xiamen University); Huang, Yue (Xiamen University); Liang, Dong (Shenzhen Institutes of Advanced Technology); Yu, Yizhou ()",5,5,,4.12,http://arxiv.org/pdf/2106.06908,https://app.dimensions.ai/details/publication/pub.1144163062,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4611 Machine Learning,
5984,pub.1146343759,10.1007/s11548-022-02590-7,35301702,,Disentangled representation and cross-modality image translation based unsupervised domain adaptation method for abdominal organ segmentation,"PurposeExisting medical image segmentation models tend to achieve satisfactory performance when the training and test data are drawn from the same distribution, while they often produce significant performance degradation when used for the evaluation of cross-modality data. To facilitate the deployment of deep learning models in real-world medical scenarios and to mitigate the performance degradation caused by domain shift, we propose an unsupervised cross-modality segmentation framework based on representation disentanglement and image-to-image translation.MethodsOur approach is based on a multimodal image translation framework, which assumes that the latent space of images can be decomposed into a content space and a style space. First, image representations are decomposed into the content and style codes by the encoders and recombined to generate cross-modality images. Second, we propose content and style reconstruction losses to preserve consistent semantic information from original images and construct content discriminators to match the content distributions between source and target domains. Synthetic images with target domain style and source domain anatomical structures are then utilized for training of the segmentation model.ResultsWe applied our framework to the bidirectional adaptation experiments on MRI and CT images of abdominal organs. Compared to the case without adaptation, the Dice similarity coefficient (DSC) increased by almost 30 and 25% and average symmetric surface distance (ASSD) dropped by 13.3 and 12.2, respectively.ConclusionThe proposed unsupervised domain adaptation framework can effectively improve the performance of cross-modality segmentation, and minimize the negative impact of domain shift. Furthermore, the translated image retains semantic information and anatomical structure. Our method significantly outperforms several competing methods.",This research work is supported by the grants from National Natural Science Foundation of China (61673007). We sincerely thank reviewers for their good advice.,This research work is supported by the grants from National Natural Science Foundation of China (61673007).,International Journal of Computer Assisted Radiology and Surgery,,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Semantics",2022-03-17,2022,2022-03-17,2022-06,17,6,1101-1113,Closed,Article,"Jiang, Kaida; Quan, Li; Gong, Tao","Jiang, Kaida (College of Information Science and Technology, Donghua University, Shanghai, China); Quan, Li (College of Information Science and Technology, Donghua University, Shanghai, China); Gong, Tao (College of Information Science and Technology, Donghua University, Shanghai, China)","Gong, Tao (Donghua University)","Jiang, Kaida (Donghua University); Quan, Li (Donghua University); Gong, Tao (Donghua University)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1146343759,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
5903,pub.1131416950,10.1007/978-3-030-60946-7_2,34113927,PMC8188902,Prediction of Type II Diabetes Onset with Computed Tomography and Electronic Medical Records,"Type II diabetes mellitus (T2DM) is a significant public health concern with multiple known risk factors (e.g., body mass index (BMI), body fat distribution, glucose levels). Improved prediction or prognosis would enable earlier intervention before possibly irreversible damage has occurred. Meanwhile, abdominal computed tomography (CT) is a relatively common imaging technique. Herein, we explore secondary use of the CT imaging data to refine the risk profile of future diagnosis of T2DM. In this work, we delineate quantitative information and imaging slices of patient history to predict onset T2DM retrieved from ICD-9 codes at least one year in the future. Furthermore, we investigate the role of five different types of electronic medical records (EMR), specifically 1) demographics; 2) pancreas volume; 3) visceral/subcutaneous fat volumes in L2 region of interest; 4) abdominal body fat distribution and 5) glucose lab tests in prediction. Next, we build a deep neural network to predict onset T2DM with pancreas imaging slices. Finally, motivated by multi-modal machine learning, we construct a merged framework to combine CT imaging slices with EMR information to refine the prediction. We empirically demonstrate our proposed joint analysis involving images and EMR leads to 4.25% and 6.93% AUC increase in predicting T2DM compared with only using images or EMR. In this study, we used case-control dataset of 997 subjects with CT scans and contextual EMR scores. To the best of our knowledge, this is the first work to show the ability to prognose T2DM using the patients’ contextual and imaging history. We believe this study has promising potential for heterogeneous data analysis and multi-modal medical applications.","This research is supported by Vanderbilt-12Sigma Research Grant, NSF CAREER 1452485, NIH 1R01EB017230 (Landman). This study was in part using the resources of the Advanced Computing Center for Research and Education (ACCRE) at Vanderbilt University, Nashville, TN. The imaging dataset(s) used for the analysis described were obtained from ImageVU, a research resource supported by the VICTR CTSA award (ULTR000445 from NCATS/NIH).",,Lecture Notes in Computer Science,Multimodal Learning for Clinical Decision Support and Clinical Image-Based Procedures,,2020-10-01,2020,2020-10-01,2020,12445,,13-23,All OA; Green,Chapter,"Tang, Yucheng; Gao, Riqiang; Lee, Ho Hin; Wells, Quinn Stanton; Spann, Ashley; Terry, James G.; Carr, John J.; Huo, Yuankai; Bao, Shunxing; Landman, Bennett A.","Tang, Yucheng (Vanderbilt University, Nashville, TN, USA); Gao, Riqiang (Vanderbilt University, Nashville, TN, USA); Lee, Ho Hin (Vanderbilt University, Nashville, TN, USA); Wells, Quinn Stanton (Vanderbilt University Medical Center, Nashville, TN, USA); Spann, Ashley (Vanderbilt University Medical Center, Nashville, TN, USA); Terry, James G. (Vanderbilt University Medical Center, Nashville, TN, USA); Carr, John J. (Vanderbilt University Medical Center, Nashville, TN, USA); Huo, Yuankai (Vanderbilt University, Nashville, TN, USA); Bao, Shunxing (Vanderbilt University, Nashville, TN, USA); Landman, Bennett A. (Vanderbilt University, Nashville, TN, USA; Vanderbilt University Medical Center, Nashville, TN, USA)","Tang, Yucheng (Vanderbilt University)","Tang, Yucheng (Vanderbilt University); Gao, Riqiang (Vanderbilt University); Lee, Ho Hin (Vanderbilt University); Wells, Quinn Stanton (Vanderbilt University Medical Center); Spann, Ashley (Vanderbilt University Medical Center); Terry, James G. (Vanderbilt University Medical Center); Carr, John J. (Vanderbilt University Medical Center); Huo, Yuankai (Vanderbilt University); Bao, Shunxing (Vanderbilt University); Landman, Bennett A. (Vanderbilt University; Vanderbilt University Medical Center)",5,5,0.5,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8188902,https://app.dimensions.ai/details/publication/pub.1131416950,46 Information and Computing Sciences,
5677,pub.1129313062,10.1088/1361-6560/ab9b57,32657281,,Self-paced DenseNet with boundary constraint for automated multi-organ segmentation on abdominal CT images,"Automated multi-organ segmentation on abdominal CT images may replace or complement manual segmentation for clinical applications including image-guided radiation therapy. However, the accuracy of auto-segmentation is challenged by low image contrast, large spatial and inter-patient anatomical variations. In this study, we propose an end-to-end segmentation network, termed self-paced DenseNet, for improved multi-organ segmentation performance, especially for the difficult-to-segment organs. Specifically, a learning-based attention mechanism and dense connection block are seamlessly integrated into the proposed self-paced DenseNet to improve the learning capability and efficiency of the backbone network. To heavily focus on the organs showing low soft-tissue contrast and motion artifacts, a boundary condition is utilized to constrain the network optimization. Additionally, to ease the large learning pace discrepancies of individual organs, a task-wise self-paced-learning strategy is employed to adaptively control the learning paces of individual organs. The proposed self-paced DenseNet was trained and evaluated on a public abdominal CT data set consisting of 90 subjects with manually labeled ground truths of eight organs (including spleen, left kidney, esophagus, gallbladder, stomach, liver, pancreas, and duodenum). For quantitative evaluation, the Dice similarity coefficient (DSC) and average surface distance (ASD) were calculated. An average DSC of 84.46% and ASD of 1.82 mm were achieved on the eight organs, which outperforms the state-of-the-art segmentation methods 2.96% on DSC under the same experimental configuration. Moreover, the proposed segmentation method shows notable improvements on the duodenum and gallbladder, obtaining an average DSC of 69.26% and 80.94% and ASD of 2.14 mm and 2.24 mm, respectively. The results are markedly superior to the average DSC of 63.12% and 76.35% and average ASD of 3.87 mm and 4.33 mm using the vanilla DenseNet, respectively, for the two organs. We demonstrated the effectiveness of the proposed self-paced DenseNet to automatically segment abdominal organs with low boundary conspicuity. The self-paced DenseNet achieved consistently superior segmentation performance on eight abdominal organs with varying segmentation difficulties. The demonstrated computational efficiency (<2 s/CT) makes it well-suited for online applications.","This work was supported in part by the National Natural Science Foundation of Shaanxi Province under Grant No. 2019ZDLGY03-02-02, in part by NIH R44CA183390, R01CA230278 and R01CA188300. N Tong was supported in part by the CSC Chinese Government Scholarship.",,Physics in Medicine and Biology,,"Abdomen; Algorithms; Artifacts; Automation; Humans; Image Processing, Computer-Assisted; Tomography, X-Ray Computed",2020-07-07,2020,2020-07-13,2020-07-07,65,13,135011,Closed,Article,"Tong, Nuo; Gou, Shuiping; Niu, Tianye; Yang, Shuyuan; Sheng, Ke","Tong, Nuo (Key Lab of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi 710071, People’s Republic of China; Department of Radiation Oncology, University of California—Los Angeles, Los Angeles, CA 90095, United States of America); Gou, Shuiping (Key Lab of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi 710071, People’s Republic of China); Niu, Tianye (Department of Radiation Oncology, University of California—Los Angeles, Los Angeles, CA 90095, United States of America); Yang, Shuyuan (Key Lab of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi 710071, People’s Republic of China); Sheng, Ke (Department of Radiation Oncology, University of California—Los Angeles, Los Angeles, CA 90095, United States of America)","Sheng, Ke (University of California, Los Angeles)","Tong, Nuo (Xidian University; University of California, Los Angeles); Gou, Shuiping (Xidian University); Niu, Tianye (University of California, Los Angeles); Yang, Shuyuan (Xidian University); Sheng, Ke (University of California, Los Angeles)",14,13,1.59,10.62,,https://app.dimensions.ai/details/publication/pub.1129313062,51 Physical Sciences; 5105 Medical and Biological Physics,
5660,pub.1133883750,10.1109/tmi.2020.3046692,33351758,,Analyzing Overfitting Under Class Imbalance in Neural Networks for Image Segmentation,"Class imbalance poses a challenge for developing unbiased, accurate predictive models. In particular, in image segmentation neural networks may overfit to the foreground samples from small structures, which are often heavily under-represented in the training set, leading to poor generalization. In this study, we provide new insights on the problem of overfitting under class imbalance by inspecting the network behavior. We find empirically that when training with limited data and strong class imbalance, at test time the distribution of logit activations may shift across the decision boundary, while samples of the well-represented class seem unaffected. This bias leads to a systematic under-segmentation of small structures. This phenomenon is consistently observed for different databases, tasks and network architectures. To tackle this problem, we introduce new asymmetric variants of popular loss functions and regularization techniques including a large margin loss, focal loss, adversarial training, mixup and data augmentation, which are explicitly designed to counter logit shift of the under-represented classes. Extensive experiments are conducted on several challenging segmentation tasks. Our results demonstrate that the proposed modifications to the objective function can lead to significantly improved segmentation accuracy compared to baselines and alternative approaches.","This work was supported by the European Research Council (ERC) through the European Union’s Horizon 2020 Research and Innovation Programme (Project MIRA, ERC-2017-STG) under Agreement 757173. The work of Zeju Li was supported by the China Scholarship Council (CSC). The work of Konstantinos Kamnitsas was supported by the U.K. Research and Innovation (UKRI) London Medical Imaging &amp; Artificial Intelligence Centre for Value-Based Healthcare.",,IEEE Transactions on Medical Imaging,,"Databases, Factual; Image Processing, Computer-Assisted; Neural Networks, Computer",2021-03-02,2021,2021-03-02,2021-03,40,3,1065-1077,All OA; Green,Article,"Li, Zeju; Kamnitsas, Konstantinos; Glocker, Ben","Li, Zeju (BioMedIA Group, Department of Computing, Imperial College London, London, SW7 2AZ, U.K.); Kamnitsas, Konstantinos (BioMedIA Group, Department of Computing, Imperial College London, London, SW7 2AZ, U.K.); Glocker, Ben (BioMedIA Group, Department of Computing, Imperial College London, London, SW7 2AZ, U.K.)","Li, Zeju (Imperial College London)","Li, Zeju (Imperial College London); Kamnitsas, Konstantinos (Imperial College London); Glocker, Ben (Imperial College London)",38,38,3.55,31.1,http://arxiv.org/pdf/2102.10365,https://app.dimensions.ai/details/publication/pub.1133883750,46 Information and Computing Sciences; 4611 Machine Learning,
5659,pub.1155132149,10.1002/mp.16280,36738103,,Surface‐GCN: Learning interaction experience for organ segmentation in 3D medical images,"BACKGROUND: Accurate segmentation of organs has a great significance for clinical diagnosis, but it is still hard work due to the obscure imaging boundaries caused by tissue adhesion on medical images. Based on the image continuity in medical image volumes, segmentation on these slices could be inferred from adjacent slices with a clear organ boundary. Radiologists can delineate a clear organ boundary by observing adjacent slices.
PURPOSE: Inspired by the radiologists' delineating procedure, we design an organ segmentation model based on boundary information of adjacent slices and a human-machine interactive learning strategy to introduce clinical experience.
METHODS: We propose an interactive organ segmentation method for medical image volume based on Graph Convolution Network (GCN) called Surface-GCN. First, we propose a Surface Feature Extraction Network (SFE-Net) to capture surface features of a target organ, and supervise it by a Mini-batch Adaptive Surface Matching (MBASM) module. Then, to predict organ boundaries precisely, we design an automatic segmentation module based on a Surface Convolution Unit (SCU), which propagates information on organ surfaces to refine the generated boundaries. In addition, an interactive segmentation module is proposed to learn radiologists' experience of interactive corrections on organ surfaces to reduce interaction clicks.
RESULTS: We evaluate the proposed method on one prostate MR image dataset and two abdominal multi-organ CT datasets. The experimental results show that our method outperforms other state-of-the-art methods. For prostate segmentation, the proposed method conducts a DSC score of 94.49% on PROMISE12 test dataset. For abdominal multi-organ segmentation, the proposed method achieves DSC scores of 95, 91, 95, and 88% for the left kidney, gallbladder, spleen, and esophagus, respectively. For interactive segmentation, the proposed method reduces 5-10 interaction clicks to reach the same accuracy.
CONCLUSIONS: To overcome the medical organ segmentation challenge, we propose a Graph Convolutional Network called Surface-GCN by imitating radiologist interactions and learning clinical experience. On single and multiple organ segmentation tasks, the proposed method could obtain more accurate segmentation boundaries compared with other state-of-the-art methods.","This work was supported by the National Natural Science Foundation of China under Grant No. 62173269, the Key Research and Development Program of Shaanxi Province of China under Grant No. 2020GXLH‐Y‐008, the Natural Science Basic Research Plan in Shaanxi Province of China under Grant No. 2022JM‐324, and the Shaanxi Provincial Social Science Fund under Grant No. 2021K014.",,Medical Physics,,,2023-02-03,2023,2023-02-10,2023-02-03,,,,Closed,Article,"Tian, Fengrui; Tian, Zhiqiang; Chen, Zhang; Zhang, Dong; Du, Shaoyi","Tian, Fengrui (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China); Tian, Zhiqiang (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China); Chen, Zhang (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China); Zhang, Dong (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China; School of Automation Science and Engineering, Xi'an Jiaotong University, Xi'an, China); Du, Shaoyi (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China)","Tian, Zhiqiang (Xi'an Jiaotong University); Du, Shaoyi (Xi'an Jiaotong University)","Tian, Fengrui (Xi'an Jiaotong University; Xi'an Jiaotong University); Tian, Zhiqiang (Xi'an Jiaotong University); Chen, Zhang (Xi'an Jiaotong University); Zhang, Dong (Xi'an Jiaotong University; Xi'an Jiaotong University); Du, Shaoyi (Xi'an Jiaotong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155132149,51 Physical Sciences; 5105 Medical and Biological Physics,
5648,pub.1140450173,10.1109/tmi.2021.3105046,34398751,,Deep Symmetric Adaptation Network for Cross-Modality Medical Image Segmentation,"Unsupervised domain adaptation (UDA) methods have shown their promising performance in the cross-modality medical image segmentation tasks. These typical methods usually utilize a translation network to transform images from the source domain to target domain or train the pixel-level classifier merely using translated source images and original target images. However, when there exists a large domain shift between source and target domains, we argue that this asymmetric structure, to some extent, could not fully eliminate the domain gap. In this paper, we present a novel deep symmetric architecture of UDA for medical image segmentation, which consists of a segmentation sub-network, and two symmetric source and target domain translation sub-networks. To be specific, based on two translation sub-networks, we introduce a bidirectional alignment scheme via a shared encoder and two private decoders to simultaneously align features 1) from source to target domain and 2) from target to source domain, which is able to effectively mitigate the discrepancy between domains. Furthermore, for the segmentation sub-network, we train a pixel-level classifier using not only original target images and translated source images, but also original source images and translated target images, which could sufficiently leverage the semantic information from the images with different styles. Extensive experiments demonstrate that our method has remarkable advantages compared to the state-of-the-art methods in three segmentation tasks, such as cross-modality cardiac, BraTS, and abdominal multi-organ segmentation.","This work was supported in part by the National Key Research and Development Plan under Grant 2019YFC0118300 and in part by the China Postdoctoral Science Foundation under Project 2021M690609. The authors would like to thank Zhentao Chen (Intel) for the help and comments, Prof. Qi Dou (CUHK), and Prof. Pingkun Yan (RPI) for providing the data source.",,IEEE Transactions on Medical Imaging,,"Heart; Image Processing, Computer-Assisted; Semantics",2021-12-30,2021,2021-12-30,2022-01,41,1,121-132,All OA; Green,Article,"Han, Xiaoting; Qi, Lei; Yu, Qian; Zhou, Ziqi; Zheng, Yefeng; Shi, Yinghuan; Gao, Yang","Han, Xiaoting (State Key Laboratory for Novel Software Technology, Collaborative Innovation Center of Novel Software Technology and Industrialization, National Institute for Healthcare Data Science, Nanjing University, Nanjing, 210046, China); Qi, Lei (Key Laboratory of Computer Network and Information Integration (Ministry of Education), School of Computer Science and Engineering, Southeast University, Nanjing, 211189, China); Yu, Qian (School of Data and Computer Science, Shandong Women’s University, Jinan, 250300, China); Zhou, Ziqi (State Key Laboratory for Novel Software Technology, Collaborative Innovation Center of Novel Software Technology and Industrialization, National Institute for Healthcare Data Science, Nanjing University, Nanjing, 210046, China); Zheng, Yefeng (Tencent Jarvis Lab, Shenzhen, 518057, China); Shi, Yinghuan (State Key Laboratory for Novel Software Technology, Collaborative Innovation Center of Novel Software Technology and Industrialization, National Institute for Healthcare Data Science, Nanjing University, Nanjing, 210046, China); Gao, Yang (State Key Laboratory for Novel Software Technology, Collaborative Innovation Center of Novel Software Technology and Industrialization, National Institute for Healthcare Data Science, Nanjing University, Nanjing, 210046, China)","Shi, Yinghuan (Nanjing University)","Han, Xiaoting (Nanjing University); Qi, Lei (Southeast University); Yu, Qian (Shandong Women’s University); Zhou, Ziqi (Nanjing University); Zheng, Yefeng (); Shi, Yinghuan (Nanjing University); Gao, Yang (Nanjing University)",13,13,2.51,10.29,http://arxiv.org/pdf/2101.06853,https://app.dimensions.ai/details/publication/pub.1140450173,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4611 Machine Learning,
5347,pub.1139548365,10.1016/j.media.2021.102156,34274689,,Incorporating the hybrid deformable model for improving the performance of abdominal CT segmentation via multi-scale feature fusion network,"Automated multi-organ abdominal Computed Tomography (CT) image segmentation can assist the treatment planning, diagnosis, and improve many clinical workflows' efficiency. The 3-D Convolutional Neural Network (CNN) recently attained state-of-the-art accuracy, which typically relies on supervised training with many manual annotated data. Many methods used the data augmentation strategy with a rigid or affine spatial transformation to alleviate the over-fitting problem and improve the network's robustness. However, the rigid or affine spatial transformation fails to capture the complex voxel-based deformation in the abdomen, filled with many soft organs. We developed a novel Hybrid Deformable Model (HDM), which consists of the inter-and intra-patient deformation for more effective data augmentation to tackle this issue. The inter-patient deformations were extracted from the learning-based deformable registration between different patients, while the intra-patient deformations were formed using the random 3-D Thin-Plate-Spline (TPS) transformation. Incorporating the HDM enabled the network to capture many of the subtle deformations of abdominal organs. To find a better solution and achieve faster convergence for network training, we fused the pre-trained multi-scale features into the a 3-D attention U-Net. We directly compared the segmentation accuracy of the proposed method to the previous techniques on several centers' datasets via cross-validation. The proposed method achieves the average Dice Similarity Coefficient (DSC) 0.852, which outperformed the other state-of-the-art on multi-organ abdominal CT segmentation results.","This work is supported in part by grants from the National Key Research and Develop Program of China (2016YFC0105102, 2018YFA0704102), the National Natural Science Foundation of China (62001464, 61871374, 81871433, 62073309, 81827805), the Leading Talent of Special Support Project in Guangdong (2016TX03R139), Shenzhen matching project (GJHS20170314155751703), Fundamental Research Program of Shenzhen (JCYJ20170413162458312), Science Foundation of Guangdong (2017B020229002, 2015B020233011), Basic Research Layout Project of Shenzhen (JCYJ20200109114610201, JCYJ20200109114812361), Project of Shenzhen International Cooperation Foundation (GJHZ20180926165402083) and Shenzhen Interventional Diagnosis and Treatment Integration Key Technology and Engineering Laboratory. CAS Key Laboratory of Health Informatics, Shenzhen Institutes of Advanced Technology, the Key Laboratory of Health Informatics in Chinese Academy of Sciences.",,Medical Image Analysis,,"Abdomen; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Tomography, X-Ray Computed",2021-07-09,2021,2021-07-09,2021-10,73,,102156,All OA; Bronze,Article,"Liang, Xiaokun; Li, Na; Zhang, Zhicheng; Xiong, Jing; Zhou, Shoujun; Xie, Yaoqin","Liang, Xiaokun (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong 518055, China; Shenzhen Colleges of Advanced Technology, University of Chinese Academy of Sciences, Shenzhen, Guangdong 518055, China. Electronic address: xk.liang@qq.com.); Li, Na (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong 518055, China; Shenzhen Colleges of Advanced Technology, University of Chinese Academy of Sciences, Shenzhen, Guangdong 518055, China.); Zhang, Zhicheng (Department of Radiation Oncology, Stanford University, Stanford, CA 94305, USA.); Xiong, Jing (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong 518055, China.); Zhou, Shoujun (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong 518055, China. Electronic address: sj.zhou@siat.ac.cn.); Xie, Yaoqin (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong 518055, China. Electronic address: yq.xie@siat.ac.cn.)","Liang, Xiaokun (Shenzhen Institutes of Advanced Technology; University of Chinese Academy of Sciences)","Liang, Xiaokun (Shenzhen Institutes of Advanced Technology; University of Chinese Academy of Sciences); Li, Na (Shenzhen Institutes of Advanced Technology; University of Chinese Academy of Sciences); Zhang, Zhicheng (Stanford University); Xiong, Jing (Shenzhen Institutes of Advanced Technology); Zhou, Shoujun (Shenzhen Institutes of Advanced Technology); Xie, Yaoqin (Shenzhen Institutes of Advanced Technology)",17,17,1.92,14.91,https://www.sciencedirect.com/science/article/am/pii/S1361841521002024,https://app.dimensions.ai/details/publication/pub.1139548365,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
5342,pub.1145408181,10.1109/tmi.2022.3150682,35139014,,Self-Supervised Learning for Few-Shot Medical Image Segmentation,"Fully-supervised deep learning segmentation models are inflexible when encountering new unseen semantic classes and their fine-tuning often requires significant amounts of annotated data. Few-shot semantic segmentation (FSS) aims to solve this inflexibility by learning to segment an arbitrary unseen semantically meaningful class by referring to only a few labeled examples, without involving fine-tuning. State-of-the-art FSS methods are typically designed for segmenting natural images and rely on abundant annotated data of training classes to learn image representations that generalize well to unseen testing classes. However, such a training mechanism is impractical in annotation-scarce medical imaging scenarios. To address this challenge, in this work, we propose a novel self-supervised FSS framework for medical images, named SSL-ALPNet, in order to bypass the requirement for annotations during training. The proposed method exploits superpixel-based pseudo-labels to provide supervision signals. In addition, we propose a simple yet effective adaptive local prototype pooling module which is plugged into the prototype networks to further boost segmentation accuracy. We demonstrate the general applicability of the proposed approach using three different tasks: organ segmentation of abdominal CT and MRI images respectively, and cardiac segmentation of MRI images. The proposed method yields higher Dice scores than conventional FSS methods which require manual annotations for training in our experiments.",,This work was supported in part by the Engineering and Physical Sciences Research Council (EPSRC) Program under Grant EP/P001009/1 (SmartHeart) and in part by the Innovate U.K. under Grant 104691 (London Medical Imaging and Artificial Intelligence Centre for Value-Based Healthcare).,IEEE Transactions on Medical Imaging,,"Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Supervised Machine Learning",2022-06-30,2022,2022-06-30,2022-07,41,7,1837-1848,All OA; Green,Article,"Ouyang, Cheng; Biffi, Carlo; Chen, Chen; Kart, Turkay; Qiu, Huaqi; Rueckert, Daniel","Ouyang, Cheng (Department of Computing, Imperial College London, London, SW7 2RH, U.K.); Biffi, Carlo (Department of Computing, Imperial College London, London, SW7 2RH, U.K.); Chen, Chen (Department of Computing, Imperial College London, London, SW7 2RH, U.K.); Kart, Turkay (Department of Computing, Imperial College London, London, SW7 2RH, U.K.); Qiu, Huaqi (Department of Computing, Imperial College London, London, SW7 2RH, U.K.); Rueckert, Daniel (Department of Computing, Imperial College London, London, SW7 2RH, U.K.; Institute for AI and Informatics in Medicine, Klinikum rechts der Isar, Technical University of Munich, 81675, Münich, Germany)","Ouyang, Cheng (Imperial College London)","Ouyang, Cheng (Imperial College London); Biffi, Carlo (Imperial College London); Chen, Chen (Imperial College London); Kart, Turkay (Imperial College London); Qiu, Huaqi (Imperial College London); Rueckert, Daniel (Imperial College London; Technical University of Munich; Rechts der Isar Hospital)",8,8,,,http://spiral.imperial.ac.uk/bitstream/10044/1/95289/2/few_shot_segmentation_no_template.pdf,https://app.dimensions.ai/details/publication/pub.1145408181,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4605 Data Management and Data Science; 4611 Machine Learning,
5342,pub.1138505272,10.1038/s41598-021-90294-4,34075061,PMC8169882,Domain adaptation for segmentation of critical structures for prostate cancer therapy,"Preoperative assessment of the proximity of critical structures to the tumors is crucial in avoiding unnecessary damage during prostate cancer treatment. A patient-specific 3D anatomical model of those structures, namely the neurovascular bundles (NVB) and the external urethral sphincters (EUS), can enable physicians to perform such assessments intuitively. As a crucial step to generate
 a patient-specific anatomical model from preoperative MRI in a clinical routine, we propose a multi-class automatic segmentation based on an anisotropic convolutional network. Our specific challenge is to train the network model on a unique source dataset only available at a single clinical site and deploy it to another target site without sharing the original images or labels. As network models trained on data from a single source suffer from quality loss due to the domain shift, we propose a semi-supervised domain adaptation (DA) method to refine the model’s performance in the target domain. Our DA method combines transfer learning and uncertainty guided self-learning based on deep ensembles. Experiments on the segmentation of the prostate, NVB, and EUS, show significant performance gain with the combination of those techniques compared to pure TL and the combination of TL with simple self-learning (p<0.005\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}<0.005$$\end{document} for all structures using a Wilcoxon’s signed-rank test). Results on a different task and data (Pancreas CT segmentation) demonstrate our method’s generic application capabilities. Our method has the advantage that it does not require any further data from the source domain, unlike the majority of recent domain adaptation strategies. This makes our method suitable for clinical applications, where the sharing of patient data is restricted.","The study was funded in part by the U.S. National Institutes of Health (R01EB020667, R01CA235134, P41EB015898, P41EB028741), and the EU and the federal state of Saxony-Anhalt, Germany (ZS/2016/08/80388). The content of the material is solely the responsibility of the authors and does not necessarily represent the official views of these agencies. The Titan Xp used for this research was donated by the NVIDIA Corporation.",Open Access funding enabled and organized by Projekt DEAL.,Scientific Reports,,"Humans; Male; Neural Networks, Computer; Prostate; Prostatic Neoplasms; Tomography, X-Ray Computed",2021-06-01,2021,2021-06-01,,11,1,11480,All OA; Gold,Article,"Meyer, Anneke; Mehrtash, Alireza; Rak, Marko; Bashkanov, Oleksii; Langbein, Bjoern; Ziaei, Alireza; Kibel, Adam S.; Tempany, Clare M.; Hansen, Christian; Tokuda, Junichi","Meyer, Anneke (Department of Simulation and Graphics and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany); Mehrtash, Alireza (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA); Rak, Marko (Department of Simulation and Graphics and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany); Bashkanov, Oleksii (Department of Simulation and Graphics and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany); Langbein, Bjoern (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA); Ziaei, Alireza (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA); Kibel, Adam S. (Division of Urology, Department of Surgery, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA); Tempany, Clare M. (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA); Hansen, Christian (Department of Simulation and Graphics and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany); Tokuda, Junichi (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA)","Meyer, Anneke (Otto-von-Guericke University Magdeburg)","Meyer, Anneke (Otto-von-Guericke University Magdeburg); Mehrtash, Alireza (Brigham and Women's Hospital; Harvard University); Rak, Marko (Otto-von-Guericke University Magdeburg); Bashkanov, Oleksii (Otto-von-Guericke University Magdeburg); Langbein, Bjoern (Brigham and Women's Hospital; Harvard University); Ziaei, Alireza (Brigham and Women's Hospital; Harvard University); Kibel, Adam S. (Brigham and Women's Hospital; Harvard University); Tempany, Clare M. (Brigham and Women's Hospital; Harvard University); Hansen, Christian (Otto-von-Guericke University Magdeburg); Tokuda, Junichi (Brigham and Women's Hospital; Harvard University)",2,2,0.41,1.51,https://www.nature.com/articles/s41598-021-90294-4.pdf,https://app.dimensions.ai/details/publication/pub.1138505272,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences; 3211 Oncology and Carcinogenesis; 46 Information and Computing Sciences; 4611 Machine Learning,
5337,pub.1128801387,10.1016/j.media.2020.101766,32623276,,Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation,"Although having achieved great success in medical image segmentation, deep learning-based approaches usually require large amounts of well-annotated data, which can be extremely expensive in the field of medical image analysis. Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised learning and unsupervised domain adaptation both take the advantage of unlabeled data, and they are closely related to each other. In this paper, we propose uncertainty-aware multi-view co-training (UMCT), a unified framework that addresses these two tasks for volumetric medical image segmentation. Our framework is capable of efficiently utilizing unlabeled data for better performance. We firstly rotate and permute the 3D volumes into multiple views and train a 3D deep network on each view. We then apply co-training by enforcing multi-view consistency on unlabeled data, where an uncertainty estimation of each view is utilized to achieve accurate labeling. Experiments on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset show state-of-the-art performance of the proposed framework on semi-supervised medical image segmentation. Under unsupervised domain adaptation settings, we validate the effectiveness of this work by adapting our multi-organ segmentation model to two pathological organs from the Medical Segmentation Decathlon Datasets. Additionally, we show that our UMCT-DA model can even effectively handle the challenging situation where labeled source data is inaccessible, demonstrating strong potentials for real-world applications.",,,Medical Image Analysis,,Humans; Supervised Machine Learning; Uncertainty,2020-06-27,2020,2020-06-27,2020-10,65,,101766,All OA; Green,Article,"Xia, Yingda; Yang, Dong; Yu, Zhiding; Liu, Fengze; Cai, Jinzheng; Yu, Lequan; Zhu, Zhuotun; Xu, Daguang; Yuille, Alan; Roth, Holger","Xia, Yingda (Johns Hopkins Unversity, Baltimore, MD, 21218, USA.); Yang, Dong (NVIDIA Corporation, Bethesda, MD, 20814, USA.); Yu, Zhiding (NVIDIA Corporation, Bethesda, MD, 20814, USA.); Liu, Fengze (Johns Hopkins Unversity, Baltimore, MD, 21218, USA.); Cai, Jinzheng (University of Florida, Gainesville, FL, 32611, USA.); Yu, Lequan (The Chinese University of Hong Kong, Hong Kong, China.); Zhu, Zhuotun (Johns Hopkins Unversity, Baltimore, MD, 21218, USA.); Xu, Daguang (NVIDIA Corporation, Bethesda, MD, 20814, USA.); Yuille, Alan (Johns Hopkins Unversity, Baltimore, MD, 21218, USA.); Roth, Holger (NVIDIA Corporation, Bethesda, MD, 20814, USA. Electronic address: hroth@nvidia.com.)","Roth, Holger (Nvidia (United States))","Xia, Yingda (); Yang, Dong (Nvidia (United States)); Yu, Zhiding (Nvidia (United States)); Liu, Fengze (); Cai, Jinzheng (University of Florida); Yu, Lequan (Chinese University of Hong Kong); Zhu, Zhuotun (); Xu, Daguang (Nvidia (United States)); Yuille, Alan (); Roth, Holger (Nvidia (United States))",82,81,5.04,,http://arxiv.org/pdf/2006.16806,https://app.dimensions.ai/details/publication/pub.1128801387,32 Biomedical and Clinical Sciences; 40 Engineering,
5334,pub.1129610687,10.1109/tmi.2020.3011626,32746148,PMC7757913,PSIGAN: Joint Probabilistic Segmentation and Image Distribution Matching for Unpaired Cross-Modality Adaptation-Based MRI Segmentation,"We developed a new joint probabilistic segmentation and image distribution matching generative adversarial network (PSIGAN) for unsupervised domain adaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI) images. Our UDA approach models the co-dependency between images and their segmentation as a joint probability distribution using a new structure discriminator. The structure discriminator computes structure of interest focused adversarial loss by combining the generated pseudo MRI with probabilistic segmentations produced by a simultaneously trained segmentation sub-network. The segmentation sub-network is trained using the pseudo MRI produced by the generator sub-network. This leads to a cyclical optimization of both the generator and segmentation sub-networks that are jointly trained as part of an end-to-end network. Extensive experiments and comparisons against multiple state-of-the-art methods were done on four different MRI sequences totalling 257 scans for generating multi-organ and tumor segmentation. The experiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20 T2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right kidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for parotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our method achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the abdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for lung tumors.",Thisworkwas supported by the MSK Cancer Center under Grant P30 CA008748.,,IEEE Transactions on Medical Imaging,,"Humans; Image Processing, Computer-Assisted; Lung Neoplasms; Magnetic Resonance Imaging; Spleen",2020-11-30,2020,2020-11-30,2020-12,39,12,4071-4084,All OA; Green,Article,"Jiang, Jue; Hu, Yu-Chi; Tyagi, Neelam; Rimner, Andreas; Lee, Nancy; Deasy, Joseph O.; Berry, Sean; Veeraraghavan, Harini","Jiang, Jue (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA); Hu, Yu-Chi (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA); Tyagi, Neelam (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA); Rimner, Andreas (Department of Radiation Oncology, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA); Lee, Nancy (Department of Radiation Oncology, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA); Deasy, Joseph O. (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA); Berry, Sean (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA); Veeraraghavan, Harini (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA)","Veeraraghavan, Harini (Memorial Sloan Kettering Cancer Center)","Jiang, Jue (Memorial Sloan Kettering Cancer Center); Hu, Yu-Chi (Memorial Sloan Kettering Cancer Center); Tyagi, Neelam (Memorial Sloan Kettering Cancer Center); Rimner, Andreas (Memorial Sloan Kettering Cancer Center); Lee, Nancy (Memorial Sloan Kettering Cancer Center); Deasy, Joseph O. (Memorial Sloan Kettering Cancer Center); Berry, Sean (Memorial Sloan Kettering Cancer Center); Veeraraghavan, Harini (Memorial Sloan Kettering Cancer Center)",17,16,1.34,8.76,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7757913,https://app.dimensions.ai/details/publication/pub.1129610687,46 Information and Computing Sciences; 4611 Machine Learning,
5328,pub.1148072911,10.1002/mp.15765,35598077,PMC9908007,Nested block self‐attention multiple resolution residual network for multiorgan segmentation from CT,"BACKGROUND: Fast and accurate multiorgans segmentation from computed tomography (CT) scans is essential for radiation treatment planning. Self-attention(SA)-based deep learning methodologies provide higher accuracies than standard methods but require memory and computationally intensive calculations, which restricts their use to relatively shallow networks.
PURPOSE: Our goal was to develop and test a new computationally fast and memory-efficient bidirectional SA method called nested block self-attention (NBSA), which is applicable to shallow and deep multiorgan segmentation networks.
METHODS: A new multiorgan segmentation method combining a deep multiple resolution residual network with computationally efficient SA called nested block SA (MRRN-NBSA) was developed and evaluated to segment 18 different organs from head and neck (HN) and abdomen organs. MRRN-NBSA combines features from multiple image resolutions and feature levels with SA to extract organ-specific contextual features. Computational efficiency is achieved by using memory blocks of fixed spatial extent for SA calculation combined with bidirectional attention flow. Separate models were trained for HN (n = 238) and abdomen (n = 30) and tested on set aside open-source grand challenge data sets for HN (n = 10) using a public domain database of computational anatomy and blinded testing on 20 cases from Beyond the Cranial Vault data set with overall accuracy provided by the grand challenge website for abdominal organs. Robustness to two-rater segmentations was also evaluated for HN cases using the open-source data set. Statistical comparison of MRRN-NBSA against Unet, convolutional network-based SA using criss-cross attention (CCA), dual SA, and transformer-based (UNETR) methods was done by measuring the differences in the average Dice similarity coefficient (DSC) accuracy for all HN organs using the Kruskall-Wallis test, followed by individual method comparisons using paired, two-sided Wilcoxon-signed rank tests at 95% confidence level with Bonferroni correction used for multiple comparisons.
RESULTS: MRRN-NBSA produced an average high DSC of 0.88 for HN and 0.86 for the abdomen that exceeded current methods. MRRN-NBSA was more accurate than the computationally most efficient CCA (average DSC of 0.845 for HN, 0.727 for abdomen). Kruskal-Wallis test showed significant difference between evaluated methods (p=0.00025). Pair-wise comparisons showed significant differences between MRRN-NBSA than Unet (p=0.0003), CCA (p=0.030), dual (p=0.038), and UNETR methods (p=0.012) after Bonferroni correction. MRRN-NBSA produced less variable segmentations for submandibular glands (0.82 ± 0.06) compared to two raters (0.75 ± 0.31).
CONCLUSIONS: MRRN-NBSA produced more accurate multiorgan segmentations than current methods on two different public data sets. Testing on larger institutional cohorts is required to establish feasibility for clinical use.",This work was supported by the Memorial Sloan Kettering (MSK) Cancer Center Support Grant/Core Grant NCI P30 CA008748.,,Medical Physics,,"Abdomen; Attention; Head; Image Processing, Computer-Assisted; Tomography, X-Ray Computed",2022-06-08,2022,2022-06-08,2022-08,49,8,5244-5257,Closed,Article,"Jiang, Jue; Elguindi, Sharif; Berry, Sean L.; Onochie, Ifeanyirochukwu; Cervino, Laura; Deasy, Joseph O.; Veeraraghavan, Harini","Jiang, Jue (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA); Elguindi, Sharif (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA); Berry, Sean L. (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA); Onochie, Ifeanyirochukwu (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA); Cervino, Laura (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA); Deasy, Joseph O. (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA); Veeraraghavan, Harini (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, USA)","Veeraraghavan, Harini (Memorial Sloan Kettering Cancer Center)","Jiang, Jue (Memorial Sloan Kettering Cancer Center); Elguindi, Sharif (Memorial Sloan Kettering Cancer Center); Berry, Sean L. (Memorial Sloan Kettering Cancer Center); Onochie, Ifeanyirochukwu (Memorial Sloan Kettering Cancer Center); Cervino, Laura (Memorial Sloan Kettering Cancer Center); Deasy, Joseph O. (Memorial Sloan Kettering Cancer Center); Veeraraghavan, Harini (Memorial Sloan Kettering Cancer Center)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1148072911,51 Physical Sciences; 5105 Medical and Biological Physics,
5061,pub.1153241598,10.1109/tmi.2022.3225667,36449590,,BowelNet: Joint Semantic-Geometric Ensemble Learning for Bowel Segmentation from Both Partially and Fully Labeled CT Images,"Accurate bowel segmentation is essential for diagnosis and treatment of bowel cancers. Unfortunately, segmenting the entire bowel in CT images is quite challenging due to unclear boundary, large shape, size, and appearance variations, as well as diverse filling status within the bowel. In this paper, we present a novel two-stage framework, named BowelNet, to handle the challenging task of bowel segmentation in CT images, with two stages of 1) jointly localizing all types of the bowel, and 2) finely segmenting each type of the bowel. Specifically, in the first stage, we learn a unified localization network from both partially- and fully-labeled CT images to robustly detect all types of the bowel. To better capture unclear bowel boundary and learn complex bowel shapes, in the second stage, we propose to jointly learn semantic information (i.e., bowel segmentation mask) and geometric representations (i.e., bowel boundary and bowel skeleton) for fine bowel segmentation in a multi-task learning scheme. Moreover, we further propose to learn a meta segmentation network via pseudo labels to improve segmentation accuracy. By evaluating on a large abdominal CT dataset, our proposed BowelNet method can achieve Dice scores of 0.764, 0.848, 0.835, 0.774, and 0.824 in segmenting the duodenum, jejunum-ileum, colon, sigmoid, and rectum, respectively. These results demonstrate the effectiveness of our proposed BowelNet framework in segmenting the entire bowel from CT images.",,,IEEE Transactions on Medical Imaging,,,2022-11-30,2022,2022-11-30,2022-11-30,PP,99,1-1,Closed,Article,"Wang, Chong; Cui, Zhiming; Yang, Junwei; Han, Miaofei; Carneiro, Gustavo; Shen, Dinggang","Wang, Chong (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Cui, Zhiming (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Yang, Junwei (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Han, Miaofei (Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China); Carneiro, Gustavo (Australian Institute for Machine Learning, University of Adelaide, SA, Australia); Shen, Dinggang (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China)",,"Wang, Chong (ShanghaiTech University); Cui, Zhiming (ShanghaiTech University); Yang, Junwei (ShanghaiTech University); Han, Miaofei (); Carneiro, Gustavo (University of Adelaide); Shen, Dinggang (ShanghaiTech University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153241598,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
5061,pub.1150990474,10.1016/j.media.2022.102616,36179380,,Fast and Low-GPU-memory abdomen CT organ segmentation: The FLARE challenge,"Automatic segmentation of abdominal organs in CT scans plays an important role in clinical practice. However, most existing benchmarks and datasets only focus on segmentation accuracy, while the model efficiency and its accuracy on the testing cases from different medical centers have not been evaluated. To comprehensively benchmark abdominal organ segmentation methods, we organized the first Fast and Low GPU memory Abdominal oRgan sEgmentation (FLARE) challenge, where the segmentation methods were encouraged to achieve high accuracy on the testing cases from different medical centers, fast inference speed, and low GPU memory consumption, simultaneously. The winning method surpassed the existing state-of-the-art method, achieving a 19× faster inference speed and reducing the GPU memory consumption by 60% with comparable accuracy. We provide a summary of the top methods, make their code and Docker containers publicly available, and give practical suggestions on building accurate and efficient abdominal organ segmentation models. The FLARE challenge remains open for future submissions through a live platform for benchmarking further methodology developments at https://flare.grand-challenge.org/.","This project is supported by China’s Ministry of Science and Technology (No. 2020YFA0713800) and National Natural Science Foundation of China (No. 11971229, No. 12090023). The authors would like to thank NVIDIA for supporting the evaluation platform.",,Medical Image Analysis,,"Humans; Algorithms; Tomography, X-Ray Computed; Abdomen; Benchmarking; Image Processing, Computer-Assisted",2022-09-13,2022,2022-09-13,2022-11,82,,102616,Closed,Article,"Ma, Jun; Zhang, Yao; Gu, Song; An, Xingle; Wang, Zhihe; Ge, Cheng; Wang, Congcong; Zhang, Fan; Wang, Yu; Xu, Yinan; Gou, Shuiping; Thaler, Franz; Payer, Christian; Štern, Darko; Henderson, Edward G A; McSweeney, Dónal M; Green, Andrew; Jackson, Price; McIntosh, Lachlan; Nguyen, Quoc-Cuong; Qayyum, Abdul; Conze, Pierre-Henri; Huang, Ziyan; Zhou, Ziqi; Fan, Deng-Ping; Xiong, Huan; Dong, Guoqiang; Zhu, Qiongjie; He, Jian; Yang, Xiaoping","Ma, Jun (Department of Mathematics, Nanjing University of Science and Technology, 210094, Nanjing, China.); Zhang, Yao (Institute of Computing Technology, Chinese Academy of Sciences and the University of Chinese Academy of Sciences, 100019, Beijing, China.); Gu, Song (Department of Image Reconstruction, Nanjing Anke Medical Technology Co., Ltd., 211113, Nanjing, China.); An, Xingle (Infervision Technology Co. Ltd., 100020, Beijing, China.); Wang, Zhihe (Shenzhen Haichuang Medical Co., Ltd., 518049, Shenzhen, China.); Ge, Cheng (Institute of Bioinformatics and Medical Engineering, Jiangsu University of Technology, 213001, Changzhou, China.); Wang, Congcong (School of Computer Science and Engineering, Tianjin University of Technology, 300384, Tianjin, China; Engineering Research Center of Learning-Based Intelligent System, Ministry of Education, 300384, Tianjin, China.); Zhang, Fan (Radiological Algorithm, Fosun Aitrox Information Technology Co., Ltd., 200033, Shanghai, China.); Wang, Yu (Radiological Algorithm, Fosun Aitrox Information Technology Co., Ltd., 200033, Shanghai, China.); Xu, Yinan (Key Lab of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, 710071, Shaanxi, China.); Gou, Shuiping (Key Lab of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, 710071, Shaanxi, China.); Thaler, Franz (Gottfried Schatz Research Center: Biophysics, Medical University of Graz, 8010, Graz, Austria; Institute of Computer Graphics and Vision, Graz University of Technology, 8010, Graz, Austria.); Payer, Christian (Institute of Computer Graphics and Vision, Graz University of Technology, 8010, Graz, Austria.); Štern, Darko (Gottfried Schatz Research Center: Biophysics, Medical University of Graz, 8010, Graz, Austria.); Henderson, Edward G A (Division of Cancer Sciences, The University of Manchester, M139PL, Manchester, UK; Radiotherapy Related Research, The Christie NHS Foundation Trust, M139PL, Manchester, UK.); McSweeney, Dónal M (Division of Cancer Sciences, The University of Manchester, M139PL, Manchester, UK; Radiotherapy Related Research, The Christie NHS Foundation Trust, M139PL, Manchester, UK.); Green, Andrew (Division of Cancer Sciences, The University of Manchester, M139PL, Manchester, UK; Radiotherapy Related Research, The Christie NHS Foundation Trust, M139PL, Manchester, UK.); Jackson, Price (Peter MacCallum Cancer Centre, 3000, Melbourne, Australia.); McIntosh, Lachlan (Peter MacCallum Cancer Centre, 3000, Melbourne, Australia.); Nguyen, Quoc-Cuong (University of Information Technology, VNU-HCM, 700000, Ho Chi Minh City, Viet Nam.); Qayyum, Abdul (Brest National School of Engineering, UMR CNRS 6285 LabSTICC, 29280, Brest, France.); Conze, Pierre-Henri (IMT Atlantique, LaTIM UMR 1101, Inserm, 29238, Brest, France.); Huang, Ziyan (Institute of Medical Robotics, Shanghai Jiao Tong University, 200240, Shanghai, China.); Zhou, Ziqi (Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, 518000, Shenzhen, China.); Fan, Deng-Ping (College of Computer Science, Nankai University, 300071, Tianjin, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates.); Xiong, Huan (Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Harbin Institute of Technology, 150001, Harbin, China.); Dong, Guoqiang (Department of Nuclear Medicine, Nanjing Drum Tower Hospital, the Affiliated Hospital of Nanjing University Medical School, 210008, Nanjing, China; Department of Interventional Radiology, The Second Affiliated Hospital of Bengbu Medical College, 233017, Bengbu, China.); Zhu, Qiongjie (Department of Nuclear Medicine, Nanjing Drum Tower Hospital, the Affiliated Hospital of Nanjing University Medical School, 210008, Nanjing, China; Department of Radiology, Shidong Hospital, 200438, Shanghai, China.); He, Jian (Department of Nuclear Medicine, Nanjing Drum Tower Hospital, the Affiliated Hospital of Nanjing University Medical School, 210008, Nanjing, China.); Yang, Xiaoping (Department of Mathematics, Nanjing University, 210093, Nanjing, China. Electronic address: xpyang@nju.edu.cn.)","Yang, Xiaoping (Nanjing University)","Ma, Jun (Nanjing University of Science and Technology); Zhang, Yao (Institute of Computing Technology); Gu, Song (); An, Xingle (); Wang, Zhihe (); Ge, Cheng (Jiangsu University of Technology); Wang, Congcong (Tianjin University of Technology; Ministry of Education of the People's Republic of China); Zhang, Fan (); Wang, Yu (); Xu, Yinan (Xidian University); Gou, Shuiping (Xidian University); Thaler, Franz (Medical University of Graz; Graz University of Technology); Payer, Christian (Graz University of Technology); Štern, Darko (Medical University of Graz); Henderson, Edward G A (University of Manchester; Christie Hospital NHS Foundation Trust); McSweeney, Dónal M (University of Manchester; Christie Hospital NHS Foundation Trust); Green, Andrew (University of Manchester; Christie Hospital NHS Foundation Trust); Jackson, Price (Peter MacCallum Cancer Centre); McIntosh, Lachlan (Peter MacCallum Cancer Centre); Nguyen, Quoc-Cuong (Vietnam National University, Ho Chi Minh City); Qayyum, Abdul (); Conze, Pierre-Henri (IMT Atlantique); Huang, Ziyan (Shanghai Jiao Tong University); Zhou, Ziqi (Shenzhen University); Fan, Deng-Ping (Nankai University; Inception Institute of Artificial Intelligence); Xiong, Huan (Mohamed bin Zayed University of Artificial Intelligence; Harbin Institute of Technology); Dong, Guoqiang (The Second Affiliated Hospital of Bengbu Medical College); Zhu, Qiongjie (); He, Jian (); Yang, Xiaoping (Nanjing University)",17,17,,,,https://app.dimensions.ai/details/publication/pub.1150990474,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
5061,pub.1133288818,10.1038/s41592-020-01008-z,33288961,,nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation,"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.","This work was co-funded by the National Center for Tumor Diseases (NCT) in Heidelberg and the Helmholtz Imaging Platform (HIP). We thank our colleagues at DKFZ who were involved in the various challenge contributions, especially A. Klein, D. Zimmerer, J. Wasserthal, G. Koehler, T. Norajitra and S. Wirkert, who contributed to the Decathlon submission. We also thank the MITK team, which supported us in producing all medical dataset visualizations. We are also thankful to all the challenge organizers, who provided an important basis for our work. We want to especially mention N. Heller, who enabled the collection of all the details from the KiTS challenge through excellent challenge design, E. Kavur from the CHAOS team, who generated comprehensive leaderboard information for us, C. Petitjean, who provided detailed leaderboard information of the SegTHOR entries from ISBI 2019 and M. Maška, who patiently supported us during our Cell Tracking Challenge submission. We thank M. Wiesenfarth for his helpful advice concerning the ranking of methods and the visualization of rankings. We further thank C. Pape and T. Wollman for their crucial introductions to the CREMI and Cell Tracking Challenges, respectively. Last but not least, we thank O. Ronneberger and L. Maier-Hein for their important feedback on this manuscript.",,Nature Methods,,"Algorithms; Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer",2020-12-07,2020,2020-12-07,2021-02,18,2,203-211,All OA; Green,Article,"Isensee, Fabian; Jaeger, Paul F.; Kohl, Simon A. A.; Petersen, Jens; Maier-Hein, Klaus H.","Isensee, Fabian (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Faculty of Biosciences, University of Heidelberg, Heidelberg, Germany); Jaeger, Paul F. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany); Kohl, Simon A. A. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; DeepMind, London, UK); Petersen, Jens (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Faculty of Physics & Astronomy, University of Heidelberg, Heidelberg, Germany); Maier-Hein, Klaus H. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany)","Maier-Hein, Klaus H. (German Cancer Research Center; University Hospital Heidelberg)","Isensee, Fabian (German Cancer Research Center; Heidelberg University); Jaeger, Paul F. (German Cancer Research Center); Kohl, Simon A. A. (German Cancer Research Center; DeepMind (United Kingdom)); Petersen, Jens (German Cancer Research Center; Heidelberg University); Maier-Hein, Klaus H. (German Cancer Research Center; University Hospital Heidelberg)",1224,1222,142.07,,http://arxiv.org/pdf/1904.08128,https://app.dimensions.ai/details/publication/pub.1133288818,31 Biological Sciences,
5053,pub.1124882702,10.1109/tmi.2020.2974159,32078541,,Image-to-Images Translation for Multi-Task Organ Segmentation and Bone Suppression in Chest X-Ray Radiography,"Chest X-ray radiography is one of the earliest medical imaging technologies and remains one of the most widely-used for diagnosis, screening, and treatment follow up of diseases related to lungs and heart. The literature in this field of research reports many interesting studies dealing with the challenging tasks of bone suppression and organ segmentation but performed separately, limiting any learning that comes with the consolidation of parameters that could optimize both processes. This study, and for the first time, introduces a multitask deep learning model that generates simultaneously the bone-suppressed image and the organ-segmented image, enhancing the accuracy of tasks, minimizing the number of parameters needed by the model and optimizing the processing time, all by exploiting the interplay between the network parameters to benefit the performance of both tasks. The architectural design of this model, which relies on a conditional generative adversarial network, reveals the process on how the wellestablished pix2pix network (image-to-image network) is modified to fit the need for multitasking and extending it to the new image-to-images architecture. The developed source code of this multitask model is shared publicly on Github as the first attempt for providing the two-task pix2pix extension, a supervised/paired/aligned/registered image-to-images translation which would be useful in many multitask applications. Dilated convolutions are also used to improve the results through a more effective receptive field assessment. The comparison with state-of-the-art al-gorithms along with ablation study and a demonstration video1 are provided to evaluate the efficacy and gauge the merits of the proposed approach.","This work was supported by the National Science Foundation (NSF) under NSFGrant CNS-1920182, Grant CNS-1532061, and Grant CNS-1551221. The work of Mohammad Eslami was also supported by ESLA White Labs GmbH. The work of Shadi Albarqouni was supported by the PRIME Programme of the German Academic Exchange Service (DAAD) with funds from the German Federal Ministry of Education and Research (BMBF).",,IEEE Transactions on Medical Imaging,,,2020-02-14,2020,2020-02-14,2020-02-14,39,7,2553-2565,All OA; Hybrid,Article,"Eslami, Mohammad; Tabarestani, Solale; Albarqouni, Shadi; Adeli, Ehsan; Navab, Nassir; Adjouadi, Malek","Eslami, Mohammad (CAMP-TUM, 80333, Munich, Germany; Center for Advanced Technology and Education (CATE), Florida International University, Miami, FL, 33174, USA); Tabarestani, Solale (Center for Advanced Technology and Education (CATE), Florida International University, Miami, FL, 33174, USA); Albarqouni, Shadi (Computer Aided Medical Procedures and Augmented Reality (CAMP), Technical University of Munich (TUM), 80333, Munich, Germany; Computer Vision Lab (CVL), Department of Information Technology and Electrical Engineering, ETH Zürich, 8092, Zürich, Switzerland); Adeli, Ehsan (School of Medicine and Computer Science Department, Stanford University, Stanford, CA, 94305, USA); Navab, Nassir (Computer Aided Medical Procedures and Augmented Reality (CAMP), Technical University of Munich (TUM), 80333, Munich, Germany; Computer Aided Medical Procedures (CAMP), Johns Hopkins University, Baltimore, MD, 21218, USA); Adjouadi, Malek (Center for Advanced Technology and Education (CATE), Florida International University, Miami, FL, 33174, USA)","Eslami, Mohammad (; Florida International University)","Eslami, Mohammad (Florida International University); Tabarestani, Solale (Florida International University); Albarqouni, Shadi (Technical University of Munich; ETH Zurich); Adeli, Ehsan (Stanford University); Navab, Nassir (Technical University of Munich; Johns Hopkins University); Adjouadi, Malek (Florida International University)",49,36,2.65,25.25,https://doi.org/10.1109/tmi.2020.2974159,https://app.dimensions.ai/details/publication/pub.1124882702,46 Information and Computing Sciences; 4611 Machine Learning,
4810,pub.1151489432,10.1016/j.media.2022.102642,36223682,,"WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image","Whole abdominal organ segmentation is important in diagnosing abdomen lesions, radiotherapy, and follow-up. However, oncologists' delineating all abdominal organs from 3D volumes is time-consuming and very expensive. Deep learning-based medical image segmentation has shown the potential to reduce manual delineation efforts, but it still requires a large-scale fine annotated dataset for training, and there is a lack of large-scale datasets covering the whole abdomen region with accurate and detailed annotations for the whole abdominal organ segmentation. In this work, we establish a new large-scale Whole abdominal ORgan Dataset (WORD) for algorithm research and clinical application development. This dataset contains 150 abdominal CT volumes (30495 slices). Each volume has 16 organs with fine pixel-level annotations and scribble-based sparse annotations, which may be the largest dataset with whole abdominal organ annotation. Several state-of-the-art segmentation methods are evaluated on this dataset. And we also invited three experienced oncologists to revise the model predictions to measure the gap between the deep learning method and oncologists. Afterwards, we investigate the inference-efficient learning on the WORD, as the high-resolution image requires large GPU memory and a long inference time in the test stage. We further evaluate the scribble-based annotation-efficient learning on this dataset, as the pixel-wise manual annotation is time-consuming and expensive. The work provided a new benchmark for the abdominal multi-organ segmentation task, and these experiments can serve as the baseline for future research and clinical application development.","This work was supported by the National Natural Science Foundation of China [81771921, 61901084], the National Key Research and Development Program, China [2020YFB1711503] and also by key research and development project of Sichuan province, China [20ZDYF2817]. We would like to thank Mr. Zhiqiang Hu and Guofeng Lv from the SenseTime Research for constructive discussions and suggestions and also thank M.D. J. Xiao and W. Liao and their team members for data collection, annotation, checking and user study. We also would like to thank the Shanghai AI Lab and Shanghai SenseTime Research for their high-performance computation support.",,Medical Image Analysis,,"Humans; Tomography, X-Ray Computed; Benchmarking; Algorithms; Abdomen; Image Processing, Computer-Assisted",2022-09-30,2022,2022-09-30,2022-11,82,,102642,All OA; Green,Article,"Luo, Xiangde; Liao, Wenjun; Xiao, Jianghong; Chen, Jieneng; Song, Tao; Zhang, Xiaofan; Li, Kang; Metaxas, Dimitris N; Wang, Guotai; Zhang, Shaoting","Luo, Xiangde (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China.); Liao, Wenjun (Department of Radiation Oncology, Sichuan Cancer Hospital and Institute, Sichuan Cancer Center, Chengdu, China; School of Medicine, University of Electronic Science and Technology of China, Chengdu, China.); Xiao, Jianghong (Department of Radiation Oncology, Cancer Center West China Hospital, Sichuan University, Chengdu, China. Electronic address: xiaojh@scu.edu.cn.); Chen, Jieneng (Department of Computer Science, Johns Hopkins University, Baltimore, USA.); Song, Tao (SenseTime Research, Shanghai, China.); Zhang, Xiaofan (Shanghai Artificial Intelligence Laboratory, Shanghai, China.); Li, Kang (West China Hospital-SenseTime Joint Lab, West China Biomedical Big Data Center, Sichuan University, Chengdu, China.); Metaxas, Dimitris N (Department of Computer Science, Rutgers University, Piscataway, NJ, USA.); Wang, Guotai (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China. Electronic address: guotai.wang@uestc.edu.cn.); Zhang, Shaoting (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China. Electronic address: zhangshaoting@uestc.edu.cn.)","Xiao, Jianghong (Sichuan University); Wang, Guotai (University of Electronic Science and Technology of China; Shanghai Artificial Intelligence Laboratory); Zhang, Shaoting (University of Electronic Science and Technology of China; Shanghai Artificial Intelligence Laboratory)","Luo, Xiangde (University of Electronic Science and Technology of China; Shanghai Artificial Intelligence Laboratory); Liao, Wenjun (Sichuan Cancer Hospital; University of Electronic Science and Technology of China); Xiao, Jianghong (Sichuan University); Chen, Jieneng (Johns Hopkins University); Song, Tao (); Zhang, Xiaofan (Shanghai Artificial Intelligence Laboratory); Li, Kang (Sichuan University); Metaxas, Dimitris N (Rutgers, The State University of New Jersey); Wang, Guotai (University of Electronic Science and Technology of China; Shanghai Artificial Intelligence Laboratory); Zhang, Shaoting (University of Electronic Science and Technology of China; Shanghai Artificial Intelligence Laboratory)",2,2,,,http://arxiv.org/pdf/2111.02403,https://app.dimensions.ai/details/publication/pub.1151489432,32 Biomedical and Clinical Sciences; 40 Engineering,
4801,pub.1105237849,10.1109/tmi.2018.2851780,29994393,,Automatic Multiorgan Segmentation via Multiscale Registration and Graph Cut,"We propose an automatic multiorgan segmentation method for 3-D radiological images of different anatomical contents and modalities. The approach is based on a simultaneous multilabel graph cut optimization of location, appearance, and spatial configuration criteria of target structures. Organ location is defined by target-specific probabilistic atlases (PA) constructed from a training dataset using a fast (2+1)D SURF-based multiscale registration method involving a simple four-parameter transformation. PAs are also used to derive target-specific organ appearance models represented as intensity histograms. The spatial configuration prior is derived from shortest-path constraints defined on the adjacency graph of structures. Thorough evaluations on Visceral project benchmarks and training dataset, as well as comparisons with the state-of-the-art confirm that our approach is comparable to and often outperforms similar approaches in multiorgan segmentation, thus proving that the combination of multiple suboptimal but complementary information sources can yield very good performance.",We thank the Visceral Consortium for allowing us to use the Visceral training dataset in independent evaluations.,,IEEE Transactions on Medical Imaging,,"Algorithms; Databases, Factual; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Radiography, Abdominal; Radiography, Thoracic; Tomography, X-Ray Computed",2018-06-29,2018,2018-06-29,2018-12,37,12,2739-2749,All OA; Green,Article,"Kéchichian, Razmig; Valette, Sébastien; Desvignes, Michel","Kéchichian, Razmig (Creatis Research Center, Université de Lyon, CNRS UMR 5220, Inserm U1044, INSA-Lyon, Université Lyon 1, 69100, Villeurbanne, France); Valette, Sébastien (Creatis Research Center, Université de Lyon, CNRS UMR 5220, Inserm U1044, INSA-Lyon, Université Lyon 1, 69100, Villeurbanne, France); Desvignes, Michel (Gipsa-Lab, CNRS UMR 5216, Grenoble-INP, Université Joseph Fourier, Université Stendhal, 38400, Saint-Martin-d’Héres, France)","Kéchichian, Razmig (Claude Bernard University Lyon 1)","Kéchichian, Razmig (Claude Bernard University Lyon 1); Valette, Sébastien (Claude Bernard University Lyon 1); Desvignes, Michel ()",12,8,0.4,4.41,https://hal.archives-ouvertes.fr/hal-01902809/file/main.pdf,https://app.dimensions.ai/details/publication/pub.1105237849,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",
4780,pub.1153013167,10.1109/tmi.2022.3224067,36417741,,Causality-inspired Single-source Domain Generalization for Medical Image Segmentation,"Deep learning models usually suffer from the domain shift issue, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data are only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. In this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation scenarios: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-site prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains.",,,IEEE Transactions on Medical Imaging,,,2022-11-23,2022,2022-11-23,2022-11-23,PP,99,1-1,All OA; Green,Article,"Ouyang, Cheng; Chen, Chen; Li, Surui; Li, Zeju; Qin, Chen; Bai, Wenjia; Rueckert, Daniel","Ouyang, Cheng (Department of Computing, Imperial College London, UK); Chen, Chen (Department of Computing, Imperial College London, UK); Li, Surui (Department of Computing, Imperial College London, UK); Li, Zeju (Department of Computing, Imperial College London, UK); Qin, Chen (Department of Electrical and Electronic Engineering, Imperial College London, UK); Bai, Wenjia (Department of Computing, Imperial College London, UK); Rueckert, Daniel (Department of Computing, Imperial College London, UK)",,"Ouyang, Cheng (Imperial College London); Chen, Chen (Imperial College London); Li, Surui (Imperial College London); Li, Zeju (Imperial College London); Qin, Chen (Imperial College London); Bai, Wenjia (Imperial College London); Rueckert, Daniel (Imperial College London)",5,5,,,http://arxiv.org/pdf/2111.12525,https://app.dimensions.ai/details/publication/pub.1153013167,46 Information and Computing Sciences; 4611 Machine Learning,
4525,pub.1101047038,10.1109/tmi.2018.2806309,29994628,PMC6076994,Automatic Multi-Organ Segmentation on Abdominal CT With Dense V-Networks,"Automatic segmentation of abdominal anatomy on computed tomography (CT) images can support diagnosis, treatment planning, and treatment delivery workflows. Segmentation methods using statistical models and multi-atlas label fusion (MALF) require inter-subject image registrations, which are challenging for abdominal images, but alternative methods without registration have not yet achieved higher accuracy for most abdominal organs. We present a registration-free deep-learning-based segmentation algorithm for eight organs that are relevant for navigation in endoscopic pancreatic and biliary procedures, including the pancreas, the gastrointestinal tract (esophagus, stomach, and duodenum) and surrounding organs (liver, spleen, left kidney, and gallbladder). We directly compared the segmentation accuracy of the proposed method to the existing deep learning and MALF methods in a cross-validation on a multi-centre data set with 90 subjects. The proposed method yielded significantly higher Dice scores for all organs and lower mean absolute distances for most organs, including Dice scores of 0.78 versus 0.71, 0.74, and 0.74 for the pancreas, 0.90 versus 0.85, 0.87, and 0.83 for the stomach, and 0.76 versus 0.68, 0.69, and 0.66 for the esophagus. We conclude that the deep-learning-based segmentation represents a registration-free method for multi-organ abdominal CT segmentation whose accuracy can surpass current methods, potentially supporting image-guided navigation in gastrointestinal endoscopy procedures.",This work was supported in part by the Cancer Research U.K. under Multidisciplinary Grant C28070/A19985 and the National Institute for Health Research UCL/UCL Hospitals Biomedical Research Centre.,,IEEE Transactions on Medical Imaging,,"Algorithms; Digestive System; Humans; Kidney; Radiographic Image Interpretation, Computer-Assisted; Radiography, Abdominal; Spleen; Tomography, X-Ray Computed",2018-02-14,2018,2018-02-14,2018-08,37,8,1822-1834,All OA; Green,Article,"Gibson, Eli; Giganti, Francesco; Hu, Yipeng; Bonmati, Ester; Bandula, Steve; Gurusamy, Kurinchi; Davidson, Brian; Pereira, Stephen P.; Clarkson, Matthew J.; Barratt, Dean C.","Gibson, Eli (UCL Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, WC1E 6BT, U.K.; Wellcome/EPSRC Centre for Interventional and Surgical Sciences University College London, London, WC1E 6BT, U.K.); Giganti, Francesco (Department of Radiology, University College Hospital Trust, London, NW1 2BU, U.K.; Division of Surgery and Interventional Science, University College London, London, WC1E 6BT, U.K.); Hu, Yipeng (UCL Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, WC1E 6BT, U.K.; Wellcome/EPSRC Centre for Interventional and Surgical Sciences University College London, London, WC1E 6BT, U.K.); Bonmati, Ester (UCL Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, WC1E 6BT, U.K.; Wellcome/EPSRC Centre for Interventional and Surgical Sciences University College London, London, WC1E 6BT, U.K.); Bandula, Steve (UCL Centre for Medical Imaging, University College London, London, WC1E 6BT, U.K.); Gurusamy, Kurinchi (Division of Surgery and Interventional Science, University College London, London, WC1E 6BT, U.K.); Davidson, Brian (Division of Surgery and Interventional Science, University College London, London, WC1E 6BT, U.K.); Pereira, Stephen P. (Institute for Liver and Digestive Health, University College London, London, WC1E 6BT, U.K.); Clarkson, Matthew J. (UCL Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, WC1E 6BT, U.K.; Wellcome/EPSRC Centre for Interventional and Surgical Sciences University College London, London, WC1E 6BT, U.K.); Barratt, Dean C. (UCL Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, WC1E 6BT, U.K.; Wellcome/EPSRC Centre for Interventional and Surgical Sciences University College London, London, WC1E 6BT, U.K.)","Gibson, Eli (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences)","Gibson, Eli (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Giganti, Francesco (University College Hospital; University College London); Hu, Yipeng (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Bonmati, Ester (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Bandula, Steve (University College London); Gurusamy, Kurinchi (University College London); Davidson, Brian (University College London); Pereira, Stephen P. (University College London); Clarkson, Matthew J. (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Barratt, Dean C. (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences)",369,226,17.62,,https://discovery.ucl.ac.uk/10044219/1/TMI2806309.pdf,https://app.dimensions.ai/details/publication/pub.1101047038,40 Engineering; 46 Information and Computing Sciences,
4391,pub.1133615584,10.1109/tmi.2020.3045775,33338014,,A Unified Framework for Generalized Low-Shot Medical Image Segmentation With Scarce Data,"Medical image segmentation has achieved remarkable advancements using deep neural networks (DNNs). However, DNNs often need big amounts of data and annotations for training, both of which can be difficult and costly to obtain. In this work, we propose a unified framework for generalized low-shot (one- and few-shot) medical image segmentation based on distance metric learning (DML). Unlike most existing methods which only deal with the lack of annotations while assuming abundance of data, our framework works with extreme scarcity of both, which is ideal for rare diseases. Via DML, the framework learns a multimodal mixture representation for each category, and performs dense predictions based on cosine distances between the pixels' deep embeddings and the category representations. The multimodal representations effectively utilize the inter-subject similarities and intraclass variations to overcome overfitting due to extremely limited data. In addition, we propose adaptive mixing coefficients for the multimodal mixture distributions to adaptively emphasize the modes better suited to the current input. The representations are implicitly embedded as weights of the fc layer, such that the cosine distances can be computed efficiently via forward propagation. In our experiments on brain MRI and abdominal CT datasets, the proposed framework achieves superior performances for low-shot segmentation towards standard DNN-based (3D U-Net) and classical registration-based (ANTs) methods, e.g., achieving mean Dice coefficients of 81%/69% for brain tissue/abdominal multi-organ segmentation using a single training sample, as compared to 52%/31% and 72%/35% by the U-Net and ANTs, respectively.","This work was supported in part by the General Program (Key Program, Major Research Plan) of National Natural Science Foundation of China under Grant 61876032, in part by the Key-Area Research and Development Program of Guangdong Province, China, under Grant 2018B010111001, in part by the National Key R&amp;D Program of China under Grant 2018YFC2000702, and in part by the Scientific and Technical Innovation 2030-“New Generation Artificial Intelligence” under Grant 2020AAA0104100. H. Cui contributed to this work as an intern at Tencent Jarvis Lab.",,IEEE Transactions on Medical Imaging,,"Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer",2021-09-30,2021,2021-09-30,2021-10,40,10,2656-2671,All OA; Green,Article,"Cui, Hengji; Wei, Dong; Ma, Kai; Gu, Shi; Zheng, Yefeng","Cui, Hengji (School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China); Wei, Dong (Tencent Jarvis Lab, Shenzhen, 518057, China); Ma, Kai (Tencent Jarvis Lab, Shenzhen, 518057, China); Gu, Shi (School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China); Zheng, Yefeng (Tencent Jarvis Lab, Shenzhen, 518057, China)","Gu, Shi (University of Electronic Science and Technology of China)","Cui, Hengji (University of Electronic Science and Technology of China); Wei, Dong (); Ma, Kai (); Gu, Shi (University of Electronic Science and Technology of China); Zheng, Yefeng ()",15,15,2.58,11.94,http://arxiv.org/pdf/2110.09260,https://app.dimensions.ai/details/publication/pub.1133615584,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
4267,pub.1121175877,10.1016/j.compmedimag.2019.05.006,31550670,,A novel robust kernel principal component analysis for nonlinear statistical shape modeling from erroneous data,"Statistical Shape Models (SSMs) have achieved considerable success in medical image segmentation. A high quality SSM is able to approximate the main plausible variances of a given anatomical structure to guide segmentation. However, it is technically challenging to derive such a quality model because: (1) the distribution of shape variance is often nonlinear or multi-modal which cannot be modeled by standard approaches assuming Gaussian distribution; (2) as the quality of annotations in training data usually varies, heavy corruption will degrade the quality of the model as a whole. In this work, these challenges are addressed by introducing a generic SSM that is able to model nonlinear distribution and is robust to outliers in training data. Without losing generality and assuming a sparsity in nonlinear distribution, a novel Robust Kernel Principal Component Analysis (RKPCA) for statistical shape modeling is proposed with the aim of constructing a low-rank nonlinear subspace where outliers are discarded. The proposed approach is validated on two different datasets: a set of 30 public CT kidney pairs and a set of 49 MRI ankle bones volumes. Experimental results demonstrate a significantly better performance on outlier recovery and a higher quality of the proposed model as well as lower segmentation errors compared to the state-of-the-art techniques.","This research is supported by the National Research Foundation, Prime Minister&#x27;s Office, Singapore under its International Research Centres in Singapore Funding Initiative. This work is partially supported by a grant AcRF RGC 2017-T1-001-053 by Ministry of Education, Singapore.",,Computerized Medical Imaging and Graphics,,"Ankle; Datasets as Topic; Humans; Kidney; Magnetic Resonance Imaging; Models, Statistical; Principal Component Analysis; Tomography, X-Ray Computed",2019-09-21,2019,2019-09-21,2019-10,77,,101638,Closed,Article,"Ma, Jingting; Wang, Anqi; Lin, Feng; Wesarg, Stefan; Erdt, Marius","Ma, Jingting (Nanyang Technological University, Nanyang Avenue 50, Singapore 639798, Singapore); Wang, Anqi (Fraunhofer IGD, Darmstadt 64283, Germany); Lin, Feng (Nanyang Technological University, Nanyang Avenue 50, Singapore 639798, Singapore); Wesarg, Stefan (Fraunhofer IGD, Darmstadt 64283, Germany); Erdt, Marius (Nanyang Technological University, Nanyang Avenue 50, Singapore 639798, Singapore; Fraunhofer Singapore, Nanyang Avenue 50, Singapore 639798, Singapore)","Ma, Jingting (Nanyang Technological University)","Ma, Jingting (Nanyang Technological University); Wang, Anqi (Fraunhofer Institute for Computer Graphics Research); Lin, Feng (Nanyang Technological University); Wesarg, Stefan (Fraunhofer Institute for Computer Graphics Research); Erdt, Marius (Nanyang Technological University; Fraunhofer Singapore)",12,5,0.22,4.48,,https://app.dimensions.ai/details/publication/pub.1121175877,40 Engineering; 46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
4252,pub.1106114410,10.1002/mp.13122,30098025,PMC6192047,Development of deep neural network for individualized hepatobiliary toxicity prediction after liver SBRT,"BACKGROUND: Accurate prediction of radiation toxicity of healthy organs-at-risks (OARs) critically determines the radiation therapy (RT) success. The existing dose-volume histogram-based metric may grossly under/overestimate the therapeutic toxicity after 27% in liver RT and 50% in head-and-neck RT. We propose the novel paradigm for toxicity prediction by leveraging the enormous potential of deep learning and go beyond the existing dose/volume histograms.
EXPERIMENTAL DESIGN: We employed a database of 125 liver stereotactic body RT (SBRT) cases with follow-up data to train deep learning-based toxicity predictor. Convolutional neural networks (CNNs) were applied to discover the consistent patterns in 3D dose plans associated with toxicities. To enhance the predicting power, we first pretrain the CNNs with transfer learning from 3D CT images of 2644 human organs. CNNs were then trained on liver SBRT cases. Furthermore, nondosimetric pretreatment features, such as patients' demographics, underlying liver diseases, liver-directed therapies, were inputted into the fully connected neural network for more comprehensive prediction. The saliency maps of CNNs were used to estimate the toxicity risks associated with irradiation of anatomical regions of specific OARs. In addition, we applied machine learning solutions to map numerical pretreatment features with hepatobiliary toxicity manifestation.
RESULTS: Among 125 liver SBRT patients, 58 were treated for liver metastases, 36 for hepatocellular carcinoma, 27 for cholangiocarcinoma, and 4 for other histologies. We observed that CNN we able to achieve accurate hepatobiliary toxicity prediction with the AUC of 0.79, whereas combining CNN for 3D dose plan analysis and fully connected neural networks for numerical feature analysis resulted in AUC of 0.85. Deep learning produces almost two times fewer false-positive toxicity predictions in comparison to DVH-based predictions, when the number of false negatives, i.e., missed toxicities, was minimized. The CNN saliency maps automatically estimated the toxicity risks for portal vein (PV) regions. We discovered that irradiation of the proximal portal vein is associated with two times higher toxicity risks (risk score: 0.66) that irradiation of the left portal vein (risk score: 0.31).
CONCLUSIONS: The framework offers clinically accurate tools for hepatobiliary toxicity prediction and automatic identification of anatomical regions that are critical to spare during SBRT.","This work was partially supported by NIH (1R01 CA176553 and EB016777), and Varian research grants.",,Medical Physics,,"Biliary Tract; Humans; Liver; Neural Networks, Computer; Organs at Risk; Precision Medicine; Radiosurgery; Radiotherapy Dosage; Radiotherapy Planning, Computer-Assisted; Risk; Treatment Failure",2018-09-10,2018,2018-09-10,2018-10,45,10,4763-4774,All OA; Green,Article,"Ibragimov, Bulat; Toesca, Diego; Chang, Daniel; Yuan, Yixuan; Koong, Albert; Xing, Lei","Ibragimov, Bulat (Department of Radiation Oncology, Stanford University School of Medicine, Stanford, CA, USA); Toesca, Diego (Department of Radiation Oncology, Stanford University School of Medicine, Stanford, CA, USA); Chang, Daniel (Department of Radiation Oncology, Stanford University School of Medicine, Stanford, CA, USA); Yuan, Yixuan (Department of Electronic Engineering, City University of Hong Kong, Hong Kong, China); Koong, Albert (Department of Radiation Oncology, MD Anderson Cancer Center, Houston, TX, USA); Xing, Lei (Department of Radiation Oncology, Stanford University School of Medicine, Stanford, CA, USA)","Xing, Lei (Stanford University)","Ibragimov, Bulat (Stanford University); Toesca, Diego (Stanford University); Chang, Daniel (Stanford University); Yuan, Yixuan (City University of Hong Kong); Koong, Albert (The University of Texas MD Anderson Cancer Center); Xing, Lei (Stanford University)",81,49,4.59,49.33,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6192047,https://app.dimensions.ai/details/publication/pub.1106114410,51 Physical Sciences; 5105 Medical and Biological Physics,
4026,pub.1112688399,10.1109/jbhi.2019.2904078,30869633,,Neural Networks for Deep Radiotherapy Dose Analysis and Prediction of Liver SBRT Outcomes,"Stereotactic body radiation therapy (SBRT) is a relatively novel treatment modality, with little post-treatment prognostic information reported. This study proposes a novel neural network based paradigm for accurate prediction of liver SBRT outcomes. We assembled a database of patients treated with liver SBRT at our institution. Together with a three-dimensional (3-D) dose delivery plans for each SBRT treatment, other variables such as patients' demographics, quantified abdominal anatomy, history of liver comorbidities, other liver-directed therapies, and liver function tests were collected. We developed a multi-path neural network with the convolutional path for 3-D dose plan analysis and fully connected path for other variables analysis, where the network was trained to predict post-SBRT survival and local cancer progression. To enhance the network robustness, it was initially pre-trained on a large database of computed tomography images. Following n-fold cross-validation, the network automatically identified patients that are likely to have longer survival or late cancer recurrence, i.e., patients with the positive predicted outcome (PPO) of SBRT, and vice versa, i.e., negative predicted outcome (NPO). The predicted results agreed with actual SBRT outcomes with 56% of PPO patients and 0% NPO patients with primary liver cancer survived more than two years after SBRT. Similarly, 82% of PPO patients and 0% of NPO patients with metastatic liver cancer survived two-year threshold. The obtained results were superior to the performance of support vector machine and random forest classifiers. Furthermore, the network was able to identify the critical-to-spare liver regions, and the critical clinical features associated with the highest risks of negative SBRT outcomes.",This work was supported in part by a research grant from Varian Medical Systems and in part by the National Institutes of Health under Grant 1R01CA176553.,,IEEE Journal of Biomedical and Health Informatics,,"Adult; Aged; Aged, 80 and over; Algorithms; Deep Learning; Disease Progression; Female; Humans; Kaplan-Meier Estimate; Liver; Liver Neoplasms; Male; Middle Aged; Neural Networks, Computer; ROC Curve; Radiosurgery; Radiotherapy Planning, Computer-Assisted",2019-03-11,2019,2019-03-11,2019-09,23,5,1821-1833,Closed,Article,"Ibragimov, Bulat; Toesca, Diego A. S.; Yuan, Yixuan; Koong, Albert C.; Chang, Daniel T.; Xing, Lei","Ibragimov, Bulat (Department of Radiation Oncology, Stanford University, Palo Alto, CA, 94304, USA; Department of Computer Science, University of Copenhagen, Copenhagen, 2100, Denmark); Toesca, Diego A. S. (Department of Radiation Oncology, Stanford University, Palo Alto, CA, 94304, USA); Yuan, Yixuan (Department of Electrical Engineering, City University of Hong Kong, Hong Kong); Koong, Albert C. (Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, 77030, USA); Chang, Daniel T. (Department of Radiation Oncology, Stanford University, Palo Alto, CA, 94304, USA); Xing, Lei (Department of Radiation Oncology, Stanford University, Palo Alto, CA, 94304, USA)","Ibragimov, Bulat (Stanford University; University of Copenhagen)","Ibragimov, Bulat (Stanford University; University of Copenhagen); Toesca, Diego A. S. (Stanford University); Yuan, Yixuan (City University of Hong Kong); Koong, Albert C. (The University of Texas MD Anderson Cancer Center); Chang, Daniel T. (Stanford University); Xing, Lei (Stanford University)",22,18,1.66,11.36,,https://app.dimensions.ai/details/publication/pub.1112688399,42 Health Sciences; 4203 Health Services and Systems; 46 Information and Computing Sciences; 4601 Applied Computing,
4004,pub.1139995701,10.1109/tpami.2021.3100536,34314356,,AbdomenCT-1K: Is Abdominal Organ Segmentation a Solved Problem?,"With the unprecedented developments in deep learning, automatic segmentation of main abdominal organs seems to be a solved problem as state-of-the-art (SOTA) methods have achieved comparable results with inter-rater variability on many benchmark datasets. However, most of the existing abdominal datasets only contain single-center, single-phase, single-vendor, or single-disease cases, and it is unclear whether the excellent performance can generalize on diverse datasets. This paper presents a large and diverse abdominal CT organ segmentation dataset, termed AbdomenCT-1K, with more than 1000 (1K) CT scans from 12 medical centers, including multi-phase, multi-vendor, and multi-disease cases. Furthermore, we conduct a large-scale study for liver, kidney, spleen, and pancreas segmentation and reveal the unsolved segmentation problems of the SOTA methods, such as the limited generalization ability on distinct medical centers, phases, and unseen diseases. To advance the unsolved problems, we further build four organ segmentation benchmarks for fully supervised, semi-supervised, weakly supervised, and continual learning, which are currently challenging and active research topics. Accordingly, we develop a simple and effective method for each benchmark, which can be used as out-of-the-box methods and strong baselines. We believe the AbdomenCT-1K dataset will promote future in-depth research towards clinical applicable abdominal organ segmentation methods.","This work was supported by China’s Ministry of Science and Technology under Grant 2020YFA0713800 and National Natural Science Foundation of China under Grants 11971229, No. 12090023.","This work was supported by China's Ministry of Science and Technology under Grant 2020YFA0713800 and National Natural Science Foundation of China under Grants 11971229, No. 12090023.",IEEE Transactions on Pattern Analysis and Machine Intelligence,,"Abdomen; Algorithms; Pancreas; Spleen; Tomography, X-Ray Computed",2022-09-14,2022,2022-09-14,2022-10,44,10,6695-6714,All OA; Green,Article,"Ma, Jun; Zhang, Yao; Gu, Song; Zhu, Cheng; Ge, Cheng; Zhang, Yichi; An, Xingle; Wang, Congcong; Wang, Qiyuan; Liu, Xin; Cao, Shucheng; Zhang, Qi; Liu, Shangqing; Wang, Yunpeng; Li, Yuhui; He, Jian; Yang, Xiaoping","Ma, Jun (Department of Mathematics, Nanjing University of Science and Technology, Nanjing, 210094, China); Zhang, Yao (Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100864, China; University of Chinese Academy of Sciences, Beijing, 100049, China); Gu, Song (School of Automation, Nanjing University of Information Science and Technology, Nanjing, 210044, China); Zhu, Cheng (Shenzhen Haichuang Medical CO., LTD., Shenzhen, 518000, China); Ge, Cheng (Institute of Bioinformatics and Medical Engineering, Jiangsu University of Technology, Changzhou, 213001, China); Zhang, Yichi (School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China); An, Xingle (Beijing Infervision Technology CO. LTD., Beijing, 100089, China); Wang, Congcong (School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, 300222, China; Department of Computer Science, Norwegian University of Science and Technology, 7491, Trondheim, Norway); Wang, Qiyuan (School of Electronic Science and Engineering, Nanjing University, Nanjing, 210023, China); Liu, Xin (Suzhou LungCare Medical Technology Co., Ltd, Suzhou, 215021, China); Cao, Shucheng (Bioengineering, Biological and Environmental Science and Engineering Division, King Abdullah University of Science and Technology, Thuwal, 23955, Saudi Arabia); Zhang, Qi (Department of Computer and Information Science, Faculty of Science and Technology, University of Macau, Taipa, Macau, 999078, China); Liu, Shangqing (School of Biomedical Engineering, Southern Medical University, Guangzhou, 510515, China); Wang, Yunpeng (Institutes of Biomedical Sciences, Fudan University, Shanghai, 200433, China); Li, Yuhui (Computational Biology, University of Southern California, Los Angeles, CA, 90007, USA); He, Jian (Department of Nuclear Medicine, Nanjing Drum Tower Hospital, Nanjing, 210008, China); Yang, Xiaoping (Department of Mathematics, Nanjing University, Nanjing, 210023, China)","Ma, Jun (Nanjing University of Science and Technology)","Ma, Jun (Nanjing University of Science and Technology); Zhang, Yao (Institute of Computing Technology; University of Chinese Academy of Sciences); Gu, Song (Nanjing University of Information Science and Technology); Zhu, Cheng (); Ge, Cheng (Jiangsu University of Technology); Zhang, Yichi (Beihang University); An, Xingle (InferVision (China)); Wang, Congcong (Tianjin University of Technology; Norwegian University of Science and Technology); Wang, Qiyuan (Nanjing University); Liu, Xin (); Cao, Shucheng (King Abdullah University of Science and Technology); Zhang, Qi (University of Macau); Liu, Shangqing (Southern Medical University); Wang, Yunpeng (Fudan University); Li, Yuhui (University of Southern California); He, Jian (Nanjing Drum Tower Hospital); Yang, Xiaoping (Nanjing University)",65,65,,,https://repository.kaust.edu.sa/bitstream/10754/665793/1/Preprintfile1.pdf,https://app.dimensions.ai/details/publication/pub.1139995701,46 Information and Computing Sciences; 4611 Machine Learning,
3824,pub.1061696797,10.1109/tmi.2016.2600502,27541630,,3D Statistical Shape Models Incorporating Landmark-Wise Random Regression Forests for Omni-Directional Landmark Detection,"3D Statistical Shape Models (3D-SSM) are widely used for medical image segmentation. However, during segmentation, they typically perform a very limited unidirectional search for suitable landmark positions in the image, relying on weak learners or use-case specific appearance models that solely take local image information into account. As a consequence, segmentation errors arise, and results in general depend on the accuracy of a previous model initialization. Furthermore, these methods become subject to a tedious and use-case dependent parameter tuning in order to obtain optimized results. To overcome these limitations, we propose an extension of 3D-SSM by landmark-wise random regression forests that perform an enhanced omni-directional search for landmark positions, thereby taking rich non-local image information into account. In addition, we provide a long distance model fitting based on a multi-scale approach, that allows an accurate and reproducible segmentation even from distant image positions, thus enabling an application without model initialization. Finally, translation of the proposed method to different organs is straightforward and requires no adaptation of the training process. In segmentation experiments on 45 clinical CT volumes, the proposed omni-directional search significantly increased accuracy and displayed great precision regardless of model initialization. Furthermore, for liver, spleen and kidney segmentation in a competitive multi-organ labeling challenge on publicly available data, the proposed method achieved similar or better results than the state of the art. Finally, liver segmentation results were obtained that successfully compete with specialized state-of-the-art methods from the well-known liver segmentation challenge SLIVER.","This work was carried out with the support of the German Research Foundation (DFG) as part of project I02, SFB/TRR 125 Cognition-Guided Surgery.",,IEEE Transactions on Medical Imaging,,"Algorithms; Humans; Models, Statistical; Reproducibility of Results; Tomography, X-Ray Computed",2016-08-16,2016,2016-08-16,2017-01,36,1,155-168,Closed,Article,"Norajitra, Tobias; Maier-Hein, Klaus H.","Norajitra, Tobias (Junior Research Group Medical Image Computing, German Cancer Research Center (DFKZ), Heidelberg, 69120, Germany); Maier-Hein, Klaus H. (Junior Research Group Medical Image Computing, German Cancer Research Center (DFKZ), Heidelberg, 69120, Germany)","Maier-Hein, Klaus H. (German Cancer Research Center)","Norajitra, Tobias (German Cancer Research Center); Maier-Hein, Klaus H. (German Cancer Research Center)",22,8,1.17,6.12,,https://app.dimensions.ai/details/publication/pub.1061696797,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
2186,pub.1154965332,10.1016/j.media.2023.102762,36738650,,"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives","Transformer, one of the latest technological advances of deep learning, has gained prevalence in natural language processing or computer vision. Since medical imaging bear some resemblance to computer vision, it is natural to inquire about the status quo of Transformers in medical imaging and ask the question: can the Transformer models transform medical imaging? In this paper, we attempt to make a response to the inquiry. After a brief introduction of the fundamentals of Transformers, especially in comparison with convolutional neural networks (CNNs), and highlighting key defining properties that characterize the Transformers, we offer a comprehensive review of the state-of-the-art Transformer-based approaches for medical imaging and exhibit current research progresses made in the areas of medical image segmentation, recognition, detection, registration, reconstruction, enhancement, etc. In particular, what distinguishes our review lies in its organization based on the Transformer's key defining properties, which are mostly derived from comparing the Transformer and CNN, and its type of architecture, which specifies the manner in which the Transformer and CNN are combined, all helping the readers to best understand the rationale behind the reviewed approaches. We conclude with discussions of future perspectives.","Li and Zhou are supported by National Natural Science Foundation of China (NSFC) under grant No. 62271465. Chen is supported by U01-CA140204 and R01-EB031023 from the National Institutes of Health, USA .",,Medical Image Analysis,,"Humans; Diagnostic Imaging; Neural Networks, Computer",2023-01-31,2023,2023-01-31,2023-04,85,,102762,All OA; Green,Article,"Li, Jun; Chen, Junyu; Tang, Yucheng; Wang, Ce; Landman, Bennett A; Zhou, S Kevin","Li, Jun (Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China.); Chen, Junyu (Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins Medical Institutes, Baltimore, MD, USA.); Tang, Yucheng (Department of Electrical and Computer Engineering, Vanderbilt University, Nashville, TN, USA.); Wang, Ce (Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China.); Landman, Bennett A (Department of Electrical and Computer Engineering, Vanderbilt University, Nashville, TN, USA.); Zhou, S Kevin (Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China; School of Biomedical Engineering & Suzhou Institute for Advanced Research, Center for Medical Imaging, Robotics, and Analytic Computing & Learning (MIRACLE), University of Science and Technology of China, Suzhou 215123, China. Electronic address: skevinzhou@ustc.edu.cn.)","Zhou, S Kevin (Institute of Computing Technology; University of Science and Technology of China)","Li, Jun (Institute of Computing Technology); Chen, Junyu (Johns Hopkins University); Tang, Yucheng (Vanderbilt University); Wang, Ce (Institute of Computing Technology); Landman, Bennett A (Vanderbilt University); Zhou, S Kevin (Institute of Computing Technology; University of Science and Technology of China)",3,3,,,http://arxiv.org/pdf/2206.01136,https://app.dimensions.ai/details/publication/pub.1154965332,32 Biomedical and Clinical Sciences; 40 Engineering,
2063,pub.1135470352,10.18178/joig.9.1.9-14,,,A Supervoxel Classification Based Method for Multi-organ Segmentation from Abdominal CT Images,,,,Journal of Image and Graphics,,,2021,2021,2021,2021,9,1,9-14,All OA; Hybrid,Article,"Wu, Jiaqi; Graduated School of Engineering, Kyushu Institute of Technology; Li, Guangxu; Lu, Huimin; Kamiy, Tohru","Wu, Jiaqi (); Graduated School of Engineering, Kyushu Institute of Technology (); Li, Guangxu (); Lu, Huimin (); Kamiy, Tohru ()",,"Wu, Jiaqi (); Graduated School of Engineering, Kyushu Institute of Technology (); Li, Guangxu (); Lu, Huimin (); Kamiy, Tohru ()",12,12,,,https://doi.org/10.18178/joig.9.1.9-14,https://app.dimensions.ai/details/publication/pub.1135470352,,
1834,pub.1154464932,10.1109/icicml57342.2022.10009865,,,Hybrid Transformer and Convolution for Medical Image Segmentation,"Using medical image segmentation algorithm to automatically label diseased organs or tissues can effectively help doctors to diagnose and treat various diseases. Most segmentation models are designed based on convolution. However, due to the inherent limitations of convolution operation, its receptive field is usually limited and cannot capture the global information. With transformer, it can capture long distance dependencies, but the lack of local detail can lead to limited predictive capability. Therefore, aiming at the above problems, we propose a segmentation method based on hybrid transformer and convolution. It effectively merges the classic convolution layer with the transformer layer to help guide accurate segmentation. Specifically, a lightweight convolutional layer is first used to extract low-level features and reduce the amount of data. This is followed by a mix of transformer blocks and convolution blocks to aid in the extraction of high-level information. This hybrid convolution layer and transformer layer help to improve the generalization and robustness of the learned representation. In the decoding part, the feature maps of adjacent dimensions are up-sampled into higher resolution feature maps and sent to transformer layer for decoding. The cross-fusion attention mechanism is used to adaptively screen the valid features from the encoded part by reducing the semantic gap between the feature maps of the encoder and decoder subnets. We have done full experiments on Synapse dataset and proved that the proposed method has great competitiveness and advantages compared with other segmentation methods.",This work was supported by the National Natural Science Foundation of China (No. 62041108); the Natural Science Foundation of Ningxia (No. 2020AAC03029); Innovation and Entrepreneurship Project for Returnees in Ningxia 2020.,This work was supported by the National Natural Science Foundation of China (No. 62041108); the Natural Science Foundation of Ningxia (No. 2020AAC03029); Innovation and Entrepreneurship Project for Returnees in Ningxia 2020.,,"2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)",,2022-10-30,2022,,2022-10-30,00,,156-159,Closed,Proceeding,"Wang, Fan; Wang, Bo","Wang, Fan (School of Physics and Electronic-Electrical Engineering, Ningxia University, Yinchuan, China); Wang, Bo (School of Physics and Electronic-Electrical Engineering, Ningxia University, Yinchuan, China)","Wang, Fan (Ningxia University)","Wang, Fan (Ningxia University); Wang, Bo (Ningxia University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154464932,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1832,pub.1112456890,10.1117/12.2512883,,,Multi-organ segmentation in clinical-computed tomography for patient-specific image quality and dose metrology,"The purpose of this study was to develop a robust, automated multi-organ segmentation model for clinical adult and pediatric CT and implement the model as part of a patient-specific safety and quality monitoring system. 3D convolutional neural network (Unet) models were setup to segment 30 different organs and structures at the diagnostic image resolution. For each organ, 200 manually-labeled cases were used to train the network, fitting it to different clinical imaging resolutions and contrast enhancement stages. The dataset was randomly shuffled, and divided with 6/2/2 train/validation/test set split. The model was deployed to automatically segment 1200 clinical CT images as a demonstration of the utility of the method. Each case was made into a patient-specific phantom based on the segmentation masks, with unsegmented organs and structures filled in by deforming a template XCAT phantom of similar anatomy. The organ doses were then estimated using a validated scanner-specific MC-GPU package using the actual scan information. The segmented organ information was likewise used to assess contrast, noise, and detectability index within each organ. The neural network segmentation model showed dice similarity coefficients (DSC) above 0.85 for the majority of organs. Notably, the lungs and liver showed a DSC of 0.95 and 0.94, respectively. The segmentation results produced patient-specific dose and quality values across the tested 1200 patients with representative the histogram distributions. The measurements were compared in global-to-organ (e.g. CTDvol vs. liver dose) and organ-to-organ (e.g. liver dose vs. spleen dose) manner. The global-to-organ measurements (liver dose vs. CTDIvol: 𝑅 = 0.62; liver vs. global d’: 𝑅 = 0.78; liver vs. global noise: 𝑅 = 0.55) were less correlated compared to the organ-to-organ measurements (liver vs. spleen dose: 𝑅 = 0.93; liver vs. spleen d’: 𝑅 = 0.82; liver vs. spleen noise: 𝑅 = 0.78). This variation of measurement is more prominent for iterative reconstruction kernel compared to the filtered back projection kernel (liver vs. global noise: 𝑅𝐼𝑅 = 0.47 vs. 𝑅𝐹𝐵𝑃 = 0.75; liver vs. global d’: 𝑅𝐼𝑅 = 0.74 vs. 𝑅𝐹𝐵𝑃 = 0.83). The results can help derive meaningful relationships between image quality, organ doses, and patient attributes.",,,Progress in Biomedical Optics and Imaging,Medical Imaging 2019: Physics of Medical Imaging,,2019-03-01,2019,,,10948,,1094829-1094829-6,Closed,Proceeding,"Fu, Wanyi; Sharma, Shobhit; Smith, Taylor; Hou, Rui; Abadi, Ehsan; Selvakumaran, Vignesh; Tang, Ruixiang; Lo, Joseph Y.; Segars, W. Paul; Kapadia, Anuj J.; Solomon, Justin B.; Rubin, Geoffrey D.; Samei, Ehsan","Fu, Wanyi (Duke Univ. (United States)); Sharma, Shobhit (Duke Univ. (United States)); Smith, Taylor (Duke Univ. (United States)); Hou, Rui (Duke Univ. (United States)); Abadi, Ehsan (Duke Univ. (United States)); Selvakumaran, Vignesh (Duke Univ. (United States)); Tang, Ruixiang (Duke Univ. (United States)); Lo, Joseph Y. (Duke Univ. (United States)); Segars, W. Paul (Duke Univ. (United States)); Kapadia, Anuj J. (Duke Univ. (United States)); Solomon, Justin B. (Duke Univ. (United States)); Rubin, Geoffrey D. (Duke Univ. (United States)); Samei, Ehsan (Duke Univ. (United States))",,"Fu, Wanyi (Duke University); Sharma, Shobhit (Duke University); Smith, Taylor (Duke University); Hou, Rui (Duke University); Abadi, Ehsan (Duke University); Selvakumaran, Vignesh (Duke University); Tang, Ruixiang (Duke University); Lo, Joseph Y. (Duke University); Segars, W. Paul (Duke University); Kapadia, Anuj J. (Duke University); Solomon, Justin B. (Duke University); Rubin, Geoffrey D. (Duke University); Samei, Ehsan (Duke University)",9,4,,3.93,,https://app.dimensions.ai/details/publication/pub.1112456890,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences; 3211 Oncology and Carcinogenesis; 51 Physical Sciences; 5105 Medical and Biological Physics,
1831,pub.1129811746,10.1109/isbiworkshops50223.2020.9153392,,,Joint Low Dose CT Denoising And Kidney Segmentation,"In this research, both image denoising and kidney segmentation tasks are addressed jointly via one multitask deep convolutional network. This multitasking scheme yields better results for both tasks compared to separate single-task methods. Also, to the best of our knowledge, this is a first time attempt at addressing these joint tasks in low-dose CT scans (LDCT). This new network is a conditional generative adversarial network (C-GAN) and is an extension of the image-to-image translation network. To investigate the generalized nature of the network, two other conventional single task networks are also exploited, including the well-known 2D UNet method for segmentation and the more recently proposed method WGAN for LDCT denoising. Implementation results proved that the proposed method outperforms UNet and WGAN for both tasks.","This research is supported by the National Science Foundation (NSF) under grants CNS-1920182, CNS-1532061, CNS-1338922 and CNS-1551221.",,,2020 IEEE 17th International Symposium on Biomedical Imaging Workshops (ISBI Workshops),,2020-04-04,2020,,2020-04-04,00,,1-4,Closed,Proceeding,"Eslami, Mohammad; Tabarestani, Solale; Adjouadi, Malek","Eslami, Mohammad (Center for Advanced Technology and Education (CATE), Department of Electrical and Computer Engineering, Florida International University, Miami, USA, https://cate.fiu.edu/); Tabarestani, Solale (Center for Advanced Technology and Education (CATE), Department of Electrical and Computer Engineering, Florida International University, Miami, USA, https://cate.fiu.edu/); Adjouadi, Malek (Center for Advanced Technology and Education (CATE), Department of Electrical and Computer Engineering, Florida International University, Miami, USA, https://cate.fiu.edu/)","Eslami, Mohammad (Florida International University)","Eslami, Mohammad (Florida International University); Tabarestani, Solale (Florida International University); Adjouadi, Malek (Florida International University)",4,4,,1.96,,https://app.dimensions.ai/details/publication/pub.1129811746,40 Engineering; 4006 Communications Engineering; 46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1830,pub.1152214914,10.48550/arxiv.2210.14845,,,Synthetic Tumors Make AI Segment Tumors Better,"We develop a novel strategy to generate synthetic tumors. Unlike existing
works, the tumors generated by our strategy have two intriguing advantages: (1)
realistic in shape and texture, which even medical professionals can confuse
with real tumors; (2) effective for AI model training, which can perform liver
tumor segmentation similarly to a model trained on real tumors - this result is
unprecedented because no existing work, using synthetic tumors only, has thus
far reached a similar or even close performance to the model trained on real
tumors. This result also implies that manual efforts for developing per-voxel
annotation of tumors (which took years to create) can be considerably reduced
for training AI models in the future. Moreover, our synthetic tumors have the
potential to improve the success rate of small tumor detection by automatically
generating enormous examples of small (or tiny) synthetic tumors.",,,arXiv,,,2022-10-26,2022,,,,,,All OA; Green,Preprint,"Hu, Qixin; Xiao, Junfei; Chen, Yixiong; Sun, Shuwen; Chen, Jie-Neng; Yuille, Alan; Zhou, Zongwei","Hu, Qixin (); Xiao, Junfei (); Chen, Yixiong (); Sun, Shuwen (); Chen, Jie-Neng (); Yuille, Alan (); Zhou, Zongwei ()",,"Hu, Qixin (); Xiao, Junfei (); Chen, Yixiong (); Sun, Shuwen (); Chen, Jie-Neng (); Yuille, Alan (); Zhou, Zongwei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152214914,32 Biomedical and Clinical Sciences; 3211 Oncology and Carcinogenesis; 46 Information and Computing Sciences,
1820,pub.1154683311,10.1007/s10489-022-04417-z,,,Siamese few-shot network: a novel and efficient network for medical image segmentation,"Few-shot learning is attracting more researchers due to its outstanding ability to find unseen classes with less data. Meanwhile, we noticed that medical data is difficult to collect and label, but there is a major need for higher accuracy in either organ segmentation or disease classification. Therefore, we propose a few-shot learning model with a Siamese core, the Siamese few-shot network (SFN) to improve medical image segmentation. To the beset of our knowledge, SFN is the first model to introduce few-shot learning combined with the Siamese idea to medical image segmentation. Furthermore, we also design a grid attention(GA) module to locally focus semantic information, especially in medical images. The results prove that our method outperforms the state-of-the-art model on abdominal organ segmentation for CT and MRI.",This work is supported by the National Natural Science Foundation of China under Grant U2003208.,,Applied Intelligence,,,2023-01-19,2023,2023-01-19,,,,1-13,Closed,Article,"Xiao, Guangli; Tian, Shengwei; Yu, Long; Zhou, Zhicheng; Zeng, Xuanli","Xiao, Guangli (College of Software Engineering, Xinjiang University, 830000, Urumqi, China; Key Laboratory of Software Engineering Technology, Xinjiang University, 830000, Urumqi, China); Tian, Shengwei (College of Software Engineering, Xinjiang University, 830000, Urumqi, China; Key Laboratory of Software Engineering Technology, Xinjiang University, 830000, Urumqi, China); Yu, Long (Network Center, Xinjiang University, 830046, Urumqi, China); Zhou, Zhicheng (College of Software Engineering, Xinjiang University, 830000, Urumqi, China; Key Laboratory of Software Engineering Technology, Xinjiang University, 830000, Urumqi, China); Zeng, Xuanli (College of Software Engineering, Xinjiang University, 830000, Urumqi, China; Key Laboratory of Software Engineering Technology, Xinjiang University, 830000, Urumqi, China)","Tian, Shengwei (Xinjiang University; Xinjiang University)","Xiao, Guangli (Xinjiang University; Xinjiang University); Tian, Shengwei (Xinjiang University; Xinjiang University); Yu, Long (Xinjiang University); Zhou, Zhicheng (Xinjiang University; Xinjiang University); Zeng, Xuanli (Xinjiang University; Xinjiang University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154683311,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1747,pub.1120844513,10.1007/978-3-030-30642-7_46,,,Multiple Organs Segmentation in Abdomen CT Scans Using a Cascade of CNNs,"Automatic organ segmentation is a vital prerequisite of many clinical application in radiology. The anatomical variability of organs in the abdomen makes it difficult for many methods to obtain good segmentations for all organs. In this paper, we present a particular ensemble of convolutional neural networks, combining technologies that analyze the images with either a local or a global perspective. In particular, we implemented a cascade of models combining the advantages of using local and global processing. We have evaluated our proposed system on CT scan of 30 subjects in a nested cross-validation framework, showing a significant performance improvement if compared with state-of-the-art methods.",,,Lecture Notes in Computer Science,Image Analysis and Processing – ICIAP 2019,,2019-09-02,2019,2019-09-02,2019,11751,,509-516,Closed,Chapter,"Akbar, Muhammad Usman; Aslani, Shahab; Murino, Vitorio; Sona, Diego","Akbar, Muhammad Usman (Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Genova, Italy; Department of Electrical, Electronics and Telecommunication Engineering and Naval Architecture, Università degli Studi di Genova, Genoa, Italy); Aslani, Shahab (Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Genova, Italy; Department of Electrical, Electronics and Telecommunication Engineering and Naval Architecture, Università degli Studi di Genova, Genoa, Italy); Murino, Vitorio (Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Genova, Italy; Department of Computer Science, Università di Verona, Verona, Italy); Sona, Diego (Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Genova, Italy; Neuroinformatics Laboratory, Fondazione Bruno Kessler, Trento, Italy)","Akbar, Muhammad Usman (Italian Institute of Technology; University of Genoa)","Akbar, Muhammad Usman (Italian Institute of Technology; University of Genoa); Aslani, Shahab (Italian Institute of Technology; University of Genoa); Murino, Vitorio (Italian Institute of Technology; University of Verona); Sona, Diego (Italian Institute of Technology; Fondazione Bruno Kessler)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1120844513,46 Information and Computing Sciences,
1708,pub.1120213905,10.48550/arxiv.1908.02182,,,An attempt at beating the 3D U-Net,"The U-Net is arguably the most successful segmentation architecture in the
medical domain. Here we apply a 3D U-Net to the 2019 Kidney and Kidney Tumor
Segmentation Challenge and attempt to improve upon it by augmenting it with
residual and pre-activation residual blocks. Cross-validation results on the
training cases suggest only very minor, barely measurable improvements. Due to
marginally higher dice scores, the residual 3D U-Net is chosen for test set
prediction. With a Composite Dice score of 91.23 on the test set, our method
outperformed all 105 competing teams and won the KiTS2019 challenge by a small
margin.",,,arXiv,,,2019-08-06,2019,,,,,,All OA; Green,Preprint,"Isensee, Fabian; Maier-Hein, Klaus H.","Isensee, Fabian (); Maier-Hein, Klaus H. ()",,"Isensee, Fabian (); Maier-Hein, Klaus H. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1120213905,32 Biomedical and Clinical Sciences; 3211 Oncology and Carcinogenesis; 46 Information and Computing Sciences,
1675,pub.1147455887,10.1109/icassp43922.2022.9747097,,,Contrastive Translation Learning For Medical Image Segmentation,"Unsupervised domain adaptation commonly uses cycle generative networks to produce synthesis data from source to target domains. Unfortunately, translated samples cannot effectively preserve semantic information from input sources, resulting in bad or low adaptability of the network to segment target data. This work proposes an advantageous domain translation mechanism to improve the perceptual ability of the network for accurate unlabeled target data segmentation. Our domain translation employs patchwise contrastive learning to improve the semantic content consistency between input and translated images. Our approach was applied to unsupervised domain adaptation based abdominal organ segmentation. The experimental results demonstrate the effectiveness of our framework that outperforms other methods.",,This work was supported partly by the National Natural Science Foundation of China under Grant 61971367 and the Fujian Provincial Technology Innovation Joint Funds under Grant 2019Y9091.,,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,2022-05-27,2022,,2022-05-27,00,,2395-2399,Closed,Proceeding,"Zeng, Wankang; Fan, Wenkang; Shen, Dongfang; Chen, Yinran; Luo, Xiongbiao","Zeng, Wankang (Department of Computer Science, Xiamen University, Xiamen, 361005, China); Fan, Wenkang (Department of Computer Science, Xiamen University, Xiamen, 361005, China); Shen, Dongfang (Department of Computer Science, Xiamen University, Xiamen, 361005, China); Chen, Yinran (Department of Computer Science, Xiamen University, Xiamen, 361005, China); Luo, Xiongbiao (Department of Computer Science, Xiamen University, Xiamen, 361005, China)","Luo, Xiongbiao (Xiamen University)","Zeng, Wankang (Xiamen University); Fan, Wenkang (Xiamen University); Shen, Dongfang (Xiamen University); Chen, Yinran (Xiamen University); Luo, Xiongbiao (Xiamen University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1147455887,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4611 Machine Learning,
1675,pub.1118997523,10.48550/arxiv.1802.07800,,,Liver segmentation in CT images using three dimensional to two  dimensional fully convolutional network,"The need for CT scan analysis is growing for pre-diagnosis and therapy of
abdominal organs. Automatic organ segmentation of abdominal CT scan can help
radiologists analyze the scans faster and segment organ images with fewer
errors. However, existing methods are not efficient enough to perform the
segmentation process for victims of accidents and emergencies situations. In
this paper we propose an efficient liver segmentation with our 3D to 2D fully
connected network (3D-2D-FCN). The segmented mask is enhanced by means of
conditional random field on the organ's border. Consequently, we segment a
target liver in less than a minute with Dice score of 93.52.",,,arXiv,,,2018-02-21,2018,,,,,,All OA; Green,Preprint,"Rafiei, Shima; Nasr-Esfahani, Ebrahim; Soroushmehr, S. M. Reza; Karimi, Nader; Samavi, Shadrokh; Najarian, Kayvan","Rafiei, Shima (); Nasr-Esfahani, Ebrahim (); Soroushmehr, S. M. Reza (); Karimi, Nader (); Samavi, Shadrokh (); Najarian, Kayvan ()",,"Rafiei, Shima (); Nasr-Esfahani, Ebrahim (); Soroushmehr, S. M. Reza (); Karimi, Nader (); Samavi, Shadrokh (); Najarian, Kayvan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1118997523,51 Physical Sciences; 5105 Medical and Biological Physics,
1673,pub.1119920831,10.48550/arxiv.1907.10982,,,Overfitting of neural nets under class imbalance: Analysis and  improvements for segmentation,"Overfitting in deep learning has been the focus of a number of recent works,
yet its exact impact on the behavior of neural networks is not well understood.
This study analyzes overfitting by examining how the distribution of logits
alters in relation to how much the model overfits. Specifically, we find that
when training with few data samples, the distribution of logit activations when
processing unseen test samples of an under-represented class tends to shift
towards and even across the decision boundary, while the over-represented class
seems unaffected. In image segmentation, foreground samples are often heavily
under-represented. We observe that sensitivity of the model drops as a result
of overfitting, while precision remains mostly stable. Based on our analysis,
we derive asymmetric modifications of existing loss functions and regularizers
including a large margin loss, focal loss, adversarial training and mixup,
which specifically aim at reducing the shift observed when embedding unseen
samples of the under-represented class. We study the case of binary
segmentation of brain tumor core and show that our proposed simple
modifications lead to significantly improved segmentation performance over the
symmetric variants.",,,arXiv,,,2019-07-25,2019,,,,,,All OA; Green,Preprint,"Li, Zeju; Kamnitsas, Konstantinos; Glocker, Ben","Li, Zeju (); Kamnitsas, Konstantinos (); Glocker, Ben ()",,"Li, Zeju (); Kamnitsas, Konstantinos (); Glocker, Ben ()",1,1,,0.38,,https://app.dimensions.ai/details/publication/pub.1119920831,46 Information and Computing Sciences; 4611 Machine Learning,
1670,pub.1147398373,10.1109/isbi52829.2022.9761568,,,Unsupervised Ensemble Distillation for Multi-Organ Segmentation,"Multi-organ segmentation is a fundamental task in medical image processing. This paper explores a novel privacy friendly setting to train a multi-organ segmentation model, i.e., learning directly from multiple pre-trained single-organ segmentation models. We formulate it into a special unsupervised ensemble distillation problem. To solve the problem, a multi-teacher knowledge distillation framework is proposed, which leverages the soft labels predicted by pre-trained teacher models to train a student model, i.e., the multi-organ segmentation model. Considering the difference of the teachers in task speciality, the pseudo labels for each organ come from the corresponding teacher, and those of the background region are from all teachers. To deal with the problem of unmatched dimensionality between the teacher models and the student model, an output transformation method is introduced. The entire learning process requires only access to pre-trained models and a reasonable set of unsupervised target data, achieving a good balance between the privacy protection and model performance. Experimental results on a widely adopted benchmark dataset have shown the promise of the proposed method.","This work is partially supported by the Science and Technology Commission of Shanghai Municipality (STCSM) (Grant No. 21DZ1100100, Grant No. 18DZ2270700)","This work is partially supported by the Science and Technology Commission of Shanghai Municipality (STCSM) (Grant No. 21DZ1100100, Grant No. 18DZ2270700)",,2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),,2022-03-31,2022,,2022-03-31,00,,1-5,Closed,Proceeding,"Zhang, Lefei; Feng, Shixiang; Wang, Yu; Wang, Yanfeng; Zhang, Ya; Chen, Xin; Tian, Qi","Zhang, Lefei (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University); Feng, Shixiang (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University); Wang, Yu (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai AI Laboratory); Wang, Yanfeng (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai AI Laboratory); Zhang, Ya (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai AI Laboratory); Chen, Xin (Cloud & AI, Huawei Technologies); Tian, Qi (Cloud & AI, Huawei Technologies)","Zhang, Lefei (Shanghai Jiao Tong University)","Zhang, Lefei (Shanghai Jiao Tong University); Feng, Shixiang (Shanghai Jiao Tong University); Wang, Yu (Shanghai Jiao Tong University); Wang, Yanfeng (Shanghai Jiao Tong University); Zhang, Ya (Shanghai Jiao Tong University); Chen, Xin (); Tian, Qi ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1147398373,46 Information and Computing Sciences; 4611 Machine Learning,
1670,pub.1147403745,10.1109/aipr52630.2021.9762167,,,A Deep Learning Approach for Contour Interpolation,"Contour interpolation is an important tool to expedite manual segmentation of anatomical structures. The conventional shape-based interpolation (SBI) algorithm, based on distance map calculation then interpolation, often performs sub-optimally when the two adjacent to-be-interpolated manual contours differ dramatically, especially near the superior and inferior borders of organs and for the gastrointestinal structures. In this study, we present a deep learning solution to improve the robustness and accuracy for contour interpolation, especially for these historically difficult cases. The deep contour interpolation model achieved a dice score of 0.95±0.06 and a mean DTA value of 1.08±2.31 mm, averaged on 3167 testing cases of all 16 organs. In a comparison, the results by the conventional SBI method were 0.94±0.08 and 1.50±3.63mm, respectively. For the difficult cases, the dice score and DTA value were 0.91±0.09 and 1.71±2.25 mm by the deep model, compared to 0. 86± 0. 13 and 3.42±5.88 mm by the conventional SBI method. A student t-test was applied to confirm that the performance improvements were statistically significant (p< 0. 05) for all cases. It could be useful to expedite the tasks of manual segmentation of organs and structures in the medical images.",This study was supported by National Institute of Biomedical Imaging and Bioengineering (NIBIB) grant R03-EB028427.,This study was supported by National Institute of Biomedical Imaging and Bioengineering (NIBIB) grant R03-EB028427.,,2021 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),,2021-10-14,2021,,2021-10-14,00,,1-5,Closed,Proceeding,"Zhao, Chenxi; Duan, Ye; Yang, Deshan","Zhao, Chenxi (EECS Department, University of Missouri, Columbia, USA); Duan, Ye (EECS Deparment, University of Missouri, Columbia, USA); Yang, Deshan (Department of Radiation Oncology, Washington University, St. Louis, USA)",,"Zhao, Chenxi (University of Missouri); Duan, Ye (University of Missouri); Yang, Deshan (Washington University in St. Louis)",1,1,,1.25,,https://app.dimensions.ai/details/publication/pub.1147403745,32 Biomedical and Clinical Sciences; 51 Physical Sciences; 5105 Medical and Biological Physics,
1670,pub.1107018773,10.1007/978-3-030-00934-2_76,,,Deep 3D Dose Analysis for Prediction of Outcomes After Liver Stereotactic Body Radiation Therapy,"Accurate and precise dose delivery is the key factor for radiation therapy (RT) success. Currently, RT planning is based on optimization of oversimplified dose-volume metrics that consider all human organs to be homogeneous. The limitations of such an approach result in suboptimal treatments with poor outcomes: short survival, early cancer recurrence and radiation-induced toxicities of healthy organs. This paper pioneers the concept of deep 3D dose analysis for outcome prediction after liver stereotactic body RT (SBRT). The presented work develops tools for unification of dose plans into the same anatomy space, classifies dose plan using convolutional neural networks with transfer learning form anatomy images, and assembles the first volumetric liver atlas of the critical-to-spare liver regions. The concept is validated on prediction of post-SBRT survival and local cancer progression using a clinical database of primary and metastatic liver SBRTs. The risks of negative SBRT outcomes are quantitatively estimated for individual liver segments.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2018,,2018-09-26,2018,2018-09-26,2018,11071,,684-692,Closed,Chapter,"Ibragimov, Bulat; Toesca, Diego A. S.; Yuan, Yixuan; Koong, Albert C.; Chang, Daniel T.; Xing, Lei","Ibragimov, Bulat (Department of Radiation Oncology, Stanford University, Stanford, USA); Toesca, Diego A. S. (Department of Radiation Oncology, Stanford University, Stanford, USA); Yuan, Yixuan (Department of Radiation Oncology, Stanford University, Stanford, USA; Department of Electronic Engineering, City Univeristy of Hong Kong, Hong Kong, China); Koong, Albert C. (Department of Radiation Oncology, MD Anderson Cancer Center, Houston, USA); Chang, Daniel T. (Department of Radiation Oncology, Stanford University, Stanford, USA); Xing, Lei (Department of Radiation Oncology, Stanford University, Stanford, USA)","Ibragimov, Bulat (Stanford University)","Ibragimov, Bulat (Stanford University); Toesca, Diego A. S. (Stanford University); Yuan, Yixuan (Stanford University; City University of Hong Kong); Koong, Albert C. (The University of Texas MD Anderson Cancer Center); Chang, Daniel T. (Stanford University); Xing, Lei (Stanford University)",2,0,,,,https://app.dimensions.ai/details/publication/pub.1107018773,46 Information and Computing Sciences,
1670,pub.1121619828,10.1007/978-3-030-32248-9_45,,,Overfitting of Neural Nets Under Class Imbalance: Analysis and Improvements for Segmentation,"Abstract
Overfitting in deep learning has been the focus of a number of recent works, yet its exact impact on the behavior of neural networks is not well understood. This study analyzes overfitting by examining how the distribution of logits alters in relation to how much the model overfits. Specifically, we find that when training with few data samples, the distribution of logit activations when processing unseen test samples of an under-represented class tends to shift towards and even across the decision boundary, while the over-represented class seems unaffected. In image segmentation, foreground samples are often heavily under-represented. We observe that sensitivity of the model drops as a result of overfitting, while precision remains mostly stable. Based on our analysis, we derive asymmetric modifications of existing loss functions and regularizers including a large margin loss, focal loss, adversarial training and mixup, which specifically aim at reducing the shift observed when embedding unseen samples of the under-represented class. We study the case of binary segmentation of brain tumor core and show that our proposed simple modifications lead to significantly improved segmentation performance over the symmetric variants.","ZL is grateful for a China Scholarship Council (CSC) Imperial Scholarship. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant No 757173, project MIRA, ERC-2017-STG) and EPSRC (EP/R511547/1).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2019,,2019-10-10,2019,2019-10-10,2019,11766,,402-410,All OA; Green,Chapter,"Li, Zeju; Kamnitsas, Konstantinos; Glocker, Ben","Li, Zeju (Biomedical Image Analysis Group, Imperial College London, London, UK); Kamnitsas, Konstantinos (Biomedical Image Analysis Group, Imperial College London, London, UK); Glocker, Ben (Biomedical Image Analysis Group, Imperial College London, London, UK)","Li, Zeju (Imperial College London)","Li, Zeju (Imperial College London); Kamnitsas, Konstantinos (Imperial College London); Glocker, Ben (Imperial College London)",56,39,,21.46,http://arxiv.org/pdf/1907.10982,https://app.dimensions.ai/details/publication/pub.1121619828,46 Information and Computing Sciences; 4611 Machine Learning,
1670,pub.1154628678,10.48550/arxiv.2301.07074,,,SegViz: A Federated Learning Framework for Medical Image Segmentation  from Distributed Datasets with Different and Incomplete Annotations,"Segmentation is one of the primary tasks in the application of deep learning
in medical imaging, owing to its multiple downstream clinical applications. As
a result, many large-scale segmentation datasets have been curated and released
for the segmentation of different anatomical structures. However, these
datasets focus on the segmentation of a subset of anatomical structures in the
body, therefore, training a model for each dataset would potentially result in
hundreds of models and thus limit their clinical translational utility.
Furthermore, many of these datasets share the same field of view but have
different subsets of annotations, thus making individual dataset annotations
incomplete. To that end, we developed SegViz, a federated learning framework
for aggregating knowledge from distributed medical image segmentation datasets
with different and incomplete annotations into a `global` meta-model. The
SegViz framework was trained to build a single model capable of segmenting both
liver and spleen aggregating knowledge from both these nodes by aggregating the
weights after every 10 epochs. The global SegViz model was tested on an
external dataset, Beyond the Cranial Vault (BTCV), comprising both liver and
spleen annotations using the dice similarity (DS) metric. The baseline
individual segmentation models for spleen and liver trained on their respective
datasets produced a DS score of 0.834 and 0.878 on the BTCV test set. In
comparison, the SegViz model produced comparable mean DS scores of 0.829 and
0.899 for the segmentation of the spleen and liver respectively. Our results
demonstrate SegViz as an essential first step towards training clinically
translatable multi-task segmentation models from distributed datasets with
disjoint incomplete annotations with excellent performance.",,,arXiv,,,2023-01-17,2023,,,,,,All OA; Green,Preprint,"Kanhere, Adway U.; Kulkarni, Pranav; Yi, Paul H.; Parekh, Vishwa S.","Kanhere, Adway U. (); Kulkarni, Pranav (); Yi, Paul H. (); Parekh, Vishwa S. ()",,"Kanhere, Adway U. (); Kulkarni, Pranav (); Yi, Paul H. (); Parekh, Vishwa S. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154628678,46 Information and Computing Sciences; 4605 Data Management and Data Science; 51 Physical Sciences,
1667,pub.1129107497,10.48550/arxiv.2007.03868,,,Marginal loss and exclusion loss for partially supervised multi-organ  segmentation,"Annotating multiple organs in medical images is both costly and
time-consuming; therefore, existing multi-organ datasets with labels are often
low in sample size and mostly partially labeled, that is, a dataset has a few
organs labeled but not all organs. In this paper, we investigate how to learn a
single multi-organ segmentation network from a union of such datasets. To this
end, we propose two types of novel loss function, particularly designed for
this scenario: (i) marginal loss and (ii) exclusion loss. Because the
background label for a partially labeled image is, in fact, a `merged' label of
all unlabelled organs and `true' background (in the sense of full labels), the
probability of this `merged' background label is a marginal probability,
summing the relevant probabilities before merging. This marginal probability
can be plugged into any existing loss function (such as cross entropy loss,
Dice loss, etc.) to form a marginal loss. Leveraging the fact that the organs
are non-overlapping, we propose the exclusion loss to gauge the dissimilarity
between labeled organs and the estimated segmentation of unlabelled organs.
Experiments on a union of five benchmark datasets in multi-organ segmentation
of liver, spleen, left and right kidneys, and pancreas demonstrate that using
our newly proposed loss functions brings a conspicuous performance improvement
for state-of-the-art methods without introducing any extra computation.",,,arXiv,,,2020-07-07,2020,,,,,,All OA; Green,Preprint,"Shi, Gonglei; Xiao, Li; Chen, Yang; Zhou, S. Kevin","Shi, Gonglei (); Xiao, Li (); Chen, Yang (); Zhou, S. Kevin ()",,"Shi, Gonglei (); Xiao, Li (); Chen, Yang (); Zhou, S. Kevin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129107497,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1640,pub.1107026723,10.1007/978-3-030-00937-3_58,,,Inter-site Variability in Prostate Segmentation Accuracy Using Deep Learning,"Deep-learning-based segmentation tools have yielded higher reported segmentation accuracies for many medical imaging applications. However, inter-site variability in image properties can challenge the translation of these tools to data from ‘unseen’ sites not included in the training data. This study quantifies the impact of inter-site variability on the accuracy of deep-learning-based segmentations of the prostate from magnetic resonance (MR) images, and evaluates two strategies for mitigating the reduced accuracy for data from unseen sites: training on multi-site data and training with limited additional data from the unseen site. Using 376 T2-weighted prostate MR images from six sites, we compare the segmentation accuracy (Dice score and boundary distance) of three deep-learning-based networks trained on data from a single site and on various configurations of data from multiple sites. We found that the segmentation accuracy of a single-site network was substantially worse on data from unseen sites than on data from the training site. Training on multi-site data yielded marginally improved accuracy and robustness. However, including as few as 8 subjects from the unseen site, e.g. during commissioning of a new clinical system, yielded substantial improvement (regaining 75% of the difference in Dice score).",This publication presents independent research supported by Cancer Research UK (Multidisciplinary C28070/A19985).,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2018,,2018-09-13,2018,2018-09-13,2018,11073,,506-514,All OA; Green,Chapter,"Gibson, Eli; Hu, Yipeng; Ghavami, Nooshin; Ahmed, Hashim U.; Moore, Caroline; Emberton, Mark; Huisman, Henkjan J.; Barratt, Dean C.","Gibson, Eli (University College London, London, UK); Hu, Yipeng (University College London, London, UK); Ghavami, Nooshin (University College London, London, UK); Ahmed, Hashim U. (Imperial College, London, UK); Moore, Caroline (University College London, London, UK); Emberton, Mark (University College London, London, UK); Huisman, Henkjan J. (Radboud University Medical Center, Nijmegen, The Netherlands); Barratt, Dean C. (University College London, London, UK)","Gibson, Eli (University College London)","Gibson, Eli (University College London); Hu, Yipeng (University College London); Ghavami, Nooshin (University College London); Ahmed, Hashim U. (Imperial College London); Moore, Caroline (University College London); Emberton, Mark (University College London); Huisman, Henkjan J. (Radboud University Nijmegen Medical Centre); Barratt, Dean C. (University College London)",42,25,,,https://discovery.ucl.ac.uk/10057753/1/Gibson_MICCAI2018.pdf,https://app.dimensions.ai/details/publication/pub.1107026723,46 Information and Computing Sciences,
1637,pub.1106861335,10.1109/icip.2018.8451238,,,Liver Segmentation in CT Images Using Three Dimensional to Two Dimensional Fully Convolutional Network,"The need for CT scan analysis is growing for diagnosis and therapy of abdominal organs. Automatic organ segmentation of abdominal CT scan can help radiologists analyze the scans faster, and diagnose disease and injury more accurately. However, existing methods are not efficient enough to perform the segmentation process for victims of accidents and emergency situations. In this paper, we propose an efficient liver segmentation with our 3D to 2D fully convolution network (3D-2D-FCN). The segmented mask is enhanced using the conditional random field on the organ's border. Consequently, we segment a target liver in less than a minute with Dice score of 93.52%.",We deeply appreciate Dr. Mattias Heinrich the author of [4] for providing us the result of 10 first training set of MICCAI 2015 datasets.,,,2018 25th IEEE International Conference on Image Processing (ICIP),,2018-10-07,2018,,2018-10-07,00,,2067-2071,All OA; Green,Proceeding,"Rafiei, Shima; Nasr-Esfahani, Ebrahim; Najarian, Kayvan; Karimi, Nader; Samavi, Shadrokh; Soroushmehr, S.M. Reza","Rafiei, Shima (Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, 84156-83111, Iran); Nasr-Esfahani, Ebrahim (Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, 84156-83111, Iran); Najarian, Kayvan (Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor, MI, U.S.A.; Michigan Center for Integrative Research in Critical Care, University of Michigan, Ann Arbor, MI, U.S.A.); Karimi, Nader (Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, 84156-83111, Iran); Samavi, Shadrokh (Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, 84156-83111, Iran; Michigan Center for Integrative Research in Critical Care, University of Michigan, Ann Arbor, MI, U.S.A.); Soroushmehr, S.M. Reza (Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor, MI, U.S.A.; Michigan Center for Integrative Research in Critical Care, University of Michigan, Ann Arbor, MI, U.S.A.)",,"Rafiei, Shima (Isfahan University of Technology); Nasr-Esfahani, Ebrahim (Isfahan University of Technology); Najarian, Kayvan (University of Michigan–Ann Arbor; University of Michigan–Ann Arbor); Karimi, Nader (Isfahan University of Technology); Samavi, Shadrokh (Isfahan University of Technology; University of Michigan–Ann Arbor); Soroushmehr, S.M. Reza (University of Michigan–Ann Arbor; University of Michigan–Ann Arbor)",33,17,,20.1,http://arxiv.org/pdf/1802.07800,https://app.dimensions.ai/details/publication/pub.1106861335,51 Physical Sciences; 5105 Medical and Biological Physics,3 Good Health and Well Being
1635,pub.1152818253,10.48550/arxiv.2211.08533,,,A Point in the Right Direction: Vector Prediction for Spatially-aware  Self-supervised Volumetric Representation Learning,"High annotation costs and limited labels for dense 3D medical imaging tasks
have recently motivated an assortment of 3D self-supervised pretraining methods
that improve transfer learning performance. However, these methods commonly
lack spatial awareness despite its centrality in enabling effective 3D image
analysis. More specifically, position, scale, and orientation are not only
informative but also automatically available when generating image crops for
training. Yet, to date, no work has proposed a pretext task that distills all
key spatial features. To fulfill this need, we develop a new self-supervised
method, VectorPOSE, which promotes better spatial understanding with two novel
pretext tasks: Vector Prediction (VP) and Boundary-Focused Reconstruction
(BFR). VP focuses on global spatial concepts (i.e., properties of 3D patches)
while BFR addresses weaknesses of recent reconstruction methods to learn more
effective local representations. We evaluate VectorPOSE on three 3D medical
image segmentation tasks, showing that it often outperforms state-of-the-art
methods, especially in limited annotation settings.",,,arXiv,,,2022-11-15,2022,,,,,,All OA; Green,Preprint,"Zhang, Yejia; Gu, Pengfei; Sapkota, Nishchal; Zheng, Hao; Liang, Peixian; Chen, Danny Z.","Zhang, Yejia (); Gu, Pengfei (); Sapkota, Nishchal (); Zheng, Hao (); Liang, Peixian (); Chen, Danny Z. ()",,"Zhang, Yejia (); Gu, Pengfei (); Sapkota, Nishchal (); Zheng, Hao (); Liang, Peixian (); Chen, Danny Z. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152818253,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",
1634,pub.1127839860,10.1109/isbi45749.2020.9098615,,,Liver Segmentation in CT with MRI Data: Zero-Shot Domain Adaptation by Contour Extraction and Shape Priors,"In this work we address the problem of domain adaptation for segmentation tasks with deep convolutional neural networks. We focus on managing the domain shift from MRI to CT volumes on the example of 3D liver segmentation. Domain adaptation between modalities is particularly of practical importance, as different hospital departments usually tend to use different imaging modalities and protocols in their clinical routine. Thus, training a model with source data from one department may not be sufficient for application in another institution. Most adaptation strategies make use of target domain samples and often additionally incorporate the corresponding ground truths from the target domain during the training process. In contrast to these approaches, we investigate the possibility of training our model solely on source domain data sets, i.e. we apply zero-shot domain adaptation. To compensate the missing target domain data, we use prior knowledge about both modalities to steer the model towards more general features during the training process. We particularly make use of fixed Sobel kernels to enhance contour information and apply anatomical priors, learned separately by a convolutional autoencoder. Although we completely discard including the target domain in the training process, our proposed approach improves a vanilla U-Net implementation drastically and yields promising segmentation results.",,,,2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI),,2020-04-03,2020,,2020-04-03,00,,1538-1542,Closed,Proceeding,"Pham, D. D.; Dovletov, G.; Pauli, J.","Pham, D. D. (Intelligent Systems, Faculty of Engineering, University of Duisburg-Essen, Germany); Dovletov, G. (Intelligent Systems, Faculty of Engineering, University of Duisburg-Essen, Germany); Pauli, J. (Intelligent Systems, Faculty of Engineering, University of Duisburg-Essen, Germany)",,"Pham, D. D. (University of Duisburg-Essen); Dovletov, G. (University of Duisburg-Essen); Pauli, J. (University of Duisburg-Essen)",5,5,,2.58,,https://app.dimensions.ai/details/publication/pub.1127839860,46 Information and Computing Sciences; 4611 Machine Learning,
1634,pub.1138108937,10.48550/arxiv.2105.06986,,,Evaluating the Robustness of Self-Supervised Learning in Medical Imaging,"Self-supervision has demonstrated to be an effective learning strategy when
training target tasks on small annotated data-sets. While current research
focuses on creating novel pretext tasks to learn meaningful and reusable
representations for the target task, these efforts obtain marginal performance
gains compared to fully-supervised learning. Meanwhile, little attention has
been given to study the robustness of networks trained in a self-supervised
manner. In this work, we demonstrate that networks trained via self-supervised
learning have superior robustness and generalizability compared to
fully-supervised learning in the context of medical imaging. Our experiments on
pneumonia detection in X-rays and multi-organ segmentation in CT yield
consistent results exposing the hidden benefits of self-supervision for
learning robust feature representations.",,,arXiv,,,2021-05-14,2021,,,,,,All OA; Green,Preprint,"Navarro, Fernando; Watanabe, Christopher; Shit, Suprosanna; Sekuboyina, Anjany; Peeken, Jan C.; Combs, Stephanie E.; Menze, Bjoern H.","Navarro, Fernando (); Watanabe, Christopher (); Shit, Suprosanna (); Sekuboyina, Anjany (); Peeken, Jan C. (); Combs, Stephanie E. (); Menze, Bjoern H. ()",,"Navarro, Fernando (); Watanabe, Christopher (); Shit, Suprosanna (); Sekuboyina, Anjany (); Peeken, Jan C. (); Combs, Stephanie E. (); Menze, Bjoern H. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1138108937,46 Information and Computing Sciences; 4611 Machine Learning,
1606,pub.1151979824,10.48550/arxiv.2210.08066,,,Optimizing Vision Transformers for Medical Image Segmentation,"For medical image semantic segmentation (MISS), Vision Transformers have
emerged as strong alternatives to convolutional neural networks thanks to their
inherent ability to capture long-range correlations. However, existing research
uses off-the-shelf vision Transformer blocks based on linear projections and
feature processing which lack spatial and local context to refine organ
boundaries. Furthermore, Transformers do not generalize well on small medical
imaging datasets and rely on large-scale pre-training due to limited inductive
biases. To address these problems, we demonstrate the design of a compact and
accurate Transformer network for MISS, CS-Unet, which introduces convolutions
in a multi-stage design for hierarchically enhancing spatial and local modeling
ability of Transformers. This is mainly achieved by our well-designed
Convolutional Swin Transformer (CST) block which merges convolutions with
Multi-Head Self-Attention and Feed-Forward Networks for providing inherent
localized spatial context and inductive biases. Experiments demonstrate CS-Unet
without pre-training outperforms other counterparts by large margins on
multi-organ and cardiac datasets with fewer parameters and achieves
state-of-the-art performance. Our code is available at Github.",,,arXiv,,,2022-10-14,2022,,,,,,All OA; Green,Preprint,"Liu, Qianying; Kaul, Chaitanya; Wang, Jun; Anagnostopoulos, Christos; Murray-Smith, Roderick; Deligianni, Fani","Liu, Qianying (); Kaul, Chaitanya (); Wang, Jun (); Anagnostopoulos, Christos (); Murray-Smith, Roderick (); Deligianni, Fani ()",,"Liu, Qianying (); Kaul, Chaitanya (); Wang, Jun (); Anagnostopoulos, Christos (); Murray-Smith, Roderick (); Deligianni, Fani ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151979824,46 Information and Computing Sciences; 4611 Machine Learning,
1603,pub.1140549364,10.48550/arxiv.2108.08537,,,Multi-task Federated Learning for Heterogeneous Pancreas Segmentation,"Federated learning (FL) for medical image segmentation becomes more
challenging in multi-task settings where clients might have different
categories of labels represented in their data. For example, one client might
have patient data with ""healthy'' pancreases only while datasets from other
clients may contain cases with pancreatic tumors. The vanilla federated
averaging algorithm makes it possible to obtain more generalizable deep
learning-based segmentation models representing the training data from multiple
institutions without centralizing datasets. However, it might be sub-optimal
for the aforementioned multi-task scenarios. In this paper, we investigate
heterogeneous optimization methods that show improvements for the automated
segmentation of pancreas and pancreatic tumors in abdominal CT images with FL
settings.",,,arXiv,,,2021-08-19,2021,,,,,,All OA; Green,Preprint,"Shen, Chen; Wang, Pochuan; Roth, Holger R.; Yang, Dong; Xu, Daguang; Oda, Masahiro; Wang, Weichung; Fuh, Chiou-Shann; Chen, Po-Ting; Liu, Kao-Lang; Liao, Wei-Chih; Mori, Kensaku","Shen, Chen (); Wang, Pochuan (); Roth, Holger R. (); Yang, Dong (); Xu, Daguang (); Oda, Masahiro (); Wang, Weichung (); Fuh, Chiou-Shann (); Chen, Po-Ting (); Liu, Kao-Lang (); Liao, Wei-Chih (); Mori, Kensaku ()",,"Shen, Chen (); Wang, Pochuan (); Roth, Holger R. (); Yang, Dong (); Xu, Daguang (); Oda, Masahiro (); Wang, Weichung (); Fuh, Chiou-Shann (); Chen, Po-Ting (); Liu, Kao-Lang (); Liao, Wei-Chih (); Mori, Kensaku ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140549364,46 Information and Computing Sciences; 4611 Machine Learning,
1603,pub.1086113928,10.1007/978-3-319-59129-2_4,,,Robust Abdominal Organ Segmentation Using Regional Convolutional Neural Networks,"A fully automatic system for abdominal organ segmentation is presented. As a first step, an organ localization is obtained via a robust and efficient feature registration method where the center of the organ is estimated together with a region of interest surrounding the center. Then, a convolutional neural network performing voxelwise classification is applied. The convolutional neural network consists of several full 3D convolutional layers and takes both low and high resolution image data as input, which is designed to ensure both local and global consistency. Despite limited training data, our experimental results are on par with state-of-the-art approaches that have been developed over many years. More specifically the method is applied to the MICCAI2015 challenge “Multi-Atlas Labeling Beyond the Cranial Vault” in the free competition for organ segmentation in the abdomen. It achieved the best results for 3 out of the 13 organs with a total mean Dice coefficient of \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {0.757}$$\end{document} for all organs. Top scores were achieved for the gallbladder, the aorta and the right adrenal gland.",,,Lecture Notes in Computer Science,Image Analysis,,2017-05-19,2017,2017-05-19,2017,10270,,41-52,All OA; Green,Chapter,"Larsson, Måns; Zhang, Yuhang; Kahl, Fredrik","Larsson, Måns (Chalmers University of Technology, Gothenburg, Sweden); Zhang, Yuhang (Chalmers University of Technology, Gothenburg, Sweden); Kahl, Fredrik (Chalmers University of Technology, Gothenburg, Sweden; Centre for Mathematical Sciences, Lund University, Lund, Sweden)","Larsson, Måns (Chalmers University of Technology)","Larsson, Måns (Chalmers University of Technology); Zhang, Yuhang (Chalmers University of Technology); Kahl, Fredrik (Chalmers University of Technology; Lund University)",14,4,,3.77,http://publications.lib.chalmers.se/records/fulltext/250295/local_250295.pdf,https://app.dimensions.ai/details/publication/pub.1086113928,46 Information and Computing Sciences; 4611 Machine Learning,
1603,pub.1139662333,10.48550/arxiv.2107.05471,,,The Power of Proxy Data and Proxy Networks for Hyper-Parameter  Optimization in Medical Image Segmentation,"Deep learning models for medical image segmentation are primarily
data-driven. Models trained with more data lead to improved performance and
generalizability. However, training is a computationally expensive process
because multiple hyper-parameters need to be tested to find the optimal setting
for best performance. In this work, we focus on accelerating the estimation of
hyper-parameters by proposing two novel methodologies: proxy data and proxy
networks. Both can be useful for estimating hyper-parameters more efficiently.
We test the proposed techniques on CT and MR imaging modalities using
well-known public datasets. In both cases using one dataset for building proxy
data and another data source for external evaluation. For CT, the approach is
tested on spleen segmentation with two datasets. The first dataset is from the
medical segmentation decathlon (MSD), where the proxy data is constructed, the
secondary dataset is utilized as an external validation dataset. Similarly, for
MR, the approach is evaluated on prostate segmentation where the first dataset
is from MSD and the second dataset is PROSTATEx. First, we show higher
correlation to using full data for training when testing on the external
validation set using smaller proxy data than a random selection of the proxy
data. Second, we show that a high correlation exists for proxy networks when
compared with the full network on validation Dice score. Third, we show that
the proposed approach of utilizing a proxy network can speed up an AutoML
framework for hyper-parameter search by 3.3x, and by 4.4x if proxy data and
proxy network are utilized together.",,,arXiv,,,2021-07-12,2021,,,,,,All OA; Green,Preprint,"Nath, Vishwesh; Yang, Dong; Hatamizadeh, Ali; Abidin, Anas A.; Myronenko, Andriy; Roth, Holger; Xu, Daguang","Nath, Vishwesh (); Yang, Dong (); Hatamizadeh, Ali (); Abidin, Anas A. (); Myronenko, Andriy (); Roth, Holger (); Xu, Daguang ()",,"Nath, Vishwesh (); Yang, Dong (); Hatamizadeh, Ali (); Abidin, Anas A. (); Myronenko, Andriy (); Roth, Holger (); Xu, Daguang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139662333,46 Information and Computing Sciences; 51 Physical Sciences; 5105 Medical and Biological Physics,
1601,pub.1151817536,10.48550/arxiv.2210.05151,,,UGformer for Robust Left Atrium and Scar Segmentation Across Scanners,"Thanks to the capacity for long-range dependencies and robustness to
irregular shapes, vision transformers and deformable convolutions are emerging
as powerful vision techniques of segmentation.Meanwhile, Graph Convolution
Networks (GCN) optimize local features based on global topological relationship
modeling. Particularly, they have been proved to be effective in addressing
issues in medical imaging segmentation tasks including multi-domain
generalization for low-quality images. In this paper, we present a novel,
effective, and robust framework for medical image segmentation, namely,
UGformer. It unifies novel transformer blocks, GCN bridges, and convolution
decoders originating from U-Net to predict left atriums (LAs) and LA scars. We
have identified two appealing findings of the proposed UGformer: 1). an
enhanced transformer module with deformable convolutions to improve the
blending of the transformer information with convolutional information and help
predict irregular LAs and scar shapes. 2). Using a bridge incorporating GCN to
further overcome the difficulty of capturing condition inconsistency across
different Magnetic Resonance Images scanners with various inconsistent domain
information. The proposed UGformer model exhibits outstanding ability to
segment the left atrium and scar on the LAScarQS 2022 dataset, outperforming
several recent state-of-the-arts.",,,arXiv,,,2022-10-11,2022,,,,,,All OA; Green,Preprint,"Liu, Tianyi; Hou, Size; Zhu, Jiayuan; Zhao, Zilong; Jiang, Haochuan","Liu, Tianyi (); Hou, Size (); Zhu, Jiayuan (); Zhao, Zilong (); Jiang, Haochuan ()",,"Liu, Tianyi (); Hou, Size (); Zhu, Jiayuan (); Zhao, Zilong (); Jiang, Haochuan ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151817536,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",
1601,pub.1141326833,10.1007/978-3-030-87199-4_43,,,The Power of Proxy Data and Proxy Networks for Hyper-parameter Optimization in Medical Image Segmentation,"Deep learning models for medical image segmentation are primarily data-driven. Models trained with more data lead to improved performance and generalizability. However, training is a computationally expensive process because multiple hyper-parameters need to be tested to find the optimal setting for best performance. In this work, we focus on accelerating the estimation of hyper-parameters by proposing two novel methodologies: proxy data and proxy networks. Both can be useful for estimating hyper-parameters more efficiently. We test the proposed techniques on CT and MR imaging modalities using well-known public datasets. In both cases using one dataset for building proxy data and another data source for external evaluation. For CT, the approach is tested on spleen segmentation with two datasets. The first dataset is from the medical segmentation decathlon (MSD), where the proxy data is constructed, the secondary dataset is utilized as an external validation dataset. Similarly, for MR, the approach is evaluated on prostate segmentation where the first dataset is from MSD and the second dataset is PROSTATEx. First, we show higher correlation to using full data for training when testing on the external validation set using smaller proxy data than a random selection of the proxy data. Second, we show that a high correlation exists for proxy networks when compared with the full network on validation Dice score. Third, we show that the proposed approach of utilizing a proxy network can speed up an AutoML framework for hyper-parameter search by 3.3×\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}, and by 4.4×\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document} if proxy data and proxy network are utilized together.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12903,,456-465,All OA; Green,Chapter,"Nath, Vishwesh; Yang, Dong; Hatamizadeh, Ali; Abidin, Anas A.; Myronenko, Andriy; Roth, Holger R.; Xu, Daguang","Nath, Vishwesh (NVIDIA, Santa Clara, USA); Yang, Dong (NVIDIA, Santa Clara, USA); Hatamizadeh, Ali (NVIDIA, Santa Clara, USA); Abidin, Anas A. (NVIDIA, Santa Clara, USA); Myronenko, Andriy (NVIDIA, Santa Clara, USA); Roth, Holger R. (NVIDIA, Santa Clara, USA); Xu, Daguang (NVIDIA, Santa Clara, USA)","Nath, Vishwesh (Nvidia (United States))","Nath, Vishwesh (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Hatamizadeh, Ali (Nvidia (United States)); Abidin, Anas A. (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Roth, Holger R. (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",4,4,,,http://arxiv.org/pdf/2107.05471,https://app.dimensions.ai/details/publication/pub.1141326833,46 Information and Computing Sciences,
1601,pub.1139702105,10.48550/arxiv.2107.05938,,,Learning from Partially Overlapping Labels: Image Segmentation under  Annotation Shift,"Scarcity of high quality annotated images remains a limiting factor for
training accurate image segmentation models. While more and more annotated
datasets become publicly available, the number of samples in each individual
database is often small. Combining different databases to create larger amounts
of training data is appealing yet challenging due to the heterogeneity as a
result of differences in data acquisition and annotation processes, often
yielding incompatible or even conflicting information. In this paper, we
investigate and propose several strategies for learning from partially
overlapping labels in the context of abdominal organ segmentation. We find that
combining a semi-supervised approach with an adaptive cross entropy loss can
successfully exploit heterogeneously annotated data and substantially improve
segmentation accuracy compared to baseline and alternative approaches.",,,arXiv,,,2021-07-13,2021,,,,,,All OA; Green,Preprint,"Filbrandt, Gregory; Kamnitsas, Konstantinos; Bernstein, David; Taylor, Alexandra; Glocker, Ben","Filbrandt, Gregory (); Kamnitsas, Konstantinos (); Bernstein, David (); Taylor, Alexandra (); Glocker, Ben ()",,"Filbrandt, Gregory (); Kamnitsas, Konstantinos (); Bernstein, David (); Taylor, Alexandra (); Glocker, Ben ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139702105,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1600,pub.1151032966,10.1007/978-3-031-16440-8_62,,,Context-Aware Voxel-Wise Contrastive Learning for Label Efficient Multi-organ Segmentation,"Medical image segmentation is a prerequisite for many clinical applications including disease diagnosis, surgical planning and computer assisted interventions. Due to the challenges in obtaining expert-level accurate, densely annotated multi-organ dataset, existing datasets for multi-organ segmentation either have small number of samples, or only have annotations of a few organs instead of all organs, which are termed as partially labeled data. There exist previous attempts to develop label efficient segmentation method to make use of these partially labeled dataset for improving the performance of multi-organ segmentation. However, most of these methods suffer from the limitation that they only use the labeled information in the dataset without taking advantage of the large amount of unlabeled data. To this end, we propose a context-aware voxel-wise contrastive learning method to take full advantage of both labeled and unlabeled data in partially labeled dataset for an improvement of multi-organ segmentation performance. Experimental Results demonstrated that our proposed method achieved superior performance than other state-of-the-art methods.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13434,,653-662,Closed,Chapter,"Liu, Peng; Zheng, Guoyan","Liu, Peng (Institute of Medical Robotics, School of Biomedical Engineering, Shanghai Jiao Tong University, No. 800, Dongchuan Road, 200240, Shanghai, China); Zheng, Guoyan (Institute of Medical Robotics, School of Biomedical Engineering, Shanghai Jiao Tong University, No. 800, Dongchuan Road, 200240, Shanghai, China)","Zheng, Guoyan (Shanghai Jiao Tong University)","Liu, Peng (Shanghai Jiao Tong University); Zheng, Guoyan (Shanghai Jiao Tong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151032966,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1600,pub.1141327111,10.1007/978-3-030-87722-4_12,,,Learning from Partially Overlapping Labels: Image Segmentation Under Annotation Shift,"Scarcity of high quality annotated images remains a limiting factor for training accurate image segmentation models. While more and more annotated datasets become publicly available, the number of samples in each individual database is often small. Combining different databases to create larger amounts of training data is appealing yet challenging due to the heterogeneity as a result of differences in data acquisition and annotation processes, often yielding incompatible or even conflicting information. In this paper, we investigate and propose several strategies for learning from partially overlapping labels in the context of abdominal organ segmentation. We find that combining a semi-supervised approach with an adaptive cross entropy loss can successfully exploit heterogeneously annotated data and substantially improve segmentation accuracy compared to baseline and alternative approaches.","This work received funding from the UKRI London Medical Imaging &amp; Artificial Intelligence Centre for Value Based Healthcare and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 757173, project MIRA, ERC-2017-STG). AT receives a grant from Lady Garden Foundation.",,Lecture Notes in Computer Science,"Domain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health",,2021-09-21,2021,2021-09-21,2021,12968,,123-132,All OA; Green,Chapter,"Filbrandt, Gregory; Kamnitsas, Konstantinos; Bernstein, David; Taylor, Alexandra; Glocker, Ben","Filbrandt, Gregory (Department of Computing, Imperial College London, London, UK); Kamnitsas, Konstantinos (Department of Computing, Imperial College London, London, UK); Bernstein, David (Joint Department of Physics, The Institute of Cancer Research and The Royal Marsden NHS Foundation Trust, London, UK); Taylor, Alexandra (Department of Radiotherapy, The Royal Marsden NHS Foundation Trust, London, UK); Glocker, Ben (Department of Computing, Imperial College London, London, UK)","Glocker, Ben (Imperial College London)","Filbrandt, Gregory (Imperial College London); Kamnitsas, Konstantinos (Imperial College London); Bernstein, David (Institute of Cancer Research); Taylor, Alexandra (Royal Marsden NHS Foundation Trust); Glocker, Ben (Imperial College London)",0,0,,0.0,http://arxiv.org/pdf/2107.05938,https://app.dimensions.ai/details/publication/pub.1141327111,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1600,pub.1151694531,10.1007/978-3-031-18523-6_6,,,Joint Multi Organ and Tumor Segmentation from Partial Labels Using Federated Learning,"Segmentation studies in medical image analysis are always associated with a particular task scenario. However, building datasets to train models to segment multiple types of organs and pathologies is challenging. For example, a dataset annotated for the pancreas and pancreatic tumors will result in a model that cannot segment other organs, like the liver and spleen, visible in the same abdominal computed tomography image. The lack of a well-annotated dataset is one limitation resulting in a lack of universal segmentation models. Federated learning (FL) is ideally suited for addressing this issue in the real-world context. In this work, we show that each medical center can use training data for distinct tasks to collaboratively build more generalizable segmentation models for multiple segmentation tasks without the requirement to centralize datasets in one place. The main challenge of this research is the heterogeneity of training data from various institutions and segmentation tasks. In this paper, we propose a multi-task segmentation framework using FL to learn segmentation models using several independent datasets with different annotations of organs or tumors. We include experiments on four publicly available single-task datasets, including MSD liver (w/ tumor), MSD spleen, MSD pancreas (w/ tumor), and KITS19. Experimental results on an external validation set to highlight the advantages of employing FL in multi-task organ and tumor segmentation.",,,Lecture Notes in Computer Science,"Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health",,2022-10-07,2022,2022-10-07,2022,13573,,58-67,Closed,Chapter,"Shen, Chen; Wang, Pochuan; Yang, Dong; Xu, Daguang; Oda, Masahiro; Chen, Po-Ting; Liu, Kao-Lang; Liao, Wei-Chih; Fuh, Chiou-Shann; Mori, Kensaku; Wang, Weichung; Roth, Holger R.","Shen, Chen (Nagoya University, Nagoya, Japan); Wang, Pochuan (National Taiwan University, Taipei, Taiwan); Yang, Dong (NVIDIA Corporation, Santa Clara, USA); Xu, Daguang (NVIDIA Corporation, Santa Clara, USA); Oda, Masahiro (Nagoya University, Nagoya, Japan); Chen, Po-Ting (National Taiwan University Hospital, Taipei, Taiwan); Liu, Kao-Lang (National Taiwan University Hospital, Taipei, Taiwan); Liao, Wei-Chih (National Taiwan University Hospital, Taipei, Taiwan); Fuh, Chiou-Shann (National Taiwan University, Taipei, Taiwan); Mori, Kensaku (Nagoya University, Nagoya, Japan); Wang, Weichung (National Taiwan University, Taipei, Taiwan); Roth, Holger R. (NVIDIA Corporation, Santa Clara, USA)","Wang, Weichung (National Taiwan University)","Shen, Chen (Nagoya University); Wang, Pochuan (National Taiwan University); Yang, Dong (Nvidia (United States)); Xu, Daguang (Nvidia (United States)); Oda, Masahiro (Nagoya University); Chen, Po-Ting (National Taiwan University Hospital); Liu, Kao-Lang (National Taiwan University Hospital); Liao, Wei-Chih (National Taiwan University Hospital); Fuh, Chiou-Shann (National Taiwan University); Mori, Kensaku (Nagoya University); Wang, Weichung (National Taiwan University); Roth, Holger R. (Nvidia (United States))",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151694531,46 Information and Computing Sciences; 4611 Machine Learning,
1600,pub.1142567293,10.1007/978-3-030-90874-4_10,,,Multi-task Federated Learning for Heterogeneous Pancreas Segmentation,"Federated learning (FL) for medical image segmentation becomes more challenging in multi-task settings where clients might have different categories of labels represented in their data. For example, one client might have patient data with “healthy” pancreases only while datasets from other clients may contain cases with pancreatic tumors. The vanilla federated averaging algorithm makes it possible to obtain more generalizable deep learning-based segmentation models representing the training data from multiple institutions without centralizing datasets. However, it might be sub-optimal for the aforementioned multi-task scenarios. In this paper, we investigate heterogeneous optimization methods that show improvements for the automated segmentation of pancreas and pancreatic tumors in abdominal CT images with FL settings.","Parts of this research was supported by the MEXT/JSPS KAKENHI (894030, 17H00867).",,Lecture Notes in Computer Science,"Clinical Image-Based Procedures, Distributed and Collaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning",,2021-11-14,2021,2021-11-14,2021,12969,,101-110,All OA; Green,Chapter,"Shen, Chen; Wang, Pochuan; Roth, Holger R.; Yang, Dong; Xu, Daguang; Oda, Masahiro; Wang, Weichung; Fuh, Chiou-Shann; Chen, Po-Ting; Liu, Kao-Lang; Liao, Wei-Chih; Mori, Kensaku","Shen, Chen (Nagoya University, Nagoya, Japan); Wang, Pochuan (National Taiwan University, Taipei, Taiwan); Roth, Holger R. (NVIDIA Corporation, Santa Clara, USA); Yang, Dong (NVIDIA Corporation, Santa Clara, USA); Xu, Daguang (NVIDIA Corporation, Santa Clara, USA); Oda, Masahiro (Nagoya University, Nagoya, Japan); Wang, Weichung (National Taiwan University, Taipei, Taiwan); Fuh, Chiou-Shann (National Taiwan University, Taipei, Taiwan); Chen, Po-Ting (National Taiwan University Hospital, Taipei, Taiwan); Liu, Kao-Lang (National Taiwan University Hospital, Taipei, Taiwan); Liao, Wei-Chih (National Taiwan University Hospital, Taipei, Taiwan); Mori, Kensaku (Nagoya University, Nagoya, Japan)","Wang, Weichung (National Taiwan University)","Shen, Chen (Nagoya University); Wang, Pochuan (National Taiwan University); Roth, Holger R. (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Xu, Daguang (Nvidia (United States)); Oda, Masahiro (Nagoya University); Wang, Weichung (National Taiwan University); Fuh, Chiou-Shann (National Taiwan University); Chen, Po-Ting (National Taiwan University Hospital); Liu, Kao-Lang (National Taiwan University Hospital); Liao, Wei-Chih (National Taiwan University Hospital); Mori, Kensaku (Nagoya University)",5,5,,4.09,http://arxiv.org/pdf/2108.08537,https://app.dimensions.ai/details/publication/pub.1142567293,46 Information and Computing Sciences; 4611 Machine Learning,
1539,pub.1141301976,10.1007/978-3-030-87193-2_40,,,Selective Learning from External Data for CT Image Segmentation,"Learning from external data is an effective and efficient way of training deep networks, which can substantially alleviate the burden on collecting training data and annotations. It is of great significance in improving the performance of CT image segmentation tasks, where collecting a large amount of voxel-wise annotations is expensive or even impractical. In this paper, we propose a generic selective learning method to maximize the performance gains of harnessing external data in CT image segmentation. The key idea is to learn a weight for each external data such that ‘good’ data can have large weights and thus contribute more to the training loss, thereby implicitly encouraging the network to mine more valuable knowledge from informative external data while suppressing to memorize irrelevant patterns from ‘useless’ or even ‘harmful’ data. Particularly, we formulate our idea as a constrained non-linear programming problem, solved by an iterative solution that alternatively conducts weights estimating and network updating. Extensive experiments on abdominal multi-organ CT segmentation datasets show the efficacy and performance gains of our method against existing methods. The code is publicly available (Released at https://github.com/YouyiSong/Codes-for-Selective-Learning).","The work described in this paper is supported by two grants from the Hong Kong Research Grants Council under General Research Fund scheme (Project No. PolyU 152035/17E and 15205919), and a grant from HKU Startup Fund and HKU Seed Fund for Basic Research (Project No. 202009185079).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12901,,420-430,Closed,Chapter,"Song, Youyi; Yu, Lequan; Lei, Baiying; Choi, Kup-Sze; Qin, Jing","Song, Youyi (Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong, China); Yu, Lequan (Department of Statistics and Actuarial Science, The University of Hong Kong, Hong Kong, China); Lei, Baiying (School of Biomedical Engineering, Shenzhen University, Shenzhen, China); Choi, Kup-Sze (Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong, China); Qin, Jing (Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong, China)","Song, Youyi (Hong Kong Polytechnic University)","Song, Youyi (Hong Kong Polytechnic University); Yu, Lequan (University of Hong Kong); Lei, Baiying (Shenzhen University); Choi, Kup-Sze (Hong Kong Polytechnic University); Qin, Jing (Hong Kong Polytechnic University)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141301976,46 Information and Computing Sciences; 4611 Machine Learning,
1539,pub.1154711687,10.1007/978-3-031-23911-3_25,,,Combining Self-training and Hybrid Architecture for Semi-supervised Abdominal Organ Segmentation,"Abdominal organ segmentation has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis. However, manually annotating organs from CT scans is time-consuming and labor-intensive. Semi-supervised learning has shown the potential to alleviate this challenge by learning from a large set of unlabeled images and limited labeled samples. In this work, we follow the self-training strategy and employ a high-performance hybrid architecture (PHTrans) consisting of CNN and Swin Transformer for the teacher model to generate precise pseudo labels for unlabeled data. Afterward, we introduce them with labeled data together into a two-stage segmentation framework with lightweight PHTrans for training to improve the performance and generalization ability of the model while remaining efficient. Experiments on the validation set of FLARE2022 demonstrate that our method achieves excellent segmentation performance as well as fast and low-resource model inference. The average DSC and NSD are 0.8956 and 0.9316, respectively. Under our development environments, the average inference time is 18.62 s, the average maximum GPU memory is 1995.04 MB, and the area under the GPU memory-time curve and the average area under the CPU utilization-time curve are 23196.84 and 319.67. The code is available at https://github.com/lseventeen/FLARE22-TwoStagePHTrans.",The authors of this paper declare that the segmentation method they implemented for participation in the FLARE 2022 challenge has not used any pre-trained models nor additional datasets other than those provided by the organizers. The proposed solution is fully automatic without any manual intervention.,,Lecture Notes in Computer Science,Fast and Low-Resource Semi-supervised Abdominal Organ Segmentation,,2022,2022,2023-01-21,2022,13816,,281-292,All OA; Green,Chapter,"Liu, Wentao; Xu, Weijin; Yan, Songlin; Wang, Lemeng; Li, Haoyuan; Yang, Huihua","Liu, Wentao (School of Artificial Intelligence, Beijing University of Posts and Telecommunications, 100876, Beijing, China); Xu, Weijin (School of Artificial Intelligence, Beijing University of Posts and Telecommunications, 100876, Beijing, China); Yan, Songlin (School of Artificial Intelligence, Beijing University of Posts and Telecommunications, 100876, Beijing, China); Wang, Lemeng (School of Artificial Intelligence, Beijing University of Posts and Telecommunications, 100876, Beijing, China); Li, Haoyuan (School of Artificial Intelligence, Beijing University of Posts and Telecommunications, 100876, Beijing, China); Yang, Huihua (School of Artificial Intelligence, Beijing University of Posts and Telecommunications, 100876, Beijing, China; School of Computer Science and Information Security, Guilin University of Electronic Technology, 541004, Guilin, China)","Yang, Huihua (Beijing University of Posts and Telecommunications; Guilin University of Electronic Technology)","Liu, Wentao (Beijing University of Posts and Telecommunications); Xu, Weijin (Beijing University of Posts and Telecommunications); Yan, Songlin (Beijing University of Posts and Telecommunications); Wang, Lemeng (Beijing University of Posts and Telecommunications); Li, Haoyuan (Beijing University of Posts and Telecommunications); Yang, Huihua (Beijing University of Posts and Telecommunications; Guilin University of Electronic Technology)",0,0,,,http://arxiv.org/pdf/2207.11512,https://app.dimensions.ai/details/publication/pub.1154711687,46 Information and Computing Sciences; 4611 Machine Learning,
1539,pub.1146094497,10.48550/arxiv.2203.02098,,,Universal Segmentation of 33 Anatomies,"In the paper, we present an approach for learning a single model that
universally segments 33 anatomical structures, including vertebrae, pelvic
bones, and abdominal organs. Our model building has to address the following
challenges. Firstly, while it is ideal to learn such a model from a
large-scale, fully-annotated dataset, it is practically hard to curate such a
dataset. Thus, we resort to learn from a union of multiple datasets, with each
dataset containing the images that are partially labeled. Secondly, along the
line of partial labelling, we contribute an open-source, large-scale vertebra
segmentation dataset for the benefit of spine analysis community, CTSpine1K,
boasting over 1,000 3D volumes and over 11K annotated vertebrae. Thirdly, in a
3D medical image segmentation task, due to the limitation of GPU memory, we
always train a model using cropped patches as inputs instead a whole 3D volume,
which limits the amount of contextual information to be learned. To this, we
propose a cross-patch transformer module to fuse more information in adjacent
patches, which enlarges the aggregated receptive field for improved
segmentation performance. This is especially important for segmenting, say, the
elongated spine. Based on 7 partially labeled datasets that collectively
contain about 2,800 3D volumes, we successfully learn such a universal model.
Finally, we evaluate the universal model on multiple open-source datasets,
proving that our model has a good generalization performance and can
potentially serve as a solid foundation for downstream tasks.",,,arXiv,,,2022-03-03,2022,,,,,,All OA; Green,Preprint,"Liu, Pengbo; Deng, Yang; Wang, Ce; Hui, Yuan; Li, Qian; Li, Jun; Luo, Shiwei; Sun, Mengke; Quan, Quan; Yang, Shuxin; Hao, You; Xiao, Honghu; Zhao, Chunpeng; Wu, Xinbao; Zhou, S. Kevin","Liu, Pengbo (); Deng, Yang (); Wang, Ce (); Hui, Yuan (); Li, Qian (); Li, Jun (); Luo, Shiwei (); Sun, Mengke (); Quan, Quan (); Yang, Shuxin (); Hao, You (); Xiao, Honghu (); Zhao, Chunpeng (); Wu, Xinbao (); Zhou, S. Kevin ()",,"Liu, Pengbo (); Deng, Yang (); Wang, Ce (); Hui, Yuan (); Li, Qian (); Li, Jun (); Luo, Shiwei (); Sun, Mengke (); Quan, Quan (); Yang, Shuxin (); Hao, You (); Xiao, Honghu (); Zhao, Chunpeng (); Wu, Xinbao (); Zhou, S. Kevin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146094497,46 Information and Computing Sciences; 4611 Machine Learning,
1538,pub.1149253264,10.48550/arxiv.2207.01919,,,Vector Quantisation for Robust Segmentation,"The reliability of segmentation models in the medical domain depends on the
model's robustness to perturbations in the input space. Robustness is a
particular challenge in medical imaging exhibiting various sources of image
noise, corruptions, and domain shifts. Obtaining robustness is often attempted
via simulating heterogeneous environments, either heuristically in the form of
data augmentation or by learning to generate specific perturbations in an
adversarial manner. We propose and justify that learning a discrete
representation in a low dimensional embedding space improves robustness of a
segmentation model. This is achieved with a dictionary learning method called
vector quantisation. We use a set of experiments designed to analyse robustness
in both the latent and output space under domain shift and noise perturbations
in the input space. We adapt the popular UNet architecture, inserting a
quantisation block in the bottleneck. We demonstrate improved segmentation
accuracy and better robustness on three segmentation tasks. Code is available
at
\url{https://github.com/AinkaranSanthi/Vector-Quantisation-for-Robust-Segmentation}",,,arXiv,,,2022-07-05,2022,,,,,,All OA; Green,Preprint,"Santhirasekaram, Ainkaran; Kori, Avinash; Winkler, Mathias; Rockall, Andrea; Glocker, Ben","Santhirasekaram, Ainkaran (); Kori, Avinash (); Winkler, Mathias (); Rockall, Andrea (); Glocker, Ben ()",,"Santhirasekaram, Ainkaran (); Kori, Avinash (); Winkler, Mathias (); Rockall, Andrea (); Glocker, Ben ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149253264,46 Information and Computing Sciences; 4611 Machine Learning,
1537,pub.1148083959,10.48550/arxiv.2205.10342,,,Self-supervised 3D anatomy segmentation using self-distilled masked  image transformer (SMIT),"Vision transformers, with their ability to more efficiently model long-range
context, have demonstrated impressive accuracy gains in several computer vision
and medical image analysis tasks including segmentation. However, such methods
need large labeled datasets for training, which is hard to obtain for medical
image analysis. Self-supervised learning (SSL) has demonstrated success in
medical image segmentation using convolutional networks. In this work, we
developed a \underline{s}elf-distillation learning with \underline{m}asked
\underline{i}mage modeling method to perform SSL for vision
\underline{t}ransformers (SMIT) applied to 3D multi-organ segmentation from CT
and MRI. Our contribution is a dense pixel-wise regression within masked
patches called masked image prediction, which we combined with masked patch
token distillation as pretext task to pre-train vision transformers. We show
our approach is more accurate and requires fewer fine tuning datasets than
other pretext tasks. Unlike prior medical image methods, which typically used
image sets arising from disease sites and imaging modalities corresponding to
the target tasks, we used 3,643 CT scans (602,708 images) arising from head and
neck, lung, and kidney cancers as well as COVID-19 for pre-training and applied
it to abdominal organs segmentation from MRI pancreatic cancer patients as well
as publicly available 13 different abdominal organs segmentation from CT. Our
method showed clear accuracy improvement (average DSC of 0.875 from MRI and
0.878 from CT) with reduced requirement for fine-tuning datasets over commonly
used pretext tasks. Extensive comparisons against multiple current SSL methods
were done. Code will be made available upon acceptance for publication.",,,arXiv,,,2022-05-20,2022,,,,,,All OA; Green,Preprint,"Jiang, Jue; Tyagi, Neelam; Tringale, Kathryn; Crane, Christopher; Veeraraghavan, Harini","Jiang, Jue (); Tyagi, Neelam (); Tringale, Kathryn (); Crane, Christopher (); Veeraraghavan, Harini ()",,"Jiang, Jue (); Tyagi, Neelam (); Tringale, Kathryn (); Crane, Christopher (); Veeraraghavan, Harini ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148083959,32 Biomedical and Clinical Sciences; 46 Information and Computing Sciences; 51 Physical Sciences; 5105 Medical and Biological Physics,
1536,pub.1153158793,10.48550/arxiv.2211.14051,,,Open-Source Skull Reconstruction with MONAI,"We present a deep learning-based approach for skull reconstruction for MONAI,
which has been pre-trained on the MUG500+ skull dataset. The implementation
follows the MONAI contribution guidelines, hence, it can be easily tried out
and used, and extended by MONAI users. The primary goal of this paper lies in
the investigation of open-sourcing codes and pre-trained deep learning models
under the MONAI framework. Nowadays, open-sourcing software, especially
(pre-trained) deep learning models, has become increasingly important. Over the
years, medical image analysis experienced a tremendous transformation. Over a
decade ago, algorithms had to be implemented and optimized with low-level
programming languages, like C or C++, to run in a reasonable time on a desktop
PC, which was not as powerful as today's computers. Nowadays, users have
high-level scripting languages like Python, and frameworks like PyTorch and
TensorFlow, along with a sea of public code repositories at hand. As a result,
implementations that had thousands of lines of C or C++ code in the past, can
now be scripted with a few lines and in addition executed in a fraction of the
time. To put this even on a higher level, the Medical Open Network for
Artificial Intelligence (MONAI) framework tailors medical imaging research to
an even more convenient process, which can boost and push the whole field. The
MONAI framework is a freely available, community-supported, open-source and
PyTorch-based framework, that also enables to provide research contributions
with pre-trained models to others. Codes and pre-trained weights for skull
reconstruction are publicly available at:
https://github.com/Project-MONAI/research-contributions/tree/master/SkullRec",,,arXiv,,,2022-11-25,2022,,,,,,All OA; Green,Preprint,"Li, Jianning; Ferreira, André; Puladi, Behrus; Alves, Victor; Kamp, Michael; Kim, Moon-Sung; Nensa, Felix; Kleesiek, Jens; Ahmadi, Seyed-Ahmad; Egger, Jan","Li, Jianning (); Ferreira, André (); Puladi, Behrus (); Alves, Victor (); Kamp, Michael (); Kim, Moon-Sung (); Nensa, Felix (); Kleesiek, Jens (); Ahmadi, Seyed-Ahmad (); Egger, Jan ()",,"Li, Jianning (); Ferreira, André (); Puladi, Behrus (); Alves, Victor (); Kamp, Michael (); Kim, Moon-Sung (); Nensa, Felix (); Kleesiek, Jens (); Ahmadi, Seyed-Ahmad (); Egger, Jan ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153158793,46 Information and Computing Sciences; 4601 Applied Computing,
1536,pub.1151033114,10.1007/978-3-031-16452-1_8,,,Adversarially Robust Prototypical Few-Shot Segmentation with Neural-ODEs,"Few-shot Learning (FSL) methods are being adopted in settings where data is not abundantly available. This is especially seen in medical domains where the annotations are expensive to obtain. Deep Neural Networks have been shown to be vulnerable to adversarial attacks. This is even more severe in the case of FSL due to the lack of a large number of training examples. In this paper, we provide a framework to make few-shot segmentation models adversarially robust in the medical domain where such attacks can severely impact the decisions made by clinicians who use them. We propose a novel robust few-shot segmentation framework, Prototypical Neural Ordinary Differential Equation (PNODE), that provides defense against gradient-based adversarial attacks. We show that our framework is more robust compared to traditional adversarial defense mechanisms such as adversarial training. Adversarial training involves increased training time and shows robustness to limited types of attacks depending on the type of adversarial examples seen during training. Our proposed framework generalises well to common adversarial attacks like FGSM, PGD and SMIA while having the model parameters comparable to the existing few-shot segmentation models. We show the effectiveness of our proposed approach on three publicly available multi-organ segmentation datasets in both in-domain and cross-domain settings by attacking the support and query sets without the need for ad-hoc adversarial training.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13438,,77-87,All OA; Green,Chapter,"Pandey, Prashant; Vardhan, Aleti; Chasmai, Mustafa; Sur, Tanuj; Lall, Brejesh","Pandey, Prashant (Indian Institute of Technology Delhi, New Delhi, India); Vardhan, Aleti (Manipal Institute of Technology, Manipal, India); Chasmai, Mustafa (Indian Institute of Technology Delhi, New Delhi, India); Sur, Tanuj (Chennai Mathematical Institute, Chennai, India); Lall, Brejesh (Indian Institute of Technology Delhi, New Delhi, India)","Pandey, Prashant (Indian Institute of Technology Delhi)","Pandey, Prashant (Indian Institute of Technology Delhi); Vardhan, Aleti (Manipal Academy of Higher Education); Chasmai, Mustafa (Indian Institute of Technology Delhi); Sur, Tanuj (Chennai Mathematical Institute); Lall, Brejesh (Indian Institute of Technology Delhi)",0,0,,,http://arxiv.org/pdf/2210.03429,https://app.dimensions.ai/details/publication/pub.1151033114,46 Information and Computing Sciences; 4611 Machine Learning,
1536,pub.1141301964,10.1007/978-3-030-87193-2_3,,,Pancreas CT Segmentation by Predictive Phenotyping,"Pancreas CT segmentation offers promise at understanding the structural manifestation of metabolic conditions. To date, the medical primary record of conditions that impact the pancreas is in the electronic health record (EHR) in terms of diagnostic phenotype data (e.g., ICD-10 codes). We posit that similar structural phenotypes could be revealed by studying subjects with similar medical outcomes. Segmentation is mainly driven by imaging data, but this direct approach may not consider differing canonical appearances with different underlying conditions (e.g., pancreatic atrophy versus pancreatic cysts). To this end, we exploit clinical features from EHR data to complement image features for enhancing the pancreas segmentation, especially in high-risk outcomes. Specifically, we propose, to the best of our knowledge, the first phenotype embedding model for pancreas segmentation by predicting representatives that share similar comorbidities. Such an embedding strategy can adaptively refine the segmentation outcome based on the discriminative contexts distilled from clinical features. Experiments with 2000 patients’ EHR data and 300 CT images with the healthy pancreas, type II diabetes, and pancreatitis subjects show that segmentation by predictive phenotyping significantly improves performance over state-of-the-arts (Dice score 0.775 to 0.791, p<0.05\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p < 0.05$$\end{document}, Wilcoxon signed-rank test). The proposed method additionally achieves superior performance on two public testing datasets, BTCV MICCAI Challenge 2015 and TCIA pancreas CT. Our approach provides a promising direction of advancing segmentation with phenotype features while without requiring EHR data as input during testing.","This research is supported by NIH Common Fund and National Institute of Diabetes, Digestive and Kidney Diseases U54DK120058, NSF CAREER 1452485, NIH grants, 2R01EB006136, 1R01EB017230 (Landman), and R01NS09529. The identified datasets used for the analysis described were obtained from the Research Derivative (RD), database of clinical and related data. The imaging dataset(s) used for the analysis described were obtained from ImageVU, a research repository of medical imaging data and image-related metadata. ImageVU and RD are supported by the VICTR CTSA award (ULTR000445 from NCATS/NIH) and Vanderbilt University Medical Center institutional funding. ImageVU pilot work was also funded by PCORI (contract CDRN-1306-04869).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12901,,25-35,Closed,Chapter,"Tang, Yucheng; Gao, Riqiang; Lee, Hohin; Yang, Qi; Yu, Xin; Zhou, Yuyin; Bao, Shunxing; Huo, Yuankai; Spraggins, Jeffrey; Virostko, Jack; Xu, Zhoubing; Landman, Bennett A.","Tang, Yucheng (Vanderbilt University, 37203, Nashville, TN, USA); Gao, Riqiang (Vanderbilt University, 37203, Nashville, TN, USA); Lee, Hohin (Vanderbilt University, 37203, Nashville, TN, USA); Yang, Qi (Vanderbilt University, 37203, Nashville, TN, USA); Yu, Xin (Vanderbilt University, 37203, Nashville, TN, USA); Zhou, Yuyin (Stanford University, 94305, Stanford, CA, USA); Bao, Shunxing (Vanderbilt University, 37203, Nashville, TN, USA); Huo, Yuankai (Vanderbilt University, 37203, Nashville, TN, USA); Spraggins, Jeffrey (Vanderbilt University, 37203, Nashville, TN, USA; Vanderbilt University Medical Center, 37235, Nashville, TN, USA); Virostko, Jack (University of Texas at Austin, 78705, Austin, TX, USA); Xu, Zhoubing (Siemens Healthineers, 08540, Princeton, NJ, USA); Landman, Bennett A. (Vanderbilt University, 37203, Nashville, TN, USA; Vanderbilt University Medical Center, 37235, Nashville, TN, USA)","Tang, Yucheng (Vanderbilt University)","Tang, Yucheng (Vanderbilt University); Gao, Riqiang (Vanderbilt University); Lee, Hohin (Vanderbilt University); Yang, Qi (Vanderbilt University); Yu, Xin (Vanderbilt University); Zhou, Yuyin (Stanford University); Bao, Shunxing (Vanderbilt University); Huo, Yuankai (Vanderbilt University); Spraggins, Jeffrey (Vanderbilt University; Vanderbilt University Medical Center); Virostko, Jack (The University of Texas at Austin); Xu, Zhoubing (Siemens Healthcare (United States)); Landman, Bennett A. (Vanderbilt University; Vanderbilt University Medical Center)",4,4,,,,https://app.dimensions.ai/details/publication/pub.1141301964,46 Information and Computing Sciences,
1535,pub.1129504481,10.48550/arxiv.2007.09669,,,Unified cross-modality feature disentangler for unsupervised  multi-domain MRI abdomen organs segmentation,"Our contribution is a unified cross-modality feature disentagling approach
for multi-domain image translation and multiple organ segmentation. Using CT as
the labeled source domain, our approach learns to segment multi-modal
(T1-weighted and T2-weighted) MRI having no labeled data. Our approach uses a
variational auto-encoder (VAE) to disentangle the image content from style. The
VAE constrains the style feature encoding to match a universal prior (Gaussian)
that is assumed to span the styles of all the source and target modalities. The
extracted image style is converted into a latent style scaling code, which
modulates the generator to produce multi-modality images according to the
target domain code from the image content features. Finally, we introduce a
joint distribution matching discriminator that combines the translated images
with task-relevant segmentation probability maps to further constrain and
regularize image-to-image (I2I) translations. We performed extensive
comparisons to multiple state-of-the-art I2I translation and segmentation
methods. Our approach resulted in the lowest average multi-domain image
reconstruction error of 1.34$\pm$0.04. Our approach produced an average Dice
similarity coefficient (DSC) of 0.85 for T1w and 0.90 for T2w MRI for
multi-organ segmentation, which was highly comparable to a fully supervised MRI
multi-organ segmentation network (DSC of 0.86 for T1w and 0.90 for T2w MRI).",,,arXiv,,,2020-07-19,2020,,,,,,All OA; Green,Preprint,"Jiang, Jue; Veeraraghavan, Harini","Jiang, Jue (); Veeraraghavan, Harini ()",,"Jiang, Jue (); Veeraraghavan, Harini ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129504481,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 51 Physical Sciences,
1531,pub.1154197453,10.48550/arxiv.2301.00785,,,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,"An increasing number of public datasets have shown a marked clinical impact
on assessing anatomical structures. However, each of the datasets is small,
partially labeled, and rarely investigates severe tumor subjects. Moreover,
current models are limited to segmenting specific organs/tumors, which can not
be extended to novel domains and classes. To tackle these limitations, we
introduce embedding learned from Contrastive Language-Image Pre-training (CLIP)
to segmentation models, dubbed the CLIP-Driven Universal Model. The Universal
Model can better segment 25 organs and 6 types of tumors by exploiting the
semantic relationship between abdominal structures. The model is developed from
an assembly of 14 datasets with 3,410 CT scans and evaluated on 6,162 external
CT scans from 3 datasets. We achieve the state-of-the-art results on Beyond The
Cranial Vault (BTCV). Compared with dataset-specific models, the Universal
Model is computationally more efficient (6x faster), generalizes better to CT
scans from varying sites, and shows stronger transfer learning performance on
novel tasks. The design of CLIP embedding enables the Universal Model to be
easily extended to new classes without catastrophically forgetting the
previously learned classes.",,,arXiv,,,2023-01-02,2023,,,,,,All OA; Green,Preprint,"Liu, Jie; Zhang, Yixiao; Chen, Jie-Neng; Xiao, Junfei; Lu, Yongyi; Landman, Bennett A.; Yuan, Yixuan; Yuille, Alan; Tang, Yucheng; Zhou, Zongwei","Liu, Jie (); Zhang, Yixiao (); Chen, Jie-Neng (); Xiao, Junfei (); Lu, Yongyi (); Landman, Bennett A. (); Yuan, Yixuan (); Yuille, Alan (); Tang, Yucheng (); Zhou, Zongwei ()",,"Liu, Jie (); Zhang, Yixiao (); Chen, Jie-Neng (); Xiao, Junfei (); Lu, Yongyi (); Landman, Bennett A. (); Yuan, Yixuan (); Yuille, Alan (); Tang, Yucheng (); Zhou, Zongwei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154197453,46 Information and Computing Sciences; 4611 Machine Learning,
1483,pub.1144687130,10.1109/bibm52615.2021.9669727,,,PoissonSeg: Semi-Supervised Few-Shot Medical Image Segmentation via Poisson Learning,"The application of deep learning to medical image segmentation has been hampered due to the lack of abundant pixel-level annotated data. Few-shot Semantic Segmentation (FSS) is a promising strategy for breaking the deadlock. However, a high-performing FSS model still requires sufficient pixel-level annotated classes for training to avoid overfitting, which leads to its performance bottleneck in medical image segmentation due to the unmet need for annotations. Thus, semi-supervised FSS for medical images is accordingly proposed to utilize unlabeled data for further performance improvement. Nevertheless, existing semi-supervised FSS methods has two obvious defects: (1) neglecting the relationship between the labeled and unlabeled data; (2) using unlabeled data directly for end-to-end training leads to degenerated representation learning. To address these problems, we propose a novel semi-supervised FSS framework for medical image segmentation. The proposed framework employs Poisson learning for modeling data relationship and propagating supervision signals, and Spatial Consistency Calibration for encouraging the model to learn more coherent representations. In this process, unlabeled samples do not involve in end-to-end training, but provide supervisory information for query image segmentation through graph-based learning. We conduct extensive experiments on three medical image segmentation datasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for MRI and abdominal organs segmentation for CT) to demonstrate the state-of-the-art performance and broad applicability of the proposed framework.","This work was supported by the General Program of National Natural Science Foundation of China (NSFC) under Grant No. 61806147 and No. 62102259, and the Shanghai Sailing Program (21YF1431600).",,,2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),,2021-12-12,2021,,2021-12-12,00,,1513-1518,All OA; Green,Proceeding,"Shen, Xiaoang; Zhang, Guokai; Lai, Huilin; Luo, Jihao; Lu, Jianwei; Luo, Ye","Shen, Xiaoang (School of Software Engineering, Tongji University, Shanghai, China); Zhang, Guokai (School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China); Lai, Huilin (School of Software Engineering, Tongji University, Shanghai, China); Luo, Jihao (School of Computing, National University of Singapore, Singapore); Lu, Jianwei (School of Software Engineering, Tongji University, Shanghai, China); Luo, Ye (School of Software Engineering, Tongji University, Shanghai, China)","Lu, Jianwei (Tongji University); Luo, Ye (Tongji University)","Shen, Xiaoang (Tongji University); Zhang, Guokai (University of Shanghai for Science and Technology); Lai, Huilin (Tongji University); Luo, Jihao (National University of Singapore); Lu, Jianwei (Tongji University); Luo, Ye (Tongji University)",2,2,,1.59,http://arxiv.org/pdf/2108.11694,https://app.dimensions.ai/details/publication/pub.1144687130,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1480,pub.1136235371,10.48550/arxiv.2103.04526,,,Incremental Learning for Multi-organ Segmentation with Partially Labeled  Datasets,"There exists a large number of datasets for organ segmentation, which are
partially annotated, and sequentially constructed. A typical dataset is
constructed at a certain time by curating medical images and annotating the
organs of interest. In other words, new datasets with annotations of new organ
categories are built over time. To unleash the potential behind these partially
labeled, sequentially-constructed datasets, we propose to learn a multi-organ
segmentation model through incremental learning (IL). In each IL stage, we lose
access to the previous annotations, whose knowledge is assumingly captured by
the current model, and gain the access to a new dataset with annotations of new
organ categories, from which we learn to update the organ segmentation model to
include the new organs. We give the first attempt to conjecture that the
different distribution is the key reason for 'catastrophic forgetting' that
commonly exists in IL methods, and verify that IL has the natural adaptability
to medical image scenarios. Extensive experiments on five open-sourced datasets
are conducted to prove the effectiveness of our method and the conjecture
mentioned above.",,,arXiv,,,2021-03-07,2021,,,,,,All OA; Green,Preprint,"Liu, Pengbo; Xiao, Li; Zhou, S. Kevin","Liu, Pengbo (); Xiao, Li (); Zhou, S. Kevin ()",,"Liu, Pengbo (); Xiao, Li (); Zhou, S. Kevin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136235371,46 Information and Computing Sciences; 4611 Machine Learning,
1477,pub.1152233846,10.1007/978-3-031-18910-4_12,,,Gradient-Rebalanced Uncertainty Minimization for Cross-Site Adaptation of Medical Image Segmentation,"Automatically adapting image segmentation across data sites benefits to reduce the data annotation burden in medical image analysis. Due to variations in image collection procedures, there usually exists moderate domain gap between medical image datasets from different sites. Increasing the prediction certainty is beneficial for gradually reducing the category-wise domain shift. However, uncertainty minimization naturally leads to bias towards major classes since the target object usually occupies a small portion of pixels in the input image. In this paper, we propose a gradient-rebalanced uncertainty minimization scheme which is capable of eliminating the learning bias. First, the foreground pixels and background pixels are reweighted according to the total gradient amplitude of every class. Furthermore, we devise a feature-level adaptation scheme to reduce the overall domain gap between source and target datasets, based on feature norm regularization and adversarial learning. Experiments on CT pancreas segmentation and MRI prostate segmentation validate that, our method outperforms existing cross-site adaptation algorithms by around 3% on the DICE similarity coefficient.",,,Lecture Notes in Computer Science,Pattern Recognition and Computer Vision,,2022-10-27,2022,2022-10-27,2022,13535,,138-151,Closed,Chapter,"Li, Jiaming; Fang, Chaowei; Li, Guanbin","Li, Jiaming (School of Data and Computer Science, Sun Yet-Sen University, 510006, Guangzhou, China); Fang, Chaowei (School of Artificial Intelligence, Xidian University, 710071, Xi’an, China); Li, Guanbin (School of Data and Computer Science, Sun Yet-Sen University, 510006, Guangzhou, China)","Fang, Chaowei (Xidian University)","Li, Jiaming (Sun Yat-sen University); Fang, Chaowei (Xidian University); Li, Guanbin (Sun Yat-sen University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152233846,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1477,pub.1131397010,10.1007/978-3-030-59719-1_31,,,Unsupervised Learning for CT Image Segmentation via Adversarial Redrawing,"We propose a novel adversarial learning framework for unsupervised training of CNNs in CT image segmentation. It is motivated by difficulties in collecting voxel-wise annotations, which is laborious, time-consuming and expensive. It is conceptually simple, allowing us to train an effective segmentation network without any human annotation. Specifically, we design the generator with a CNN producing the segmentation results and a decoder redrawing the CT volume based on the segmentation results. The CNN is then implicitly trained in the adversarial learning framework where a discriminator gradually enforcing the generator to generate CT volumes whose distribution well matches the distribution of the training data. We further propose two constrains as regularization schemes for the training procedure to drive the model towards optimal segmentation by avoiding some unreasonable results. We conducted extensive experiments to evaluate the proposed method on a famous publicly available dataset, and the experimental results demonstrate the effectiveness of the proposed method.","The work described in this paper is supported by grants from the Hong Kong Research Grants Council (Project No. PolyU 152035/17E and Project No. 15205919), a grant from the Natural Foundation of China (Grant No. 61902232), a grant from the Hong Kong Innovation and Technology Commission (Project No. ITS/398/17FP), and a grant from the Li Ka Shing Foundation Cross-Disciplinary Research (Grant no. 2020LKSFG05D).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12264,,309-320,Closed,Chapter,"Song, Youyi; Zhou, Teng; Teoh, Jeremy Yuen-Chun; Zhang, Jing; Qin, Jing","Song, Youyi (Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong, China); Zhou, Teng (Department of Compute Science, Shantou University, Shantou, China); Teoh, Jeremy Yuen-Chun (Department of Surgery, The Chinese University of Hong Kong, Hong Kong SAR, China); Zhang, Jing (College of Electrical Engineering, Sichuan University, Chengdu, China); Qin, Jing (Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong, China)","Song, Youyi (Hong Kong Polytechnic University)","Song, Youyi (Hong Kong Polytechnic University); Zhou, Teng (Shantou University); Teoh, Jeremy Yuen-Chun (Chinese University of Hong Kong); Zhang, Jing (Sichuan University); Qin, Jing (Hong Kong Polytechnic University)",6,6,,3.09,,https://app.dimensions.ai/details/publication/pub.1131397010,46 Information and Computing Sciences; 4611 Machine Learning,
1477,pub.1151032972,10.1007/978-3-031-16440-8_68,,,Learning Incrementally to Segment Multiple Organs in a CT Image,"There exists a large number of datasets for organ segmentation, which are partially annotated and sequentially constructed. A typical dataset is constructed at a certain time by curating medical images and annotating the organs of interest. In other words, new datasets with annotations of new organ categories are built over time. To unleash the potential behind these partially labeled, sequentially-constructed datasets, we propose to incrementally learn a multi-organ segmentation model. In each incremental learning (IL) stage, we lose the access to previous data and annotations, whose knowledge is assumingly captured by the current model, and gain the access to a new dataset with annotations of new organ categories, from which we learn to update the organ segmentation model to include the new organs. While IL is notorious for its ‘catastrophic forgetting’ weakness in the context of natural image analysis, we experimentally discover that such a weakness mostly disappears for CT multi-organ segmentation. To further stabilize the model performance across the IL stages, we introduce a light memory module and some loss functions to restrain the representation of different categories in feature space, aggregating feature representation of the same class and separating feature representation of different classes. Extensive experiments on five open-sourced datasets are conducted to illustrate the effectiveness of our method.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13434,,714-724,All OA; Green,Chapter,"Liu, Pengbo; Wang, Xia; Fan, Mengsi; Pan, Hongli; Yin, Minmin; Zhu, Xiaohong; Du, Dandan; Zhao, Xiaoying; Xiao, Li; Ding, Lian; Wu, Xingwang; Zhou, S. Kevin","Liu, Pengbo (Center for Medical Imaging, Robotics, Analytic Computing and Learning (MIRACLE), School of Biomedical Engineering and Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China); Wang, Xia (The First Affiliated Hospital of Anhui Medical University, Anhui, China); Fan, Mengsi (The First Affiliated Hospital of Anhui Medical University, Anhui, China); Pan, Hongli (The First Affiliated Hospital of Anhui Medical University, Anhui, China); Yin, Minmin (The First Affiliated Hospital of Anhui Medical University, Anhui, China); Zhu, Xiaohong (The First Affiliated Hospital of Anhui Medical University, Anhui, China); Du, Dandan (The First Affiliated Hospital of Anhui Medical University, Anhui, China); Zhao, Xiaoying (The First Affiliated Hospital of Anhui Medical University, Anhui, China); Xiao, Li (Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China); Ding, Lian (Huawei Cloud Computing Technology Co. Ltd., Dongguan, China); Wu, Xingwang (The First Affiliated Hospital of Anhui Medical University, Anhui, China); Zhou, S. Kevin (Center for Medical Imaging, Robotics, Analytic Computing and Learning (MIRACLE), School of Biomedical Engineering and Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China)","Zhou, S. Kevin (University of Science and Technology of China; Institute of Computing Technology)","Liu, Pengbo (University of Science and Technology of China; Institute of Computing Technology); Wang, Xia (First Affiliated Hospital of Anhui Medical University); Fan, Mengsi (First Affiliated Hospital of Anhui Medical University); Pan, Hongli (First Affiliated Hospital of Anhui Medical University); Yin, Minmin (First Affiliated Hospital of Anhui Medical University); Zhu, Xiaohong (First Affiliated Hospital of Anhui Medical University); Du, Dandan (First Affiliated Hospital of Anhui Medical University); Zhao, Xiaoying (First Affiliated Hospital of Anhui Medical University); Xiao, Li (Institute of Computing Technology); Ding, Lian (); Wu, Xingwang (First Affiliated Hospital of Anhui Medical University); Zhou, S. Kevin (University of Science and Technology of China; Institute of Computing Technology)",0,0,,,http://arxiv.org/pdf/2203.02100,https://app.dimensions.ai/details/publication/pub.1151032972,46 Information and Computing Sciences; 4611 Machine Learning,
1477,pub.1131399627,10.1007/978-3-030-59719-1_18,,,Learning 3D Features with 2D CNNs via Surface Projection for CT Volume Segmentation,"Abstract3D features are desired in nature for segmenting CT volumes. It is, however, computationally expensive to employ a 3D convolutional neural network (CNN) to learn 3D features. Existing methods hence learn 3D features by still relying on 2D CNNs while attempting to consider more 2D slices, but up until now it is difficulty for them to consider the whole volumetric data, resulting in information loss and performance degradation. In this paper, we propose a simple and effective technique that allows a 2D CNN to learn 3D features for segmenting CT volumes. Our key insight is that all boundary voxels of a 3D object form a surface that can be represented by using a 2D matrix, and therefore they can be perfectly recognized by a 2D CNN in theory. We hence learn 3D features for recognizing these boundary voxels by learning the projection distance between a set of prescribed spherical surfaces and the object’s surface, which can be readily performed by a 2D CNN. By doing so, we can consider the whole volumetric data when spherical surfaces are sampled sufficiently dense, without any information loss. We assessed the proposed method on a publicly available dataset. The experimental evidence shows that the proposed method is effective, outperforming existing methods.","The work described in this paper is supported by a grant from the Hong Kong Research Grants Council (Project No. PolyU 152035/17E), a grant from the Natural Foundation of China (Grant No. 61902232), a grant from the Li Ka Shing Foundation Cross-Disciplinary Research (Grant no. 2020LKSFG05D), a grant from the Innovative Technology Fund (Grant No. MRP/015/18), and a grant from the General Research Fund (Grant No. PolyU 152006/19E).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12264,,176-186,Closed,Chapter,"Song, Youyi; Yu, Zhen; Zhou, Teng; Teoh, Jeremy Yuen-Chun; Lei, Baiying; Choi, Kup-Sze; Qin, Jing","Song, Youyi (Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong, China); Yu, Zhen (Central Clinical School, Monash University, Melbourne, Australia); Zhou, Teng (Department of Compute Science, Shantou University, Shantou, China); Teoh, Jeremy Yuen-Chun (Department of Surgery, The Chinese University of Hong Kong, Hong Kong, China); Lei, Baiying (School of Biomedical Engineering, Shenzhen University, Shenzhen, China); Choi, Kup-Sze (Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong, China); Qin, Jing (Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong, China)","Song, Youyi (Hong Kong Polytechnic University)","Song, Youyi (Hong Kong Polytechnic University); Yu, Zhen (Monash University); Zhou, Teng (Shantou University); Teoh, Jeremy Yuen-Chun (Chinese University of Hong Kong); Lei, Baiying (Shenzhen University); Choi, Kup-Sze (Hong Kong Polytechnic University); Qin, Jing (Hong Kong Polytechnic University)",7,7,,4.25,,https://app.dimensions.ai/details/publication/pub.1131399627,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",
1475,pub.1146094499,10.48550/arxiv.2203.02100,,,Learning Incrementally to Segment Multiple Organs in a CT Image,"There exists a large number of datasets for organ segmentation, which are
partially annotated and sequentially constructed. A typical dataset is
constructed at a certain time by curating medical images and annotating the
organs of interest. In other words, new datasets with annotations of new organ
categories are built over time. To unleash the potential behind these partially
labeled, sequentially-constructed datasets, we propose to incrementally learn a
multi-organ segmentation model. In each incremental learning (IL) stage, we
lose the access to previous data and annotations, whose knowledge is assumingly
captured by the current model, and gain the access to a new dataset with
annotations of new organ categories, from which we learn to update the organ
segmentation model to include the new organs. While IL is notorious for its
`catastrophic forgetting' weakness in the context of natural image analysis, we
experimentally discover that such a weakness mostly disappears for CT
multi-organ segmentation. To further stabilize the model performance across the
IL stages, we introduce a light memory module and some loss functions to
restrain the representation of different categories in feature space,
aggregating feature representation of the same class and separating feature
representation of different classes. Extensive experiments on five open-sourced
datasets are conducted to illustrate the effectiveness of our method.",,,arXiv,,,2022-03-03,2022,,,,,,All OA; Green,Preprint,"Liu, Pengbo; Wang, Xia; Fan, Mengsi; Pan, Hongli; Yin, Minmin; Zhu, Xiaohong; Du, Dandan; Zhao, Xiaoying; Xiao, Li; Ding, Lian; Wu, Xingwang; Zhou, S. Kevin","Liu, Pengbo (); Wang, Xia (); Fan, Mengsi (); Pan, Hongli (); Yin, Minmin (); Zhu, Xiaohong (); Du, Dandan (); Zhao, Xiaoying (); Xiao, Li (); Ding, Lian (); Wu, Xingwang (); Zhou, S. Kevin ()",,"Liu, Pengbo (); Wang, Xia (); Fan, Mengsi (); Pan, Hongli (); Yin, Minmin (); Zhu, Xiaohong (); Du, Dandan (); Zhao, Xiaoying (); Xiao, Li (); Ding, Lian (); Wu, Xingwang (); Zhou, S. Kevin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146094499,46 Information and Computing Sciences; 4611 Machine Learning,
1475,pub.1139021897,10.1007/978-3-030-78191-0_35,,,Generalized Organ Segmentation by Imitating One-Shot Reasoning Using Anatomical Correlation,"Learning by imitation is one of the most significant abilities of human beings and plays a vital role in human’s computational neural system. In medical image analysis, given several exemplars (anchors), experienced radiologist has the ability to delineate unfamiliar organs by imitating the reasoning process learned from existing types of organs. Inspired by this observation, we propose OrganNet which learns a generalized organ concept from a set of annotated organ classes and then transfer this concept to unseen classes. In this paper, we show that such process can be integrated into the one-shot segmentation task which is a very challenging but meaningful topic. We propose pyramid reasoning modules (PRMs) to model the anatomical correlation between anchor and target volumes. In practice, the proposed module first computes a correlation matrix between target and anchor computerized tomography (CT) volumes. Then, this matrix is used to transform the feature representations of both anchor volume and its segmentation mask. Finally, OrganNet learns to fuse the representations from various inputs and predicts segmentation results for target volume. Extensive experiments show that OrganNet can effectively resist the wide variations in organ morphology and produce state-of-the-art results in one-shot segmentation task. Moreover, even when compared with fully-supervised segmentation models, OrganNet is still able to produce satisfying segmentation results.",,,Lecture Notes in Computer Science,Information Processing in Medical Imaging,,2021-06-14,2021,2021-06-14,2021,12729,,452-464,All OA; Green,Chapter,"Zhou, Hong-Yu; Liu, Hualuo; Cao, Shilei; Wei, Dong; Lu, Chixiang; Yu, Yizhou; Ma, Kai; Zheng, Yefeng","Zhou, Hong-Yu (The University of Hong Kong, Pok Fu Lam, Hong Kong); Liu, Hualuo (Tencent, Shenzhen, China); Cao, Shilei (Tencent, Shenzhen, China); Wei, Dong (Tencent, Shenzhen, China); Lu, Chixiang (Huazhong University of Science and Technology, Wuhan, China); Yu, Yizhou (The University of Hong Kong, Pok Fu Lam, Hong Kong); Ma, Kai (Tencent, Shenzhen, China); Zheng, Yefeng (Tencent, Shenzhen, China)","Wei, Dong (Tencent (China))","Zhou, Hong-Yu (University of Hong Kong); Liu, Hualuo (Tencent (China)); Cao, Shilei (Tencent (China)); Wei, Dong (Tencent (China)); Lu, Chixiang (Huazhong University of Science and Technology); Yu, Yizhou (University of Hong Kong); Ma, Kai (Tencent (China)); Zheng, Yefeng (Tencent (China))",2,2,,1.65,http://arxiv.org/pdf/2103.16344,https://app.dimensions.ai/details/publication/pub.1139021897,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4611 Machine Learning,
1428,pub.1138607130,10.48550/arxiv.2106.01596,,,Semantic-Aware Contrastive Learning for Multi-object Medical Image  Segmentation,"Medical image segmentation, or computing voxelwise semantic masks, is a
fundamental yet challenging task to compute a voxel-level semantic mask. To
increase the ability of encoder-decoder neural networks to perform this task
across large clinical cohorts, contrastive learning provides an opportunity to
stabilize model initialization and enhance encoders without labels. However,
multiple target objects (with different semantic meanings) may exist in a
single image, which poses a problem for adapting traditional contrastive
learning methods from prevalent 'image-level classification' to 'pixel-level
segmentation'. In this paper, we propose a simple semantic-aware contrastive
learning approach leveraging attention masks to advance multi-object semantic
segmentation. Briefly, we embed different semantic objects to different
clusters rather than the traditional image-level embeddings. We evaluate our
proposed method on a multi-organ medical image segmentation task with both
in-house data and MICCAI Challenge 2015 BTCV datasets. Compared with current
state-of-the-art training strategies, our proposed pipeline yields a
substantial improvement of 5.53% and 6.09% on Dice score for both medical image
segmentation cohorts respectively (p-value<0.01). The performance of the
proposed method is further assessed on natural images via the PASCAL VOC 2012
dataset, and achieves a substantial improvement of 2.75% on mIoU
(p-value<0.01).",,,arXiv,,,2021-06-03,2021,,,,,,All OA; Green,Preprint,"Lee, Ho Hin; Tang, Yucheng; Yang, Qi; Yu, Xin; Bao, Shunxing; Cai, Leon Y.; Remedios, Lucas W.; Landman, Bennett A.; Huo, Yuankai","Lee, Ho Hin (); Tang, Yucheng (); Yang, Qi (); Yu, Xin (); Bao, Shunxing (); Cai, Leon Y. (); Remedios, Lucas W. (); Landman, Bennett A. (); Huo, Yuankai ()",,"Lee, Ho Hin (); Tang, Yucheng (); Yang, Qi (); Yu, Xin (); Bao, Shunxing (); Cai, Leon Y. (); Remedios, Lucas W. (); Landman, Bennett A. (); Huo, Yuankai ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1138607130,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4605 Data Management and Data Science; 4611 Machine Learning,
1428,pub.1136816955,10.48550/arxiv.2103.16344,,,Generalized Organ Segmentation by Imitating One-shot Reasoning using  Anatomical Correlation,"Learning by imitation is one of the most significant abilities of human
beings and plays a vital role in human's computational neural system. In
medical image analysis, given several exemplars (anchors), experienced
radiologist has the ability to delineate unfamiliar organs by imitating the
reasoning process learned from existing types of organs. Inspired by this
observation, we propose OrganNet which learns a generalized organ concept from
a set of annotated organ classes and then transfer this concept to unseen
classes. In this paper, we show that such process can be integrated into the
one-shot segmentation task which is a very challenging but meaningful topic. We
propose pyramid reasoning modules (PRMs) to model the anatomical correlation
between anchor and target volumes. In practice, the proposed module first
computes a correlation matrix between target and anchor computerized tomography
(CT) volumes. Then, this matrix is used to transform the feature
representations of both anchor volume and its segmentation mask. Finally,
OrganNet learns to fuse the representations from various inputs and predicts
segmentation results for target volume. Extensive experiments show that
OrganNet can effectively resist the wide variations in organ morphology and
produce state-of-the-art results in one-shot segmentation task. Moreover, even
when compared with fully-supervised segmentation models, OrganNet is still able
to produce satisfying segmentation results.",,,arXiv,,,2021-03-30,2021,,,,,,All OA; Green,Preprint,"Zhou, Hong-Yu; Liu, Hualuo; Cao, Shilei; Wei, Dong; Lu, Chixiang; Yu, Yizhou; Ma, Kai; Zheng, Yefeng","Zhou, Hong-Yu (); Liu, Hualuo (); Cao, Shilei (); Wei, Dong (); Lu, Chixiang (); Yu, Yizhou (); Ma, Kai (); Zheng, Yefeng ()",,"Zhou, Hong-Yu (); Liu, Hualuo (); Cao, Shilei (); Wei, Dong (); Lu, Chixiang (); Yu, Yizhou (); Ma, Kai (); Zheng, Yefeng ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136816955,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4611 Machine Learning,
1425,pub.1140705878,10.48550/arxiv.2108.11694,,,PoissonSeg: Semi-Supervised Few-Shot Medical Image Segmentation via  Poisson Learning,"The application of deep learning to medical image segmentation has been
hampered due to the lack of abundant pixel-level annotated data. Few-shot
Semantic Segmentation (FSS) is a promising strategy for breaking the deadlock.
However, a high-performing FSS model still requires sufficient pixel-level
annotated classes for training to avoid overfitting, which leads to its
performance bottleneck in medical image segmentation due to the unmet need for
annotations. Thus, semi-supervised FSS for medical images is accordingly
proposed to utilize unlabeled data for further performance improvement.
Nevertheless, existing semi-supervised FSS methods has two obvious defects: (1)
neglecting the relationship between the labeled and unlabeled data; (2) using
unlabeled data directly for end-to-end training leads to degenerated
representation learning. To address these problems, we propose a novel
semi-supervised FSS framework for medical image segmentation. The proposed
framework employs Poisson learning for modeling data relationship and
propagating supervision signals, and Spatial Consistency Calibration for
encouraging the model to learn more coherent representations. In this process,
unlabeled samples do not involve in end-to-end training, but provide
supervisory information for query image segmentation through graph-based
learning. We conduct extensive experiments on three medical image segmentation
datasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for
MRI and abdominal organs segmentation for CT) to demonstrate the
state-of-the-art performance and broad applicability of the proposed framework.",,,arXiv,,,2021-08-26,2021,,,,,,All OA; Green,Preprint,"Shen, Xiaoang; Zhang, Guokai; Lai, Huilin; Luo, Jihao; Lu, Jianwei; Luo, Ye","Shen, Xiaoang (); Zhang, Guokai (); Lai, Huilin (); Luo, Jihao (); Lu, Jianwei (); Luo, Ye ()",,"Shen, Xiaoang (); Zhang, Guokai (); Lai, Huilin (); Luo, Jihao (); Lu, Jianwei (); Luo, Ye ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140705878,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1425,pub.1104301883,10.1016/j.asoc.2018.05.038,,,Robust abdominal organ segmentation using regional convolutional neural networks,"A fully automatic system for abdominal organ segmentation is presented. As a first step, an organ localization is obtained via a robust and efficient feature registration method where the center of the organ is estimated together with a region of interest surrounding the center. Then, a convolutional neural network performing voxelwise classification is applied. Two convolutional neural networks of different architecture are compared. The first one has a structure similar to networks used for classification and is applied using a sliding window approach. The second one has a structure allowing it to be applied in a fully convolutional manner reducing computation time. Despite limited training data, our experimental results are on par with state-of-the-art approaches that have been developed over many years. More specifically the method is applied to the MICCAI2015 challenge “Multi-Atlas Labeling Beyond the Cranial Vault” in the free competition for organ segmentation in the abdomen. The method performed well for both types of convolutional neural networks. For the fully convolutional network a mean Dice coefficient of 0.767 was achieved, for the network applied with sliding window this number was 0.757.",,,Applied Soft Computing,,,2018-09,2018,,2018-09,70,,465-471,All OA; Green,Article,"Larsson, Måns; Zhang, Yuhang; Kahl, Fredrik","Larsson, Måns (Chalmers University of Technology, Gothenburg, Sweden); Zhang, Yuhang (Chalmers University of Technology, Gothenburg, Sweden); Kahl, Fredrik (Chalmers University of Technology, Gothenburg, Sweden; Centre for Mathematical Sciences, Lund University, Lund, Sweden)","Larsson, Måns (Chalmers University of Technology)","Larsson, Måns (Chalmers University of Technology); Zhang, Yuhang (Chalmers University of Technology); Kahl, Fredrik (Chalmers University of Technology; Lund University)",33,15,,10.42,http://publications.lib.chalmers.se/records/fulltext/250295/local_250295.pdf,https://app.dimensions.ai/details/publication/pub.1104301883,46 Information and Computing Sciences; 4611 Machine Learning,
1422,pub.1150171807,10.18178/joig.10.3.95-101,,,Multi-organ Statistical Shape Model Building Using a Non-rigid ICP Based Surface Registration,,,,Journal of Image and Graphics,,,2022,2022,2022,2022,10,3,,All OA; Hybrid,Article,"Wu, Jiaqi; Li, Guangxu; Kamiya, Tohru","Wu, Jiaqi (); Li, Guangxu (); Kamiya, Tohru ()",,"Wu, Jiaqi (); Li, Guangxu (); Kamiya, Tohru ()",0,0,,,https://doi.org/10.18178/joig.10.3.95-101,https://app.dimensions.ai/details/publication/pub.1150171807,,
1422,pub.1133613127,10.48550/arxiv.2012.08721,,,Deep Learning to Segment Pelvic Bones: Large-scale CT Datasets and  Baseline Models,"Purpose: Pelvic bone segmentation in CT has always been an essential step in
clinical diagnosis and surgery planning of pelvic bone diseases. Existing
methods for pelvic bone segmentation are either hand-crafted or semi-automatic
and achieve limited accuracy when dealing with image appearance variations due
to the multi-site domain shift, the presence of contrasted vessels, coprolith
and chyme, bone fractures, low dose, metal artifacts, etc. Due to the lack of a
large-scale pelvic CT dataset with annotations, deep learning methods are not
fully explored. Methods: In this paper, we aim to bridge the data gap by
curating a large pelvic CT dataset pooled from multiple sources and different
manufacturers, including 1, 184 CT volumes and over 320, 000 slices with
different resolutions and a variety of the above-mentioned appearance
variations. Then we propose for the first time, to the best of our knowledge,
to learn a deep multi-class network for segmenting lumbar spine, sacrum, left
hip, and right hip, from multiple-domain images simultaneously to obtain more
effective and robust feature representations. Finally, we introduce a
post-processing tool based on the signed distance function (SDF) to eliminate
false predictions while retaining correctly predicted bone fragments. Results:
Extensive experiments on our dataset demonstrate the effectiveness of our
automatic method, achieving an average Dice of 0.987 for a metal-free volume.
SDF post-processor yields a decrease of 10.5% in hausdorff distance by
maintaining important bone fragments in post-processing phase. Conclusion: We
believe this large-scale dataset will promote the development of the whole
community and plan to open source the images, annotations, codes, and trained
baseline models at https://github.com/ICT-MIRACLE-lab/CTPelvic1K.",,,arXiv,,,2020-12-15,2020,,,,,,All OA; Green,Preprint,"Liu, Pengbo; Han, Hu; Du, Yuanqi; Zhu, Heqin; Li, Yinhao; Gu, Feng; Xiao, Honghu; Li, Jun; Zhao, Chunpeng; Xiao, Li; Wu, Xinbao; Zhou, S. Kevin","Liu, Pengbo (); Han, Hu (); Du, Yuanqi (); Zhu, Heqin (); Li, Yinhao (); Gu, Feng (); Xiao, Honghu (); Li, Jun (); Zhao, Chunpeng (); Xiao, Li (); Wu, Xinbao (); Zhou, S. Kevin ()",,"Liu, Pengbo (); Han, Hu (); Du, Yuanqi (); Zhu, Heqin (); Li, Yinhao (); Gu, Feng (); Xiao, Honghu (); Li, Jun (); Zhao, Chunpeng (); Xiao, Li (); Wu, Xinbao (); Zhou, S. Kevin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1133613127,32 Biomedical and Clinical Sciences; 51 Physical Sciences; 5105 Medical and Biological Physics,
1374,pub.1151124913,10.1007/978-3-031-16852-9_13,,,CateNorm: Categorical Normalization for Robust Medical Image Segmentation,"Batch normalization (BN) uniformly shifts and scales the activations based on the statistics of a batch of images. However, the intensity distribution of the background pixels often dominates the BN statistics because the background accounts for a large proportion of the entire image. This paper focuses on enhancing BN with the intensity distribution of foreground pixels, the one that really matters for image segmentation. We propose a new normalization strategy, named categorical normalization (CateNorm), to normalize the activations according to categorical statistics. The categorical statistics are obtained by dynamically modulating specific regions in an image that belong to the foreground. CateNorm demonstrates both precise and robust segmentation results across five public datasets obtained from different domains, covering complex and variable data distributions. It is attributable to the ability of CateNorm to capture domain-invariant information from multiple domains (institutions) of medical data.Code is available at https://github.com/lambert-x/CateNorm.",This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research. We also thank Quande Liu for the discussion.,,Lecture Notes in Computer Science,Domain Adaptation and Representation Transfer,,2022-09-15,2022,2022-09-15,2022,13542,,129-146,All OA; Green,Chapter,"Xiao, Junfei; Yu, Lequan; Zhou, Zongwei; Bai, Yutong; Xing, Lei; Yuille, Alan; Zhou, Yuyin","Xiao, Junfei (Johns Hopkins University, Baltimore, USA); Yu, Lequan (The University of Hong Kong, Pok Fu Lam, Hong Kong); Zhou, Zongwei (Johns Hopkins University, Baltimore, USA); Bai, Yutong (Johns Hopkins University, Baltimore, USA); Xing, Lei (Stanford University, Stanford, USA); Yuille, Alan (Johns Hopkins University, Baltimore, USA); Zhou, Yuyin (UC Santa Cruz, Santa Cruz, USA)","Xiao, Junfei (Johns Hopkins University)","Xiao, Junfei (Johns Hopkins University); Yu, Lequan (University of Hong Kong); Zhou, Zongwei (Johns Hopkins University); Bai, Yutong (Johns Hopkins University); Xing, Lei (Stanford University); Yuille, Alan (Johns Hopkins University); Zhou, Yuyin (University of California, Santa Cruz)",1,1,,,http://arxiv.org/pdf/2103.15858,https://app.dimensions.ai/details/publication/pub.1151124913,46 Information and Computing Sciences,
1372,pub.1154178383,10.1109/bibm55620.2022.9995254,,,Unsupervised Domain Adaptation with Dual U-DenseTransformer Generation,"Unsupervised domain adaptation is to transfer knowledge from a well-annotated source domain and learn an accurate classifier for an unlabeled target domain, which is particularly useful in multimodal medical image processing. Currently available adaptation approaches strongly reduce the domain bias or inconsistency in the latent space, deteriorating inherent data structures. To appropriately leverage the reduction of the domain discrepancy and the maintenance of the intrinsic structure, this paper proposes a dual U-DenseTransformer generation domain adaptation framework to bridge the gap between source and target domains and achieve translation. Specifically, we create a DenseTransformer with multi-head attention embedded in U-shape network to establish a dual-generator strategy, which is further enhanced by a new hybrid loss function and an edge-aware mechanism that preserve inherent data structure consistent. We apply our proposed method to medical image segmentation, with the experimental results showing that it works more effective and stable than currently available approaches. Particularly, the dice similarity was improved from 79.3% to 82.8%, while the average symmetric surface distance was reduced from 2.5 to 1.9.",,"This work was supported partly by the Fujian Provincial Technology Innovation Joint Funds under Grant 2019Y9091, the Natural Science Foundation of Fujian Province of China under Grant 2020J01004, and the National Natural Science Foundation of China under Grant 61971367.",,2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),,2022-12-08,2022,,2022-12-08,00,,1170-1177,Closed,Proceeding,"Shen, Dongfang; Wu, Ming; Zheng, Song; Chen, Jianhui; Chen, Yijiang; Chen, Yinran; Luo, Xiongbiao","Shen, Dongfang (Department of Computer Science, Xiamen University, Xiamen, 361005, China); Wu, Ming (Department of Computer Science, Xiamen University, Xiamen, 361005, China); Zheng, Song (Union Hospital, Fujian Medical University, Fuzhou, 350001, China); Chen, Jianhui (Union Hospital, Fujian Medical University, Fuzhou, 350001, China); Chen, Yijiang (Department of Computer Science, Xiamen University, Xiamen, 361005, China); Chen, Yinran (Department of Computer Science, Xiamen University, Xiamen, 361005, China); Luo, Xiongbiao (Department of Computer Science, Xiamen University, Xiamen, 361005, China)","Shen, Dongfang (Xiamen University); Wu, Ming (Xiamen University)","Shen, Dongfang (Xiamen University); Wu, Ming (Xiamen University); Zheng, Song (Union Hospital); Chen, Jianhui (Union Hospital); Chen, Yijiang (Xiamen University); Chen, Yinran (Xiamen University); Luo, Xiongbiao (Xiamen University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154178383,46 Information and Computing Sciences; 4602 Artificial Intelligence,
1330,pub.1132796657,10.48550/arxiv.2011.09608,,,Bidirectional RNN-based Few Shot Learning for 3D Medical Image  Segmentation,"Segmentation of organs of interest in 3D medical images is necessary for
accurate diagnosis and longitudinal studies. Though recent advances using deep
learning have shown success for many segmentation tasks, large datasets are
required for high performance and the annotation process is both time consuming
and labor intensive. In this paper, we propose a 3D few shot segmentation
framework for accurate organ segmentation using limited training samples of the
target organ annotation. To achieve this, a U-Net like network is designed to
predict segmentation by learning the relationship between 2D slices of support
data and a query image, including a bidirectional gated recurrent unit (GRU)
that learns consistency of encoded features between adjacent slices. Also, we
introduce a transfer learning method to adapt the characteristics of the target
image and organ by updating the model before testing with arbitrary support and
query data sampled from the support data. We evaluate our proposed model using
three 3D CT datasets with annotations of different organs. Our model yielded
significantly improved performance over state-of-the-art few shot segmentation
models and was comparable to a fully supervised model trained with more target
training data.",,,arXiv,,,2020-11-18,2020,,,,,,All OA; Green,Preprint,"Kim, Soopil; An, Sion; Chikontwe, Philip; Park, Sang Hyun","Kim, Soopil (); An, Sion (); Chikontwe, Philip (); Park, Sang Hyun ()",,"Kim, Soopil (); An, Sion (); Chikontwe, Philip (); Park, Sang Hyun ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1132796657,46 Information and Computing Sciences; 4611 Machine Learning,
1326,pub.1133409509,10.48550/arxiv.2012.05440,,,Few-shot Medical Image Segmentation using a Global Correlation Network  with Discriminative Embedding,"Despite deep convolutional neural networks achieved impressive progress in
medical image computing and analysis, its paradigm of supervised learning
demands a large number of annotations for training to avoid overfitting and
achieving promising results. In clinical practices, massive semantic
annotations are difficult to acquire in some conditions where specialized
biomedical expert knowledge is required, and it is also a common condition
where only few annotated classes are available. In this work, we proposed a
novel method for few-shot medical image segmentation, which enables a
segmentation model to fast generalize to an unseen class with few training
images. We construct our few-shot image segmentor using a deep convolutional
network trained episodically. Motivated by the spatial consistency and
regularity in medical images, we developed an efficient global correlation
module to capture the correlation between a support and query image and
incorporate it into the deep network called global correlation network.
Moreover, we enhance discriminability of deep embedding to encourage clustering
of the feature domains of the same class while keep the feature domains of
different organs far apart. Ablation Study proved the effectiveness of the
proposed global correlation module and discriminative embedding loss. Extensive
experiments on anatomical abdomen images on both CT and MRI modalities are
performed to demonstrate the state-of-the-art performance of our proposed
model.",,,arXiv,,,2020-12-09,2020,,,,,,All OA; Green,Preprint,"Sun, Liyan; Li, Chenxin; Ding, Xinghao; Huang, Yue; Wang, Guisheng; Yu, Yizhou","Sun, Liyan (); Li, Chenxin (); Ding, Xinghao (); Huang, Yue (); Wang, Guisheng (); Yu, Yizhou ()",,"Sun, Liyan (); Li, Chenxin (); Ding, Xinghao (); Huang, Yue (); Wang, Guisheng (); Yu, Yizhou ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1133409509,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4611 Machine Learning,
1325,pub.1150264316,10.48550/arxiv.2208.06643,,,Medical image analysis based on transformer: A Review,"The transformer has dominated the natural language processing (NLP) field for
a long time. Recently, the transformer-based method has been adopted into the
computer vision (CV) field and shows promising results. As an important branch
of the CV field, medical image analysis joins the wave of the transformer-based
method rightfully. In this review, we illustrate the principle of the attention
mechanism, and the detailed structures of the transformer, and depict how the
transformer is adopted into medical image analysis. We organize the
transformer-based medical image analysis applications in a sequence of
different tasks, including classification, segmentation, synthesis,
registration, localization, detection, captioning, and denoising. For the
mainstream classification and segmentation tasks, we further divided the
corresponding works based on different medical imaging modalities. The datasets
corresponding to the related works are also organized. We include thirteen
modalities and more than twenty objects in our work.",,,arXiv,,,2022-08-13,2022,,,,,,All OA; Green,Preprint,"Liu, Zhaoshan; Lv, Qiujie; Lee, Chau Hung; Shen, Lei","Liu, Zhaoshan (); Lv, Qiujie (); Lee, Chau Hung (); Shen, Lei ()",,"Liu, Zhaoshan (); Lv, Qiujie (); Lee, Chau Hung (); Shen, Lei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150264316,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1324,pub.1141035050,10.48550/arxiv.2109.03201,,,nnFormer: Interleaved Transformer for Volumetric Segmentation,"Transformer, the model of choice for natural language processing, has drawn
scant attention from the medical imaging community. Given the ability to
exploit long-term dependencies, transformers are promising to help atypical
convolutional neural networks to overcome their inherent shortcomings of
spatial inductive bias. However, most of recently proposed transformer-based
segmentation approaches simply treated transformers as assisted modules to help
encode global context into convolutional representations. To address this
issue, we introduce nnFormer, a 3D transformer for volumetric medical image
segmentation. nnFormer not only exploits the combination of interleaved
convolution and self-attention operations, but also introduces local and global
volume-based self-attention mechanism to learn volume representations.
Moreover, nnFormer proposes to use skip attention to replace the traditional
concatenation/summation operations in skip connections in U-Net like
architecture. Experiments show that nnFormer significantly outperforms previous
transformer-based counterparts by large margins on three public datasets.
Compared to nnUNet, nnFormer produces significantly lower HD95 and comparable
DSC results. Furthermore, we show that nnFormer and nnUNet are highly
complementary to each other in model ensembling.",,,arXiv,,,2021-09-07,2021,,,,,,All OA; Green,Preprint,"Zhou, Hong-Yu; Guo, Jiansen; Zhang, Yinghao; Yu, Lequan; Wang, Liansheng; Yu, Yizhou","Zhou, Hong-Yu (); Guo, Jiansen (); Zhang, Yinghao (); Yu, Lequan (); Wang, Liansheng (); Yu, Yizhou ()",,"Zhou, Hong-Yu (); Guo, Jiansen (); Zhang, Yinghao (); Yu, Lequan (); Wang, Liansheng (); Yu, Yizhou ()",3,3,,2.46,,https://app.dimensions.ai/details/publication/pub.1141035050,46 Information and Computing Sciences; 4611 Machine Learning,
1286,pub.1152029707,10.1007/978-3-031-20044-1_24,,,Dual Contrastive Learning with Anatomical Auxiliary Supervision for Few-Shot Medical Image Segmentation,"Few-shot semantic segmentation is a promising solution for scarce data scenarios, especially for medical imaging challenges with limited training data. However, most of the existing few-shot segmentation methods tend to over rely on the images containing target classes, which may hinder its utilization of medical imaging data. In this paper, we present a few-shot segmentation model that employs anatomical auxiliary information from medical images without target classes for dual contrastive learning. The dual contrastive learning module performs comparison among vectors from the perspectives of prototypes and contexts, to enhance the discriminability of learned features and the data utilization. Besides, to distinguish foreground features from background features more friendly, a constrained iterative prediction module is designed to optimize the segmentation of the query image. Experiments on two medical image datasets show that the proposed method achieves performance comparable to state-of-the-art methods. Code is available at: https://github.com/cvszusparkle/AAS-DCL_FSS.","This work was supported in part by the National Natural Science Foundation of China under Grant 61973221 and Grant 62273241, the Natural Science Foundation of Guangdong Province of China under Grant 2018A030313381 and Grant 2019A1515011165.",,Lecture Notes in Computer Science,Computer Vision – ECCV 2022,,2022-10-20,2022,2022-10-20,2022,13680,,417-434,Closed,Chapter,"Wu, Huisi; Xiao, Fangyan; Liang, Chongxin","Wu, Huisi (College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China); Xiao, Fangyan (College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China); Liang, Chongxin (College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China)","Wu, Huisi (Shenzhen University)","Wu, Huisi (Shenzhen University); Xiao, Fangyan (Shenzhen University); Liang, Chongxin (Shenzhen University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152029707,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1286,pub.1148150396,10.48550/arxiv.2205.11888,,,Mind The Gap: Alleviating Local Imbalance for Unsupervised  Cross-Modality Medical Image Segmentation,"Unsupervised cross-modality medical image adaptation aims to alleviate the
severe domain gap between different imaging modalities without using the target
domain label. A key in this campaign relies upon aligning the distributions of
source and target domain. One common attempt is to enforce the global alignment
between two domains, which, however, ignores the fatal local-imbalance domain
gap problem, i.e., some local features with larger domain gap are harder to
transfer. Recently, some methods conduct alignment focusing on local regions to
improve the efficiency of model learning. While this operation may cause a
deficiency of critical information from contexts. To tackle this limitation, we
propose a novel strategy to alleviate the domain gap imbalance considering the
characteristics of medical images, namely Global-Local Union Alignment.
Specifically, a feature-disentanglement style-transfer module first synthesizes
the target-like source-content images to reduce the global domain gap. Then, a
local feature mask is integrated to reduce the 'inter-gap' for local features
by prioritizing those discriminative features with larger domain gap. This
combination of global and local alignment can precisely localize the crucial
regions in segmentation target while preserving the overall semantic
consistency. We conduct a series of experiments with two cross-modality
adaptation tasks, i,e. cardiac substructure and abdominal multi-organ
segmentation. Experimental results indicate that our method achieves
state-of-the-art performance in both tasks.",,,arXiv,,,2022-05-24,2022,,,,,,All OA; Green,Preprint,"Su, Zixian; Yao, Kai; Yang, Xi; Wang, Qiufeng; Yan, Yuyao; Sun, Jie; Huang, Kaizhu","Su, Zixian (); Yao, Kai (); Yang, Xi (); Wang, Qiufeng (); Yan, Yuyao (); Sun, Jie (); Huang, Kaizhu ()",,"Su, Zixian (); Yao, Kai (); Yang, Xi (); Wang, Qiufeng (); Yan, Yuyao (); Sun, Jie (); Huang, Kaizhu ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148150396,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1282,pub.1152094967,10.1007/978-3-031-19818-2_40,,,Learning Topological Interactions for Multi-Class Medical Image Segmentation,"Deep learning methods have achieved impressive performance for multi-class medical image segmentation. However, they are limited in their ability to encode topological interactions among different classes (e.g., containment and exclusion). These constraints naturally arise in biomedical images and can be crucial in improving segmentation quality. In this paper, we introduce a novel topological interaction module to encode the topological interactions into a deep neural network. The implementation is completely convolution-based and thus can be very efficient. This empowers us to incorporate the constraints into end-to-end training and enrich the feature representation of neural networks. The efficacy of the proposed method is validated on different types of interactions. We also demonstrate the generalizability of the method on both proprietary and public challenge datasets, in both 2D and 3D settings, as well as across different modalities such as CT and Ultrasound. Code is available at: https://github.com/TopoXLab/TopoInteraction.",We thank the anonymous reviewers for their constructive feedback. The reported research was partly supported by grants NSF IIS-1909038 and NIH 1R21CA258493-01A1.,,Lecture Notes in Computer Science,Computer Vision – ECCV 2022,,2022-10-22,2022,2022-10-22,2022,13689,,701-718,Closed,Chapter,"Gupta, Saumya; Hu, Xiaoling; Kaan, James; Jin, Michael; Mpoy, Mutshipay; Chung, Katherine; Singh, Gagandeep; Saltz, Mary; Kurc, Tahsin; Saltz, Joel; Tassiopoulos, Apostolos; Prasanna, Prateek; Chen, Chao","Gupta, Saumya (Stony Brook University, Stony Brook, NY, USA); Hu, Xiaoling (Stony Brook University, Stony Brook, NY, USA); Kaan, James (Stony Brook University, Stony Brook, NY, USA); Jin, Michael (Stony Brook University, Stony Brook, NY, USA); Mpoy, Mutshipay (Stony Brook University, Stony Brook, NY, USA); Chung, Katherine (Stony Brook University, Stony Brook, NY, USA); Singh, Gagandeep (Stony Brook University, Stony Brook, NY, USA); Saltz, Mary (Stony Brook University, Stony Brook, NY, USA); Kurc, Tahsin (Stony Brook University, Stony Brook, NY, USA); Saltz, Joel (Stony Brook University, Stony Brook, NY, USA); Tassiopoulos, Apostolos (Stony Brook University, Stony Brook, NY, USA); Prasanna, Prateek (Stony Brook University, Stony Brook, NY, USA); Chen, Chao (Stony Brook University, Stony Brook, NY, USA)","Gupta, Saumya (Stony Brook University)","Gupta, Saumya (Stony Brook University); Hu, Xiaoling (Stony Brook University); Kaan, James (Stony Brook University); Jin, Michael (Stony Brook University); Mpoy, Mutshipay (Stony Brook University); Chung, Katherine (Stony Brook University); Singh, Gagandeep (Stony Brook University); Saltz, Mary (Stony Brook University); Kurc, Tahsin (Stony Brook University); Saltz, Joel (Stony Brook University); Tassiopoulos, Apostolos (Stony Brook University); Prasanna, Prateek (Stony Brook University); Chen, Chao (Stony Brook University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1152094967,46 Information and Computing Sciences; 4611 Machine Learning,
1282,pub.1142920782,10.48550/arxiv.2111.11665,,,RadFusion: Benchmarking Performance and Fairness for Multimodal  Pulmonary Embolism Detection from CT and EHR,"Despite the routine use of electronic health record (EHR) data by
radiologists to contextualize clinical history and inform image interpretation,
the majority of deep learning architectures for medical imaging are unimodal,
i.e., they only learn features from pixel-level information. Recent research
revealing how race can be recovered from pixel data alone highlights the
potential for serious biases in models which fail to account for demographics
and other key patient attributes. Yet the lack of imaging datasets which
capture clinical context, inclusive of demographics and longitudinal medical
history, has left multimodal medical imaging underexplored. To better assess
these challenges, we present RadFusion, a multimodal, benchmark dataset of 1794
patients with corresponding EHR data and high-resolution computed tomography
(CT) scans labeled for pulmonary embolism. We evaluate several representative
multimodal fusion models and benchmark their fairness properties across
protected subgroups, e.g., gender, race/ethnicity, age. Our results suggest
that integrating imaging and EHR data can improve classification performance
and robustness without introducing large disparities in the true positive rate
between population groups.",,,arXiv,,,2021-11-23,2021,,,,,,All OA; Green,Preprint,"Zhou, Yuyin; Huang, Shih-Cheng; Fries, Jason Alan; Youssef, Alaa; Amrhein, Timothy J.; Chang, Marcello; Banerjee, Imon; Rubin, Daniel; Xing, Lei; Shah, Nigam; Lungren, Matthew P.","Zhou, Yuyin (); Huang, Shih-Cheng (); Fries, Jason Alan (); Youssef, Alaa (); Amrhein, Timothy J. (); Chang, Marcello (); Banerjee, Imon (); Rubin, Daniel (); Xing, Lei (); Shah, Nigam (); Lungren, Matthew P. ()",,"Zhou, Yuyin (); Huang, Shih-Cheng (); Fries, Jason Alan (); Youssef, Alaa (); Amrhein, Timothy J. (); Chang, Marcello (); Banerjee, Imon (); Rubin, Daniel (); Xing, Lei (); Shah, Nigam (); Lungren, Matthew P. ()",1,1,,0.88,,https://app.dimensions.ai/details/publication/pub.1142920782,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences; 46 Information and Computing Sciences,
1282,pub.1142083248,10.48550/arxiv.2110.10403,,,AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation,"Recent advances in transformer-based models have drawn attention to exploring
these techniques in medical image segmentation, especially in conjunction with
the U-Net model (or its variants), which has shown great success in medical
image segmentation, under both 2D and 3D settings. Current 2D based methods
either directly replace convolutional layers with pure transformers or consider
a transformer as an additional intermediate encoder between the encoder and
decoder of U-Net. However, these approaches only consider the attention
encoding within one single slice and do not utilize the axial-axis information
naturally provided by a 3D volume. In the 3D setting, convolution on volumetric
data and transformers both consume large GPU memory. One has to either
downsample the image or use cropped local patches to reduce GPU memory usage,
which limits its performance. In this paper, we propose Axial Fusion
Transformer UNet (AFTer-UNet), which takes both advantages of convolutional
layers' capability of extracting detailed features and transformers' strength
on long sequence modeling. It considers both intra-slice and inter-slice
long-range cues to guide the segmentation. Meanwhile, it has fewer parameters
and takes less GPU memory to train than the previous transformer-based models.
Extensive experiments on three multi-organ segmentation datasets demonstrate
that our method outperforms current state-of-the-art methods.",,,arXiv,,,2021-10-20,2021,,,,,,All OA; Green,Preprint,"Yan, Xiangyi; Tang, Hao; Sun, Shanlin; Ma, Haoyu; Kong, Deying; Xie, Xiaohui","Yan, Xiangyi (); Tang, Hao (); Sun, Shanlin (); Ma, Haoyu (); Kong, Deying (); Xie, Xiaohui ()",,"Yan, Xiangyi (); Tang, Hao (); Sun, Shanlin (); Ma, Haoyu (); Kong, Deying (); Xie, Xiaohui ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142083248,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",
1280,pub.1153198877,10.48550/arxiv.2211.14805,,,Rethinking Data Augmentation for Single-source Domain Generalization in  Medical Image Segmentation,"Single-source domain generalization (SDG) in medical image segmentation is a
challenging yet essential task as domain shifts are quite common among clinical
image datasets. Previous attempts most conduct global-only/random augmentation.
Their augmented samples are usually insufficient in diversity and
informativeness, thus failing to cover the possible target domain distribution.
In this paper, we rethink the data augmentation strategy for SDG in medical
image segmentation. Motivated by the class-level representation invariance and
style mutability of medical images, we hypothesize that unseen target data can
be sampled from a linear combination of $C$ (the class number) random
variables, where each variable follows a location-scale distribution at the
class level. Accordingly, data augmented can be readily made by sampling the
random variables through a general form. On the empirical front, we implement
such strategy with constrained B$\acute{\rm e}$zier transformation on both
global and local (i.e. class-level) regions, which can largely increase the
augmentation diversity. A Saliency-balancing Fusion mechanism is further
proposed to enrich the informativeness by engaging the gradient information,
guiding augmentation with proper orientation and magnitude. As an important
contribution, we prove theoretically that our proposed augmentation can lead to
an upper bound of the generalization risk on the unseen target domain, thus
confirming our hypothesis. Combining the two strategies, our Saliency-balancing
Location-scale Augmentation (SLAug) exceeds the state-of-the-art works by a
large margin in two challenging SDG tasks. Code is available at
https://github.com/Kaiseem/SLAug .",,,arXiv,,,2022-11-27,2022,,,,,,All OA; Green,Preprint,"Su, Zixian; Yao, Kai; Yang, Xi; Wang, Qiufeng; Sun, Jie; Huang, Kaizhu","Su, Zixian (); Yao, Kai (); Yang, Xi (); Wang, Qiufeng (); Sun, Jie (); Huang, Kaizhu ()",,"Su, Zixian (); Yao, Kai (); Yang, Xi (); Wang, Qiufeng (); Sun, Jie (); Huang, Kaizhu ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153198877,46 Information and Computing Sciences; 4611 Machine Learning,
1280,pub.1145639403,10.1109/wacv51458.2022.00181,,,UNETR: Transformers for 3D Medical Image Segmentation,"Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful ""U-shaped"" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",,,,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),,2022-01-08,2022,,2022-01-08,00,,1748-1758,All OA; Green,Proceeding,"Hatamizadeh, Ali; Tang, Yucheng; Nath, Vishwesh; Yang, Dong; Myronenko, Andriy; Landman, Bennett; Roth, Holger R.; Xu, Daguang","Hatamizadeh, Ali (NVIDIA); Tang, Yucheng (Vanderbilt University); Nath, Vishwesh (NVIDIA); Yang, Dong (NVIDIA); Myronenko, Andriy (NVIDIA); Landman, Bennett (Vanderbilt University); Roth, Holger R. (NVIDIA); Xu, Daguang (NVIDIA)","Hatamizadeh, Ali (Nvidia (United States))","Hatamizadeh, Ali (Nvidia (United States)); Tang, Yucheng (Vanderbilt University); Nath, Vishwesh (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Landman, Bennett (Vanderbilt University); Roth, Holger R. (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",218,218,,,http://arxiv.org/pdf/2103.10504,https://app.dimensions.ai/details/publication/pub.1145639403,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4611 Machine Learning,
1280,pub.1118794862,10.48550/arxiv.1704.06544,,,A 3D fully convolutional neural network and a random walker to segment  the esophagus in CT,"Precise delineation of organs at risk (OAR) is a crucial task in radiotherapy
treatment planning, which aims at delivering high dose to the tumour while
sparing healthy tissues. In recent years algorithms showed high performance and
the possibility to automate this task for many OAR. However, for some OAR
precise delineation remains challenging. The esophagus with a versatile shape
and poor contrast is among these structures. To tackle these issues we propose
a 3D fully (convolutional neural network (CNN) driven random walk (RW) approach
to automatically segment the esophagus on CT. First, a soft probability map is
generated by the CNN. Then an active contour model (ACM) is fitted on the
probability map to get a first estimation of the center line. The outputs of
the CNN and ACM are then used in addition to CT Hounsfield values to drive the
RW. Evaluation and training was done on 50 CTs with peer reviewed esophagus
contours. Results were assessed regarding spatial overlap and shape
similarities.
  The generated contours showed a mean Dice coefficient of 0.76, an average
symmetric square distance of 1.36 mm and an average Hausdorff distance of 11.68
compared to the reference. These figures translate into a very good agreement
with the reference contours and an increase in accuracy compared to other
methods.
  We show that by employing a CNN accurate estimations of esophagus location
can be obtained and refined by a post processing RW step. One of the main
advantages compared to previous methods is that our network performs
convolutions in a 3D manner, fully exploiting the 3D spatial context and
performing an efficient and precise volume-wise prediction. The whole
segmentation process is fully automatic and yields esophagus delineations in
very good agreement with the used gold standard, showing that it can compete
with previously published methods.",,,arXiv,,,2017-04-21,2017,,,,,,All OA; Green,Preprint,"Fechter, Tobias; Adebahr, Sonja; Baltas, Dimos; Ayed, Ismail Ben; Desrosiers, Christian; Dolz, Jose","Fechter, Tobias (); Adebahr, Sonja (); Baltas, Dimos (); Ayed, Ismail Ben (); Desrosiers, Christian (); Dolz, Jose ()",,"Fechter, Tobias (); Adebahr, Sonja (); Baltas, Dimos (); Ayed, Ismail Ben (); Desrosiers, Christian (); Dolz, Jose ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1118794862,32 Biomedical and Clinical Sciences; 3211 Oncology and Carcinogenesis; 51 Physical Sciences; 5105 Medical and Biological Physics,
1279,pub.1155741507,10.1007/978-3-031-26351-4_2,,,APAUNet: Axis Projection Attention UNet for Small Target in 3D Medical Segmentation,"In 3D medical image segmentation, small targets segmentation is crucial for diagnosis but still faces challenges. In this paper, we propose the Axis Projection Attention UNet, named APAUNet, for 3D medical image segmentation, especially for small targets. Considering the large proportion of the background in the 3D feature space, we introduce a projection strategy to project the 3D features into three orthogonal 2D planes to capture the contextual attention from different views. In this way, we can filter out the redundant feature information and mitigate the loss of critical information for small lesions in 3D scans. Then we utilize a dimension hybridization strategy to fuse the 3D features with attention from different axes and merge them by a weighted summation to adaptively learn the importance of different perspectives. Finally, in the APA Decoder, we concatenate both high and low resolution features in the 2D projection process, thereby obtaining more precise multi-scale information, which is vital for small lesion segmentation. Quantitative and qualitative experimental results on two public datasets (BTCV and MSD) demonstrate that our proposed APAUNet outperforms the other methods. Concretely, our APAUNet achieves an average dice score of 87.84 on BTCV, 84.48 on MSD-Liver and 69.13 on MSD-Pancreas, and significantly surpass the previous SOTA methods on small targets.","This work was supported in part by the National Key R &amp;D Program of China with grant No.2018YFB1800800, by the Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S &amp;T Cooperation Zone, by NSFC-Youth 61902335, by Shenzhen Outstanding Talents Training Fund, by Guangdong Research Project No.2017ZT07X152 and No.2019CX01X104, by the Guangdong Provincial Key Laboratory of Future Networks of Intelligence (Grant No.2022B1212010001), by zelixir biotechnology company Fund, by the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen, by Tencent Open Fund, and by ITSO at CUHKSZ.",,Lecture Notes in Computer Science,Computer Vision – ACCV 2022,,2023-02-26,2023,2023-02-26,2023,13846,,21-36,Closed,Chapter,"Jiang, Yuncheng; Zhang, Zixun; Qin, Shixi; Guo, Yao; Li, Zhen; Cui, Shuguang","Jiang, Yuncheng (FNii, CUHK-Shenzhen, Shenzhen, Guangdong, China; SSE, CUHK-Shenzhen, Shenzhen, Guangdong, China; SRIBD, CUHK-Shenzhen, Shenzhen, Guangdong, China); Zhang, Zixun (FNii, CUHK-Shenzhen, Shenzhen, Guangdong, China; SSE, CUHK-Shenzhen, Shenzhen, Guangdong, China; SRIBD, CUHK-Shenzhen, Shenzhen, Guangdong, China); Qin, Shixi (FNii, CUHK-Shenzhen, Shenzhen, Guangdong, China; SSE, CUHK-Shenzhen, Shenzhen, Guangdong, China; SRIBD, CUHK-Shenzhen, Shenzhen, Guangdong, China); Guo, Yao (Shanghai Jiao Tong University, Shanghai, China); Li, Zhen (FNii, CUHK-Shenzhen, Shenzhen, Guangdong, China; SSE, CUHK-Shenzhen, Shenzhen, Guangdong, China; SRIBD, CUHK-Shenzhen, Shenzhen, Guangdong, China); Cui, Shuguang (FNii, CUHK-Shenzhen, Shenzhen, Guangdong, China; SSE, CUHK-Shenzhen, Shenzhen, Guangdong, China; Pengcheng Laboratory, Shenzhen, Guangdong, China)","Li, Zhen (; ; )","Jiang, Yuncheng (); Zhang, Zixun (); Qin, Shixi (); Guo, Yao (Shanghai Jiao Tong University); Li, Zhen (); Cui, Shuguang (Peng Cheng Laboratory)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155741507,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",
1277,pub.1153517448,10.48550/arxiv.2212.03967,,,Few-shot Medical Image Segmentation with Cycle-resemblance Attention,"Recently, due to the increasing requirements of medical imaging applications
and the professional requirements of annotating medical images, few-shot
learning has gained increasing attention in the medical image semantic
segmentation field. To perform segmentation with limited number of labeled
medical images, most existing studies use Proto-typical Networks (PN) and have
obtained compelling success. However, these approaches overlook the query image
features extracted from the proposed representation network, failing to
preserving the spatial connection between query and support images. In this
paper, we propose a novel self-supervised few-shot medical image segmentation
network and introduce a novel Cycle-Resemblance Attention (CRA) module to fully
leverage the pixel-wise relation between query and support medical images.
Notably, we first line up multiple attention blocks to refine more abundant
relation information. Then, we present CRAPNet by integrating the CRA module
with a classic prototype network, where pixel-wise relations between query and
support features are well recaptured for segmentation. Extensive experiments on
two different medical image datasets, e.g., abdomen MRI and abdomen CT,
demonstrate the superiority of our model over existing state-of-the-art
methods.",,,arXiv,,,2022-12-07,2022,,,,,,All OA; Green,Preprint,"Ding, Hao; Sun, Changchang; Tang, Hao; Cai, Dawen; Yan, Yan","Ding, Hao (); Sun, Changchang (); Tang, Hao (); Cai, Dawen (); Yan, Yan ()",,"Ding, Hao (); Sun, Changchang (); Tang, Hao (); Cai, Dawen (); Yan, Yan ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153517448,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4605 Data Management and Data Science,
1277,pub.1136569475,10.48550/arxiv.2103.10504,,,UNETR: Transformers for 3D Medical Image Segmentation,"Fully Convolutional Neural Networks (FCNNs) with contracting and expanding
paths have shown prominence for the majority of medical image segmentation
applications since the past decade. In FCNNs, the encoder plays an integral
role by learning both global and local features and contextual representations
which can be utilized for semantic output prediction by the decoder. Despite
their success, the locality of convolutional layers in FCNNs, limits the
capability of learning long-range spatial dependencies. Inspired by the recent
success of transformers for Natural Language Processing (NLP) in long-range
sequence learning, we reformulate the task of volumetric (3D) medical image
segmentation as a sequence-to-sequence prediction problem. We introduce a novel
architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer
as the encoder to learn sequence representations of the input volume and
effectively capture the global multi-scale information, while also following
the successful ""U-shaped"" network design for the encoder and decoder. The
transformer encoder is directly connected to a decoder via skip connections at
different resolutions to compute the final semantic segmentation output. We
have validated the performance of our method on the Multi Atlas Labeling Beyond
The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical
Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation
tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV
leaderboard. Code: https://monai.io/research/unetr",,,arXiv,,,2021-03-18,2021,,,,,,All OA; Green,Preprint,"Hatamizadeh, Ali; Tang, Yucheng; Nath, Vishwesh; Yang, Dong; Myronenko, Andriy; Landman, Bennett; Roth, Holger; Xu, Daguang","Hatamizadeh, Ali (); Tang, Yucheng (); Nath, Vishwesh (); Yang, Dong (); Myronenko, Andriy (); Landman, Bennett (); Roth, Holger (); Xu, Daguang ()",,"Hatamizadeh, Ali (); Tang, Yucheng (); Nath, Vishwesh (); Yang, Dong (); Myronenko, Andriy (); Landman, Bennett (); Roth, Holger (); Xu, Daguang ()",2,2,,1.58,,https://app.dimensions.ai/details/publication/pub.1136569475,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4611 Machine Learning,
1241,pub.1136816488,10.48550/arxiv.2103.15858,,,CateNorm: Categorical Normalization for Robust Medical Image  Segmentation,"Batch normalization (BN) uniformly shifts and scales the activations based on
the statistics of a batch of images. However, the intensity distribution of the
background pixels often dominates the BN statistics because the background
accounts for a large proportion of the entire image. This paper focuses on
enhancing BN with the intensity distribution of foreground pixels, the one that
really matters for image segmentation. We propose a new normalization strategy,
named categorical normalization (CateNorm), to normalize the activations
according to categorical statistics. The categorical statistics are obtained by
dynamically modulating specific regions in an image that belong to the
foreground. CateNorm demonstrates both precise and robust segmentation results
across five public datasets obtained from different domains, covering complex
and variable data distributions. It is attributable to the ability of CateNorm
to capture domain-invariant information from multiple domains (institutions) of
medical data. Code is available at https://github.com/lambert-x/CateNorm.",,,arXiv,,,2021-03-29,2021,,,,,,All OA; Green,Preprint,"Xiao, Junfei; Yu, Lequan; Zhou, Zongwei; Bai, Yutong; Xing, Lei; Yuille, Alan; Zhou, Yuyin","Xiao, Junfei (); Yu, Lequan (); Zhou, Zongwei (); Bai, Yutong (); Xing, Lei (); Yuille, Alan (); Zhou, Yuyin ()",,"Xiao, Junfei (); Yu, Lequan (); Zhou, Zongwei (); Bai, Yutong (); Xing, Lei (); Yuille, Alan (); Zhou, Yuyin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136816488,46 Information and Computing Sciences; 49 Mathematical Sciences; 4905 Statistics,
1241,pub.1153199288,10.48550/arxiv.2211.15235,,,Reducing Domain Gap in Frequency and Spatial domain for Cross-modality  Domain Adaptation on Medical Image Segmentation,"Unsupervised domain adaptation (UDA) aims to learn a model trained on source
domain and performs well on unlabeled target domain. In medical image
segmentation field, most existing UDA methods depend on adversarial learning to
address the domain gap between different image modalities, which is ineffective
due to its complicated training process. In this paper, we propose a simple yet
effective UDA method based on frequency and spatial domain transfer uner
multi-teacher distillation framework. In the frequency domain, we first
introduce non-subsampled contourlet transform for identifying domain-invariant
and domain-variant frequency components (DIFs and DVFs), and then keep the DIFs
unchanged while replacing the DVFs of the source domain images with that of the
target domain images to narrow the domain gap. In the spatial domain, we
propose a batch momentum update-based histogram matching strategy to reduce the
domain-variant image style bias. Experiments on two cross-modality medical
image segmentation datasets (cardiac, abdominal) show that our proposed method
achieves superior performance compared to state-of-the-art methods.",,,arXiv,,,2022-11-28,2022,,,,,,All OA; Green,Preprint,"Liu, Shaolei; Yin, Siqi; Qu, Linhao; Wang, Manning","Liu, Shaolei (); Yin, Siqi (); Qu, Linhao (); Wang, Manning ()",,"Liu, Shaolei (); Yin, Siqi (); Qu, Linhao (); Wang, Manning ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153199288,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1241,pub.1145639555,10.1109/wacv51458.2022.00333,,,AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation,"Recent advances in transformer-based models have drawn attention to exploring these techniques in medical image segmentation, especially in conjunction with the UNet model (or its variants), which has shown great success in medical image segmentation, under both 2D and 3D settings. Current 2D based methods either directly replace convolutional layers with pure transformers or consider a transformer as an additional intermediate encoder between the encoder and decoder of U-Net. However, these approaches only consider the attention encoding within one single slice and do not utilize the axial-axis information naturally provided by a 3D volume. In the 3D setting, convolution on volumetric data and transformers both consume large GPU memory. One has to either downsample the image or use cropped local patches to reduce GPU memory usage, which limits its performance. In this paper, we propose Axial Fusion Transformer UNet (AFTer-UNet), which takes both advantages of convolutional layers’ capability of extracting detailed features and transformers’ strength on long sequence modeling. It considers both intra-slice and inter-slice long-range cues to guide the segmentation. Meanwhile, it has fewer parameters and takes less GPU memory to train than the previous transformer-based models. Extensive experiments on three multi-organ segmentation datasets demonstrate that our method outperforms current state-of-the-art methods.",,,,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),,2022-01-08,2022,,2022-01-08,00,,3270-3280,All OA; Green,Proceeding,"Yan, Xiangyi; Tang, Hao; Sun, Shanlin; Ma, Haoyu; Kong, Deying; Xie, Xiaohui","Yan, Xiangyi (University of California, Irvine); Tang, Hao (University of California, Irvine); Sun, Shanlin (University of California, Irvine); Ma, Haoyu (University of California, Irvine); Kong, Deying (University of California, Irvine); Xie, Xiaohui (University of California, Irvine)","Yan, Xiangyi (University of California, Irvine)","Yan, Xiangyi (University of California, Irvine); Tang, Hao (University of California, Irvine); Sun, Shanlin (University of California, Irvine); Ma, Haoyu (University of California, Irvine); Kong, Deying (University of California, Irvine); Xie, Xiaohui (University of California, Irvine)",35,35,,,http://arxiv.org/pdf/2110.10403,https://app.dimensions.ai/details/publication/pub.1145639555,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",
1241,pub.1132845467,10.48550/arxiv.2011.10217,,,DoDNet: Learning to segment multi-organ and tumors from multiple  partially labeled datasets,"Due to the intensive cost of labor and expertise in annotating 3D medical
images at a voxel level, most benchmark datasets are equipped with the
annotations of only one type of organs and/or tumors, resulting in the
so-called partially labeling issue. To address this, we propose a dynamic
on-demand network (DoDNet) that learns to segment multiple organs and tumors on
partially labeled datasets. DoDNet consists of a shared encoder-decoder
architecture, a task encoding module, a controller for generating dynamic
convolution filters, and a single but dynamic segmentation head. The
information of the current segmentation task is encoded as a task-aware prior
to tell the model what the task is expected to solve. Different from existing
approaches which fix kernels after training, the kernels in dynamic head are
generated adaptively by the controller, conditioned on both input image and
assigned task. Thus, DoDNet is able to segment multiple organs and tumors, as
done by multiple networks or a multi-head network, in a much efficient and
flexible manner. We have created a large-scale partially labeled dataset,
termed MOTS, and demonstrated the superior performance of our DoDNet over other
competitors on seven organ and tumor segmentation tasks. We also transferred
the weights pre-trained on MOTS to a downstream multi-organ segmentation task
and achieved state-of-the-art performance. This study provides a general 3D
medical image segmentation model that has been pre-trained on a large-scale
partially labelled dataset and can be extended (after fine-tuning) to
downstream volumetric medical data segmentation tasks. The dataset and code
areavailableat: https://git.io/DoDNet",,,arXiv,,,2020-11-19,2020,,,,,,All OA; Green,Preprint,"Zhang, Jianpeng; Xie, Yutong; Xia, Yong; Shen, Chunhua","Zhang, Jianpeng (); Xie, Yutong (); Xia, Yong (); Shen, Chunhua ()",,"Zhang, Jianpeng (); Xie, Yutong (); Xia, Yong (); Shen, Chunhua ()",1,1,,0.52,,https://app.dimensions.ai/details/publication/pub.1132845467,46 Information and Computing Sciences; 4611 Machine Learning,
1239,pub.1154169343,10.48550/arxiv.2212.14310,,,MagicNet: Semi-Supervised Multi-Organ Segmentation via Magic-Cube  Partition and Recovery,"We propose a novel teacher-student model for semi-supervised multi-organ
segmentation. In teacher-student model, data augmentation is usually adopted on
unlabeled data to regularize the consistent training between teacher and
student. We start from a key perspective that fixed relative locations and
variable sizes of different organs can provide distribution information where a
multi-organ CT scan is drawn. Thus, we treat the prior anatomy as a strong tool
to guide the data augmentation and reduce the mismatch between labeled and
unlabeled images for semi-supervised learning. More specifically, we propose a
data augmentation strategy based on partition-and-recovery N$^3$ cubes cross-
and within- labeled and unlabeled images. Our strategy encourages unlabeled
images to learn organ semantics in relative locations from the labeled images
(cross-branch) and enhances the learning ability for small organs
(within-branch). For within-branch, we further propose to refine the quality of
pseudo labels by blending the learned representations from small cubes to
incorporate local attributes. Our method is termed as MagicNet, since it treats
the CT volume as a magic-cube and $N^3$-cube partition-and-recovery process
matches with the rule of playing a magic-cube. Extensive experiments on two
public CT multi-organ datasets demonstrate the effectiveness of MagicNet, and
noticeably outperforms state-of-the-art semi-supervised medical image
segmentation approaches, with +7% DSC improvement on MACT dataset with 10%
labeled images.",,,arXiv,,,2022-12-29,2022,,,,,,All OA; Green,Preprint,"Chen, Duowen; Bai, Yunhao; Shen, Wei; Li, Qingli; Yu, Lequan; Wang, Yan","Chen, Duowen (); Bai, Yunhao (); Shen, Wei (); Li, Qingli (); Yu, Lequan (); Wang, Yan ()",,"Chen, Duowen (); Bai, Yunhao (); Shen, Wei (); Li, Qingli (); Yu, Lequan (); Wang, Yan ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154169343,46 Information and Computing Sciences; 4611 Machine Learning,
1239,pub.1152128571,10.1007/978-3-031-19803-8_33,,,UniMiSS: Universal Medical Self-supervised Learning via Breaking Dimensionality Barrier,"Self-supervised learning (SSL) opens up huge opportunities for medical image analysis that is well known for its lack of annotations. However, aggregating massive (unlabeled) 3D medical images like computerized tomography (CT) remains challenging due to its high imaging cost and privacy restrictions. In this paper, we advocate bringing a wealth of 2D images like chest X-rays as compensation for the lack of 3D data, aiming to build a universal medical self-supervised representation learning framework, called UniMiSS. The following problem is how to break the dimensionality barrier, i.e., making it possible to perform SSL with both 2D and 3D images? To achieve this, we design a pyramid U-like medical Transformer (MiT). It is composed of the switchable patch embedding (SPE) module and Transformers. The SPE module adaptively switches to either 2D or 3D patch embedding, depending on the input dimension. The embedded patches are converted into a sequence regardless of their original dimensions. The Transformers model the long-term dependencies in a sequence-to-sequence manner, thus enabling UniMiSS to learn representations from both 2D and 3D images. With the MiT as the backbone, we perform the UniMiSS in a self-distillation manner. We conduct expensive experiments on six 3D/2D medical image analysis tasks, including segmentation and classification. The results show that the proposed UniMiSS achieves promising performance on various downstream tasks, outperforming the ImageNet pre-training and other advanced SSL counterparts substantially. Code is available at https://github.com/YtongXie/UniMiSS-code.",Jianpeng Zhang and Yong Xia were supported by National Natural Science Foundation of China under Grants 62171377. Qi Wu was funded by ARC DE190100539.,,Lecture Notes in Computer Science,Computer Vision – ECCV 2022,,2022-10-23,2022,2022-10-23,2022,13681,,558-575,All OA; Green,Chapter,"Xie, Yutong; Zhang, Jianpeng; Xia, Yong; Wu, Qi","Xie, Yutong (The University of Adelaide, Adelaide, Australia); Zhang, Jianpeng (School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China); Xia, Yong (School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China); Wu, Qi (The University of Adelaide, Adelaide, Australia)","Wu, Qi (University of Adelaide)","Xie, Yutong (University of Adelaide); Zhang, Jianpeng (Northwestern Polytechnical University); Xia, Yong (Northwestern Polytechnical University); Wu, Qi (University of Adelaide)",1,1,,,http://arxiv.org/pdf/2112.09356,https://app.dimensions.ai/details/publication/pub.1152128571,46 Information and Computing Sciences; 4611 Machine Learning,
1235,pub.1151381613,10.1109/cvpr52688.2022.02007,,,Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis,"Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pretraining; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art on the public test leaderboards of both MSD11https://decathlon-10.grand-challenge.org/evaluation/challenge/leaderboard/ and BTCV 22https://www.synapse.org/#!Synapse:syn3193805/wiki/217785/ datasets. Code: https://monai.io/research/swin-unetr. https://decathlon-10.grand-challenge.org/evaluation/challenge/leaderboard/ https://www.synapse.org/#!Synapse:syn3193805/wiki/217785/",,,,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2022-06-24,2022,,2022-06-24,00,,20698-20708,All OA; Green,Proceeding,"Tang, Yucheng; Yang, Dong; Li, Wenqi; Roth, Holger R.; Landman, Bennett; Xu, Daguang; Nath, Vishwesh; Hatamizadeh, Ali","Tang, Yucheng (Vanderbilt University); Yang, Dong (NVIDIA); Li, Wenqi (NVIDIA); Roth, Holger R. (NVIDIA); Landman, Bennett (Vanderbilt University); Xu, Daguang (NVIDIA); Nath, Vishwesh (Vanderbilt University); Hatamizadeh, Ali (Vanderbilt University)","Hatamizadeh, Ali (Vanderbilt University)","Tang, Yucheng (Vanderbilt University); Yang, Dong (Nvidia (United States)); Li, Wenqi (Nvidia (United States)); Roth, Holger R. (Nvidia (United States)); Landman, Bennett (Vanderbilt University); Xu, Daguang (Nvidia (United States)); Nath, Vishwesh (Vanderbilt University); Hatamizadeh, Ali (Vanderbilt University)",44,44,,,http://arxiv.org/pdf/2111.14791,https://app.dimensions.ai/details/publication/pub.1151381613,46 Information and Computing Sciences; 4611 Machine Learning,
1235,pub.1142382197,10.1109/cvpr46437.2021.00125,,,DoDNet: Learning to Segment Multi-Organ and Tumors from Multiple Partially Labeled Datasets,"Due to the intensive cost of labor and expertise in annotating 3D medical images at a voxel level, most benchmark datasets are equipped with the annotations of only one type of organs and/or tumors, resulting in the so-called partially labeling issue. To address this issue, we propose a dynamic on-demand network (DoDNet) that learns to segment multiple organs and tumors on partially labeled datasets. DoD-Net consists of a shared encoder-decoder architecture, a task encoding module, a controller for dynamic filter generation, and a single but dynamic segmentation head. The information of current segmentation task is encoded as a task-aware prior to tell the model what the task is expected to achieve. Different from existing approaches which fix kernels after training, the kernels in dynamic head are generated adaptively by the controller, conditioned on both input image and assigned task. Thus, DoDNet is able to segment multiple organs and tumors, as done by multiple networks or a multi-head network, in a much efficient and flexible manner. We created a large-scale partially labeled dataset called MOTS and demonstrated the superior performance of our DoDNet over other competitors on seven organ and tumor segmentation tasks. We also transferred the weights pre-trained on MOTS to a downstream multi-organ segmentation task and achieved state-of-the-art performance. This study provides a general 3D medical image segmentation model that has been pre-trained on a large-scale partially labeled dataset and can be extended (after fine-tuning) to downstream volumetric medical data segmentation tasks. Code and models are available at: https://git.io/DoDNet","J. Zhang, Y. Xie, and Y. Xia are supported in part by the National Natural Science Foundation of China under Grants 61771397 and in part by the CAAI-Huawei MindSpore Open Fund. J. Zhang, Y. Xie, and Y. Xia are supported in part by the National Natural Science Foundation of China under Grants 61771397 and in part by the CAAI-Huawei MindSpore Open Fund.",,,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2021-06-25,2021,,2021-06-25,00,,1195-1204,All OA; Green,Proceeding,"Zhang, Jianpeng; Xie, Yutong; Xia, Yong; Shen, Chunhua","Zhang, Jianpeng (School of Computer Science and Engineering, Northwestern Polytechnical University, China; The University of Adelaide, Australia); Xie, Yutong (School of Computer Science and Engineering, Northwestern Polytechnical University, China; The University of Adelaide, Australia); Xia, Yong (School of Computer Science and Engineering, Northwestern Polytechnical University, China); Shen, Chunhua (The University of Adelaide, Australia)","Zhang, Jianpeng (Northwestern Polytechnical University; University of Adelaide)","Zhang, Jianpeng (Northwestern Polytechnical University; University of Adelaide); Xie, Yutong (Northwestern Polytechnical University; University of Adelaide); Xia, Yong (Northwestern Polytechnical University); Shen, Chunhua (University of Adelaide)",35,35,,28.65,http://arxiv.org/pdf/2011.10217,https://app.dimensions.ai/details/publication/pub.1142382197,46 Information and Computing Sciences; 4611 Machine Learning,
1206,pub.1137401673,10.48550/arxiv.2104.10195,,,Auto-FedAvg: Learnable Federated Averaging for Multi-Institutional  Medical Image Segmentation,"Federated learning (FL) enables collaborative model training while preserving
each participant's privacy, which is particularly beneficial to the medical
field. FedAvg is a standard algorithm that uses fixed weights, often
originating from the dataset sizes at each client, to aggregate the distributed
learned models on a server during the FL process. However, non-identical data
distribution across clients, known as the non-i.i.d problem in FL, could make
this assumption for setting fixed aggregation weights sub-optimal. In this
work, we design a new data-driven approach, namely Auto-FedAvg, where
aggregation weights are dynamically adjusted, depending on data distributions
across data silos and the current training progress of the models. We
disentangle the parameter set into two parts, local model parameters and global
aggregation parameters, and update them iteratively with a
communication-efficient algorithm. We first show the validity of our approach
by outperforming state-of-the-art FL methods for image recognition on a
heterogeneous data split of CIFAR-10. Furthermore, we demonstrate our
algorithm's effectiveness on two multi-institutional medical image analysis
tasks, i.e., COVID-19 lesion segmentation in chest CT and pancreas segmentation
in abdominal CT.",,,arXiv,,,2021-04-20,2021,,,,,,All OA; Green,Preprint,"Xia, Yingda; Yang, Dong; Li, Wenqi; Myronenko, Andriy; Xu, Daguang; Obinata, Hirofumi; Mori, Hitoshi; An, Peng; Harmon, Stephanie; Turkbey, Evrim; Turkbey, Baris; Wood, Bradford; Patella, Francesca; Stellato, Elvira; Carrafiello, Gianpaolo; Ierardi, Anna; Yuille, Alan; Roth, Holger","Xia, Yingda (); Yang, Dong (); Li, Wenqi (); Myronenko, Andriy (); Xu, Daguang (); Obinata, Hirofumi (); Mori, Hitoshi (); An, Peng (); Harmon, Stephanie (); Turkbey, Evrim (); Turkbey, Baris (); Wood, Bradford (); Patella, Francesca (); Stellato, Elvira (); Carrafiello, Gianpaolo (); Ierardi, Anna (); Yuille, Alan (); Roth, Holger ()",,"Xia, Yingda (); Yang, Dong (); Li, Wenqi (); Myronenko, Andriy (); Xu, Daguang (); Obinata, Hirofumi (); Mori, Hitoshi (); An, Peng (); Harmon, Stephanie (); Turkbey, Evrim (); Turkbey, Baris (); Wood, Bradford (); Patella, Francesca (); Stellato, Elvira (); Carrafiello, Gianpaolo (); Ierardi, Anna (); Yuille, Alan (); Roth, Holger ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1137401673,46 Information and Computing Sciences; 4611 Machine Learning,
1202,pub.1151550687,10.1109/access.2022.3211501,,,TransNorm: Transformer Provides a Strong Spatial Normalization Mechanism for a Deep Segmentation Model,"In the past few years, convolutional neural networks (CNNs), particularly U-Net, have been the prevailing technique in the medical image processing era. Specifically, the U-Net model, as well as its alternatives, have successfully managed to address a wide variety of medical image segmentation tasks. However, these architectures are intrinsically imperfect as they fail to exhibit long-range interactions and spatial dependencies leading to a severe performance drop in the segmentation of medical images with variable shapes and structures. Transformers, preliminary proposed for sequence-to-sequence prediction, have arisen as surrogate architectures to precisely model global information assisted by the self-attention mechanism. Despite being feasibly designed, utilizing a pure Transformer for image segmentation purposes can result in limited localization capacity stemming from inadequate low-level features. Thus, a line of research strives to design robust variants of Transformer-based U-Net. In this paper, we propose Trans-Norm, a novel deep segmentation framework which concomitantly consolidates a Transformer module into both encoder and skip-connections of the standard U-Net. We argue that the expedient design of skip-connections can be crucial for accurate segmentation as it can assist feature fusion between the expanding and contracting paths. In this respect, we derive a Spatial Normalization mechanism from the Transformer module to adaptively recalibrate the skip connection path. Extensive experiments across three typical tasks for medical image segmentation demonstrate the effectiveness of TransNorm. The codes and trained models are publicly available at github.",,,IEEE Access,,,2022-01-01,2022,2022-10-03,2022-01-01,10,,108205-108215,All OA; Gold,Article,"Azad, Reza; Al-Antary, Mohammad T.; Heidari, Moein; Merhof, Dorit","Azad, Reza (Institute of Imaging and Computer Vision, RWTH Aachen University, 52074, Aachen, Germany); Al-Antary, Mohammad T. (School of Computing and Mathematical Sciences, University of Greenwich, SE10 9LS, London, U.K.); Heidari, Moein (School of Electrical Engineering, Iran University of Science and Technology, Tehran, 13114-16846, Iran); Merhof, Dorit (Faculty of Informatics and Data Science, University of Regensburg, 93053, Regensburg, Germany; Fraunhofer Institute for Digital Medicine MEVIS, 28359, Bremen, Germany)","Merhof, Dorit (University of Regensburg; Fraunhofer Institute for Digital Medicine)","Azad, Reza (RWTH Aachen University); Al-Antary, Mohammad T. (University of Greenwich); Heidari, Moein (Iran University of Science and Technology); Merhof, Dorit (University of Regensburg; Fraunhofer Institute for Digital Medicine)",3,3,,,https://doi.org/10.1109/access.2022.3211501,https://app.dimensions.ai/details/publication/pub.1151550687,46 Information and Computing Sciences; 4611 Machine Learning,
1202,pub.1145901382,10.1109/iccv48922.2021.00389,,,Recurrent Mask Refinement for Few-Shot Medical Image Segmentation,"Although having achieved great success in medical image segmentation, deep convolutional neural networks usually require a large dataset with manual annotations for training and are difficult to generalize to unseen classes. Few-shot learning has the potential to address these challenges by learning new classes from only a few labeled examples. In this work, we propose a new framework for few-shot medical image segmentation based on prototypical networks. Our innovation lies in the design of two key modules: 1) a context relation encoder (CRE) that uses correlation to capture local relation features between foreground and background regions; and 2) a recurrent mask refinement module that repeatedly uses the CRE and a prototypical network to recapture the change of context relationship and refine the segmentation mask iteratively. Experiments on two abdomen CT datasets and an abdomen MRI dataset show the proposed method obtains substantial improvement over the state-of-the-art methods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively. Code is publicly available 1.","This work is partly supported by NSF grant IIS-1715017, a Simons Foundation grant (594598), and a hardware grant from NVIDIA.","This work is partly supported by NSF grant IIS-1715017, a Simons Foundation grant (594598), and a hardware grant from NVIDIA.",,2021 IEEE/CVF International Conference on Computer Vision (ICCV),,2021-10-17,2021,,2021-10-17,00,,3898-3908,All OA; Green,Proceeding,"Tang, Hao; Liu, Xingwei; Sun, Shanlin; Yan, Xiangyi; Xie, Xiaohui","Tang, Hao (Department of Computer Science, University of California, Irvine, California, 92697); Liu, Xingwei (Department of Computer Science, University of California, Irvine, California, 92697); Sun, Shanlin (Department of Computer Science, University of California, Irvine, California, 92697); Yan, Xiangyi (Department of Computer Science, University of California, Irvine, California, 92697); Xie, Xiaohui (Department of Computer Science, University of California, Irvine, California, 92697)","Tang, Hao (University of California, Irvine)","Tang, Hao (University of California, Irvine); Liu, Xingwei (University of California, Irvine); Sun, Shanlin (University of California, Irvine); Yan, Xiangyi (University of California, Irvine); Xie, Xiaohui (University of California, Irvine)",25,25,,20.46,http://arxiv.org/pdf/2108.00622,https://app.dimensions.ai/details/publication/pub.1145901382,46 Information and Computing Sciences; 4611 Machine Learning,
1202,pub.1148728432,10.48550/arxiv.2206.07156,,,Federated Multi-organ Segmentation with Partially Labeled Data,"Federated learning is an emerging paradigm allowing large-scale decentralized
learning without sharing data across different data owners, which helps address
the concern of data privacy in medical image analysis. However, the requirement
for label consistency across clients by the existing methods largely narrows
its application scope. In practice, each clinical site may only annotate
certain organs of interest with partial or no overlap with other sites.
Incorporating such partially labeled data into a unified federation is an
unexplored problem with clinical significance and urgency. This work tackles
the challenge by using a novel federated multi-encoding U-Net (Fed-MENU) method
for multi-organ segmentation. In our method, a multi-encoding U-Net (MENU-Net)
is proposed to extract organ-specific features through different encoding
sub-networks. Each sub-network can be seen as an expert of a specific organ and
trained for that client. Moreover, to encourage the organ-specific features
extracted by different sub-networks to be informative and distinctive, we
regularize the training of the MENU-Net by designing an auxiliary generic
decoder (AGD). Extensive experiments on four public datasets show that our
Fed-MENU method can effectively obtain a federated learning model using the
partially labeled datasets with superior performance to other models trained by
either localized or centralized learning methods. Source code will be made
publicly available at the time of paper publication.",,,arXiv,,,2022-06-14,2022,,,,,,All OA; Green,Preprint,"Xu, Xuanang; Yan, Pingkun","Xu, Xuanang (); Yan, Pingkun ()",,"Xu, Xuanang (); Yan, Pingkun ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148728432,46 Information and Computing Sciences; 4611 Machine Learning,
1202,pub.1123793466,10.48550/arxiv.2001.00208,,,Multi-organ Segmentation over Partially Labeled Datasets with  Multi-scale Feature Abstraction,"Shortage of fully annotated datasets has been a limiting factor in developing
deep learning based image segmentation algorithms and the problem becomes more
pronounced in multi-organ segmentation. In this paper, we propose a unified
training strategy that enables a novel multi-scale deep neural network to be
trained on multiple partially labeled datasets for multi-organ segmentation. In
addition, a new network architecture for multi-scale feature abstraction is
proposed to integrate pyramid input and feature analysis into a U-shape pyramid
structure. To bridge the semantic gap caused by directly merging features from
different scales, an equal convolutional depth mechanism is introduced.
Furthermore, we employ a deep supervision mechanism to refine the outputs in
different scales. To fully leverage the segmentation features from all the
scales, we design an adaptive weighting layer to fuse the outputs in an
automatic fashion. All these mechanisms together are integrated into a Pyramid
Input Pyramid Output Feature Abstraction Network (PIPO-FAN). Our proposed
method was evaluated on four publicly available datasets, including BTCV, LiTS,
KiTS and Spleen, where very promising performance has been achieved. The source
code of this work is publicly shared at https://github.com/DIAL-RPI/PIPO-FAN
for others to easily reproduce the work and build their own models with the
introduced mechanisms.",,,arXiv,,,2020-01-01,2020,,,,,,All OA; Green,Preprint,"Fang, Xi; Yan, Pingkun","Fang, Xi (); Yan, Pingkun ()",,"Fang, Xi (); Yan, Pingkun ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1123793466,46 Information and Computing Sciences; 4611 Machine Learning,
1200,pub.1153517978,10.48550/arxiv.2212.04497,,,UNETR++: Delving into Efficient and Accurate 3D Medical Image  Segmentation,"Owing to the success of transformer models, recent works study their
applicability in 3D medical segmentation tasks. Within the transformer models,
the self-attention mechanism is one of the main building blocks that strives to
capture long-range dependencies, compared to the local convolutional-based
design. However, the self-attention operation has quadratic complexity which
proves to be a computational bottleneck, especially in volumetric medical
imaging, where the inputs are 3D with numerous slices. In this paper, we
propose a 3D medical image segmentation approach, named UNETR++, that offers
both high-quality segmentation masks as well as efficiency in terms of
parameters and compute cost. The core of our design is the introduction of a
novel efficient paired attention (EPA) block that efficiently learns spatial
and channel-wise discriminative features using a pair of inter-dependent
branches based on spatial and channel attention. Our spatial attention
formulation is efficient having linear complexity with respect to the input
sequence length. To enable communication between spatial and channel-focused
branches, we share the weights of query and key mapping functions that provide
a complimentary benefit (paired attention), while also reducing the overall
network parameters. Our extensive evaluations on three benchmarks, Synapse,
BTCV and ACDC, reveal the effectiveness of the proposed contributions in terms
of both efficiency and accuracy. On Synapse dataset, our UNETR++ sets a new
state-of-the-art with a Dice Similarity Score of 87.2%, while being
significantly efficient with a reduction of over 71% in terms of both
parameters and FLOPs, compared to the best existing method in the literature.
Code: https://github.com/Amshaker/unetr_plus_plus.",,,arXiv,,,2022-12-08,2022,,,,,,All OA; Green,Preprint,"Shaker, Abdelrahman; Maaz, Muhammad; Rasheed, Hanoona; Khan, Salman; Yang, Ming-Hsuan; Khan, Fahad Shahbaz","Shaker, Abdelrahman (); Maaz, Muhammad (); Rasheed, Hanoona (); Khan, Salman (); Yang, Ming-Hsuan (); Khan, Fahad Shahbaz ()",,"Shaker, Abdelrahman (); Maaz, Muhammad (); Rasheed, Hanoona (); Khan, Salman (); Yang, Ming-Hsuan (); Khan, Fahad Shahbaz ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153517978,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,
1200,pub.1151716930,10.48550/arxiv.2210.03299,,,Topology-Preserving Segmentation Network,"Medical image segmentation aims to automatically extract anatomical or
pathological structures in the human body. Most objects or regions of interest
are of similar patterns. For example, the relative location and the relative
size of the lung and the kidney differ little among subjects. Incorporating
these morphology rules as prior knowledge into the segmentation model is
believed to be an effective way to enhance the accuracy of the segmentation
results. Motivated by this, we propose in this work the Topology-Preserving
Segmentation Network (TPSN) which can predict segmentation masks with the same
topology prescribed for specific tasks. TPSN is a deformation-based model that
yields a deformation map through an encoder-decoder architecture to warp the
template masks into a target shape approximating the region to segment.
Comparing to the segmentation framework based on pixel-wise classification,
deformation-based segmentation models that warp a template to enclose the
regions are more convenient to enforce geometric constraints. In our framework,
we carefully design the ReLU Jacobian regularization term to enforce the
bijectivity of the deformation map. As such, the predicted mask by TPSN has the
same topology as that of the template prior mask.",,,arXiv,,,2022-10-06,2022,,,,,,All OA; Green,Preprint,"Zhang, Han; Lui, Lok Ming","Zhang, Han (); Lui, Lok Ming ()",,"Zhang, Han (); Lui, Lok Ming ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151716930,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",
1200,pub.1131464527,10.1007/978-3-030-58526-6_45,,,Self-supervision with Superpixels: Training Few-Shot Medical Image Segmentation Without Annotation,"Few-shot semantic segmentation (FSS) has great potential for medical imaging applications. Most of the existing FSS techniques require abundant annotated semantic classes for training. However, these methods may not be applicable for medical images due to the lack of annotations. To address this problem we make several contributions: (1) A novel self-supervised FSS framework for medical images in order to eliminate the requirement for annotations during training. Additionally, superpixel-based pseudo-labels are generated to provide supervision; (2) An adaptive local prototype pooling module plugged into prototypical networks, to solve the common challenging foreground-background imbalance problem in medical image segmentation; (3) We demonstrate the general applicability of the proposed approach for medical images using three different tasks: abdominal organ segmentation for CT and MRI, as well as cardiac segmentation for MRI. Our results show that, for medical image segmentation, the proposed method outperforms conventional FSS methods which require manual annotations for training.",This work is supported by the EPSRC Programme Grant EP/P001009/1. This work is also supported by the UK Research and Innovation London Medical Imaging and Artificial Intelligence Centre for Value Based Healthcare. The authors would like to thank Konstantinos Kamnitsas and Zeju Li for insightful comments.,,Lecture Notes in Computer Science,Computer Vision – ECCV 2020,,2020-10-07,2020,2020-10-07,2020,12374,,762-780,All OA; Green,Chapter,"Ouyang, Cheng; Biffi, Carlo; Chen, Chen; Kart, Turkay; Qiu, Huaqi; Rueckert, Daniel","Ouyang, Cheng (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Biffi, Carlo (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Chen, Chen (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Kart, Turkay (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Qiu, Huaqi (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Rueckert, Daniel (BioMedIA Group, Department of Computing, Imperial College London, London, UK)","Ouyang, Cheng (Imperial College London)","Ouyang, Cheng (Imperial College London); Biffi, Carlo (Imperial College London); Chen, Chen (Imperial College London); Kart, Turkay (Imperial College London); Qiu, Huaqi (Imperial College London); Rueckert, Daniel (Imperial College London)",73,73,,34.75,http://arxiv.org/pdf/2007.09886,https://app.dimensions.ai/details/publication/pub.1131464527,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1200,pub.1149819041,10.48550/arxiv.2207.13415,,,TransNorm: Transformer Provides a Strong Spatial Normalization Mechanism  for a Deep Segmentation Model,"In the past few years, convolutional neural networks (CNNs), particularly
U-Net, have been the prevailing technique in the medical image processing era.
Specifically, the seminal U-Net, as well as its alternatives, have successfully
managed to address a wide variety of medical image segmentation tasks. However,
these architectures are intrinsically imperfect as they fail to exhibit
long-range interactions and spatial dependencies leading to a severe
performance drop in the segmentation of medical images with variable shapes and
structures. Transformers, preliminary proposed for sequence-to-sequence
prediction, have arisen as surrogate architectures to precisely model global
information assisted by the self-attention mechanism. Despite being feasibly
designed, utilizing a pure Transformer for image segmentation purposes can
result in limited localization capacity stemming from inadequate low-level
features. Thus, a line of research strives to design robust variants of
Transformer-based U-Net. In this paper, we propose Trans-Norm, a novel deep
segmentation framework which concomitantly consolidates a Transformer module
into both encoder and skip-connections of the standard U-Net. We argue that the
expedient design of skip-connections can be crucial for accurate segmentation
as it can assist in feature fusion between the expanding and contracting paths.
In this respect, we derive a Spatial Normalization mechanism from the
Transformer module to adaptively recalibrate the skip connection path.
Extensive experiments across three typical tasks for medical image segmentation
demonstrate the effectiveness of TransNorm. The codes and trained models are
publicly available at https://github.com/rezazad68/transnorm.",,,arXiv,,,2022-07-27,2022,,,,,,All OA; Green,Preprint,"Azad, Reza; AL-Antary, Mohammad T.; Heidari, Moein; Merhof, Dorit","Azad, Reza (); AL-Antary, Mohammad T. (); Heidari, Moein (); Merhof, Dorit ()",,"Azad, Reza (); AL-Antary, Mohammad T. (); Heidari, Moein (); Merhof, Dorit ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149819041,46 Information and Computing Sciences; 4611 Machine Learning,
1200,pub.1144119189,10.48550/arxiv.2112.11177,,,Generalizable Cross-modality Medical Image Segmentation via Style  Augmentation and Dual Normalization,"For medical image segmentation, imagine if a model was only trained using MR
images in source domain, how about its performance to directly segment CT
images in target domain? This setting, namely generalizable cross-modality
segmentation, owning its clinical potential, is much more challenging than
other related settings, e.g., domain adaptation. To achieve this goal, we in
this paper propose a novel dual-normalization model by leveraging the augmented
source-similar and source-dissimilar images during our generalizable
segmentation. To be specific, given a single source domain, aiming to simulate
the possible appearance change in unseen target domains, we first utilize a
nonlinear transformation to augment source-similar and source-dissimilar
images. Then, to sufficiently exploit these two types of augmentations, our
proposed dual-normalization based model employs a shared backbone yet
independent batch normalization layer for separate normalization. Afterward, we
put forward a style-based selection scheme to automatically choose the
appropriate path in the test stage. Extensive experiments on three publicly
available datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal
Multi-Organ datasets, have demonstrated that our method outperforms other
state-of-the-art domain generalization methods. Code is available at
https://github.com/zzzqzhou/Dual-Normalization.",,,arXiv,,,2021-12-21,2021,,,,,,All OA; Green,Preprint,"Zhou, Ziqi; Qi, Lei; Yang, Xin; Ni, Dong; Shi, Yinghuan","Zhou, Ziqi (); Qi, Lei (); Yang, Xin (); Ni, Dong (); Shi, Yinghuan ()",,"Zhou, Ziqi (); Qi, Lei (); Yang, Xin (); Ni, Dong (); Shi, Yinghuan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144119189,46 Information and Computing Sciences; 4611 Machine Learning,
1200,pub.1140154402,10.48550/arxiv.2108.00622,,,Recurrent Mask Refinement for Few-Shot Medical Image Segmentation,"Although having achieved great success in medical image segmentation, deep
convolutional neural networks usually require a large dataset with manual
annotations for training and are difficult to generalize to unseen classes.
Few-shot learning has the potential to address these challenges by learning new
classes from only a few labeled examples. In this work, we propose a new
framework for few-shot medical image segmentation based on prototypical
networks. Our innovation lies in the design of two key modules: 1) a context
relation encoder (CRE) that uses correlation to capture local relation features
between foreground and background regions; and 2) a recurrent mask refinement
module that repeatedly uses the CRE and a prototypical network to recapture the
change of context relationship and refine the segmentation mask iteratively.
Experiments on two abdomen CT datasets and an abdomen MRI dataset show the
proposed method obtains substantial improvement over the state-of-the-art
methods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively.
Code is publicly available.",,,arXiv,,,2021-08-02,2021,,,,,,All OA; Green,Preprint,"Tang, Hao; Liu, Xingwei; Sun, Shanlin; Yan, Xiangyi; Xie, Xiaohui","Tang, Hao (); Liu, Xingwei (); Sun, Shanlin (); Yan, Xiangyi (); Xie, Xiaohui ()",,"Tang, Hao (); Liu, Xingwei (); Sun, Shanlin (); Yan, Xiangyi (); Xie, Xiaohui ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140154402,46 Information and Computing Sciences; 4611 Machine Learning,
1200,pub.1129504690,10.48550/arxiv.2007.09886,,,Self-Supervision with Superpixels: Training Few-shot Medical Image  Segmentation without Annotation,"Few-shot semantic segmentation (FSS) has great potential for medical imaging
applications. Most of the existing FSS techniques require abundant annotated
semantic classes for training. However, these methods may not be applicable for
medical images due to the lack of annotations. To address this problem we make
several contributions: (1) A novel self-supervised FSS framework for medical
images in order to eliminate the requirement for annotations during training.
Additionally, superpixel-based pseudo-labels are generated to provide
supervision; (2) An adaptive local prototype pooling module plugged into
prototypical networks, to solve the common challenging foreground-background
imbalance problem in medical image segmentation; (3) We demonstrate the general
applicability of the proposed approach for medical images using three different
tasks: abdominal organ segmentation for CT and MRI, as well as cardiac
segmentation for MRI. Our results show that, for medical image segmentation,
the proposed method outperforms conventional FSS methods which require manual
annotations for training.",,,arXiv,,,2020-07-20,2020,,,,,,All OA; Green,Preprint,"Ouyang, Cheng; Biffi, Carlo; Chen, Chen; Kart, Turkay; Qiu, Huaqi; Rueckert, Daniel","Ouyang, Cheng (); Biffi, Carlo (); Chen, Chen (); Kart, Turkay (); Qiu, Huaqi (); Rueckert, Daniel ()",,"Ouyang, Cheng (); Biffi, Carlo (); Chen, Chen (); Kart, Turkay (); Qiu, Huaqi (); Rueckert, Daniel ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129504690,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1199,pub.1151381625,10.1109/cvpr52688.2022.02019,,,Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization,"For medical image segmentation, imagine if a model was only trained using MR images in source domain, how about its performance to directly segment CT images in target domain? This setting, namely generalizable cross-modality segmentation, owning its clinical potential, is much more challenging than other related settings, e.g., domain adaptation. To achieve this goal, we in this paper propose a novel dual-normalization model by leveraging the augmented source-similar and source-dissimilar images during our generalizable segmentation. To be specific, given a single source domain, aiming to simulate the possible appearance change in unseen target domains, we first utilize a nonlinear transformation to augment source-similar and source-dissimilar images. Then, to sufficiently exploit these two types of augmentations, our proposed dualnormalization based model employs a shared backbone yet independent batch normalization layer for separate normalization. Afterward, we put forward a style-based selection scheme to automatically choose the appropriate path in the test stage. Extensive experiments on three publicly available datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal Multi-Organ datasets, have demonstrated that our method outperforms other state-of-the-art domain generalization methods. Code is available athttps://github.com/zzzqzhou/Dual-Normalization.",,"This work was supported by National Key Research and Development Program of China (2019YFC0118300) NSFC Major Program (62192783), CAAI-Huawei MindSpore Project (CAAIXSJLJJ-2021-042A), China Postdoctoral Science Foundation Project (2021M690609), and Jiangsu Natural Science Foundation Project (BK20210224).",,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2022-06-24,2022,,2022-06-24,00,,20824-20833,All OA; Green,Proceeding,"Zhou, Ziqi; Qi, Lei; Yang, Xin; Ni, Dong; Shi, Yinghuan","Zhou, Ziqi (Nanjing University); Qi, Lei (Southeast University); Yang, Xin (Shenzhen University); Ni, Dong (Shenzhen University); Shi, Yinghuan (Nanjing University)",,"Zhou, Ziqi (Nanjing University); Qi, Lei (Southeast University); Yang, Xin (Shenzhen University); Ni, Dong (Shenzhen University); Shi, Yinghuan (Nanjing University)",1,1,,,http://arxiv.org/pdf/2112.11177,https://app.dimensions.ai/details/publication/pub.1151381625,46 Information and Computing Sciences; 4611 Machine Learning,
1135,pub.1154090776,10.1016/j.eswa.2022.119452,,,SwinCup: Cascaded swin transformer for histopathological structures segmentation in colorectal cancer,"Transformer models have recently become the dominant architecture in many computer vision tasks, including image classification, object detection, and image segmentation. The main reason behind their success is the ability to incorporate global context information into the learning process. By utilising self-attention, recent advancements in the Transformer architecture design enable models to consider long-range dependencies. In this paper, we propose a novel transformer, named Swin Transformer with Cascaded UPsampling (SwinCup) model for the segmentation of histopathology images. We use a hierarchical Swin Transformer with shifted windows as an encoder to extract global context features. The multi-scale feature extraction in a Swin transformer enables the model to attend to different areas in the image at different scales. A cascaded up-sampling decoder is used with an encoder to improve its feature aggregation. Experiments on GLAS and CRAG histopathology colorectal cancer datasets were used to validate the model, achieving an average 0.90 (F1 score) and surpassing the state-of-the-art by (23%).",,,Expert Systems with Applications,,,2023-04,2023,,2023-04,216,,119452,All OA; Hybrid,Article,"Zidan, Usama; Gaber, Mohamed Medhat; Abdelsamea, Mohammed M.","Zidan, Usama (School of Computing and Digital Technology, Birmingham City University, Birmingham, B4 7XG, UK); Gaber, Mohamed Medhat (School of Computing and Digital Technology, Birmingham City University, Birmingham, B4 7XG, UK; Faculty of Computer Science and Engineering, Galala University, Egypt); Abdelsamea, Mohammed M. (School of Computing and Digital Technology, Birmingham City University, Birmingham, B4 7XG, UK; Department of Computer Science, Faculty of Computers and Information, University of Assiut, Egypt)","Abdelsamea, Mohammed M. (Birmingham City University; Assiut University)","Zidan, Usama (Birmingham City University); Gaber, Mohamed Medhat (Birmingham City University); Abdelsamea, Mohammed M. (Birmingham City University; Assiut University)",0,0,,,https://doi.org/10.1016/j.eswa.2022.119452,https://app.dimensions.ai/details/publication/pub.1154090776,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4611 Machine Learning,
1135,pub.1152615828,10.1016/j.bspc.2022.104339,,,Learning multi-organ segmentation via partial- and mutual-prior from single-organ datasets,"Automatic multi-organ segmentation in medical images is crucial for many clinical applications. The art methods have reported promising results but rely on massive annotated data. However, such data is hard to obtain due to the need for considerable expertise. In contrast, obtaining a single-organ dataset is relatively easier, and many well-annotated ones are publicly available. To this end, this work raises the partially supervised problem: can we use these single-organ datasets to learn a multi-organ segmentation model? In this paper, we propose the Partial- and Mutual-Prior incorporated framework (PRIMP) to learn a robust multi-organ segmentation model by deriving knowledge from single-organ datasets. Unlike existing methods that largely ignore the organs’ anatomical prior knowledge, our PRIMP is designed with two key prior shared across different subjects and datasets: (1) partial-prior, each organ has its own character (e.g., size and shape) and (2) mutual-prior, the relative position between different organs follows the comparatively fixed anatomical structure. Specifically, we propose to incorporate partial-prior of each organ by learning from the single-organ statistics, and inject mutual-prior of organs by learning from the multi-organ statistics. By doing so, the model is encouraged to capture organs’ anatomical invariance across different subjects and datasets, thus guaranteeing the anatomical reasonableness of the predictions, narrowing down the problem of domain gaps, capturing spatial information among different slices, thereby improving organs’ segmentation performance. Experiments on four publicly available datasets (LiTS, Pancreas, KiTS, BTCV) show that our PRIMP can improve the performance on both the multi-organ and single-organ datasets (17.40% and 3.06% above the baseline model on DSC, respectively) and can surpass the comparative approaches.","This work is supported by the National Natural Science Foundation of China (No. 61876159, No. 62076116, No. 62276221), Guiding Project of Science and Technology Department of Fujian Province, China (No. 2019Y0018), and the Natural Science Foundation of Fujian Province of China (No. 2022J01002).",,Biomedical Signal Processing and Control,,,2023-02,2023,,2023-02,80,,104339,Closed,Article,"Lian, Sheng; Li, Lei; Luo, Zhiming; Zhong, Zhun; Wang, Beizhan; Li, Shaozi","Lian, Sheng (The College of Computer and Data Science, Fuzhou University, Fujian, China; The Department of Artificial Intelligence, Xiamen University, Fujian, China; Fujian Key Laboratory of Network Computing and Intelligent Information Processing (Fuzhou University), Fujian, China); Li, Lei (The Department of Software Engineering, Xiamen University, Fujian, China); Luo, Zhiming (The Department of Artificial Intelligence, Xiamen University, Fujian, China); Zhong, Zhun (University of Trento, Italy); Wang, Beizhan (The Department of Software Engineering, Xiamen University, Fujian, China); Li, Shaozi (The Department of Artificial Intelligence, Xiamen University, Fujian, China)","Luo, Zhiming (Xiamen University); Li, Shaozi (Xiamen University)","Lian, Sheng (Fuzhou University; Xiamen University; Fuzhou University); Li, Lei (Xiamen University); Luo, Zhiming (Xiamen University); Zhong, Zhun (University of Trento); Wang, Beizhan (Xiamen University); Li, Shaozi (Xiamen University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152615828,"30 Agricultural, Veterinary and Food Sciences; 3006 Food Sciences; 40 Engineering; 4003 Biomedical Engineering",
1135,pub.1143055508,10.48550/arxiv.2111.12525,,,Causality-inspired Single-source Domain Generalization for Medical Image  Segmentation,"Deep learning models usually suffer from domain shift issues, where models
trained on one source domain do not generalize well to other unseen domains. In
this work, we investigate the single-source domain generalization problem:
training a deep network that is robust to unseen domains, under the condition
that training data is only available from one source domain, which is common in
medical imaging applications. We tackle this problem in the context of
cross-domain medical image segmentation. Under this scenario, domain shifts are
mainly caused by different acquisition processes. We propose a simple
causality-inspired data augmentation approach to expose a segmentation model to
synthesized domain-shifted training examples. Specifically, 1) to make the deep
model robust to discrepancies in image intensities and textures, we employ a
family of randomly-weighted shallow networks. They augment training images
using diverse appearance transformations. 2) Further we show that spurious
correlations among objects in an image are detrimental to domain robustness.
These correlations might be taken by the network as domain-specific clues for
making predictions, and they may break on unseen domains. We remove these
spurious correlations via causal intervention. This is achieved by resampling
the appearances of potentially correlated objects independently. The proposed
approach is validated on three cross-domain segmentation tasks: cross-modality
(CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI
segmentation, and cross-center prostate MRI segmentation. The proposed approach
yields consistent performance gains compared with competitive methods when
tested on unseen domains.",,,arXiv,,,2021-11-24,2021,,,,,,All OA; Green,Preprint,"Ouyang, Cheng; Chen, Chen; Li, Surui; Li, Zeju; Qin, Chen; Bai, Wenjia; Rueckert, Daniel","Ouyang, Cheng (); Chen, Chen (); Li, Surui (); Li, Zeju (); Qin, Chen (); Bai, Wenjia (); Rueckert, Daniel ()",,"Ouyang, Cheng (); Chen, Chen (); Li, Surui (); Li, Zeju (); Qin, Chen (); Bai, Wenjia (); Rueckert, Daniel ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143055508,46 Information and Computing Sciences; 4611 Machine Learning,
1135,pub.1138895582,10.48550/arxiv.2106.06908,,,Domain Generalization on Medical Imaging Classification using Episodic  Training with Task Augmentation,"Medical imaging datasets usually exhibit domain shift due to the variations
of scanner vendors, imaging protocols, etc. This raises the concern about the
generalization capacity of machine learning models. Domain generalization (DG),
which aims to learn a model from multiple source domains such that it can be
directly generalized to unseen test domains, seems particularly promising to
medical imaging community. To address DG, recent model-agnostic meta-learning
(MAML) has been introduced, which transfers the knowledge from previous
training tasks to facilitate the learning of novel testing tasks. However, in
clinical practice, there are usually only a few annotated source domains
available, which decreases the capacity of training task generation and thus
increases the risk of overfitting to training tasks in the paradigm. In this
paper, we propose a novel DG scheme of episodic training with task augmentation
on medical imaging classification. Based on meta-learning, we develop the
paradigm of episodic training to construct the knowledge transfer from episodic
training-task simulation to the real testing task of DG. Motivated by the
limited number of source domains in real-world medical deployment, we consider
the unique task-level overfitting and we propose task augmentation to enhance
the variety during training task generation to alleviate it. With the
established learning framework, we further exploit a novel meta-objective to
regularize the deep embedding of training domains. To validate the
effectiveness of the proposed method, we perform experiments on
histopathological images and abdominal CT images.",,,arXiv,,,2021-06-12,2021,,,,,,All OA; Green,Preprint,"Li, Chenxin; Qi, Qi; Ding, Xinghao; Huang, Yue; Liang, Dong; Yu, Yizhou","Li, Chenxin (); Qi, Qi (); Ding, Xinghao (); Huang, Yue (); Liang, Dong (); Yu, Yizhou ()",,"Li, Chenxin (); Qi, Qi (); Ding, Xinghao (); Huang, Yue (); Liang, Dong (); Yu, Yizhou ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1138895582,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4611 Machine Learning,
1132,pub.1144061141,10.48550/arxiv.2112.09356,,,UniMiSS: Universal Medical Self-Supervised Learning via Breaking  Dimensionality Barrier,"Self-supervised learning (SSL) opens up huge opportunities for medical image
analysis that is well known for its lack of annotations. However, aggregating
massive (unlabeled) 3D medical images like computerized tomography (CT) remains
challenging due to its high imaging cost and privacy restrictions. In this
paper, we advocate bringing a wealth of 2D images like chest X-rays as
compensation for the lack of 3D data, aiming to build a universal medical
self-supervised representation learning framework, called UniMiSS. The
following problem is how to break the dimensionality barrier, \ie, making it
possible to perform SSL with both 2D and 3D images? To achieve this, we design
a pyramid U-like medical Transformer (MiT). It is composed of the switchable
patch embedding (SPE) module and Transformers. The SPE module adaptively
switches to either 2D or 3D patch embedding, depending on the input dimension.
The embedded patches are converted into a sequence regardless of their original
dimensions. The Transformers model the long-term dependencies in a
sequence-to-sequence manner, thus enabling UniMiSS to learn representations
from both 2D and 3D images. With the MiT as the backbone, we perform the
UniMiSS in a self-distillation manner. We conduct expensive experiments on six
3D/2D medical image analysis tasks, including segmentation and classification.
The results show that the proposed UniMiSS achieves promising performance on
various downstream tasks, outperforming the ImageNet pre-training and other
advanced SSL counterparts substantially. Code is available at
\def\UrlFont{\rm\small\ttfamily}
\url{https://github.com/YtongXie/UniMiSS-code}.",,,arXiv,,,2021-12-17,2021,,,,,,All OA; Green,Preprint,"Xie, Yutong; Zhang, Jianpeng; Xia, Yong; Wu, Qi","Xie, Yutong (); Zhang, Jianpeng (); Xia, Yong (); Wu, Qi ()",,"Xie, Yutong (); Zhang, Jianpeng (); Xia, Yong (); Wu, Qi ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144061141,46 Information and Computing Sciences; 4611 Machine Learning,
1132,pub.1125773012,10.48550/arxiv.2003.07923,,,3D medical image segmentation with labeled and unlabeled data using  autoencoders at the example of liver segmentation in CT images,"Automatic segmentation of anatomical structures with convolutional neural
networks (CNNs) constitutes a large portion of research in medical image
analysis. The majority of CNN-based methods rely on an abundance of labeled
data for proper training. Labeled medical data is often scarce, but unlabeled
data is more widely available. This necessitates approaches that go beyond
traditional supervised learning and leverage unlabeled data for segmentation
tasks. This work investigates the potential of autoencoder-extracted features
to improve segmentation with a CNN. Two strategies were considered. First,
transfer learning where pretrained autoencoder features were used as
initialization for the convolutional layers in the segmentation network.
Second, multi-task learning where the tasks of segmentation and feature
extraction, by means of input reconstruction, were learned and optimized
simultaneously. A convolutional autoencoder was used to extract features from
unlabeled data and a multi-scale, fully convolutional CNN was used to perform
the target task of 3D liver segmentation in CT images. For both strategies,
experiments were conducted with varying amounts of labeled and unlabeled
training data. The proposed learning strategies improved results in $75\%$ of
the experiments compared to training from scratch and increased the dice score
by up to $0.040$ and $0.024$ for a ratio of unlabeled to labeled training data
of about $32 : 1$ and $12.5 : 1$, respectively. The results indicate that both
training strategies are more effective with a large ratio of unlabeled to
labeled training data.",,,arXiv,,,2020-03-17,2020,,,,,,All OA; Green,Preprint,"Sital, Cheryl; Brosch, Tom; Tio, Dominique; Raaijmakers, Alexander; Weese, Jürgen","Sital, Cheryl (); Brosch, Tom (); Tio, Dominique (); Raaijmakers, Alexander (); Weese, Jürgen ()",,"Sital, Cheryl (); Brosch, Tom (); Tio, Dominique (); Raaijmakers, Alexander (); Weese, Jürgen ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1125773012,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1131,pub.1140969721,10.48550/arxiv.2109.03230,,,Self-supervised Tumor Segmentation through Layer Decomposition,"In this paper, we target self-supervised representation learning for
zero-shot tumor segmentation. We make the following contributions: First, we
advocate a zero-shot setting, where models from pre-training should be directly
applicable for the downstream task, without using any manual annotations.
Second, we take inspiration from ""layer-decomposition"", and innovate on the
training regime with simulated tumor data. Third, we conduct extensive ablation
studies to analyse the critical components in data simulation, and validate the
necessity of different proxy tasks. We demonstrate that, with sufficient
texture randomization in simulation, model trained on synthetic data can
effortlessly generalise to segment real tumor data. Forth, our approach
achieves superior results for zero-shot tumor segmentation on different
downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for
liver tumor segmentation. While evaluating the model transferability for tumor
segmentation under a low-annotation regime, the proposed approach also
outperforms all existing self-supervised approaches, opening up the usage of
self-supervised learning in practical scenarios.",,,arXiv,,,2021-09-07,2021,,,,,,All OA; Green,Preprint,"Zhang, Xiaoman; Xie, Weidi; Huang, Chaoqin; Wang, Yanfeng; Zhang, Ya; Chen, Xin; Tian, Qi","Zhang, Xiaoman (); Xie, Weidi (); Huang, Chaoqin (); Wang, Yanfeng (); Zhang, Ya (); Chen, Xin (); Tian, Qi ()",,"Zhang, Xiaoman (); Xie, Weidi (); Huang, Chaoqin (); Wang, Yanfeng (); Zhang, Ya (); Chen, Xin (); Tian, Qi ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140969721,46 Information and Computing Sciences; 4611 Machine Learning,
1131,pub.1124006131,10.48550/arxiv.2001.03111,,,Unpaired Multi-modal Segmentation via Knowledge Distillation,"Multi-modal learning is typically performed with network architectures
containing modality-specific layers and shared layers, utilizing co-registered
images of different modalities. We propose a novel learning scheme for unpaired
cross-modality image segmentation, with a highly compact architecture achieving
superior segmentation accuracy. In our method, we heavily reuse network
parameters, by sharing all convolutional kernels across CT and MRI, and only
employ modality-specific internal normalization layers which compute respective
statistics. To effectively train such a highly compact model, we introduce a
novel loss term inspired by knowledge distillation, by explicitly constraining
the KL-divergence of our derived prediction distributions between modalities.
We have extensively validated our approach on two multi-class segmentation
problems: i) cardiac structure segmentation, and ii) abdominal organ
segmentation. Different network settings, i.e., 2D dilated network and 3D
U-net, are utilized to investigate our method's general efficacy. Experimental
results on both tasks demonstrate that our novel multi-modal learning scheme
consistently outperforms single-modal training and previous multi-modal
approaches.",,,arXiv,,,2020-01-06,2020,,,,,,All OA; Green,Preprint,"Dou, Qi; Liu, Quande; Heng, Pheng Ann; Glocker, Ben","Dou, Qi (); Liu, Quande (); Heng, Pheng Ann (); Glocker, Ben ()",,"Dou, Qi (); Liu, Quande (); Heng, Pheng Ann (); Glocker, Ben ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1124006131,46 Information and Computing Sciences; 4611 Machine Learning,
1131,pub.1103819296,10.1109/access.2018.2833286,,,Preprocessing of Heteroscedastic Medical Images,"Tissue intensity distributions in medical images can have varying degrees of statistical dispersion, which is referred to as heteroscedasticity. This can influence image contrast and gradients, but can also negatively affect the performance of general-purpose distance metrics. Numerous methods to preprocess heteroscedastic images have already been proposed, though most are application-specific and rely on either manual input or certain heuristics. We therefore propose a more general and data-driven approach that relies on the notion of intensity variance around each specific intensity value, simply referred to as intensityspecific variances. First, we introduce a method for estimating these variances from an image (or a collection of images) directly, which is followed by an illustration of how they can be used to define intensity-specific distance measures. Next, we evaluate the proposed concepts through various applications using both homoand heteroscedastic CT and MR images. Finally, we present results from both qualitative and quantitative analyses that confirm the working of the proposed approaches, and support the presented concepts as valid and effective tools for (pre)processing heteroscedastic medical images.",,,IEEE Access,,,2018-06-04,2018,2018-06-04,,6,,26047-26058,All OA; Gold,Article,"Joris, Philip; Develter, Wim; Van De Voorde, Wim; Suetens, Paul; Maes, Frederik; Vandermeulen, Dirk; Claes, Peter","Joris, Philip (Dept. ESAT-PSI, KU Leuven, Leuven, Belgium); Develter, Wim (Dept. of Forensic Med., Univ. Hosp. UZ Leuven, Leuven, Belgium); Van De Voorde, Wim (Dept. of Forensic Med., Univ. Hosp. UZ Leuven, Leuven, Belgium); Suetens, Paul (Dept. ESAT-PSI, KU Leuven, Leuven, Belgium); Maes, Frederik (Dept. ESAT-PSI, KU Leuven, Leuven, Belgium); Vandermeulen, Dirk (Dept. ESAT-PSI, KU Leuven, Leuven, Belgium); Claes, Peter (Dept. ESAT-PSI, KU Leuven, Leuven, Belgium)",,"Joris, Philip (KU Leuven); Develter, Wim (Universitair Ziekenhuis Leuven); Van De Voorde, Wim (Universitair Ziekenhuis Leuven); Suetens, Paul (KU Leuven); Maes, Frederik (KU Leuven); Vandermeulen, Dirk (KU Leuven); Claes, Peter (KU Leuven)",3,2,,,https://doi.org/10.1109/access.2018.2833286,https://app.dimensions.ai/details/publication/pub.1103819296,46 Information and Computing Sciences,
1131,pub.1148758824,10.48550/arxiv.2206.08023,,,AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile  Medical Image Segmentation,"Despite the considerable progress in automatic abdominal multi-organ
segmentation from CT/MRI scans in recent years, a comprehensive evaluation of
the models' capabilities is hampered by the lack of a large-scale benchmark
from diverse clinical scenarios. Constraint by the high cost of collecting and
labeling 3D medical data, most of the deep learning models to date are driven
by datasets with a limited number of organs of interest or samples, which still
limits the power of modern deep models and makes it difficult to provide a
fully comprehensive and fair estimate of various methods. To mitigate the
limitations, we present AMOS, a large-scale, diverse, clinical dataset for
abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected
from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease
patients, each with voxel-level annotations of 15 abdominal organs, providing
challenging examples and test-bed for studying robust segmentation algorithms
under diverse targets and scenarios. We further benchmark several
state-of-the-art medical segmentation models to evaluate the status of the
existing methods on this new challenging dataset. We have made our datasets,
benchmark servers, and baselines publicly available, and hope to inspire future
research. Information can be found at https://amos22.grand-challenge.org.",,,arXiv,,,2022-06-16,2022,,,,,,All OA; Green,Preprint,"Ji, Yuanfeng; Bai, Haotian; Yang, Jie; Ge, Chongjian; Zhu, Ye; Zhang, Ruimao; Li, Zhen; Zhang, Lingyan; Ma, Wanling; Wan, Xiang; Luo, Ping","Ji, Yuanfeng (); Bai, Haotian (); Yang, Jie (); Ge, Chongjian (); Zhu, Ye (); Zhang, Ruimao (); Li, Zhen (); Zhang, Lingyan (); Ma, Wanling (); Wan, Xiang (); Luo, Ping ()",,"Ji, Yuanfeng (); Bai, Haotian (); Yang, Jie (); Ge, Chongjian (); Zhu, Ye (); Zhang, Ruimao (); Li, Zhen (); Zhang, Lingyan (); Ma, Wanling (); Wan, Xiang (); Luo, Ping ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148758824,51 Physical Sciences; 5105 Medical and Biological Physics,
1131,pub.1127205241,10.2174/1574893615999200425232601,,,An Overview of Abdominal Multi-organ Segmentation,"The segmentation of multiple abdominal organs of the human body from images with different modalities is challenging because of the inter-subject variance among abdomens, as well as the complex intra-subject variance among organs. In this paper, the recent methods proposed for abdominal multi-organ segmentation (AMOS) on medical images in the literature are reviewed. The AMOS methods can be categorized into traditional and deep learning-based methods. First, various approaches, techniques, recent advances, and related problems under both segmentation categories are explained. Second, the advantages and disadvantages of these methods are discussed. A summary of some public datasets for AMOS is provided. Finally, AMOS remains an open issue, and the combination of different methods can achieve improved segmentation performance.",Declared none.,,Current Bioinformatics,,,2021-01-01,2021,,2021-01-01,15,8,866-877,Closed,Article,"Li, Qiang; Song, Hong; Chen, Lei; Meng, Xianqi; Yang, Jian; Zhang, Le","Li, Qiang (School of Computer Science & Technology, Beijing Institute of Technology, Beijing, China, ;); Song, Hong (School of Computer Science & Technology, Beijing Institute of Technology, Beijing, China, ;); Chen, Lei (School of Computer Science & Technology, Beijing Institute of Technology, Beijing, China, ;); Meng, Xianqi (School of Optics and Electronics, Beijing Institute of Technology, Beijing, China, ;); Yang, Jian (School of Optics and Electronics, Beijing Institute of Technology, Beijing, China, ;); Zhang, Le (College of Computer Science, Sichuan University, Chengdu, China)","Song, Hong (Beijing Institute of Technology)","Li, Qiang (Beijing Institute of Technology); Song, Hong (Beijing Institute of Technology); Chen, Lei (Beijing Institute of Technology); Meng, Xianqi (Beijing Institute of Technology); Yang, Jian (Beijing Institute of Technology); Zhang, Le (Sichuan University)",1,1,,0.77,,https://app.dimensions.ai/details/publication/pub.1127205241,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1130,pub.1154903574,10.48550/arxiv.2301.10847,,,Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale  Feature Fusion Approach,"While CNN-based methods have been the cornerstone of medical image
segmentation due to their promising performance and robustness, they suffer
from limitations in capturing long-range dependencies. Transformer-based
approaches are currently prevailing since they enlarge the reception field to
model global contextual correlation. To further extract rich representations,
some extensions of the U-Net employ multi-scale feature extraction and fusion
modules and obtain improved performance. Inspired by this idea, we propose
TransCeption for medical image segmentation, a pure transformer-based U-shape
network featured by incorporating the inception-like module into the encoder
and adopting a contextual bridge for better feature fusion. The design proposed
in this work is based on three core principles: (1) The patch merging module in
the encoder is redesigned with ResInception Patch Merging (RIPM). Multi-branch
transformer (MB transformer) adopts the same number of branches as the outputs
of RIPM. Combining the two modules enables the model to capture a multi-scale
representation within a single stage. (2) We construct an Intra-stage Feature
Fusion (IFF) module following the MB transformer to enhance the aggregation of
feature maps from all the branches and particularly focus on the interaction
between the different channels of all the scales. (3) In contrast to a bridge
that only contains token-wise self-attention, we propose a Dual Transformer
Bridge that also includes channel-wise self-attention to exploit correlations
between scales at different stages from a dual perspective. Extensive
experiments on multi-organ and skin lesion segmentation tasks present the
superior performance of TransCeption compared to previous work. The code is
publicly available at \url{https://github.com/mindflow-institue/TransCeption}.",,,arXiv,,,2023-01-25,2023,,,,,,All OA; Green,Preprint,"Azad, Reza; Jia, Yiwei; Aghdam, Ehsan Khodapanah; Cohen-Adad, Julien; Merhof, Dorit","Azad, Reza (); Jia, Yiwei (); Aghdam, Ehsan Khodapanah (); Cohen-Adad, Julien (); Merhof, Dorit ()",,"Azad, Reza (); Jia, Yiwei (); Aghdam, Ehsan Khodapanah (); Cohen-Adad, Julien (); Merhof, Dorit ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154903574,40 Engineering; 46 Information and Computing Sciences; 4605 Data Management and Data Science,
1130,pub.1150567505,10.48550/arxiv.2208.12428,,,Robust Prototypical Few-Shot Organ Segmentation with Regularized  Neural-ODEs,"Despite the tremendous progress made by deep learning models in image
semantic segmentation, they typically require large annotated examples, and
increasing attention is being diverted to problem settings like Few-Shot
Learning (FSL) where only a small amount of annotation is needed for
generalisation to novel classes. This is especially seen in medical domains
where dense pixel-level annotations are expensive to obtain. In this paper, we
propose Regularized Prototypical Neural Ordinary Differential Equation
(R-PNODE), a method that leverages intrinsic properties of Neural-ODEs,
assisted and enhanced by additional cluster and consistency losses to perform
Few-Shot Segmentation (FSS) of organs. R-PNODE constrains support and query
features from the same classes to lie closer in the representation space
thereby improving the performance over the existing Convolutional Neural
Network (CNN) based FSS methods. We further demonstrate that while many
existing Deep CNN based methods tend to be extremely vulnerable to adversarial
attacks, R-PNODE exhibits increased adversarial robustness for a wide array of
these attacks. We experiment with three publicly available multi-organ
segmentation datasets in both in-domain and cross-domain FSS settings to
demonstrate the efficacy of our method. In addition, we perform experiments
with seven commonly used adversarial attacks in various settings to demonstrate
R-PNODE's robustness. R-PNODE outperforms the baselines for FSS by significant
margins and also shows superior performance for a wide array of attacks varying
in intensity and design.",,,arXiv,,,2022-08-25,2022,,,,,,All OA; Green,Preprint,"Pandey, Prashant; Chasmai, Mustafa; Sur, Tanuj; Lall, Brejesh","Pandey, Prashant (); Chasmai, Mustafa (); Sur, Tanuj (); Lall, Brejesh ()",,"Pandey, Prashant (); Chasmai, Mustafa (); Sur, Tanuj (); Lall, Brejesh ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150567505,46 Information and Computing Sciences; 4611 Machine Learning,
1128,pub.1145982160,10.1007/s11042-022-12055-3,,,Multiple organ-specific cancers classification from PET/CT images using deep learning,"As the number of cancer cases increases and the popularity of positron emission tomography/computed tomography (PET/CT), an automated cancer screening system that can assist radiologists is desired. The existing methods based on PET/CT images are mostly limited to one specific organ. In this paper, a method based on deep learning is proposed that can classify multiple organ-specific cancer to assist radiologists. In the classification model, we introduce a modality fusion module to fuse PET images, CT images, and the segmentation result of multi-organs. The segmentation result of organs is used as the attention map which can force the network to learn organ-related features from the whole-body PET/CT image. Since the low-dose computed tomography (LDCT) images are widely used in PET/CT, a grayscale transformation strategy and a double-level V-net are proposed to segment multiple organs in LDCT. The proposed grayscale transformation strategy solves insufficient annotated data, and the double-level V-net strengthens the context information of images. The proposed method can classify PET/CT images into six screening classes (health, esophageal cancer, gastric cancer, liver cancer, pancreatic cancer, and lung cancer). The experimental results demonstrate that the F-score of the classifier reaches 82.3%, indicating that it can assists radiologists in screening cancers.",This work was supported by the National Natural Science Foundation of China under Grant 61673276.,,Multimedia Tools and Applications,,,2022-03-02,2022,2022-03-02,2022-05,81,12,16133-16154,Closed,Article,"Zhang, Jiapeng; Wang, Yongxiong; Liu, Jianjun; Tang, Zhenhui; Wang, Zhe","Zhang, Jiapeng (Department of Control Science and Engineering, University of Shanghai for Science and Technology, Shanghai, China); Wang, Yongxiong (Department of Control Science and Engineering, University of Shanghai for Science and Technology, Shanghai, China); Liu, Jianjun (Department of Nuclear Medicine, Renji Hospital, School of Medicine, Shanghai Jiao Tong University, Shanghai, China); Tang, Zhenhui (Department of Automation, Key Laboratory of System Control and Information Processing, Shanghai Jiao Tong University, Shanghai, China); Wang, Zhe (Department of Control Science and Engineering, University of Shanghai for Science and Technology, Shanghai, China)","Wang, Yongxiong (University of Shanghai for Science and Technology)","Zhang, Jiapeng (University of Shanghai for Science and Technology); Wang, Yongxiong (University of Shanghai for Science and Technology); Liu, Jianjun (Shanghai Jiao Tong University; Renji Hospital); Tang, Zhenhui (Shanghai Jiao Tong University); Wang, Zhe (University of Shanghai for Science and Technology)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1145982160,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1126,pub.1154796648,10.1155/2023/3617318,,,An End-to-End Data-Adaptive Pancreas Segmentation System with an Image Quality Control Toolbox,"With the development of radiology and computer technology, diagnosis by medical imaging is heading toward precision and automation. Due to complex anatomy around the pancreatic tissue and high demands for clinical experience, the assisted pancreas segmentation system will greatly promote clinical efficiency. However, the existing segmentation model suffers from poor generalization among images from multiple hospitals. In this paper, we propose an end-to-end data-adaptive pancreas segmentation system to tackle the problems of lack of annotations and model generalizability. The system employs adversarial learning to transfer features from labeled domains to unlabeled domains, seeking a dynamic balance between domain discrimination and unsupervised segmentation. The image quality control toolbox is embedded in the system, which standardizes image quality in terms of intensity, field of view, and so on, to decrease heterogeneity among image domains. In addition, the system implements a data-adaptive process end-to-end without complex operations by doctors. The experiments are conducted on an annotated public dataset and an unannotated in-hospital dataset. The results indicate that after data adaptation, the segmentation performance measured by the dice similarity coefficient on unlabeled images improves from 58.79% to 75.43%, with a gain of 16.64%. Furthermore, the system preserves quantitatively structured information such as the pancreas’ size and volume, as well as objective and accurate visualized images, which assists clinicians in diagnosing and formulating treatment plans in a timely and accurate manner.","This work was supported in part by the National Natural Science Foundation of China (Nos. 12101571, 82172069, and 81702332), the Major Scientific Project of Zhejiang Lab (No. 2020ND8AD01), the Zhejiang Provincial Natural Science Foundation of China (No. LQ20H180001), and the Zhejiang Provincial Key Research and Development Program (No. 2020C03117).",,Journal of Healthcare Engineering,,,2023-01-24,2023,,2023-01-24,2023,,1-12,All OA; Gold,Article,"Zhu, Yan; Hu, Peijun; Li, Xiang; Tian, Yu; Bai, Xueli; Liang, Tingbo; Li, Jingsong","Zhu, Yan (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou 310027, China, meb.gov.tr); Hu, Peijun (Research Center for Healthcare Data Science, Zhejiang Lab, Hangzhou 311100, China, zhejianglab.com); Li, Xiang (Department of Hepatobiliary and Pancreatic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou 310006, China, zju.edu.cn; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou 310006, China, zju.edu.cn); Tian, Yu (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou 310027, China, meb.gov.tr); Bai, Xueli (Department of Hepatobiliary and Pancreatic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou 310006, China, zju.edu.cn; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou 310006, China, zju.edu.cn); Liang, Tingbo (Department of Hepatobiliary and Pancreatic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou 310006, China, zju.edu.cn; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou 310006, China, zju.edu.cn); Li, Jingsong (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou 310027, China, meb.gov.tr; Research Center for Healthcare Data Science, Zhejiang Lab, Hangzhou 311100, China, zhejianglab.com)","Li, Jingsong (Zhejiang University; Zhejiang Lab)","Zhu, Yan (Zhejiang University); Hu, Peijun (Zhejiang Lab); Li, Xiang (Zhejiang University; First Affiliated Hospital Zhejiang University); Tian, Yu (Zhejiang University); Bai, Xueli (Zhejiang University; First Affiliated Hospital Zhejiang University); Liang, Tingbo (Zhejiang University; First Affiliated Hospital Zhejiang University); Li, Jingsong (Zhejiang University; Zhejiang Lab)",0,0,,,https://downloads.hindawi.com/journals/jhe/2023/3617318.pdf,https://app.dimensions.ai/details/publication/pub.1154796648,40 Engineering; 4003 Biomedical Engineering,
1126,pub.1153402310,10.48550/arxiv.2212.01703,,,Active learning using adaptable task-based prioritisation,"Supervised machine learning-based medical image computing applications
necessitate expert label curation, while unlabelled image data might be
relatively abundant. Active learning methods aim to prioritise a subset of
available image data for expert annotation, for label-efficient model training.
We develop a controller neural network that measures priority of images in a
sequence of batches, as in batch-mode active learning, for multi-class
segmentation tasks. The controller is optimised by rewarding positive
task-specific performance gain, within a Markov decision process (MDP)
environment that also optimises the task predictor. In this work, the task
predictor is a segmentation network. A meta-reinforcement learning algorithm is
proposed with multiple MDPs, such that the pre-trained controller can be
adapted to a new MDP that contains data from different institutes and/or
requires segmentation of different organs or structures within the abdomen. We
present experimental results using multiple CT datasets from more than one
thousand patients, with segmentation tasks of nine different abdominal organs,
to demonstrate the efficacy of the learnt prioritisation controller function
and its cross-institute and cross-organ adaptability. We show that the proposed
adaptable prioritisation metric yields converging segmentation accuracy for the
novel class of kidney, unseen in training, using between approximately 40\% to
60\% of labels otherwise required with other heuristic or random prioritisation
metrics. For clinical datasets of limited size, the proposed adaptable
prioritisation offers a performance improvement of 22.6\% and 10.2\% in Dice
score, for tasks of kidney and liver vessel segmentation, respectively,
compared to random prioritisation and alternative active sampling strategies.",,,arXiv,,,2022-12-03,2022,,,,,,All OA; Green,Preprint,"Saeed, Shaheer U.; Ramalhinho, João; Pinnock, Mark; Shen, Ziyi; Fu, Yunguan; Montaña-Brown, Nina; Bonmati, Ester; Barratt, Dean C.; Pereira, Stephen P.; Davidson, Brian; Clarkson, Matthew J.; Hu, Yipeng","Saeed, Shaheer U. (); Ramalhinho, João (); Pinnock, Mark (); Shen, Ziyi (); Fu, Yunguan (); Montaña-Brown, Nina (); Bonmati, Ester (); Barratt, Dean C. (); Pereira, Stephen P. (); Davidson, Brian (); Clarkson, Matthew J. (); Hu, Yipeng ()",,"Saeed, Shaheer U. (); Ramalhinho, João (); Pinnock, Mark (); Shen, Ziyi (); Fu, Yunguan (); Montaña-Brown, Nina (); Bonmati, Ester (); Barratt, Dean C. (); Pereira, Stephen P. (); Davidson, Brian (); Clarkson, Matthew J. (); Hu, Yipeng ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153402310,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1069,pub.1154995257,10.1109/jbhi.2023.3240844,,,Self-supervised Tumor Segmentation with Sim2Real Adaptation,"This paper targets on self-supervised tumor segmentation. We make the following contributions: (i) we take inspiration from the observation that tumors are often characterised independently of their contexts, we propose a novel proxy task “layer-decomposition”, that closely matches the goal of the downstream task, and design a scalable pipeline for generating synthetic tumor data for pre-training; (ii) we propose a two-stage Sim2Real training regime for unsupervised tumor segmentation, where we first pre-train a model with simulated tumors, and then adopt a self-training strategy for downstream data adaptation; (iii) when evaluating on different tumor segmentation benchmarks, e.g. BraTS2018 for brain tumor segmentation and LiTS2017 for liver tumor segmentation, our approach achieves state-of-the-art segmentation performance under the unsupervised setting. While transferring the model for tumor segmentation under a low-annotation regime, the proposed approach also outperforms all existing self-supervised approaches; (iv) we conduct extensive ablation studies to analyse the critical components in data simulation, and validate the necessity of different proxy tasks. We demonstrate that, with sufficient texture randomization in simulation, model trained on synthetic data can effortlessly generalise to datasets with real tumors.",,,IEEE Journal of Biomedical and Health Informatics,,,2023-01-31,2023,2023-01-31,,PP,99,1-13,Closed,Article,"Zhang, Xiaoman; Xie, Weidi; Huang, Chaoqin; Zhang, Ya; Chen, Xin; Tian, Qi; Wang, Yanfeng","Zhang, Xiaoman (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China); Xie, Weidi (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China); Huang, Chaoqin (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China); Zhang, Ya (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China); Chen, Xin (Huawei Cloud, Shanghai, China); Tian, Qi (Huawei Cloud, Shanghai, China); Wang, Yanfeng (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China)",,"Zhang, Xiaoman (Shanghai Jiao Tong University); Xie, Weidi (Shanghai Jiao Tong University); Huang, Chaoqin (Shanghai Jiao Tong University); Zhang, Ya (Shanghai Jiao Tong University); Chen, Xin (); Tian, Qi (); Wang, Yanfeng (Shanghai Jiao Tong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154995257,46 Information and Computing Sciences; 4611 Machine Learning,
1068,pub.1128884240,10.48550/arxiv.2006.16806,,,Uncertainty-aware multi-view co-training for semi-supervised medical  image segmentation and domain adaptation,"Although having achieved great success in medical image segmentation, deep
learning-based approaches usually require large amounts of well-annotated data,
which can be extremely expensive in the field of medical image analysis.
Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised
learning and unsupervised domain adaptation both take the advantage of
unlabeled data, and they are closely related to each other. In this paper, we
propose uncertainty-aware multi-view co-training (UMCT), a unified framework
that addresses these two tasks for volumetric medical image segmentation. Our
framework is capable of efficiently utilizing unlabeled data for better
performance. We firstly rotate and permute the 3D volumes into multiple views
and train a 3D deep network on each view. We then apply co-training by
enforcing multi-view consistency on unlabeled data, where an uncertainty
estimation of each view is utilized to achieve accurate labeling. Experiments
on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset
show state-of-the-art performance of the proposed framework on semi-supervised
medical image segmentation. Under unsupervised domain adaptation settings, we
validate the effectiveness of this work by adapting our multi-organ
segmentation model to two pathological organs from the Medical Segmentation
Decathlon Datasets. Additionally, we show that our UMCT-DA model can even
effectively handle the challenging situation where labeled source data is
inaccessible, demonstrating strong potentials for real-world applications.",,,arXiv,,,2020-06-28,2020,,,,,,All OA; Green,Preprint,"Xia, Yingda; Yang, Dong; Yu, Zhiding; Liu, Fengze; Cai, Jinzheng; Yu, Lequan; Zhu, Zhuotun; Xu, Daguang; Yuille, Alan; Roth, Holger","Xia, Yingda (); Yang, Dong (); Yu, Zhiding (); Liu, Fengze (); Cai, Jinzheng (); Yu, Lequan (); Zhu, Zhuotun (); Xu, Daguang (); Yuille, Alan (); Roth, Holger ()",,"Xia, Yingda (); Yang, Dong (); Yu, Zhiding (); Liu, Fengze (); Cai, Jinzheng (); Yu, Lequan (); Zhu, Zhuotun (); Xu, Daguang (); Yuille, Alan (); Roth, Holger ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1128884240,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1067,pub.1148115869,10.48550/arxiv.2205.11096,,,FedNorm: Modality-Based Normalization in Federated Learning for  Multi-Modal Liver Segmentation,"Given the high incidence and effective treatment options for liver diseases,
they are of great socioeconomic importance. One of the most common methods for
analyzing CT and MRI images for diagnosis and follow-up treatment is liver
segmentation. Recent advances in deep learning have demonstrated encouraging
results for automatic liver segmentation. Despite this, their success depends
primarily on the availability of an annotated database, which is often not
available because of privacy concerns. Federated Learning has been recently
proposed as a solution to alleviate these challenges by training a shared
global model on distributed clients without access to their local databases.
Nevertheless, Federated Learning does not perform well when it is trained on a
high degree of heterogeneity of image data due to multi-modal imaging, such as
CT and MRI, and multiple scanner types. To this end, we propose Fednorm and its
extension \fednormp, two Federated Learning algorithms that use a
modality-based normalization technique. Specifically, Fednorm normalizes the
features on a client-level, while Fednorm+ employs the modality information of
single slices in the feature normalization. Our methods were validated using
428 patients from six publicly available databases and compared to
state-of-the-art Federated Learning algorithms and baseline models in
heterogeneous settings (multi-institutional, multi-modal data). The
experimental results demonstrate that our methods show an overall acceptable
performance, achieve Dice per patient scores up to 0.961, consistently
outperform locally trained models, and are on par or slightly better than
centralized models.",,,arXiv,,,2022-05-23,2022,,,,,,All OA; Green,Preprint,"Bernecker, Tobias; Peters, Annette; Schlett, Christopher L.; Bamberg, Fabian; Theis, Fabian; Rueckert, Daniel; Weiß, Jakob; Albarqouni, Shadi","Bernecker, Tobias (); Peters, Annette (); Schlett, Christopher L. (); Bamberg, Fabian (); Theis, Fabian (); Rueckert, Daniel (); Weiß, Jakob (); Albarqouni, Shadi ()",,"Bernecker, Tobias (); Peters, Annette (); Schlett, Christopher L. (); Bamberg, Fabian (); Theis, Fabian (); Rueckert, Daniel (); Weiß, Jakob (); Albarqouni, Shadi ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148115869,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences; 46 Information and Computing Sciences,
1066,pub.1143469147,10.48550/arxiv.2111.14791,,,Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image  Analysis,"Vision Transformers (ViT)s have shown great performance in self-supervised
learning of global and local representations that can be transferred to
downstream applications. Inspired by these results, we introduce a novel
self-supervised learning framework with tailored proxy tasks for medical image
analysis. Specifically, we propose: (i) a new 3D transformer-based model,
dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for
self-supervised pre-training; (ii) tailored proxy tasks for learning the
underlying pattern of human anatomy. We demonstrate successful pre-training of
the proposed model on 5,050 publicly available computed tomography (CT) images
from various body organs. The effectiveness of our approach is validated by
fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV)
Segmentation Challenge with 13 abdominal organs and segmentation tasks from the
Medical Segmentation Decathlon (MSD) dataset. Our model is currently the
state-of-the-art (i.e. ranked 1st) on the public test leaderboards of both MSD
and BTCV datasets. Code: https://monai.io/research/swin-unetr",,,arXiv,,,2021-11-29,2021,,,,,,All OA; Green,Preprint,"Tang, Yucheng; Yang, Dong; Li, Wenqi; Roth, Holger; Landman, Bennett; Xu, Daguang; Nath, Vishwesh; Hatamizadeh, Ali","Tang, Yucheng (); Yang, Dong (); Li, Wenqi (); Roth, Holger (); Landman, Bennett (); Xu, Daguang (); Nath, Vishwesh (); Hatamizadeh, Ali ()",,"Tang, Yucheng (); Yang, Dong (); Li, Wenqi (); Roth, Holger (); Landman, Bennett (); Xu, Daguang (); Nath, Vishwesh (); Hatamizadeh, Ali ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143469147,46 Information and Computing Sciences; 4611 Machine Learning,
1063,pub.1155159969,10.1109/tmi.2023.3242838,,,Context Label Learning: Improving Background Class Representations in Semantic Segmentation,"Background samples provide key contextual information for segmenting regions of interest (ROIs). However, they always cover a diverse set of structures, causing difficulties for the segmentation model to learn good decision boundaries with high sensitivity and precision. The issue concerns the highly heterogeneous nature of the background class, resulting in multi-modal distributions. Empirically, we find that neural networks trained with heterogeneous background struggle to map the corresponding contextual samples to compact clusters in feature space. As a result, the distribution over background logit activations may shift across the decision boundary, leading to systematic over-segmentation across different datasets and tasks. In this study, we propose context label learning (CoLab) to improve the context representations by decomposing the background class into several subclasses. Specifically, we train an auxiliary network as a task generator, along with the primary segmentation model, to automatically generate context labels that positively affect the ROI segmentation accuracy. Extensive experiments are conducted on several challenging segmentation tasks and datasets. The results demonstrate that CoLab can guide the segmentation model to map the logits of background samples away from the decision boundary, resulting in significantly improved segmentation accuracy. Code is available.",,,IEEE Transactions on Medical Imaging,,,2023-02-07,2023,2023-02-07,,PP,99,1-1,All OA; Hybrid,Article,"Li, Zeju; Kamnitsas, Konstantinos; Ouyang, Cheng; Chen, Chen; Glocker, Ben","Li, Zeju (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom); Kamnitsas, Konstantinos (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom); Ouyang, Cheng (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom); Chen, Chen (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom); Glocker, Ben (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom)",,"Li, Zeju (Imperial College London); Kamnitsas, Konstantinos (Imperial College London); Ouyang, Cheng (Imperial College London); Chen, Chen (Imperial College London); Glocker, Ben (Imperial College London)",0,0,,,https://ieeexplore.ieee.org/ielx7/42/4359023/10038608.pdf,https://app.dimensions.ai/details/publication/pub.1155159969,46 Information and Computing Sciences; 4611 Machine Learning,
1013,pub.1148868686,10.48550/arxiv.2206.10571,,,Toward Unpaired Multi-modal Medical Image Segmentation via Learning  Structured Semantic Consistency,"Integrating multi-modal data to improve medical image analysis has received
great attention recently. However, due to the modal discrepancy, how to use a
single model to process the data from multiple modalities is still an open
issue. In this paper, we propose a novel scheme to achieve better pixel-level
segmentation for unpaired multi-modal medical images. Different from previous
methods which adopted both modality-specific and modality-shared modules to
accommodate the appearance variance of different modalities while extracting
the common semantic information, our method is based on a single Transformer
with a carefully designed External Attention Module (EAM) to learn the
structured semantic consistency (i.e. semantic class representations and their
correlations) between modalities in the training phase. In practice, the
above-mentioned structured semantic consistency across modalities can be
progressively achieved by implementing the consistency regularization at the
modality-level and image-level respectively. The proposed EAMs are adopted to
learn the semantic consistency for different scale representations and can be
discarded once the model is optimized. Therefore, during the testing phase, we
only need to maintain one Transformer for all modal predictions, which nicely
balances the model's ease of use and simplicity. To demonstrate the
effectiveness of the proposed method, we conduct the experiments on two medical
image segmentation scenarios: (1) cardiac structure segmentation, and (2)
abdominal multi-organ segmentation. Extensive results show that the proposed
method outperforms the state-of-the-art methods by a wide margin, and even
achieves competitive performance with extremely limited training samples (e.g.,
1 or 3 annotated CT or MRI images) for one specific modality.",,,arXiv,,,2022-06-21,2022,,,,,,All OA; Green,Preprint,"Yang, Jie; Zhu, Ye; Zhang, Ruimao; Wang, Chaoqun; Li, Zhen; Wan, Xiang","Yang, Jie (); Zhu, Ye (); Zhang, Ruimao (); Wang, Chaoqun (); Li, Zhen (); Wan, Xiang ()",,"Yang, Jie (); Zhu, Ye (); Zhang, Ruimao (); Wang, Chaoqun (); Li, Zhen (); Wan, Xiang ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148868686,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4605 Data Management and Data Science,
1012,pub.1129504284,10.48550/arxiv.2007.09465,,,PSIGAN: Joint probabilistic segmentation and image distribution matching  for unpaired cross-modality adaptation based MRI segmentation,"We developed a new joint probabilistic segmentation and image distribution
matching generative adversarial network (PSIGAN) for unsupervised domain
adaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI)
images. Our UDA approach models the co-dependency between images and their
segmentation as a joint probability distribution using a new structure
discriminator. The structure discriminator computes structure of interest
focused adversarial loss by combining the generated pseudo MRI with
probabilistic segmentations produced by a simultaneously trained segmentation
sub-network. The segmentation sub-network is trained using the pseudo MRI
produced by the generator sub-network. This leads to a cyclical optimization of
both the generator and segmentation sub-networks that are jointly trained as
part of an end-to-end network. Extensive experiments and comparisons against
multiple state-of-the-art methods were done on four different MRI sequences
totalling 257 scans for generating multi-organ and tumor segmentation. The
experiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20
T2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right
kidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for
parotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our
method achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the
abdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for
lung tumors.",,,arXiv,,,2020-07-18,2020,,,,,,All OA; Green,Preprint,"Jiang, Jue; Hu, Yu Chi; Tyagi, Neelam; Rimner, Andreas; Lee, Nancy; Deasy, Joseph O.; Berry, Sean; Veeraraghavan, Harini","Jiang, Jue (); Hu, Yu Chi (); Tyagi, Neelam (); Rimner, Andreas (); Lee, Nancy (); Deasy, Joseph O. (); Berry, Sean (); Veeraraghavan, Harini ()",,"Jiang, Jue (); Hu, Yu Chi (); Tyagi, Neelam (); Rimner, Andreas (); Lee, Nancy (); Deasy, Joseph O. (); Berry, Sean (); Veeraraghavan, Harini ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129504284,46 Information and Computing Sciences; 4611 Machine Learning,
1010,pub.1151333024,10.1101/2022.09.24.22280071,,,The FELIX Project: Deep Networks To Detect Pancreatic Neoplasms,"Tens of millions of abdominal images are obtained with computed tomography (CT) in the U.S. each year but pancreatic cancers are sometimes not initially detected in these images. We here describe a suite of algorithms (named FELIX) that can recognize pancreatic lesions from CT images without human input. Using FELIX, >95% of patients with pancreatic ductal adenocarcinomas were detected at a specificity of >95% in patients without pancreatic disease. FELIX may be able to assist radiologists in identifying pancreatic cancers earlier, when surgery and other treatments offer more hope for long-term survival.","This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research, the Virginia and D.K. Ludwig Fund for Cancer Research, the Sol Goldman Charitable Trust, and NIH Grant #CA06973.",This work was funded by the Lustgarten Foundation for Pancreatic Cancer Research.,medRxiv,,,2022-09-25,2022,2022-09-25,,,,2022.09.24.22280071,All OA; Green,Preprint,"Xia, Yingda; Yu, Qihang; Chu, Linda; Kawamoto, Satomi; Park, Seyoun; Liu, Fengze; Chen, Jieneng; Zhu, Zhuotun; Li, Bowen; Zhou, Zongwei; Lu, Yongyi; Wang, Yan; Shen, Wei; Xie, Lingxi; Zhou, Yuyin; Wolfgang, Christopher; Javed, Ammar; Fouladi, Daniel Fadaei; Shayesteh, Shahab; Graves, Jefferson; Blanco, Alejandra; Zinreich, Eva S.; Klauss, Miriam; Mayer, Philipp; Kinny-Koster, Benedict; Kinzler, Kenneth; Hruban, Ralph H.; Vogelstein, Bert; Yuille, Alan L.; Fishman, Elliot K.","Xia, Yingda (); Yu, Qihang (); Chu, Linda (); Kawamoto, Satomi (); Park, Seyoun (); Liu, Fengze (); Chen, Jieneng (); Zhu, Zhuotun (); Li, Bowen (); Zhou, Zongwei (); Lu, Yongyi (); Wang, Yan (); Shen, Wei (); Xie, Lingxi (); Zhou, Yuyin (); Wolfgang, Christopher (); Javed, Ammar (); Fouladi, Daniel Fadaei (); Shayesteh, Shahab (); Graves, Jefferson (); Blanco, Alejandra (); Zinreich, Eva S. (); Klauss, Miriam (); Mayer, Philipp (); Kinny-Koster, Benedict (); Kinzler, Kenneth (); Hruban, Ralph H. (); Vogelstein, Bert (); Yuille, Alan L. (); Fishman, Elliot K. ()","Yuille, Alan L. ; Fishman, Elliot K. ","Xia, Yingda (); Yu, Qihang (); Chu, Linda (); Kawamoto, Satomi (); Park, Seyoun (); Liu, Fengze (); Chen, Jieneng (); Zhu, Zhuotun (); Li, Bowen (); Zhou, Zongwei (); Lu, Yongyi (); Wang, Yan (); Shen, Wei (); Xie, Lingxi (); Zhou, Yuyin (); Wolfgang, Christopher (); Javed, Ammar (); Fouladi, Daniel Fadaei (); Shayesteh, Shahab (); Graves, Jefferson (); Blanco, Alejandra (); Zinreich, Eva S. (); Klauss, Miriam (); Mayer, Philipp (); Kinny-Koster, Benedict (); Kinzler, Kenneth (); Hruban, Ralph H. (); Vogelstein, Bert (); Yuille, Alan L. (); Fishman, Elliot K. ()",0,0,,,https://www.medrxiv.org/content/medrxiv/early/2022/09/25/2022.09.24.22280071.full.pdf,https://app.dimensions.ai/details/publication/pub.1151333024,32 Biomedical and Clinical Sciences; 3211 Oncology and Carcinogenesis,
1010,pub.1119401204,10.48550/arxiv.1906.10089,,,Image to Images Translation for Multi-Task Organ Segmentation and Bone  Suppression in Chest X-Ray Radiography,"Chest X-ray radiography is one of the earliest medical imaging technologies
and remains one of the most widely-used for diagnosis, screening, and treatment
follow up of diseases related to lungs and heart. The literature in this field
of research reports many interesting studies dealing with the challenging tasks
of bone suppression and organ segmentation but performed separately, limiting
any learning that comes with the consolidation of parameters that could
optimize both processes. This study, and for the first time, introduces a
multitask deep learning model that generates simultaneously the bone-suppressed
image and the organ-segmented image, enhancing the accuracy of tasks,
minimizing the number of parameters needed by the model and optimizing the
processing time, all by exploiting the interplay between the network parameters
to benefit the performance of both tasks. The architectural design of this
model, which relies on a conditional generative adversarial network, reveals
the process on how the well-established pix2pix network (image-to-image
network) is modified to fit the need for multitasking and extending it to the
new image-to-images architecture. The developed source code of this multitask
model is shared publicly on Github as the first attempt for providing the
two-task pix2pix extension, a supervised/paired/aligned/registered
image-to-images translation which would be useful in many multitask
applications. Dilated convolutions are also used to improve the results through
a more effective receptive field assessment. The comparison with
state-of-the-art algorithms along with ablation study and a demonstration video
are provided to evaluate efficacy and gauge the merits of the proposed
approach.",,,arXiv,,,2019-06-24,2019,,,,,,All OA; Green,Preprint,"Eslami, Mohammad; Tabarestani, Solale; Albarqouni, Shadi; Adeli, Ehsan; Navab, Nassir; Adjouadi, Malek","Eslami, Mohammad (); Tabarestani, Solale (); Albarqouni, Shadi (); Adeli, Ehsan (); Navab, Nassir (); Adjouadi, Malek ()",,"Eslami, Mohammad (); Tabarestani, Solale (); Albarqouni, Shadi (); Adeli, Ehsan (); Navab, Nassir (); Adjouadi, Malek ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119401204,46 Information and Computing Sciences; 4611 Machine Learning,
965,pub.1152748225,10.48550/arxiv.2211.06894,,,Learning from partially labeled data for multi-organ and tumor  segmentation,"Medical image benchmarks for the segmentation of organs and tumors suffer
from the partially labeling issue due to its intensive cost of labor and
expertise. Current mainstream approaches follow the practice of one network
solving one task. With this pipeline, not only the performance is limited by
the typically small dataset of a single task, but also the computation cost
linearly increases with the number of tasks. To address this, we propose a
Transformer based dynamic on-demand network (TransDoDNet) that learns to
segment organs and tumors on multiple partially labeled datasets. Specifically,
TransDoDNet has a hybrid backbone that is composed of the convolutional neural
network and Transformer. A dynamic head enables the network to accomplish
multiple segmentation tasks flexibly. Unlike existing approaches that fix
kernels after training, the kernels in the dynamic head are generated
adaptively by the Transformer, which employs the self-attention mechanism to
model long-range organ-wise dependencies and decodes the organ embedding that
can represent each organ. We create a large-scale partially labeled Multi-Organ
and Tumor Segmentation benchmark, termed MOTS, and demonstrate the superior
performance of our TransDoDNet over other competitors on seven organ and tumor
segmentation tasks. This study also provides a general 3D medical image
segmentation model, which has been pre-trained on the large-scale MOTS
benchmark and has demonstrated advanced performance over BYOL, the current
predominant self-supervised learning method. Code will be available at
\url{https://git.io/DoDNet}.",,,arXiv,,,2022-11-13,2022,,,,,,All OA; Green,Preprint,"Xie, Yutong; Zhang, Jianpeng; Xia, Yong; Shen, Chunhua","Xie, Yutong (); Zhang, Jianpeng (); Xia, Yong (); Shen, Chunhua ()",,"Xie, Yutong (); Zhang, Jianpeng (); Xia, Yong (); Shen, Chunhua ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152748225,46 Information and Computing Sciences; 4611 Machine Learning,
965,pub.1142403659,10.48550/arxiv.2111.02403,,,"WORD: A large scale dataset, benchmark and clinical applicable study for  abdominal organ segmentation from CT image","Whole abdominal organ segmentation is important in diagnosing abdomen
lesions, radiotherapy, and follow-up. However, oncologists' delineating all
abdominal organs from 3D volumes is time-consuming and very expensive. Deep
learning-based medical image segmentation has shown the potential to reduce
manual delineation efforts, but it still requires a large-scale fine annotated
dataset for training, and there is a lack of large-scale datasets covering the
whole abdomen region with accurate and detailed annotations for the whole
abdominal organ segmentation. In this work, we establish a new large-scale
\textit{W}hole abdominal \textit{OR}gan \textit{D}ataset (\textit{WORD}) for
algorithm research and clinical application development. This dataset contains
150 abdominal CT volumes (30495 slices). Each volume has 16 organs with fine
pixel-level annotations and scribble-based sparse annotations, which may be the
largest dataset with whole abdominal organ annotation. Several state-of-the-art
segmentation methods are evaluated on this dataset. And we also invited three
experienced oncologists to revise the model predictions to measure the gap
between the deep learning method and oncologists. Afterwards, we investigate
the inference-efficient learning on the WORD, as the high-resolution image
requires large GPU memory and a long inference time in the test stage. We
further evaluate the scribble-based annotation-efficient learning on this
dataset, as the pixel-wise manual annotation is time-consuming and expensive.
The work provided a new benchmark for the abdominal multi-organ segmentation
task, and these experiments can serve as the baseline for future research and
clinical application development.",,,arXiv,,,2021-11-02,2021,,,,,,All OA; Green,Preprint,"Luo, Xiangde; Liao, Wenjun; Xiao, Jianghong; Chen, Jieneng; Song, Tao; Zhang, Xiaofan; Li, Kang; Metaxas, Dimitris N.; Wang, Guotai; Zhang, Shaoting","Luo, Xiangde (); Liao, Wenjun (); Xiao, Jianghong (); Chen, Jieneng (); Song, Tao (); Zhang, Xiaofan (); Li, Kang (); Metaxas, Dimitris N. (); Wang, Guotai (); Zhang, Shaoting ()",,"Luo, Xiangde (); Liao, Wenjun (); Xiao, Jianghong (); Chen, Jieneng (); Song, Tao (); Zhang, Xiaofan (); Li, Kang (); Metaxas, Dimitris N. (); Wang, Guotai (); Zhang, Shaoting ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142403659,51 Physical Sciences; 5105 Medical and Biological Physics,
962,pub.1153777520,10.48550/arxiv.2212.08423,,,Context Label Learning: Improving Background Class Representations in  Semantic Segmentation,"Background samples provide key contextual information for segmenting regions
of interest (ROIs). However, they always cover a diverse set of structures,
causing difficulties for the segmentation model to learn good decision
boundaries with high sensitivity and precision. The issue concerns the highly
heterogeneous nature of the background class, resulting in multi-modal
distributions. Empirically, we find that neural networks trained with
heterogeneous background struggle to map the corresponding contextual samples
to compact clusters in feature space. As a result, the distribution over
background logit activations may shift across the decision boundary, leading to
systematic over-segmentation across different datasets and tasks. In this
study, we propose context label learning (CoLab) to improve the context
representations by decomposing the background class into several subclasses.
Specifically, we train an auxiliary network as a task generator, along with the
primary segmentation model, to automatically generate context labels that
positively affect the ROI segmentation accuracy. Extensive experiments are
conducted on several challenging segmentation tasks and datasets. The results
demonstrate that CoLab can guide the segmentation model to map the logits of
background samples away from the decision boundary, resulting in significantly
improved segmentation accuracy. Code is available.",,,arXiv,,,2022-12-16,2022,,,,,,All OA; Green,Preprint,"Li, Zeju; Kamnitsas, Konstantinos; Ouyang, Cheng; Chen, Chen; Glocker, Ben","Li, Zeju (); Kamnitsas, Konstantinos (); Ouyang, Cheng (); Chen, Chen (); Glocker, Ben ()",,"Li, Zeju (); Kamnitsas, Konstantinos (); Ouyang, Cheng (); Chen, Chen (); Glocker, Ben ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153777520,46 Information and Computing Sciences; 4611 Machine Learning,
961,pub.1151766002,10.48550/arxiv.2210.04285,,,Improved Abdominal Multi-Organ Segmentation via 3D Boundary-Constrained  Deep Neural Networks,"Quantitative assessment of the abdominal region from clinically acquired CT
scans requires the simultaneous segmentation of abdominal organs. Thanks to the
availability of high-performance computational resources, deep learning-based
methods have resulted in state-of-the-art performance for the segmentation of
3D abdominal CT scans. However, the complex characterization of organs with
fuzzy boundaries prevents the deep learning methods from accurately segmenting
these anatomical organs. Specifically, the voxels on the boundary of organs are
more vulnerable to misprediction due to the highly-varying intensity of
inter-organ boundaries. This paper investigates the possibility of improving
the abdominal image segmentation performance of the existing 3D encoder-decoder
networks by leveraging organ-boundary prediction as a complementary task. To
address the problem of abdominal multi-organ segmentation, we train the 3D
encoder-decoder network to simultaneously segment the abdominal organs and
their corresponding boundaries in CT scans via multi-task learning. The network
is trained end-to-end using a loss function that combines two task-specific
losses, i.e., complete organ segmentation loss and boundary prediction loss. We
explore two different network topologies based on the extent of weights shared
between the two tasks within a unified multi-task framework. To evaluate the
utilization of complementary boundary prediction task in improving the
abdominal multi-organ segmentation, we use three state-of-the-art
encoder-decoder networks: 3D UNet, 3D UNet++, and 3D Attention-UNet. The
effectiveness of utilizing the organs' boundary information for abdominal
multi-organ segmentation is evaluated on two publically available abdominal CT
datasets. A maximum relative improvement of 3.5% and 3.6% is observed in Mean
Dice Score for Pancreas-CT and BTCV datasets, respectively.",,,arXiv,,,2022-10-09,2022,,,,,,All OA; Green,Preprint,"Irshad, Samra; Gomes, Douglas P. S.; Kim, Seong Tae","Irshad, Samra (); Gomes, Douglas P. S. (); Kim, Seong Tae ()",,"Irshad, Samra (); Gomes, Douglas P. S. (); Kim, Seong Tae ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151766002,46 Information and Computing Sciences; 4611 Machine Learning; 51 Physical Sciences,
919,pub.1131126270,10.1007/s11831-020-09497-z,,,A Systematic Review of the Techniques for the Automatic Segmentation of Organs-at-Risk in Thoracic Computed Tomography Images,"The standard treatment for the cancer is the radiotherapy where the organs nearby the target volumes get affected during treatment called the Organs-at-risk. Segmentation of Organs-at-risk is crucial but important for the proper planning of radiotherapy treatment. Manual segmentation is time consuming and tedious in regular practices and results may vary from experts to experts. The automatic segmentation will produce robust results with precise accuracy. The aim of this systematic review is to study various techniques for the automatic segmentation of organs-at-risk in thoracic computed tomography images and to discuss the best technique which give the higher accuracy in terms of segmentation among all other techniques proposed in the literature. PRISMA guidelines had been used to conduct this systematic review. Three online databases had been used for the identification of the related papers and a query had been formed for the search purpose. The papers were shortlisted based on the various inclusion and exclusion criteria. Four research questions had been designed and answers of those were explored. After reviewing all the techniques, the best technique had been selected and discussed in detail which gave the precise accuracy based on Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD). Both DSC and HD were used in the literature to evaluate the performance of their proposed technique for the automatic segmentation of four organs (esophagus, heart, trachea and aorta). However, the value of these parameters vary as per the validation sample size. Consequently, various challenges faced by the researchers had been listed. This paper includes the summary of the various automatic segmentation techniques for the Organs-at-risk in thoracic computed tomography images in terms of four research questions. Different techniques, Datasets, Performance accuracy and various challenges had been discussed.",,,Archives of Computational Methods in Engineering,,,2020-09-23,2020,2020-09-23,2021-06,28,4,3245-3267,Closed,Article,"Ashok, Malvika; Gupta, Abhishek","Ashok, Malvika (School of Computer Science and Engineering, Shri Mata Vaishno Devi University, 182320, Kakryal, Katra, Jammu and Kashmir, India); Gupta, Abhishek (School of Computer Science and Engineering, Shri Mata Vaishno Devi University, 182320, Kakryal, Katra, Jammu and Kashmir, India)","Gupta, Abhishek (Shri Mata Vaishno Devi University)","Ashok, Malvika (Shri Mata Vaishno Devi University); Gupta, Abhishek (Shri Mata Vaishno Devi University)",16,16,,,,https://app.dimensions.ai/details/publication/pub.1131126270,40 Engineering; 49 Mathematical Sciences,
913,pub.1145979358,10.48550/arxiv.2203.00131,,,"A Data-scalable Transformer for Medical Image Segmentation:  Architecture, Model Efficiency, and Benchmark","Transformer, as a new generation of neural architecture, has demonstrated
remarkable performance in natural language processing and computer vision.
However, existing vision Transformers struggle to learn with limited medical
data and are unable to generalize on diverse medical image tasks. To tackle
these challenges, we present MedFormer as a data-scalable Transformer towards
generalizable medical image segmentation. The key designs incorporate desirable
inductive bias, hierarchical modeling with linear-complexity attention, and
multi-scale feature fusion in a spatially and semantically global manner.
MedFormer can learn across tiny- to large-scale data without pre-training.
Extensive experiments demonstrate the potential of MedFormer as a general
segmentation backbone, outperforming CNNs and vision Transformers on three
public datasets with multiple modalities (e.g., CT and MRI) and diverse medical
targets (e.g., healthy organ, diseased tissue, and tumor). We make the models
and evaluation pipeline publicly available, offering solid baselines and
unbiased comparisons for promoting a wide range of downstream clinical
applications.",,,arXiv,,,2022-02-28,2022,,,,,,All OA; Green,Preprint,"Gao, Yunhe; Zhou, Mu; Liu, Di; Yan, Zhennan; Zhang, Shaoting; Metaxas, Dimitris N.","Gao, Yunhe (); Zhou, Mu (); Liu, Di (); Yan, Zhennan (); Zhang, Shaoting (); Metaxas, Dimitris N. ()",,"Gao, Yunhe (); Zhou, Mu (); Liu, Di (); Yan, Zhennan (); Zhang, Shaoting (); Metaxas, Dimitris N. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1145979358,46 Information and Computing Sciences; 4611 Machine Learning,
878,pub.1154688911,10.1109/tmi.2023.3238067,,,Partial Unbalanced Feature Transport for Cross-Modality Cardiac Image Segmentation,"Deep learning based approaches have achieved great success on the automatic cardiac image segmentation task. However, the achieved segmentation performance remains limited due to the significant difference across image domains, which is referred to as domain shift. Unsupervised domain adaptation (UDA), as a promising method to mitigate this effect, trains a model to reduce the domain discrepancy between the source (with labels) and the target (without labels) domains in a common latent feature space. In this work, we propose a novel framework, named Partial Unbalanced Feature Transport (PUFT), for cross-modality cardiac image segmentation. Our model facilities UDA leveraging two Continuous Normalizing Flow-based Variational Auto-Encoders (CNF-VAE) and a Partial Unbalanced Optimal Transport (PUOT) strategy. Instead of directly using VAE for UDA in previous works where the latent features from both domains are approximated by a parameterized variational form, we introduce continuous normalizing flows (CNF) into the extended VAE to estimate the probabilistic posterior and alleviate the inference bias. To remove the remaining domain shift, PUOT exploits the label information in the source domain to constrain the OT plan and extracts structural information of both domains, which are often neglected in classical OT for UDA. We evaluate our proposed model on two cardiac datasets and an abdominal dataset. The experimental results demonstrate that PUFT achieves superior performance compared with state-of-the-art segmentation methods for most structural segmentation.",,,IEEE Transactions on Medical Imaging,,,2023-01-19,2023,2023-01-19,,PP,99,1-1,Closed,Article,"Dong, Shunjie; Pan, Zixuan; Fu, Yu; Xu, Dongwei; Shi, Kuangyu; Yang, Qianqian; Shi, Yiyu; Zhuo, Cheng","Dong, Shunjie (College of Information Science &#x0026; Electronic Engineering, Zhejiang University, Hangzhou, China); Pan, Zixuan (Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, USA); Fu, Yu (College of Information Science &#x0026; Electronic Engineering, Zhejiang University, Hangzhou, China); Xu, Dongwei (College of Information Science &#x0026; Electronic Engineering, Zhejiang University, Hangzhou, China); Shi, Kuangyu (Department of Nuclear Medicine, Inselspital University Hospital Bern, Bern, Switzerland); Yang, Qianqian (College of Information Science &#x0026; Electronic Engineering, Zhejiang University, Hangzhou, China); Shi, Yiyu (Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, USA); Zhuo, Cheng (College of Information Science &#x0026; Electronic Engineering, Zhejiang University, Hangzhou, China)",,"Dong, Shunjie (Zhejiang University); Pan, Zixuan (University of Notre Dame); Fu, Yu (Zhejiang University); Xu, Dongwei (Zhejiang University); Shi, Kuangyu (University Hospital of Bern); Yang, Qianqian (Zhejiang University); Shi, Yiyu (University of Notre Dame); Zhuo, Cheng (Zhejiang University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154688911,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
878,pub.1135670476,10.48550/arxiv.2102.10365,,,Analyzing Overfitting under Class Imbalance in Neural Networks for Image  Segmentation,"Class imbalance poses a challenge for developing unbiased, accurate
predictive models. In particular, in image segmentation neural networks may
overfit to the foreground samples from small structures, which are often
heavily under-represented in the training set, leading to poor generalization.
In this study, we provide new insights on the problem of overfitting under
class imbalance by inspecting the network behavior. We find empirically that
when training with limited data and strong class imbalance, at test time the
distribution of logit activations may shift across the decision boundary, while
samples of the well-represented class seem unaffected. This bias leads to a
systematic under-segmentation of small structures. This phenomenon is
consistently observed for different databases, tasks and network architectures.
To tackle this problem, we introduce new asymmetric variants of popular loss
functions and regularization techniques including a large margin loss, focal
loss, adversarial training, mixup and data augmentation, which are explicitly
designed to counter logit shift of the under-represented classes. Extensive
experiments are conducted on several challenging segmentation tasks. Our
results demonstrate that the proposed modifications to the objective function
can lead to significantly improved segmentation accuracy compared to baselines
and alternative approaches.",,,arXiv,,,2021-02-20,2021,,,,,,All OA; Green,Preprint,"Li, Zeju; Kamnitsas, Konstantinos; Glocker, Ben","Li, Zeju (); Kamnitsas, Konstantinos (); Glocker, Ben ()",,"Li, Zeju (); Kamnitsas, Konstantinos (); Glocker, Ben ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1135670476,46 Information and Computing Sciences; 4611 Machine Learning,
873,pub.1152730442,10.1109/tcsvt.2022.3221925,,,Weak-Boundary Sensitive Superpixel Segmentation Based on Local Adaptive Distance,"Superpixel segmentation provides a way to capture object boundaries unsupervised and has benefited many compute vision applications. However, under-segmentation for weak boundaries and poor compatibility with image feature representations often limit its wide application. In this paper, we propose a new weak-boundary sensitive superpixel generation method and provide an all-in-one solution for images with different feature representations. We first design a local adaptive distance (LAD) to be more sensitive to feature changes in low-contrast regions. LAD leverages image local standard deviation as region contrast clues. It adaptively increases the feature distances in low-contrast regions to avoid feature space distances of weak boundaries being inundated by regularity constraints. LAD is scale-invariant that can be compatible with high bit-depth and multi-feature images. Then, based on LAD, we introduce a novel morphological contour evolution model to generate superpixels iteratively. Leveraging morphological dilation of superpixel shapes, the new model is more conducive to the boundary detection of irregular or slender objects. Extensive experiments demonstrate that our method favorably outperforms state-of-the-art methods, especially regarding the under-segmentation error and segmentation accuracy.",,,IEEE Transactions on Circuits and Systems for Video Technology,,,2022-11-14,2022,2022-11-14,,PP,99,1-1,Closed,Article,"Sun, Limin; Ma, Dongyang; Pan, Xiao; Zhou, Yuanfeng","Sun, Limin (School of Software, Shandong University, Jinan, China); Ma, Dongyang (School of Computer Science, Peking University, Beijing, China); Pan, Xiao (School of Computer Science and Technology, and the Shandong Key Laboratory of Digital Media Technology, Shandong University of Finance and Economics, Jinan, China); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, China)",,"Sun, Limin (Shandong University); Ma, Dongyang (Peking University); Pan, Xiao (Shandong University of Finance and Economics); Zhou, Yuanfeng (Shandong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152730442,"46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4607 Graphics, Augmented Reality and Games",
840,pub.1142015215,10.48550/arxiv.2110.09260,,,A Unified Framework for Generalized Low-Shot Medical Image Segmentation  with Scarce Data,"Medical image segmentation has achieved remarkable advancements using deep
neural networks (DNNs). However, DNNs often need big amounts of data and
annotations for training, both of which can be difficult and costly to obtain.
In this work, we propose a unified framework for generalized low-shot (one- and
few-shot) medical image segmentation based on distance metric learning (DML).
Unlike most existing methods which only deal with the lack of annotations while
assuming abundance of data, our framework works with extreme scarcity of both,
which is ideal for rare diseases. Via DML, the framework learns a multimodal
mixture representation for each category, and performs dense predictions based
on cosine distances between the pixels' deep embeddings and the category
representations. The multimodal representations effectively utilize the
inter-subject similarities and intraclass variations to overcome overfitting
due to extremely limited data. In addition, we propose adaptive mixing
coefficients for the multimodal mixture distributions to adaptively emphasize
the modes better suited to the current input. The representations are
implicitly embedded as weights of the fc layer, such that the cosine distances
can be computed efficiently via forward propagation. In our experiments on
brain MRI and abdominal CT datasets, the proposed framework achieves superior
performances for low-shot segmentation towards standard DNN-based (3D U-Net)
and classical registration-based (ANTs) methods, e.g., achieving mean Dice
coefficients of 81%/69% for brain tissue/abdominal multiorgan segmentation
using a single training sample, as compared to 52%/31% and 72%/35% by the U-Net
and ANTs, respectively.",,,arXiv,,,2021-10-18,2021,,,,,,All OA; Green,Preprint,"Cui, Hengji; Wei, Dong; Ma, Kai; Gu, Shi; Zheng, Yefeng","Cui, Hengji (); Wei, Dong (); Ma, Kai (); Gu, Shi (); Zheng, Yefeng ()",,"Cui, Hengji (); Wei, Dong (); Ma, Kai (); Gu, Shi (); Zheng, Yefeng ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142015215,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
830,pub.1152860345,10.48550/arxiv.2211.09233,,,Prompt Tuning for Parameter-efficient Medical Image Segmentation,"Neural networks pre-trained on a self-supervision scheme have become the
standard when operating in data rich environments with scarce annotations. As
such, fine-tuning a model to a downstream task in a parameter-efficient but
effective way, e.g. for a new set of classes in the case of semantic
segmentation, is of increasing importance. In this work, we propose and
investigate several contributions to achieve a parameter-efficient but
effective adaptation for semantic segmentation on two medical imaging datasets.
Relying on the recently popularized prompt tuning approach, we provide a
prompt-able UNet (PUNet) architecture, that is frozen after pre-training, but
adaptable throughout the network by class-dependent learnable prompt tokens. We
pre-train this architecture with a dedicated dense self-supervision scheme
based on assignments to online generated prototypes (contrastive prototype
assignment, CPA) of a student teacher combination alongside a concurrent
segmentation loss on a subset of classes. We demonstrate that the resulting
neural network model is able to attenuate the gap between fully fine-tuned and
parameter-efficiently adapted models on CT imaging datasets. As such, the
difference between fully fine-tuned and prompt-tuned variants amounts to only
3.83 pp for the TCIA/BTCV dataset and 2.67 pp for the CT-ORG dataset in the
mean Dice Similarity Coefficient (DSC, in %) while only prompt tokens,
corresponding to 0.85% of the pre-trained backbone model with 6.8M frozen
parameters, are adjusted. The code for this work is available on
https://github.com/marcdcfischer/PUNet .",,,arXiv,,,2022-11-16,2022,,,,,,All OA; Green,Preprint,"Fischer, Marc; Bartler, Alexander; Yang, Bin","Fischer, Marc (); Bartler, Alexander (); Yang, Bin ()",,"Fischer, Marc (); Bartler, Alexander (); Yang, Bin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152860345,46 Information and Computing Sciences; 4611 Machine Learning,
799,pub.1149847036,10.48550/arxiv.2207.14191,,,Learning with Limited Annotations: A Survey on Deep Semi-Supervised  Learning for Medical Image Segmentation,"Medical image segmentation is a fundamental and critical step in many
image-guided clinical approaches. Recent success of deep learning-based
segmentation methods usually relies on a large amount of labeled data, which is
particularly difficult and costly to obtain especially in the medical imaging
domain where only experts can provide reliable and accurate annotations.
Semi-supervised learning has emerged as an appealing strategy and been widely
applied to medical image segmentation tasks to train deep models with limited
annotations. In this paper, we present a comprehensive review of recently
proposed semi-supervised learning methods for medical image segmentation and
summarized both the technical novelties and empirical results. Furthermore, we
analyze and discuss the limitations and several unsolved problems of existing
approaches. We hope this review could inspire the research community to explore
solutions for this challenge and further promote the developments in medical
image segmentation field.",,,arXiv,,,2022-07-28,2022,,,,,,All OA; Green,Preprint,"Jiao, Rushi; Zhang, Yichi; Ding, Le; Cai, Rong; Zhang, Jicong","Jiao, Rushi (); Zhang, Yichi (); Ding, Le (); Cai, Rong (); Zhang, Jicong ()",,"Jiao, Rushi (); Zhang, Yichi (); Ding, Le (); Cai, Rong (); Zhang, Jicong ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149847036,46 Information and Computing Sciences; 4611 Machine Learning,
798,pub.1124236165,10.48550/arxiv.2001.06535,,,CHAOS Challenge -- Combined (CT-MR) Healthy Abdominal Organ Segmentation,"Segmentation of abdominal organs has been a comprehensive, yet unresolved,
research field for many years. In the last decade, intensive developments in
deep learning (DL) have introduced new state-of-the-art segmentation systems.
In order to expand the knowledge on these topics, the CHAOS - Combined (CT-MR)
Healthy Abdominal Organ Segmentation challenge has been organized in
conjunction with IEEE International Symposium on Biomedical Imaging (ISBI),
2019, in Venice, Italy. CHAOS provides both abdominal CT and MR data from
healthy subjects for single and multiple abdominal organ segmentation. Five
different but complementary tasks have been designed to analyze the
capabilities of current approaches from multiple perspectives. The results are
investigated thoroughly, compared with manual annotations and interactive
methods. The analysis shows that the performance of DL models for single
modality (CT / MR) can show reliable volumetric analysis performance (DICE:
0.98 $\pm$ 0.00 / 0.95 $\pm$ 0.01) but the best MSSD performance remain limited
(21.89 $\pm$ 13.94 / 20.85 $\pm$ 10.63 mm). The performances of participating
models decrease significantly for cross-modality tasks for the liver (DICE:
0.88 $\pm$ 0.15 MSSD: 36.33 $\pm$ 21.97 mm) and all organs (DICE: 0.85 $\pm$
0.21 MSSD: 33.17 $\pm$ 38.93 mm). Despite contrary examples on different
applications, multi-tasking DL models designed to segment all organs seem to
perform worse compared to organ-specific ones (performance drop around 5\%).
Besides, such directions of further research for cross-modality segmentation
would significantly support real-world clinical applications. Moreover, having
more than 1500 participants, another important contribution of the paper is the
analysis on shortcomings of challenge organizations such as the effects of
multiple submissions and peeking phenomena.",,,arXiv,,,2020-01-17,2020,,,,,,All OA; Green,Preprint,"Kavur, A. Emre; Gezer, N. Sinem; Barış, Mustafa; Aslan, Sinem; Conze, Pierre-Henri; Groza, Vladimir; Pham, Duc Duy; Chatterjee, Soumick; Ernst, Philipp; Özkan, Savaş; Baydar, Bora; Lachinov, Dmitry; Han, Shuo; Pauli, Josef; Isensee, Fabian; Perkonigg, Matthias; Sathish, Rachana; Rajan, Ronnie; Sheet, Debdoot; Dovletov, Gurbandurdy; Speck, Oliver; Nürnberger, Andreas; Maier-Hein, Klaus H.; Akar, Gözde Bozdağı; Ünal, Gözde; Dicle, Oğuz; Selver, M. Alper","Kavur, A. Emre (); Gezer, N. Sinem (); Barış, Mustafa (); Aslan, Sinem (); Conze, Pierre-Henri (); Groza, Vladimir (); Pham, Duc Duy (); Chatterjee, Soumick (); Ernst, Philipp (); Özkan, Savaş (); Baydar, Bora (); Lachinov, Dmitry (); Han, Shuo (); Pauli, Josef (); Isensee, Fabian (); Perkonigg, Matthias (); Sathish, Rachana (); Rajan, Ronnie (); Sheet, Debdoot (); Dovletov, Gurbandurdy (); Speck, Oliver (); Nürnberger, Andreas (); Maier-Hein, Klaus H. (); Akar, Gözde Bozdağı (); Ünal, Gözde (); Dicle, Oğuz (); Selver, M. Alper ()",,"Kavur, A. Emre (); Gezer, N. Sinem (); Barış, Mustafa (); Aslan, Sinem (); Conze, Pierre-Henri (); Groza, Vladimir (); Pham, Duc Duy (); Chatterjee, Soumick (); Ernst, Philipp (); Özkan, Savaş (); Baydar, Bora (); Lachinov, Dmitry (); Han, Shuo (); Pauli, Josef (); Isensee, Fabian (); Perkonigg, Matthias (); Sathish, Rachana (); Rajan, Ronnie (); Sheet, Debdoot (); Dovletov, Gurbandurdy (); Speck, Oliver (); Nürnberger, Andreas (); Maier-Hein, Klaus H. (); Akar, Gözde Bozdağı (); Ünal, Gözde (); Dicle, Oğuz (); Selver, M. Alper ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1124236165,51 Physical Sciences; 5105 Medical and Biological Physics,
738,pub.1147377575,10.1016/j.displa.2022.102223,,,Evolution of multiorgan segmentation techniques from traditional to deep learning in abdominal CT images – A systematic review,"Abdominal organ segmentation is the crucial research direction in computer assisted diagnostic systems. Segmentation of multiple organs in medical images is known as Multiorgan segmentation. It is a widespread subject of research in the realm of medical image analysis. The purpose of this study is to provide the comprehensive systematic literature review on segmentation of multiple organs in abdomen CT scans. This paper focuses on the progression of state-of-art methods from traditional techniques to deep learning models. Firstly, the methods are classified into three categories: atlas based, statistical shape models and deep learning models. Secondly, research is carried out to determine which organs require more attention. The liver, kidney, and spleen are the most often selected organs, whereas the esophagus, duodenum, and portal vein are rarely picked. When medical images are taken into account for research, datasets play a vital role. This paper sheds light on publicly available datasets along with their size, no of organ classes and, related challenges which make the current study more effective and useful for the researchers in the same field. Further, evaluation metrics along with their scope and characteristics are presented. We conclude with a discussion of challenges and future directions which will open pathways for researchers. Based on the surveyed research papers, Dense-Net came out as an optimal choice. Recently, the standard practice in multi organ segmentation is two step deep learning models in sequential manner, which can take leverage of two models.",,,Displays,,,2022-07,2022,,2022-07,73,,102223,Closed,Article,"Kaur, Harinder; Kaur, Navjot; Neeru, Nirvair","Kaur, Harinder (Department of Computer Engineering, Punjabi University Patiala, 147002 Punjab, India); Kaur, Navjot (Department of Computer Engineering, Punjabi University Patiala, 147002 Punjab, India); Neeru, Nirvair (Department of Computer Engineering, Punjabi University Patiala, 147002 Punjab, India)","Kaur, Harinder (Punjabi University)","Kaur, Harinder (Punjabi University); Kaur, Navjot (Punjabi University); Neeru, Nirvair (Punjabi University)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1147377575,"46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4607 Graphics, Augmented Reality and Games",
734,pub.1150456349,10.1016/j.imed.2022.07.002,,,Transformers in Medical Image Analysis: A Review,"Transformers have dominated the field of natural language processing and have recently made an impact in the area of computer vision. In the field of medical image analysis, transformers have also been successfully used in to full-stack clinical applications, including image synthesis/reconstruction, registration, segmentation, detection, and diagnosis. This paper aims to promote awareness of the applications of transformers in medical image analysis. Specifically, we first provide an overview of the core concepts of the attention mechanism built into transformers and other basic components. Second, we review various transformer architectures tailored for medical image applications and discuss their limitations. Within this review, we investigate key challenges including the use of transformers in different learning paradigms, improving model efficiency, and coupling with other techniques. We hope this review will provide a comprehensive picture of transformers to readers with an interest in medical image analysis.",,"This work was supported by the National Natural Science Foundation of China (Grant No. 62106101), the Natural Science Foundation of Jiangsu Province (Grant No. BK20210180), and Jiangsu Provincial Key R D Program (Grant Nos. BE2020620 and BE2020723).",Intelligent Medicine,,,2022-08,2022,,2022-08,,,,All OA; Gold,Article,"He, Kelei; Gan, Chen; Li, Zhuoyuan; Rekik, Islem; Yin, Zihao; Ji, Wen; Gao, Yang; Wang, Qian; Zhang, Junfeng; Shen, Dinggang","He, Kelei (Medical School of Nanjing University, Nanjing, P.R.China; National Institute of Healthcare Data Science at Nanjing University, Nanjing, P.R.China); Gan, Chen (National Institute of Healthcare Data Science at Nanjing University, Nanjing, P.R.China); Li, Zhuoyuan (Medical School of Nanjing University, Nanjing, P.R.China; National Institute of Healthcare Data Science at Nanjing University, Nanjing, P.R.China); Rekik, Islem (BASIRA Laboratory, Faculty of Computer and Informatics Engineering, Istanbul Technical University, Istanbul, Turkey; School of Science and Engineering, Computing, University of Dundee, UK); Yin, Zihao (National Institute of Healthcare Data Science at Nanjing University, Nanjing, P.R.China); Ji, Wen (National Institute of Healthcare Data Science at Nanjing University, Nanjing, P.R.China); Gao, Yang (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, P.R.China; National Institute of Healthcare Data Science at Nanjing University, Nanjing, P.R.China); Wang, Qian (School of Biomedical Engineering, ShanghaiTech University, Shanghai, P.R.China); Zhang, Junfeng (Medical School of Nanjing University, Nanjing, P.R.China; National Institute of Healthcare Data Science at Nanjing University, Nanjing, P.R.China); Shen, Dinggang (School of Biomedical Engineering, ShanghaiTech University, Shanghai, P.R.China; Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, P.R.China)","Wang, Qian (ShanghaiTech University); Zhang, Junfeng (Nanjing University; Nanjing University); Shen, Dinggang (ShanghaiTech University; )","He, Kelei (Nanjing University; Nanjing University); Gan, Chen (Nanjing University); Li, Zhuoyuan (Nanjing University; Nanjing University); Rekik, Islem (Istanbul Technical University; University of Dundee); Yin, Zihao (Nanjing University); Ji, Wen (Nanjing University); Gao, Yang (Nanjing University; Nanjing University); Wang, Qian (ShanghaiTech University); Zhang, Junfeng (Nanjing University; Nanjing University); Shen, Dinggang (ShanghaiTech University)",10,10,,,https://doi.org/10.1016/j.imed.2022.07.002,https://app.dimensions.ai/details/publication/pub.1150456349,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
734,pub.1136340442,10.48550/arxiv.2103.06384,,,Automated liver tissues delineation techniques: A systematic survey on  machine learning current trends and future orientations,"Machine learning and computer vision techniques have grown rapidly in recent
years due to their automation, suitability, and ability to generate astounding
results. Hence, in this paper, we survey the key studies that are published
between 2014 and 2022, showcasing the different machine learning algorithms
researchers have used to segment the liver, hepatic tumors, and
hepatic-vasculature structures. We divide the surveyed studies based on the
tissue of interest (hepatic-parenchyma, hepatic-tumors, or hepatic-vessels),
highlighting the studies that tackle more than one task simultaneously.
Additionally, the machine learning algorithms are classified as either
supervised or unsupervised, and they are further partitioned if the amount of
work that falls under a certain scheme is significant. Moreover, different
datasets and challenges found in literature and websites containing masks of
the aforementioned tissues are thoroughly discussed, highlighting the
organizers' original contributions and those of other researchers. Also, the
metrics used excessively in literature are mentioned in our review, stressing
their relevance to the task at hand. Finally, critical challenges and future
directions are emphasized for innovative researchers to tackle, exposing gaps
that need addressing, such as the scarcity of many studies on the vessels'
segmentation challenge and why their absence needs to be dealt with sooner than
later.",,,arXiv,,,2021-03-10,2021,,,,,,All OA; Green,Preprint,"Al-Kababji, Ayman; Bensaali, Faycal; Dakua, Sarada Prasad; Himeur, Yassine","Al-Kababji, Ayman (); Bensaali, Faycal (); Dakua, Sarada Prasad (); Himeur, Yassine ()",,"Al-Kababji, Ayman (); Bensaali, Faycal (); Dakua, Sarada Prasad (); Himeur, Yassine ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136340442,46 Information and Computing Sciences; 4611 Machine Learning,
687,pub.1145843869,10.48550/arxiv.2202.12165,,,Transformers in Medical Image Analysis: A Review,"Transformers have dominated the field of natural language processing, and
recently impacted the computer vision area. In the field of medical image
analysis, Transformers have also been successfully applied to full-stack
clinical applications, including image synthesis/reconstruction, registration,
segmentation, detection, and diagnosis. Our paper aims to promote awareness and
application of Transformers in the field of medical image analysis.
Specifically, we first overview the core concepts of the attention mechanism
built into Transformers and other basic components. Second, we review various
Transformer architectures tailored for medical image applications and discuss
their limitations. Within this review, we investigate key challenges revolving
around the use of Transformers in different learning paradigms, improving the
model efficiency, and their coupling with other techniques. We hope this review
can give a comprehensive picture of Transformers to the readers in the field of
medical image analysis.",,,arXiv,,,2022-02-24,2022,,,,,,All OA; Green,Preprint,"He, Kelei; Gan, Chen; Li, Zhuoyuan; Rekik, Islem; Yin, Zihao; Ji, Wen; Gao, Yang; Wang, Qian; Zhang, Junfeng; Shen, Dinggang","He, Kelei (); Gan, Chen (); Li, Zhuoyuan (); Rekik, Islem (); Yin, Zihao (); Ji, Wen (); Gao, Yang (); Wang, Qian (); Zhang, Junfeng (); Shen, Dinggang ()",,"He, Kelei (); Gan, Chen (); Li, Zhuoyuan (); Rekik, Islem (); Yin, Zihao (); Ji, Wen (); Gao, Yang (); Wang, Qian (); Zhang, Junfeng (); Shen, Dinggang ()",2,2,,,,https://app.dimensions.ai/details/publication/pub.1145843869,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
601,pub.1148408535,10.48550/arxiv.2206.01136,,,"Transforming medical imaging with Transformers? A comparative review of  key properties, current progresses, and future perspectives","Transformer, the latest technological advance of deep learning, has gained
prevalence in natural language processing or computer vision. Since medical
imaging bear some resemblance to computer vision, it is natural to inquire
about the status quo of Transformers in medical imaging and ask the question:
can the Transformer models transform medical imaging? In this paper, we attempt
to make a response to the inquiry. After a brief introduction of the
fundamentals of Transformers, especially in comparison with convolutional
neural networks (CNNs), and highlighting key defining properties that
characterize the Transformers, we offer a comprehensive review of the
state-of-the-art Transformer-based approaches for medical imaging and exhibit
current research progresses made in the areas of medical image segmentation,
recognition, detection, registration, reconstruction, enhancement, etc. In
particular, what distinguishes our review lies in its organization based on the
Transformer's key defining properties, which are mostly derived from comparing
the Transformer and CNN, and its type of architecture, which specifies the
manner in which the Transformer and CNN are combined, all helping the readers
to best understand the rationale behind the reviewed approaches. We conclude
with discussions of future perspectives.",,,arXiv,,,2022-06-02,2022,,,,,,All OA; Green,Preprint,"Li, Jun; Chen, Junyu; Tang, Yucheng; Wang, Ce; Landman, Bennett A.; Zhou, S. Kevin","Li, Jun (); Chen, Junyu (); Tang, Yucheng (); Wang, Ce (); Landman, Bennett A. (); Zhou, S. Kevin ()",,"Li, Jun (); Chen, Junyu (); Tang, Yucheng (); Wang, Ce (); Landman, Bennett A. (); Zhou, S. Kevin ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1148408535,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
600,pub.1119426691,10.48550/arxiv.1904.08128,,,Automated Design of Deep Learning Methods for Biomedical Image  Segmentation,"Biomedical imaging is a driver of scientific discovery and core component of
medical care, currently stimulated by the field of deep learning. While
semantic segmentation algorithms enable 3D image analysis and quantification in
many applications, the design of respective specialised solutions is
non-trivial and highly dependent on dataset properties and hardware conditions.
We propose nnU-Net, a deep learning framework that condenses the current domain
knowledge and autonomously takes the key decisions required to transfer a basic
architecture to different datasets and segmentation tasks. Without manual
tuning, nnU-Net surpasses most specialised deep learning pipelines in 19 public
international competitions and sets a new state of the art in the majority of
the 49 tasks. The results demonstrate a vast hidden potential in the systematic
adaptation of deep learning methods to different datasets. We make nnU-Net
publicly available as an open-source tool that can effectively be used
out-of-the-box, rendering state of the art segmentation accessible to
non-experts and catalyzing scientific progress as a framework for automated
method design.",,,arXiv,,,2019-04-17,2019,,,,,,All OA; Green,Preprint,"Isensee, Fabian; Jäger, Paul F.; Kohl, Simon A. A.; Petersen, Jens; Maier-Hein, Klaus H.","Isensee, Fabian (); Jäger, Paul F. (); Kohl, Simon A. A. (); Petersen, Jens (); Maier-Hein, Klaus H. ()",,"Isensee, Fabian (); Jäger, Paul F. (); Kohl, Simon A. A. (); Petersen, Jens (); Maier-Hein, Klaus H. ()",1,1,,0.38,,https://app.dimensions.ai/details/publication/pub.1119426691,46 Information and Computing Sciences; 4611 Machine Learning,
534,pub.1152919611,10.1016/j.engappai.2022.105532,,,Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations,"Machine learning and computer vision techniques have grown rapidly in recent years due to their automation, suitability, and ability to generate astounding results. Hence, in this paper, we survey the key studies that are published between 2014 and 2022, showcasing the different machine learning algorithms researchers have used to segment the liver, hepatic tumors, and hepatic-vasculature structures. We divide the surveyed studies based on the tissue of interest (hepatic-parenchyma, hepatic-tumors, or hepatic-vessels), highlighting the studies that tackle more than one task simultaneously. Additionally, the machine learning algorithms are classified as either supervised or unsupervised, and they are further partitioned if the amount of work that falls under a certain scheme is significant. Moreover, different datasets and challenges found in literature and websites containing masks of the aforementioned tissues are thoroughly discussed, highlighting the organizers’ original contributions and those of other researchers. Also, the metrics used excessively in the literature are mentioned in our review, stressing their relevance to the task at hand. Finally, critical challenges and future directions are emphasized for innovative researchers to tackle, exposing gaps that need addressing, such as the scarcity of many studies on the vessels’ segmentation challenge and why their absence needs to be dealt with sooner than later.",This publication was made possible by an Award [GSRA6-2-0521-19034] from Qatar National Research Fund (a member of Qatar Foundation). The contents herein are solely the responsibility of the authors. Open Access funding provided by the Qatar National Library,,Engineering Applications of Artificial Intelligence,,,2023-01,2023,,2023-01,117,,105532,All OA; Hybrid,Article,"Al-Kababji, Ayman; Bensaali, Faycal; Dakua, Sarada Prasad; Himeur, Yassine","Al-Kababji, Ayman (Department of Electrical Engineering, Qatar University, Doha, Qatar); Bensaali, Faycal (Department of Electrical Engineering, Qatar University, Doha, Qatar); Dakua, Sarada Prasad (Department of Surgery, Hamad Medical Corporation, Doha, Qatar); Himeur, Yassine (Department of Electrical Engineering, Qatar University, Doha, Qatar)","Al-Kababji, Ayman (Qatar University)","Al-Kababji, Ayman (Qatar University); Bensaali, Faycal (Qatar University); Dakua, Sarada Prasad (Hamad Medical Corporation); Himeur, Yassine (Qatar University)",1,1,,,https://doi.org/10.1016/j.engappai.2022.105532,https://app.dimensions.ai/details/publication/pub.1152919611,46 Information and Computing Sciences; 4611 Machine Learning,
479,pub.1153198901,10.48550/arxiv.2211.14830,,,Medical Image Segmentation Review: The success of U-Net,"Automatic medical image segmentation is a crucial topic in the medical domain
and successively a critical counterpart in the computer-aided diagnosis
paradigm. U-Net is the most widespread image segmentation architecture due to
its flexibility, optimized modular design, and success in all medical image
modalities. Over the years, the U-Net model achieved tremendous attention from
academic and industrial researchers. Several extensions of this network have
been proposed to address the scale and complexity created by medical tasks.
Addressing the deficiency of the naive U-Net model is the foremost step for
vendors to utilize the proper U-Net variant model for their business. Having a
compendium of different variants in one place makes it easier for builders to
identify the relevant research. Also, for ML researchers it will help them
understand the challenges of the biological tasks that challenge the model. To
address this, we discuss the practical aspects of the U-Net model and suggest a
taxonomy to categorize each network variant. Moreover, to measure the
performance of these strategies in a clinical application, we propose fair
evaluations of some unique and famous designs on well-known datasets. We
provide a comprehensive implementation library with trained models for future
research. In addition, for ease of future studies, we created an online list of
U-Net papers with their possible official implementation. All information is
gathered in https://github.com/NITR098/Awesome-U-Net repository.",,,arXiv,,,2022-11-27,2022,,,,,,All OA; Green,Preprint,"Azad, Reza; Aghdam, Ehsan Khodapanah; Rauland, Amelie; Jia, Yiwei; Avval, Atlas Haddadi; Bozorgpour, Afshin; Karimijafarbigloo, Sanaz; Cohen, Joseph Paul; Adeli, Ehsan; Merhof, Dorit","Azad, Reza (); Aghdam, Ehsan Khodapanah (); Rauland, Amelie (); Jia, Yiwei (); Avval, Atlas Haddadi (); Bozorgpour, Afshin (); Karimijafarbigloo, Sanaz (); Cohen, Joseph Paul (); Adeli, Ehsan (); Merhof, Dorit ()",,"Azad, Reza (); Aghdam, Ehsan Khodapanah (); Rauland, Amelie (); Jia, Yiwei (); Avval, Atlas Haddadi (); Bozorgpour, Afshin (); Karimijafarbigloo, Sanaz (); Cohen, Joseph Paul (); Adeli, Ehsan (); Merhof, Dorit ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153198901,46 Information and Computing Sciences; 4609 Information Systems,
436,pub.1142015108,10.48550/arxiv.2110.09148,,,Body Part Regression for CT Images,"One of the greatest challenges in the medical imaging domain is to
successfully transfer deep learning models into clinical practice. Since models
are often trained on a specific body region, a robust transfer into the clinic
necessitates the selection of images with body regions that fit the algorithm
to avoid false-positive predictions in unknown regions. Due to the insufficient
and inaccurate nature of manually-defined imaging meta-data, automated body
part recognition is a key ingredient towards the broad and reliable adoption of
medical deep learning models. While some approaches to this task have been
presented in the past, building and evaluating robust algorithms for
fine-grained body part recognition remains challenging. So far, no easy-to-use
method exists to determine the scanned body range of medical Computed
Tomography (CT) volumes. In this thesis, a self-supervised body part regression
model for CT volumes is developed and trained on a heterogeneous collection of
CT studies. Furthermore, it is demonstrated how the algorithm can contribute to
the robust and reliable transfer of medical models into the clinic. Finally,
easy application of the developed method is ensured by integrating it into the
medical platform toolkit Kaapana and providing it as a python package at
https://github.com/MIC-DKFZ/BodyPartRegression .",,,arXiv,,,2021-10-18,2021,,,,,,All OA; Green,Preprint,"Schuhegger, Sarah","Schuhegger, Sarah ()",,"Schuhegger, Sarah ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142015108,32 Biomedical and Clinical Sciences; 51 Physical Sciences; 5105 Medical and Biological Physics,
435,pub.1144976354,10.48550/arxiv.2201.09873,,,Transformers in Medical Imaging: A Survey,"Following unprecedented success on the natural language tasks, Transformers
have been successfully applied to several computer vision problems, achieving
state-of-the-art results and prompting researchers to reconsider the supremacy
of convolutional neural networks (CNNs) as {de facto} operators. Capitalizing
on these advances in computer vision, the medical imaging field has also
witnessed growing interest for Transformers that can capture global context
compared to CNNs with local receptive fields. Inspired from this transition, in
this survey, we attempt to provide a comprehensive review of the applications
of Transformers in medical imaging covering various aspects, ranging from
recently proposed architectural designs to unsolved issues. Specifically, we
survey the use of Transformers in medical image segmentation, detection,
classification, reconstruction, synthesis, registration, clinical report
generation, and other tasks. In particular, for each of these applications, we
develop taxonomy, identify application-specific challenges as well as provide
insights to solve them, and highlight recent trends. Further, we provide a
critical discussion of the field's current state as a whole, including the
identification of key challenges, open problems, and outlining promising future
directions. We hope this survey will ignite further interest in the community
and provide researchers with an up-to-date reference regarding applications of
Transformer models in medical imaging. Finally, to cope with the rapid
development in this field, we intend to regularly update the relevant latest
papers and their open-source implementations at
\url{https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging}.",,,arXiv,,,2022-01-24,2022,,,,,,All OA; Green,Preprint,"Shamshad, Fahad; Khan, Salman; Zamir, Syed Waqas; Khan, Muhammad Haris; Hayat, Munawar; Khan, Fahad Shahbaz; Fu, Huazhu","Shamshad, Fahad (); Khan, Salman (); Zamir, Syed Waqas (); Khan, Muhammad Haris (); Hayat, Munawar (); Khan, Fahad Shahbaz (); Fu, Huazhu ()",,"Shamshad, Fahad (); Khan, Salman (); Zamir, Syed Waqas (); Khan, Muhammad Haris (); Hayat, Munawar (); Khan, Fahad Shahbaz (); Fu, Huazhu ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1144976354,46 Information and Computing Sciences; 4611 Machine Learning,
301,pub.1151124908,10.1007/978-3-031-16852-9,,,"Domain Adaptation and Representation Transfer, 4th MICCAI Workshop, DART 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings","This book constitutes the refereed proceedings of the 4th MICCAI Workshop on Domain Adaptation and Representation Transfer, DART 2022, held in conjunction with MICCAI 2022, in September 2022. DART 2022 accepted 13 papers from the 25 submissions received. The workshop aims at creating a discussion forum to compare, evaluate, and discuss methodological advancements and ideas that can improve the applicability of machine learning (ML)/deep learning (DL) approaches to clinical setting by making them robust and consistent across different domains.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13542,,,All OA; Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16852-9%2F1,https://app.dimensions.ai/details/publication/pub.1151124908,46 Information and Computing Sciences; 4611 Machine Learning,
267,pub.1142567426,10.1007/978-3-030-90874-4,,,"Clinical Image-Based Procedures, Distributed and Collaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning, 10th Workshop, CLIP 2021, Second Workshop, DCL 2021, First Workshop, LL-COVID19 2021, and First Workshop and Tutorial, PPML 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27 and October 1, 2021, Proceedings","This book constitutes the refereed proceedings of the 10th International Workshop on Clinical Image-Based Procedures, CLIP 2021, Second MICCAI Workshop on Distributed and Collaborative Learning, DCL 2021, First MICCAI Workshop, LL-COVID19, First Secure and Privacy-Preserving Machine Learning for Medical Imaging Workshop and Tutorial, PPML 2021, held in conjunction with MICCAI 2021, in October 2021. The workshops were planned to take place in Strasbourg, France, but were held virtually due to the COVID-19 pandemic. CLIP 2021 accepted 9 papers from the 13 submissions received. It focuses on holistic patient models for personalized healthcare with the goal to bring basic research methods closer to the clinical practice. For DCL 2021, 4 papers from 7 submissions were accepted for publication. They deal with machine learning applied to problems where data cannot be stored in centralized databases and information privacy is a priority. LL-COVID19 2021 accepted 2 papers out of 3 submissions dealing with the use of AI models in clinical practice. And for PPML 2021, 2 papers were accepted from a total of 6 submissions, exploring the use of privacy techniques in the medical imaging community.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12969,,,All OA; Green,Edited Book,,,,,0,0,,0.0,https://link.springer.com/content/pdf/bfm%3A978-3-030-90874-4%2F1,https://app.dimensions.ai/details/publication/pub.1142567426,46 Information and Computing Sciences; 4604 Cybersecurity and Privacy,3 Good Health and Well Being
240,pub.1151694516,10.1007/978-3-031-18523-6,,,"Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health, Third MICCAI Workshop, DeCaF 2022, and Second MICCAI Workshop, FAIR 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18 and 22, 2022, Proceedings","This book constitutes the refereed proceedings of the Third MICCAI Workshop on Distributed, Collaborative, and Federated Learning, DeCaF 2022, and the Second MICCAI Workshop on Affordable AI and Healthcare, FAIR 2022, held in conjunction with MICCAI 2022, in Singapore in September 2022. FAIR 2022 was held as a hybrid event. DeCaF 2022 accepted 14 papers from the 18 submissions received. The workshop aims at creating a scientific discussion focusing on the comparison, evaluation, and discussion of methodological advancement and practical ideas about machine learning applied to problems where data cannot be stored in centralized databases or where information privacy is a priority. For FAIR 2022, 4 papers from 9 submissions were accepted for publication. The topics of the accepted submissions focus on deep ultrasound segmentation, portable OCT image quality enhancement, self-attention deep networks and knowledge distillation in low-regime setting.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13573,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1151694516,46 Information and Computing Sciences; 4606 Distributed Computing and Systems Software,3 Good Health and Well Being
198,pub.1135198814,10.1038/nde/15487091/2021/18/2,,,Nature Methods,,,,Nature Methods,,,2021,2021,,,18,2,,Closed,Article,,,,,1,1,,,,https://app.dimensions.ai/details/publication/pub.1135198814,31 Biological Sciences,
186,pub.1141327107,10.1007/978-3-030-87722-4,,,"Domain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health, Third MICCAI Workshop, DART 2021, and First MICCAI Workshop, FAIR 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27 and October 1, 2021, Proceedings","This book constitutes the refereed proceedings of the Third MICCAI Workshop on Domain Adaptation and Representation Transfer, DART 2021, and the First MICCAI Workshop on Affordable Healthcare and AI for Resource Diverse Global Health, FAIR 2021, held in conjunction with MICCAI 2021, in September/October 2021. The workshops were planned to take place in Strasbourg, France, but were held virtually due to the COVID-19 pandemic. DART 2021 accepted 13 papers from the 21 submissions received. The workshop aims at creating a discussion forum to compare, evaluate, and discuss methodological advancements and ideas that can improve the applicability of machine learning (ML)/deep learning (DL) approaches to clinical setting by making them robust and consistent across different domains. For FAIR 2021, 10 papers from 17 submissions were accepted for publication. They focus on Image-to-Image Translation particularly for low-dose or low-resolution settings; Model Compactness and Compression; Domain Adaptation and Transfer Learning; Active, Continual and Meta-Learning.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12968,,,All OA; Green,Edited Book,,,,,2,2,,1.64,https://link.springer.com/content/pdf/bfm%3A978-3-030-87722-4%2F1,https://app.dimensions.ai/details/publication/pub.1141327107,46 Information and Computing Sciences; 4611 Machine Learning,3 Good Health and Well Being
185,pub.1154711669,10.1007/978-3-031-23911-3,,,"Fast and Low-Resource Semi-supervised Abdominal Organ Segmentation, MICCAI 2022 Challenge, FLARE 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings","This book constitutes the proceedings of the MICCAI 2022 Challenge, FLARE 2022, held in Conjunction with MICCAI 2022, in Singapore, on September 22, 2022. The 28 full papers presented in this book were carefully reviewed and selected from 48 submissions. The papers present research and results for abdominal organ segmentation which has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13816,,,Closed,Edited Book,,,,,1,1,,,,https://app.dimensions.ai/details/publication/pub.1154711669,46 Information and Computing Sciences,
127,pub.1141301941,10.1007/978-3-030-87193-2,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2021, 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I","The eight-volume set LNCS 12901, 12902, 12903, 12904, 12905, 12906, 12907, and 12908 constitutes the refereed proceedings of the 24th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2021, held in Strasbourg, France, in September/October 2021.* The 531 revised full papers presented were carefully reviewed and selected from 1630 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: image segmentation Part II: machine learning - self-supervised learning; machine learning - semi-supervised learning; and machine learning - weakly supervised learning Part III: machine learning - advances in machine learning theory; machine learning - attention models; machine learning - domain adaptation; machine learning - federated learning; machine learning - interpretability / explainability; and machine learning - uncertainty Part IV: image registration; image-guided interventions and surgery; surgical data science; surgical planning and simulation; surgical skill and work flow analysis; and surgical visualization and mixed, augmented and virtual reality Part V: computer aided diagnosis; integration of imaging with non-imaging biomarkers; and outcome/disease prediction Part VI: image reconstruction; clinical applications - cardiac; and clinical applications - vascular Part VII: clinical applications - abdomen; clinical applications - breast; clinical applications - dermatology; clinical applications - fetal imaging; clinical applications - lung; clinical applications - neuroimaging - brain development; clinical applications - neuroimaging - DWI and tractography; clinical applications - neuroimaging - functional brain networks; clinical applications - neuroimaging – others; and clinical applications - oncology Part VIII: clinical applications - ophthalmology; computational (integrative) pathology; modalities - microscopy; modalities - histopathology; and modalities - ultrasound *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12901,,,All OA; Green,Edited Book,,,,,4,4,,3.27,https://link.springer.com/content/pdf/bfm%3A978-3-030-87193-2%2F1,https://app.dimensions.ai/details/publication/pub.1141301941,46 Information and Computing Sciences; 4611 Machine Learning,
126,pub.1151032907,10.1007/978-3-031-16440-8,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2022, 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part IV","The eight-volume set LNCS 13431, 13432, 13433, 13434, 13435, 13436, 13437, and 13438 constitutes the refereed proceedings of the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2022, which was held in Singapore in September 2022. The 574 revised full papers presented were carefully reviewed and selected from 1831 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: Brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; heart and lung imaging; dermatology; Part II: Computational (integrative) pathology; computational anatomy and physiology; ophthalmology; fetal imaging; Part III: Breast imaging; colonoscopy; computer aided diagnosis; Part IV: Microscopic image analysis; positron emission tomography; ultrasound imaging; video data analysis; image segmentation I; Part V: Image segmentation II; integration of imaging with non-imaging biomarkers; Part VI: Image registration; image reconstruction; Part VII: Image-Guided interventions and surgery; outcome and disease prediction; surgical data science; surgical planning and simulation; machine learning – domain adaptation and generalization; Part VIII: Machine learning – weakly-supervised learning; machine learning – model interpretation; machine learning – uncertainty; machine learning theory and methodologies.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13434,,,All OA; Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16440-8%2F1,https://app.dimensions.ai/details/publication/pub.1151032907,46 Information and Computing Sciences; 4611 Machine Learning,
115,pub.1131399582,10.1007/978-3-030-59719-1,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2020, 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part IV","The seven-volume set LNCS 12261, 12262, 12263, 12264, 12265, 12266, and 12267 constitutes the refereed proceedings of the 23rd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2020, held in Lima, Peru, in October 2020. The conference was held virtually due to the COVID-19 pandemic. The 542 revised full papers presented were carefully reviewed and selected from 1809 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: machine learning methodologies Part II: image reconstruction; prediction and diagnosis; cross-domain methods and reconstruction; domain adaptation; machine learning applications; generative adversarial networks Part III: CAI applications; image registration; instrumentation and surgical phase detection; navigation and visualization; ultrasound imaging; video image analysis Part IV: segmentation; shape models and landmark detection Part V: biological, optical, microscopic imaging; cell segmentation and stain normalization; histopathology image analysis; opthalmology Part VI: angiography and vessel analysis; breast imaging; colonoscopy; dermatology; fetal imaging; heart and lung imaging; musculoskeletal imaging Part VI: brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; positron emission tomography",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12264,,,All OA; Green,Edited Book,,,,,6,6,,3.09,https://link.springer.com/content/pdf/bfm:978-3-030-59719-1/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1131399582,46 Information and Computing Sciences; 4611 Machine Learning,
104,pub.1086114575,10.1007/978-3-319-59129-2,,,"Image Analysis, 20th Scandinavian Conference, SCIA 2017, Tromsø, Norway, June 12–14, 2017, Proceedings, Part II","The two-volume set LNCS 10269 and 10270 constitutes the refereed proceedings of the 20th Scandinavian Conference on Image Analysis, SCIA 2017, held in Tromsø, Norway, in June 2017. The 87 revised papers presented were carefully reviewed and selected from 133 submissions. The contributions are structured in topical sections on history of SCIA; motion analysis and 3D vision; pattern detection and recognition; machine learning; image processing and applications; feature extraction and segmentation; remote sensing; medical and biomedical image analysis; faces, gestures and multispectral analysis.",,,Lecture Notes in Computer Science,,,2017,2017,,2017,10270,,,All OA; Green,Edited Book,,,,,3,1,,0.86,https://oa.upm.es/51089/1/INVE_MEM_2017_278489.pdf,https://app.dimensions.ai/details/publication/pub.1086114575,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
88,pub.1120844472,10.1007/978-3-030-30642-7,,,"Image Analysis and Processing – ICIAP 2019, 20th International Conference, Trento, Italy, September 9–13, 2019, Proceedings, Part I","The two-volume set LNCS 11751 and 11752 constitutes the refereed proceedings of the 20th International Conference on Image Analysis and Processing, ICIAP 2019, held in Trento, Italy, in September 2019. The 117 papers presented were carefully reviewed and selected from 207 submissions. The papers cover both classic and the most recent trends in image processing, computer vision, and pattern recognition, addressing both theoretical and applicative aspects. They are organized in the following topical sections: Video Analysis and Understanding; Pattern Recognition and Machine Learning; Deep Learning; Multiview Geometry and 3D Computer Vision; Image Analysis, Detection and Recognition; Multimedia; Biomedical and Assistive Technology; Digital Forensics; Image processing for Cultural Heritage.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11751,,,All OA; Green,Edited Book,,,,,1,1,,0.37,https://link.springer.com/content/pdf/bfm%3A978-3-030-30642-7%2F1,https://app.dimensions.ai/details/publication/pub.1120844472,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
73,pub.1152224322,10.1007/978-3-031-18910-4,,,"Pattern Recognition and Computer Vision, 5th Chinese Conference, PRCV 2022, Shenzhen, China, November 4–7, 2022, Proceedings, Part II","The 4-volume set LNCS 13534, 13535, 13536 and 13537 constitutes the refereed proceedings of the 5th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2022, held in Shenzhen, China, in November 2022. The 233 full papers presented were carefully reviewed and selected from 564 submissions. The papers have been organized in the following topical sections: Theories and Feature Extraction; Machine learning, Multimedia and Multimodal; Optimization and Neural Network and Deep Learning; Biomedical Image Processing and Analysis; Pattern Classification and Clustering; 3D Computer Vision and Reconstruction, Robots and Autonomous Driving; Recognition, Remote Sensing; Vision Analysis and Understanding; Image Processing and Low-level Vision; Object Detection, Segmentation and Tracking.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13535,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1152224322,46 Information and Computing Sciences; 4611 Machine Learning,
72,pub.1141326795,10.1007/978-3-030-87199-4,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2021, 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III","The eight-volume set LNCS 12901, 12902, 12903, 12904, 12905, 12906, 12907, and 12908 constitutes the refereed proceedings of the 24th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2021, held in Strasbourg, France, in September/October 2021.* The 531 revised full papers presented were carefully reviewed and selected from 1630 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: image segmentation Part II: machine learning - self-supervised learning; machine learning - semi-supervised learning; and machine learning - weakly supervised learning Part III: machine learning - advances in machine learning theory; machine learning - attention models; machine learning - domain adaptation; machine learning - federated learning; machine learning - interpretability / explainability; and machine learning - uncertainty Part IV: image registration; image-guided interventions and surgery; surgical data science; surgical planning and simulation; surgical skill and work flow analysis; and surgical visualization and mixed, augmented and virtual reality Part V: computer aided diagnosis; integration of imaging with non-imaging biomarkers; and outcome/disease prediction Part VI: image reconstruction; clinical applications - cardiac; and clinical applications - vascular Part VII: clinical applications - abdomen; clinical applications - breast; clinical applications - dermatology; clinical applications - fetal imaging; clinical applications - lung; clinical applications - neuroimaging - brain development; clinical applications - neuroimaging - DWI and tractography; clinical applications - neuroimaging - functional brain networks; clinical applications - neuroimaging – others; and clinical applications - oncology Part VIII: clinical applications - ophthalmology; computational (integrative) pathology; modalities - microscopy; modalities - histopathology; and modalities - ultrasound *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12903,,,All OA; Green,Edited Book,,,,,5,5,,4.09,https://link.springer.com/content/pdf/bfm%3A978-3-030-87199-4%2F1,https://app.dimensions.ai/details/publication/pub.1141326795,46 Information and Computing Sciences; 4611 Machine Learning,
65,pub.1131394427,10.1007/978-3-030-59713-9,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2020, 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part II","The seven-volume set LNCS 12261, 12262, 12263, 12264, 12265, 12266, and 12267 constitutes the refereed proceedings of the 23rd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2020, held in Lima, Peru, in October 2020. The conference was held virtually due to the COVID-19 pandemic. The 542 revised full papers presented were carefully reviewed and selected from 1809 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: machine learning methodologies Part II: image reconstruction; prediction and diagnosis; cross-domain methods and reconstruction; domain adaptation; machine learning applications; generative adversarial networks Part III: CAI applications; image registration; instrumentation and surgical phase detection; navigation and visualization; ultrasound imaging; video image analysis Part IV: segmentation; shape models and landmark detection Part V: biological, optical, microscopic imaging; cell segmentation and stain normalization; histopathology image analysis; opthalmology Part VI: angiography and vessel analysis; breast imaging; colonoscopy; dermatology; fetal imaging; heart and lung imaging; musculoskeletal imaging Part VI: brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; positron emission tomography",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12262,,,All OA; Green,Edited Book,,,,,4,4,,2.06,https://link.springer.com/content/pdf/bfm:978-3-030-59713-9/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1131394427,46 Information and Computing Sciences; 4611 Machine Learning,
64,pub.1139024484,10.1007/978-3-030-78191-0,,,"Information Processing in Medical Imaging, 27th International Conference, IPMI 2021, Virtual Event, June 28–June 30, 2021, Proceedings","This book constitutes the proceedings of the 27th International Conference on Information Processing in Medical Imaging, IPMI 2021, which was held online during June 28-30, 2021. The conference was originally planned to take place in Bornholm, Denmark, but changed to a virtual format due to the COVID-19 pandemic. The 59 full papers presented in this volume were carefully reviewed and selected from 200 submissions. They were organized in topical sections as follows: registration; causal models and interpretability; generative modelling; shape; brain connectivity; representation learning; segmentation; sequential modelling; learning with few or low quality labels; uncertainty quantification and generative modelling; and deep learning.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12729,,,All OA; Green,Edited Book,,,,,4,4,,3.27,https://link.springer.com/content/pdf/bfm%3A978-3-030-78191-0%2F1,https://app.dimensions.ai/details/publication/pub.1139024484,46 Information and Computing Sciences; 4611 Machine Learning,
64,pub.1109706443,10.1007/978-3-030-00937-3,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part IV","The four-volume set LNCS 11070, 11071, 11072, and 11073 constitutes the refereed proceedings of the 21st International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2018, held in Granada, Spain, in September 2018. The 373 revised full papers presented were carefully reviewed and selected from 1068 submissions in a double-blind review process. The papers have been organized in the following topical sections: Part I: Image Quality and Artefacts; Image Reconstruction Methods; Machine Learning in Medical Imaging; Statistical Analysis for Medical Imaging; Image Registration Methods. Part II: Optical and Histology Applications: Optical Imaging Applications; Histology Applications; Microscopy Applications; Optical Coherence Tomography and Other Optical Imaging Applications. Cardiac, Chest and Abdominal Applications: Cardiac Imaging Applications: Colorectal, Kidney and Liver Imaging Applications; Lung Imaging Applications; Breast Imaging Applications; Other Abdominal Applications. Part III: Diffusion Tensor Imaging and Functional MRI: Diffusion Tensor Imaging; Diffusion Weighted Imaging; Functional MRI; Human Connectome. Neuroimaging and Brain Segmentation Methods: Neuroimaging; Brain Segmentation Methods. Part IV: Computer Assisted Intervention: Image Guided Interventions and Surgery; Surgical Planning, Simulation and Work Flow Analysis; Visualization and Augmented Reality. Image Segmentation Methods: General Image Segmentation Methods, Measures and Applications; Multi-Organ Segmentation; Abdominal Segmentation Methods; Cardiac Segmentation Methods; Chest, Lung and Spine Segmentation; Other Segmentation Applications.",,,Lecture Notes in Computer Science,,,2018,2018,,2018,11073,,,Closed,Edited Book,,,,,20,13,,,,https://app.dimensions.ai/details/publication/pub.1109706443,46 Information and Computing Sciences,
64,pub.1155741495,10.1007/978-3-031-26351-4,,,"Computer Vision – ACCV 2022, 16th Asian Conference on Computer Vision, Macao, China, December 4–8, 2022, Proceedings, Part VI","The 7-volume set of LNCS 13841-13847 constitutes the proceedings of the 16th Asian Conference on Computer Vision, ACCV 2022, held in Macao, China, December 2022. The total of 277 contributions included in the proceedings set was carefully reviewed and selected from 836 submissions during two rounds of reviewing and improvement. The papers focus on the following topics: Part I: 3D computer vision; optimization methods; Part II: applications of computer vision, vision for X; computational photography, sensing, and display; Part III: low-level vision, image processing; Part IV: face and gesture; pose and action; video analysis and event recognition; vision and language; biometrics; Part V: recognition: feature detection, indexing, matching, and shape representation; datasets and performance analysis; Part VI: biomedical image analysis; deep learning for computer vision; Part VII: generative models for computer vision; segmentation and grouping; motion and tracking; document image analysis; big data, large scale methods.",,,Lecture Notes in Computer Science,,,2023,2023,,2023,13846,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1155741495,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
59,pub.1121619956,10.1007/978-3-030-32248-9,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2019, 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III","The six-volume set LNCS 11764, 11765, 11766, 11767, 11768, and 11769 constitutes the refereed proceedings of the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019, held in Shenzhen, China, in October 2019. The 539 revised full papers presented were carefully reviewed and selected from 1730 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: optical imaging; endoscopy; microscopy. Part II: image segmentation; image registration; cardiovascular imaging; growth, development, atrophy and progression. Part III: neuroimage reconstruction and synthesis; neuroimage segmentation; diffusion weighted magnetic resonance imaging; functional neuroimaging (fMRI); miscellaneous neuroimaging. Part IV: shape; prediction; detection and localization; machine learning; computer-aided diagnosis; image reconstruction and synthesis. Part V: computer assisted interventions; MIC meets CAI. Part VI: computed tomography; X-ray imaging.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11766,,,All OA; Green,Edited Book,,,,,11,10,,,https://univoak.eu/islandora/object/islandora:138456/datastream/PDF/download/citation.pdf,https://app.dimensions.ai/details/publication/pub.1121619956,46 Information and Computing Sciences,
58,pub.1152128760,10.1007/978-3-031-19803-8,,,"Computer Vision – ECCV 2022, 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXI","The 39-volume set, comprising the LNCS books 13661 until 13699, constitutes the refereed proceedings of the 17th European Conference on Computer Vision, ECCV 2022, held in Tel Aviv, Israel, during October 23–27, 2022. The 1645 papers presented in these proceedings were carefully reviewed and selected from a total of 5804 submissions. The papers deal with topics such as computer vision; machine learning; deep neural networks; reinforcement learning; object recognition; image classification; image processing; object detection; semantic segmentation; human pose estimation; 3d reconstruction; stereo vision; computational photography; neural networks; image coding; image reconstruction; object recognition; motion estimation.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13681,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1152128760,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
58,pub.1152094932,10.1007/978-3-031-19818-2,,,"Computer Vision – ECCV 2022, 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIX","The 39-volume set, comprising the LNCS books 13661 until 13699, constitutes the refereed proceedings of the 17th European Conference on Computer Vision, ECCV 2022, held in Tel Aviv, Israel, during October 23–27, 2022. The 1645 papers presented in these proceedings were carefully reviewed and selected from a total of 5804 submissions. The papers deal with topics such as computer vision; machine learning; deep neural networks; reinforcement learning; object recognition; image classification; image processing; object detection; semantic segmentation; human pose estimation; 3d reconstruction; stereo vision; computational photography; neural networks; image coding; image reconstruction; object recognition; motion estimation.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13689,,,Closed,Edited Book,,,,,1,1,,,,https://app.dimensions.ai/details/publication/pub.1152094932,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
58,pub.1152053928,10.1007/978-3-031-20044-1,,,"Computer Vision – ECCV 2022, 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XX","The 39-volume set, comprising the LNCS books 13661 until 13699, constitutes the refereed proceedings of the 17th European Conference on Computer Vision, ECCV 2022, held in Tel Aviv, Israel, during October 23–27, 2022. The 1645 papers presented in these proceedings were carefully reviewed and selected from a total of 5804 submissions. The papers deal with topics such as computer vision; machine learning; deep neural networks; reinforcement learning; object recognition; image classification; image processing; object detection; semantic segmentation; human pose estimation; 3d reconstruction; stereo vision; computational photography; neural networks; image coding; image reconstruction; object recognition; motion estimation.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13680,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1152053928,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
53,pub.1108489382,10.1007/978-3-030-00934-2,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II","The four-volume set LNCS 11070, 11071, 11072, and 11073 constitutes the refereed proceedings of the 21st International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2018, held in Granada, Spain, in September 2018. The 373 revised full papers presented were carefully reviewed and selected from 1068 submissions in a double-blind review process. The papers have been organized in the following topical sections: Part I: Image Quality and Artefacts; Image Reconstruction Methods; Machine Learning in Medical Imaging; Statistical Analysis for Medical Imaging; Image Registration Methods. Part II: Optical and Histology Applications: Optical Imaging Applications; Histology Applications; Microscopy Applications; Optical Coherence Tomography and Other Optical Imaging Applications. Cardiac, Chest and Abdominal Applications: Cardiac Imaging Applications: Colorectal, Kidney and Liver Imaging Applications; Lung Imaging Applications; Breast Imaging Applications; Other Abdominal Applications. Part III: Diffusion Tensor Imaging and Functional MRI: Diffusion Tensor Imaging; Diffusion Weighted Imaging; Functional MRI; Human Connectome. Neuroimaging and Brain Segmentation Methods: Neuroimaging; Brain Segmentation Methods. Part IV: Computer Assisted Intervention: Image Guided Interventions and Surgery; Surgical Planning, Simulation and Work Flow Analysis; Visualization and Augmented Reality. Image Segmentation Methods: General Image Segmentation Methods, Measures and Applications; Multi-Organ Segmentation; Abdominal Segmentation Methods; Cardiac Segmentation Methods; Chest, Lung and Spine Segmentation; Other Segmentation Applications.",,,Lecture Notes in Computer Science,,,2018,2018,,2018,11071,,,All OA; Green,Edited Book,,,,,24,15,,,https://link.springer.com/content/pdf/bfm:978-3-030-00934-2/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1108489382,46 Information and Computing Sciences,
