"About the data: Exported on Mar 06, 2023. Criteria: '""The Medical Segmentation Decathlon""' in full data; Publication Year is 2019 or 2018. Â© 2023 Digital Science &amp; Research Solutions Inc. All rights reserved. Parts of this work may also be protected by copyright of content providers and other third parties, which together with all rights of Digital Science, user agrees not to violate. Redistribution / external use of this work (or parts thereof) is prohibited without prior written approval. Please contact info@dimensions.ai for further information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rank,Publication ID,DOI,PMID,PMCID,Title,Abstract,Acknowledgements,Funding,Source title,Anthology title,MeSH terms,Publication Date,PubYear,Publication Date (online),Publication Date (print),Volume,Issue,Pagination,Open Access,Publication Type,Authors,Authors (Raw Affiliation),Corresponding Authors,Authors Affiliations,Times cited,Recent citations,RCR,FCR,Source Linkout,Dimensions URL,Fields of Research (ANZSRC 2020),Sustainable Development Goals,,,,,,,,,,,
2830,pub.1118153008,10.1109/tmi.2019.2930068,31329113,,Reducing the Hausdorff Distance in Medical Image Segmentation With Convolutional Neural Networks,"The Hausdorff Distance (HD) is widely used in evaluating medical image segmentation methods. However, the existing segmentation methods do not attempt to reduce HD directly. In this paper, we present novel loss functions for training convolutional neural network (CNN)-based segmentation methods with the goal of reducing HD directly. We propose three methods to estimate HD from the segmentation probability map produced by a CNN. One method makes use of the distance transform of the segmentation boundary. Another method is based on applying morphological erosion on the difference between the true and estimated segmentation maps. The third method works by applying circular/spherical convolution kernels of different radii on the segmentation probability maps. Based on these three methods for estimating HD, we suggest three loss functions that can be used for training to reduce HD. We use these loss functions to train CNNs for segmentation of the prostate, liver, and pancreas in ultrasound, magnetic resonance, and computed tomography images and compare the results with commonly-used loss functions. Our results show that the proposed loss functions can lead to approximately 18-45% reduction in HD without degrading other segmentation performance criteria such as the Dice similarity coefficient. The proposed loss functions can be used for training medical image segmentation methods in order to reduce the large segmentation errors.","Thisworkwas supported in part by the Natural Science and Engineering Research Council of Canada (NSERC), in part by the Canadian Institutes of Health Research (CIHR), and in part by the Prostate Cancer Canada (PCC). The authors would like to thank the support from the Charles Laszlo Chair in Biomedical Engineering held by Professor S. Salcudean.",,IEEE Transactions on Medical Imaging,,"Algorithms; Diagnostic Imaging; Humans; Image Processing, Computer-Assisted; Liver; Male; Neural Networks, Computer; Prostate",2019-07-19,2019,2019-07-19,2020-02,39,2,499-513,All OA, Green,Article,"Karimi, Davood; Salcudean, Septimiu E.","Karimi, Davood (Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, V6T 1Z4, Canada); Salcudean, Septimiu E. (Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, V6T 1Z4, Canada)","Karimi, Davood (University of British Columbia)","Karimi, Davood (University of British Columbia); Salcudean, Septimiu E. (University of British Columbia)",209,178,11.46,80.1,http://arxiv.org/pdf/1904.10030,https://app.dimensions.ai/details/publication/pub.1118153008,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2461,pub.1117739643,10.1016/j.cmpb.2019.07.002,31416559,,Automated pancreas segmentation from computed tomography and magnetic resonance images: A systematic review,"The pancreas is a highly variable organ, the size, shape, and position of which are affected by age, sex, adiposity, the presence of diseases affecting the pancreas (e.g., diabetes, pancreatic cancer, pancreatitis) and other factors. Accurate automated segmentation of the pancreas has the potential to facilitate timely diagnosing and managing of diseases of the endocrine and exocrine pancreas. The aim was to systematically review studies reporting on automated pancreas segmentation algorithms derived from computed tomography (CT) or magnetic resonance (MR) images. The MEDLINE database and three patent databases were searched. Data on the performance of algorithms were meta-analysed, when possible. The algorithms were classified into one of four groups: multiorgan atlas-based, landmark-based, shape model-based, and neural network-based. A total of 13 cohorts suitable for meta-analysis were pooled to determine the performance of pancreas segmentation algorithms altogether using the Dice coefficient. These cohorts, comprising 1110 individuals, yielded a weighted mean Dice coefficient of 74.4%. Eight cohorts suitable for meta-analysis were pooled to determine the performance of pancreas segmentation algorithms altogether using the Jaccard index. These cohorts, comprising 636 individuals, yielded a weighted mean Jaccard index of 63.7%. Multiorgan atlas-based algorithms had a weighted mean Dice coefficient of 70.1% and a weighted mean Jaccard index of 59.8%. Neural network-based algorithms had a weighted mean Dice coefficient of 82.3% and a weighted mean Jaccard index of 70.1%. Studies using the other two types of algorithms were not meta-analysable. The above findings indicate that the automation of pancreas segmentation represents a considerable challenge as the performance of current automated pancreas segmentation algorithms is suboptimal. Adopting standardised reporting on performance of pancreas segmentation algorithms and encouraging the use of benchmark pancreas segmentation datasets will allow future algorithms to be tested and compared more easily and fairly.","This study was part of the COSMOS program. COSMOS is supported, in part, by the Royal Society of New Zealand (Rutherford Discovery Fellowship to Associate Professor Max Petrov). The authors are thankful to Heather Riley (University of Auckland) for her contribution to this manuscript.",None,Computer Methods and Programs in Biomedicine,,"Algorithms; Humans; Magnetic Resonance Imaging; Neural Networks, Computer; Pancreas; Pattern Recognition, Automated; Reproducibility of Results; Tomography, X-Ray Computed",2019-07-03,2019,2019-07-03,2019-09,178,,319-328,Closed,Article,"Kumar, Haribalan; DeSouza, Steve V; Petrov, Maxim S","Kumar, Haribalan (Auckland Bioengineering Institute, University of Auckland, Auckland, New Zealand.); DeSouza, Steve V (School of Medicine, University of Auckland, Auckland, New Zealand.); Petrov, Maxim S (School of Medicine, University of Auckland, Auckland, New Zealand. Electronic address: max.petrov@gmail.com.)","Petrov, Maxim S (University of Auckland)","Kumar, Haribalan (University of Auckland); DeSouza, Steve V (University of Auckland); Petrov, Maxim S (University of Auckland)",35,28,2.27,12.31,,https://app.dimensions.ai/details/publication/pub.1117739643,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,
2324,pub.1113280068,10.3389/fnins.2019.00285,31024229,PMC6460997,Nested Dilation Networks for Brain Tumor Segmentation Based on Magnetic Resonance Imaging,"Aim: Brain tumors are among the most fatal cancers worldwide. Diagnosing and manually segmenting tumors are time-consuming clinical tasks, and success strongly depends on the doctor's experience. Automatic quantitative analysis and accurate segmentation of brain tumors are greatly needed for cancer diagnosis. Methods:This paper presents an advanced three-dimensional multimodal segmentation algorithm called nested dilation networks (NDNs). It is inspired by the U-Net architecture, a convolutional neural network (CNN) developed for biomedical image segmentation and is modified to achieve better performance for brain tumor segmentation. Thus, we propose residual blocks nested with dilations (RnD) in the encoding part to enrich the low-level features and use squeeze-and-excitation (SE) blocks in both the encoding and decoding parts to boost significant features. To prove the reliability of the network structure, we compare our results with those of the standard U-Net and its transmutation networks. Different loss functions are considered to cope with class imbalance problems to maximize the brain tumor segmentation results. A cascade training strategy is employed to run NDNs for coarse-to-fine tumor segmentation. This strategy decomposes the multiclass segmentation problem into three binary segmentation problems and trains each task sequentially. Various augmentation techniques are utilized to increase the diversity of the data to avoid overfitting. Results: This approach achieves Dice similarity scores of 0.6652, 0.5880, and 0.6682 for edema, non-enhancing tumors, and enhancing tumors, respectively, in which the Dice loss is used for single-pass training. After cascade training, the Dice similarity scores rise to 0.7043, 0.5889, and 0.7206, respectively. Conclusion: Experiments show that the proposed deep learning algorithm outperforms other U-Net transmutation networks for brain tumor segmentation. Moreover, applying cascade training to NDNs facilitates better performance than other methods. The findings of this study provide considerable insight into the automatic and accurate segmentation of brain tumors.",,"Funding. This work was partially supported by the National Natural Science Foundation of China (Grant No. 61671399, 61601392, 61571380), National Key R&D Program of China (Grant No. 2017YFC0108703), and Fundamental Research Funds for the Central Universities (Grant No. 20720180056).",Frontiers in Neuroscience,,,2019-04-05,2019,2019-04-05,,13,,285,All OA, Gold,Article,"Wang, Liansheng; Wang, Shuxin; Chen, Rongzhen; Qu, Xiaobo; Chen, Yiping; Huang, Shaohui; Liu, Changhua","Wang, Liansheng (Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Department of Computer Science, School of Information Science and Engineering, Xiamen University, Xiamen, China); Wang, Shuxin (Department of Computer Science, School of Information Science and Engineering, Xiamen University, Xiamen, China); Chen, Rongzhen (Department of Computer Science, School of Information Science and Engineering, Xiamen University, Xiamen, China); Qu, Xiaobo (Department of Electronic Science, Fujian Provincial Key Laboratory of Plasma and Magnetic Resonance, School of Electronic Science and Engineering (National Model Microelectronics College), Xiamen University, Xiamen, China); Chen, Yiping (Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Department of Computer Science, School of Information Science and Engineering, Xiamen University, Xiamen, China); Huang, Shaohui (Department of Computer Science, School of Information Science and Engineering, Xiamen University, Xiamen, China); Liu, Changhua (Department of Medical Imaging, Chenggong Hospital Affiliated to Xiamen University, Xiamen, China)","Huang, Shaohui (Xiamen University); Liu, Changhua (Xiamen University)","Wang, Liansheng (Xiamen University; Xiamen University); Wang, Shuxin (Xiamen University); Chen, Rongzhen (Xiamen University); Qu, Xiaobo (Xiamen University); Chen, Yiping (Xiamen University; Xiamen University); Huang, Shaohui (Xiamen University); Liu, Changhua (Xiamen University)",21,18,0.92,6.29,https://www.frontiersin.org/articles/10.3389/fnins.2019.00285/pdf,https://app.dimensions.ai/details/publication/pub.1113280068,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
2061,pub.1121692044,10.1109/rbme.2019.2946868,31613783,,Automated Brain Tumor Segmentation Using Multimodal Brain Scans: A Survey Based on Models Submitted to the BraTS 20122018 Challenges,"Reliable brain tumor segmentation is essential for accurate diagnosis and treatment planning. Since manual segmentation of brain tumors is a highly time-consuming, expensive and subjective task, practical automated methods for this purpose are greatly appreciated. But since brain tumors are highly heterogeneous in terms of location, shape, and size, developing automatic segmentation methods has remained a challenging task over decades. This paper aims to review the evolution of automated models for brain tumor segmentation using multimodal MR images. In order to be able to make a just comparison between different methods, the proposed models are studied for the most famous benchmark for brain tumor segmentation, namely the BraTS challenge [1]. The BraTS 2012-2018 challenges and the state-of-the-art automated models employed each year are analysed. The changing trend of these automated methods since 2012 are studied and the main parameters that affect the performance of different models are analysed.",The work of M. Ghaffari was supported by a Macquarie University Ph.D. scholarship.,,IEEE Reviews in Biomedical Engineering,,"Algorithms; Brain; Brain Neoplasms; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Neuroimaging",2019-10-11,2019,2019-10-11,2020,13,,156-168,Closed,Article,"Ghaffari, Mina; Sowmya, Arcot; Oliver, Ruth","Ghaffari, Mina (School of Engineering, Macquarie University, Sydney, NSW, 2109, Australia); Sowmya, Arcot (School of Computer Science and Engineering, University of New South Wales, Sydney, NSW, 2052, Australia); Oliver, Ruth (School of Engineering, Macquarie University, Sydney, NSW, 2109, Australia)","Ghaffari, Mina (Macquarie University)","Ghaffari, Mina (Macquarie University); Sowmya, Arcot (UNSW Sydney); Oliver, Ruth (Macquarie University)",73,68,3.34,14.2,,https://app.dimensions.ai/details/publication/pub.1121692044,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,,
1257,pub.1111948780,10.1007/978-3-658-25326-4_7,,,Abstract: nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation,"The U-Net was presented in 2015. With its straight-forward and successful architecture it quickly evolved to a commonly used benchmark in medical image segmentation. The adaptation of the U-Net to novel problems, however, comprises several degrees of freedom regarding the exact architecture, preprocessing, training and inference.",,,Informatik aktuell,Bildverarbeitung fÃ¼r die Medizin 2019,,2019-02-07,2019,2019-02-07,2019,,,22-22,All OA, Bronze,Chapter,"Isensee, Fabian; Petersen, Jens; Klein, Andre; Zimmerer, David; Jaeger, Paul F.; Kohl, Simon; Wasserthal, Jakob; Koehler, Gregor; Norajitra, Tobias; Wirkert, Sebastian; Maier-Hein, Klaus H.","Isensee, Fabian (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Petersen, Jens (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Klein, Andre (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Zimmerer, David (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Jaeger, Paul F. (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Kohl, Simon (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Wasserthal, Jakob (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Koehler, Gregor (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Norajitra, Tobias (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Wirkert, Sebastian (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland); Maier-Hein, Klaus H. (Department of Medical Image Computing, German Cancer Research Center, Heidelberg, Deutschland)","Isensee, Fabian (German Cancer Research Center)","Isensee, Fabian (German Cancer Research Center); Petersen, Jens (German Cancer Research Center); Klein, Andre (German Cancer Research Center); Zimmerer, David (German Cancer Research Center); Jaeger, Paul F. (German Cancer Research Center); Kohl, Simon (German Cancer Research Center); Wasserthal, Jakob (German Cancer Research Center); Koehler, Gregor (German Cancer Research Center); Norajitra, Tobias (German Cancer Research Center); Wirkert, Sebastian (German Cancer Research Center); Maier-Hein, Klaus H. (German Cancer Research Center)",279,211,,94.74,https://link.springer.com/content/pdf/10.1007%2F978-3-658-25326-4_7.pdf,https://app.dimensions.ai/details/publication/pub.1111948780,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
1251,pub.1119307921,10.48550/arxiv.1809.10486,,,nnU-Net: Self-adapting Framework for U-Net-Based Medical Image  Segmentation,"The U-Net was presented in 2015. With its straight-forward and successful
architecture it quickly evolved to a commonly used benchmark in medical image
segmentation. The adaptation of the U-Net to novel problems, however, comprises
several degrees of freedom regarding the exact architecture, preprocessing,
training and inference. These choices are not independent of each other and
substantially impact the overall performance. The present paper introduces the
nnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on
the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away
superfluous bells and whistles of many proposed network designs and instead
focus on the remaining aspects that make out the performance and
generalizability of a method. We evaluate the nnU-Net in the context of the
Medical Segmentation Decathlon challenge, which measures segmentation
performance in ten disciplines comprising distinct entities, image modalities,
image geometries and dataset sizes, with no manual adjustments between datasets
allowed. At the time of manuscript submission, nnU-Net achieves the highest
mean dice scores across all classes and seven phase 1 tasks (except class 1 in
BrainTumour) in the online leaderboard of the challenge.",,,arXiv,,,2018-09-27,2018,,,,,,All OA, Green,Preprint,"Isensee, Fabian; Petersen, Jens; Klein, Andre; Zimmerer, David; Jaeger, Paul F.; Kohl, Simon; Wasserthal, Jakob; Koehler, Gregor; Norajitra, Tobias; Wirkert, Sebastian; Maier-Hein, Klaus H.","Isensee, Fabian (); Petersen, Jens (); Klein, Andre (); Zimmerer, David (); Jaeger, Paul F. (); Kohl, Simon (); Wasserthal, Jakob (); Koehler, Gregor (); Norajitra, Tobias (); Wirkert, Sebastian (); Maier-Hein, Klaus H. ()",,"Isensee, Fabian (); Petersen, Jens (); Klein, Andre (); Zimmerer, David (); Jaeger, Paul F. (); Kohl, Simon (); Wasserthal, Jakob (); Koehler, Gregor (); Norajitra, Tobias (); Wirkert, Sebastian (); Maier-Hein, Klaus H. ()",1,1,,0.32,,https://app.dimensions.ai/details/publication/pub.1119307921,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1196,pub.1119426691,10.48550/arxiv.1904.08128,,,Automated Design of Deep Learning Methods for Biomedical Image  Segmentation,"Biomedical imaging is a driver of scientific discovery and core component of
medical care, currently stimulated by the field of deep learning. While
semantic segmentation algorithms enable 3D image analysis and quantification in
many applications, the design of respective specialised solutions is
non-trivial and highly dependent on dataset properties and hardware conditions.
We propose nnU-Net, a deep learning framework that condenses the current domain
knowledge and autonomously takes the key decisions required to transfer a basic
architecture to different datasets and segmentation tasks. Without manual
tuning, nnU-Net surpasses most specialised deep learning pipelines in 19 public
international competitions and sets a new state of the art in the majority of
the 49 tasks. The results demonstrate a vast hidden potential in the systematic
adaptation of deep learning methods to different datasets. We make nnU-Net
publicly available as an open-source tool that can effectively be used
out-of-the-box, rendering state of the art segmentation accessible to
non-experts and catalyzing scientific progress as a framework for automated
method design.",,,arXiv,,,2019-04-17,2019,,,,,,All OA, Green,Preprint,"Isensee, Fabian; JÃ¤ger, Paul F.; Kohl, Simon A. A.; Petersen, Jens; Maier-Hein, Klaus H.","Isensee, Fabian (); JÃ¤ger, Paul F. (); Kohl, Simon A. A. (); Petersen, Jens (); Maier-Hein, Klaus H. ()",,"Isensee, Fabian (); JÃ¤ger, Paul F. (); Kohl, Simon A. A. (); Petersen, Jens (); Maier-Hein, Klaus H. ()",1,1,,0.38,,https://app.dimensions.ai/details/publication/pub.1119426691,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1162,pub.1122369674,10.48550/arxiv.1911.01764,,,"One Network to Segment Them All: A General, Lightweight System for  Accurate 3D Medical Image Segmentation","Many recent medical segmentation systems rely on powerful deep learning
models to solve highly specific tasks. To maximize performance, it is standard
practice to evaluate numerous pipelines with varying model topologies,
optimization parameters, pre- & postprocessing steps, and even model cascades.
It is often not clear how the resulting pipeline transfers to different tasks.
We propose a simple and thoroughly evaluated deep learning framework for
segmentation of arbitrary medical image volumes. The system requires no
task-specific information, no human interaction and is based on a fixed model
topology and a fixed hyperparameter set, eliminating the process of model
selection and its inherent tendency to cause method-level over-fitting. The
system is available in open source and does not require deep learning expertise
to use. Without task-specific modifications, the system performed better than
or similar to highly specialized deep learning methods across 3 separate
segmentation tasks. In addition, it ranked 5-th and 6-th in the first and
second round of the 2018 Medical Segmentation Decathlon comprising another 10
tasks. The system relies on multi-planar data augmentation which facilitates
the application of a single 2D architecture based on the familiar U-Net.
Multi-planar training combines the parameter efficiency of a 2D fully
convolutional neural network with a systematic train- and test-time
augmentation scheme, which allows the 2D model to learn a representation of the
3D image volume that fosters generalization.",,,arXiv,,,2019-11-05,2019,,,,,,All OA, Green,Preprint,"Perslev, Mathias; Dam, Erik BjÃ¸rnager; Pai, Akshay; Igel, Christian","Perslev, Mathias (); Dam, Erik BjÃ¸rnager (); Pai, Akshay (); Igel, Christian ()",,"Perslev, Mathias (); Dam, Erik BjÃ¸rnager (); Pai, Akshay (); Igel, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1122369674,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1153,pub.1121615276,10.1007/978-3-030-32692-0_32,,,Automatic Couinaud Segmentation from CT Volumes on Liver Using GLC-UNet,"Automatically generating Couinaud segments on liver, a prerequisite for modern surgery of the liver, from computed tomography (CT) volumes is a challenge for the computer-aided diagnosis (CAD). In this paper, we propose a novel global and local contexts UNet (GLC-UNet) for Couinaud segmentation. In this framework, intra-slice features and 3D contexts are effectively probed and jointly optimized for accurate liver and Couinaud segmentation using attention mechanism. We comprehensively evaluate our system performance (\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$98.51\%$$\end{document} in terms of Dice per case on liver segmentation, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$92.46\%$$\end{document} on Couinaud segmentation) on the Medical Segmentation Decathlon dataset (task 8, hepatic vessels and tumor) from MICCAI 2018 with our annotated 43,Â 205 CT slices on liver and Couinaud segmentation. (https://github.com/GLCUnet/dataset).",,,Lecture Notes in Computer Science,Machine Learning in Medical Imaging,,2019-10-10,2019,2019-10-10,2019,11861,,274-282,Closed,Chapter,"Tian, Jiang; Liu, Li; Shi, Zhongchao; Xu, Feiyu","Tian, Jiang (AI Lab, Lenovo Research, Beijing, China); Liu, Li (AI Lab, Lenovo Research, Beijing, China); Shi, Zhongchao (AI Lab, Lenovo Research, Beijing, China); Xu, Feiyu (AI Lab, Lenovo Research, Beijing, China)","Tian, Jiang ","Tian, Jiang (); Liu, Li (); Shi, Zhongchao (); Xu, Feiyu ()",19,16,,,,https://app.dimensions.ai/details/publication/pub.1121615276,46 Information and Computing Sciences,,,,,,,,,,,,
1127,pub.1119397031,10.48550/arxiv.1906.02817,,,V-NAS: Neural Architecture Search for Volumetric Medical Image  Segmentation,"Deep learning algorithms, in particular 2D and 3D fully convolutional neural
networks (FCNs), have rapidly become the mainstream methodology for volumetric
medical image segmentation. However, 2D convolutions cannot fully leverage the
rich spatial information along the third axis, while 3D convolutions suffer
from the demanding computation and high GPU memory consumption. In this paper,
we propose to automatically search the network architecture tailoring to
volumetric medical image segmentation problem. Concretely, we formulate the
structure learning as differentiable neural architecture search, and let the
network itself choose between 2D, 3D or Pseudo-3D (P3D) convolutions at each
layer. We evaluate our method on 3 public datasets, i.e., the NIH Pancreas
dataset, the Lung and Pancreas dataset from the Medical Segmentation Decathlon
(MSD) Challenge. Our method, named V-NAS, consistently outperforms other
state-of-the-arts on the segmentation task of both normal organ (NIH Pancreas)
and abnormal organs (MSD Lung tumors and MSD Pancreas tumors), which shows the
power of chosen architecture. Moreover, the searched architecture on one
dataset can be well generalized to other datasets, which demonstrates the
robustness and practical use of our proposed method.",,,arXiv,,,2019-06-06,2019,,,,,,All OA, Green,Preprint,"Zhu, Zhuotun; Liu, Chenxi; Yang, Dong; Yuille, Alan; Xu, Daguang","Zhu, Zhuotun (); Liu, Chenxi (); Yang, Dong (); Yuille, Alan (); Xu, Daguang ()",,"Zhu, Zhuotun (); Liu, Chenxi (); Yang, Dong (); Yuille, Alan (); Xu, Daguang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119397031,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1127,pub.1122233462,10.1109/3dv.2019.00035,,,V-NAS: Neural Architecture Search for Volumetric Medical Image Segmentation,"Deep learning algorithms, in particular 2D and 3D fully convolutional neural networks (FCNs), have rapidly become the mainstream methodology for volumetric medical image segmentation. However, 2D convolutions cannot fully leverage the rich spatial information along the third axis, while 3D convolutions suffer from the demanding computation and high GPU memory consumption. In this paper, we propose to automatically search the network architecture tailoring to volumetric medical image segmentation problem. Concretely, we formulate the structure learning as differentiable neural architecture search, and let the network itself choose between 2D, 3D or Pseudo-3D (P3D) convolutions at each layer. We evaluate our method on 3 public datasets, i.e., the NIH Pancreas dataset, the Lung and Pancreas dataset from the Medical Segmentation Decathlon (MSD) Challenge. Our method, named V-NAS, consistently outperforms other state-of-the-arts on the segmentation tasks of both normal organ (NIH Pancreas) and abnormal organs (MSD Lung tumors and MSD Pancreas tumors), which shows the power of chosen architecture. Moreover, the searched architecture on one dataset can be well generalized to other datasets, which demonstrates the robustness and practical use of our proposed method.",We thank Huiyu Wang for his insightful discussions and suggestions.,,,2019 International Conference on 3D Vision (3DV),,2019-01-19,2019,,2019-01-19,0,,240-248,All OA, Green,Proceeding,"Zhu, Zhuotun; Liu, Chenxi; Yang, Dong; Yuille, Alan; Xu, Daguang","Zhu, Zhuotun (Johns Hopkins University); Liu, Chenxi (Johns Hopkins University); Yang, Dong (NVIDIA Corporation); Yuille, Alan (Johns Hopkins University); Xu, Daguang (NVIDIA Corporation)","Zhu, Zhuotun (Johns Hopkins University)","Zhu, Zhuotun (Johns Hopkins University); Liu, Chenxi (Johns Hopkins University); Yang, Dong (); Yuille, Alan (Johns Hopkins University); Xu, Daguang ()",71,54,,27.21,http://arxiv.org/pdf/1906.02817,https://app.dimensions.ai/details/publication/pub.1122233462,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1097,pub.1119420767,10.48550/arxiv.1902.09063,,,A large annotated medical image dataset for the development and  evaluation of segmentation algorithms,"Semantic segmentation of medical images aims to associate a pixel with a
label in a medical image without human initialization. The success of semantic
segmentation algorithms is contingent on the availability of high-quality
imaging data with corresponding labels provided by experts. We sought to create
a large collection of annotated medical image datasets of various clinically
relevant anatomies available under open source license to facilitate the
development of semantic segmentation algorithms. Such a resource would allow:
1) objective assessment of general-purpose segmentation methods through
comprehensive benchmarking and 2) open and free access to medical image data
for any researcher interested in the problem domain. Through a
multi-institutional effort, we generated a large, curated dataset
representative of several highly variable segmentation tasks that was used in a
crowd-sourced challenge - the Medical Segmentation Decathlon held during the
2018 Medical Image Computing and Computer Aided Interventions Conference in
Granada, Spain. Here, we describe these ten labeled image datasets so that
these data may be effectively reused by the research community.",,,arXiv,,,2019-02-24,2019,,,,,,All OA, Green,Preprint,"Simpson, Amber L.; Antonelli, Michela; Bakas, Spyridon; Bilello, Michel; Farahani, Keyvan; van Ginneken, Bram; Kopp-Schneider, Annette; Landman, Bennett A.; Litjens, Geert; Menze, Bjoern; Ronneberger, Olaf; Summers, Ronald M.; Bilic, Patrick; Christ, Patrick F.; Do, Richard K. G.; Gollub, Marc; Golia-Pernicka, Jennifer; Heckers, Stephan H.; Jarnagin, William R.; McHugo, Maureen K.; Napel, Sandy; Vorontsov, Eugene; Maier-Hein, Lena; Cardoso, M. Jorge","Simpson, Amber L. (); Antonelli, Michela (); Bakas, Spyridon (); Bilello, Michel (); Farahani, Keyvan (); van Ginneken, Bram (); Kopp-Schneider, Annette (); Landman, Bennett A. (); Litjens, Geert (); Menze, Bjoern (); Ronneberger, Olaf (); Summers, Ronald M. (); Bilic, Patrick (); Christ, Patrick F. (); Do, Richard K. G. (); Gollub, Marc (); Golia-Pernicka, Jennifer (); Heckers, Stephan H. (); Jarnagin, William R. (); McHugo, Maureen K. (); Napel, Sandy (); Vorontsov, Eugene (); Maier-Hein, Lena (); Cardoso, M. Jorge ()",,"Simpson, Amber L. (); Antonelli, Michela (); Bakas, Spyridon (); Bilello, Michel (); Farahani, Keyvan (); van Ginneken, Bram (); Kopp-Schneider, Annette (); Landman, Bennett A. (); Litjens, Geert (); Menze, Bjoern (); Ronneberger, Olaf (); Summers, Ronald M. (); Bilic, Patrick (); Christ, Patrick F. (); Do, Richard K. G. (); Gollub, Marc (); Golia-Pernicka, Jennifer (); Heckers, Stephan H. (); Jarnagin, William R. (); McHugo, Maureen K. (); Napel, Sandy (); Vorontsov, Eugene (); Maier-Hein, Lena (); Cardoso, M. Jorge ()",2,2,,0.75,,https://app.dimensions.ai/details/publication/pub.1119420767,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1078,pub.1119396081,10.48550/arxiv.1906.02191,,,Uncertainty-based graph convolutional networks for organ segmentation  refinement,"Organ segmentation in CT volumes is an important pre-processing step in many
computer assisted intervention and diagnosis methods. In recent years,
convolutional neural networks have dominated the state of the art in this task.
However, since this problem presents a challenging environment due to high
variability in the organ's shape and similarity between tissues, the generation
of false negative and false positive regions in the output segmentation is a
common issue. Recent works have shown that the uncertainty analysis of the
model can provide us with useful information about potential errors in the
segmentation. In this context, we proposed a segmentation refinement method
based on uncertainty analysis and graph convolutional networks. We employ the
uncertainty levels of the convolutional network in a particular input volume to
formulate a semi-supervised graph learning problem that is solved by training a
graph convolutional network. To test our method we refine the initial output of
a 2D U-Net. We validate our framework with the NIH pancreas dataset and the
spleen dataset of the medical segmentation decathlon. We show that our method
outperforms the state-of-the art CRF refinement method by improving the dice
score by 1% for the pancreas and 2% for spleen, with respect to the original
U-Net's prediction. Finally, we discuss the results and current limitations of
the model for future work in this research direction. For reproducibility
purposes, we make our code publicly available.",,,arXiv,,,2019-06-05,2019,,,,,,All OA, Green,Preprint,"Soberanis-Mukul, Roger D.; Navab, Nassir; Albarqouni, Shadi","Soberanis-Mukul, Roger D. (); Navab, Nassir (); Albarqouni, Shadi ()",,"Soberanis-Mukul, Roger D. (); Navab, Nassir (); Albarqouni, Shadi ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119396081,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1067,pub.1120842303,10.48550/arxiv.1909.00548,,,Resource Optimized Neural Architecture Search for 3D Medical Image  Segmentation,"Neural Architecture Search (NAS), a framework which automates the task of
designing neural networks, has recently been actively studied in the field of
deep learning. However, there are only a few NAS methods suitable for 3D
medical image segmentation. Medical 3D images are generally very large; thus it
is difficult to apply previous NAS methods due to their GPU computational
burden and long training time. We propose the resource-optimized neural
architecture search method which can be applied to 3D medical segmentation
tasks in a short training time (1.39 days for 1GB dataset) using a small amount
of computation power (one RTX 2080Ti, 10.8GB GPU memory). Excellent performance
can also be achieved without retraining(fine-tuning) which is essential in most
NAS methods. These advantages can be achieved by using a reinforcement
learning-based controller with parameter sharing and focusing on the optimal
search space configuration of macro search rather than micro search. Our
experiments demonstrate that the proposed NAS method outperforms manually
designed networks with state-of-the-art performance in 3D medical image
segmentation.",,,arXiv,,,2019-09-02,2019,,,,,,All OA, Green,Preprint,"Bae, Woong; Lee, Seungho; Lee, Yeha; Park, Beomhee; Chung, Minki; Jung, Kyu-Hwan","Bae, Woong (); Lee, Seungho (); Lee, Yeha (); Park, Beomhee (); Chung, Minki (); Jung, Kyu-Hwan ()",,"Bae, Woong (); Lee, Seungho (); Lee, Yeha (); Park, Beomhee (); Chung, Minki (); Jung, Kyu-Hwan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1120842303,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1052,pub.1121613360,10.1007/978-3-030-32245-8_26,,,Resource Optimized Neural Architecture Search for 3D Medical Image Segmentation,"Neural Architecture Search (NAS), a framework which automates the task of designing neural networks, has recently been actively studied in the field of deep learning. However, there are only a few NAS methods suitable for 3D medical image segmentation. Medical 3D images are generally very large; thus it is difficult to apply previous NAS methods due to their GPU computational burden and long training time. We propose the resource-optimized neural architecture search method which can be applied to 3D medical segmentation tasks in a short training time (1.39 days for 1Â GB dataset) using a small amount of computation power (one RTX 2080Ti, 10.8Â GB GPU memory). Excellent performance can also be achieved without retraining (fine-tuning) which is essential in most NAS methods. These advantages can be achieved by using a reinforcement learning-based controller with parameter sharing and focusing on the optimal search space configuration of macro search rather than micro search. Our experiments demonstrate that the proposed NAS method outperforms manually designed networks with state-of-the-art performance in 3D medical image segmentation.","This research was supported by a grant of the Korea Health Technology R&amp;D Project(grant number: HI18C0673) through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health &amp; Welfare, Republic of Korea and Industrial Strategic technology development program (grant number: 10072064) funded by the Ministry of Trade Industry and Energy, Republic of Korea.",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2019,,2019-10-10,2019,2019-10-10,2019,11765,,228-236,All OA, Green,Chapter,"Bae, Woong; Lee, Seungho; Lee, Yeha; Park, Beomhee; Chung, Minki; Jung, Kyu-Hwan","Bae, Woong (VUNO Inc., Seoul, South Korea); Lee, Seungho (VUNO Inc., Seoul, South Korea); Lee, Yeha (VUNO Inc., Seoul, South Korea); Park, Beomhee (VUNO Inc., Seoul, South Korea); Chung, Minki (VUNO Inc., Seoul, South Korea); Jung, Kyu-Hwan (VUNO Inc., Seoul, South Korea)","Jung, Kyu-Hwan ","Bae, Woong (); Lee, Seungho (); Lee, Yeha (); Park, Beomhee (); Chung, Minki (); Jung, Kyu-Hwan ()",20,15,,7.66,http://arxiv.org/pdf/1909.00548,https://app.dimensions.ai/details/publication/pub.1121613360,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
999,pub.1119399508,10.48550/arxiv.1811.12506,,,3D Semi-Supervised Learning with Uncertainty-Aware Multi-View  Co-Training,"While making a tremendous impact in various fields, deep neural networks
usually require large amounts of labeled data for training which are expensive
to collect in many applications, especially in the medical domain. Unlabeled
data, on the other hand, is much more abundant. Semi-supervised learning
techniques, such as co-training, could provide a powerful tool to leverage
unlabeled data. In this paper, we propose a novel framework, uncertainty-aware
multi-view co-training (UMCT), to address semi-supervised learning on 3D data,
such as volumetric data from medical imaging. In our work, co-training is
achieved by exploiting multi-viewpoint consistency of 3D data. We generate
different views by rotating or permuting the 3D data and utilize asymmetrical
3D kernels to encourage diversified features in different sub-networks. In
addition, we propose an uncertainty-weighted label fusion mechanism to estimate
the reliability of each view's prediction with Bayesian deep learning. As one
view requires the supervision from other views in co-training, our
self-adaptive approach computes a confidence score for the prediction of each
unlabeled sample in order to assign a reliable pseudo label. Thus, our approach
can take advantage of unlabeled data during training. We show the effectiveness
of our proposed semi-supervised method on several public datasets from medical
image segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile,
a fully-supervised method based on our approach achieved state-of-the-art
performances on both the LiTS liver tumor segmentation and the Medical
Segmentation Decathlon (MSD) challenge, demonstrating the robustness and value
of our framework, even when fully supervised training is feasible.",,,arXiv,,,2018-11-29,2018,,,,,,All OA, Green,Preprint,"Xia, Yingda; Liu, Fengze; Yang, Dong; Cai, Jinzheng; Yu, Lequan; Zhu, Zhuotun; Xu, Daguang; Yuille, Alan; Roth, Holger","Xia, Yingda (); Liu, Fengze (); Yang, Dong (); Cai, Jinzheng (); Yu, Lequan (); Zhu, Zhuotun (); Xu, Daguang (); Yuille, Alan (); Roth, Holger ()",,"Xia, Yingda (); Liu, Fengze (); Yang, Dong (); Cai, Jinzheng (); Yu, Lequan (); Zhu, Zhuotun (); Xu, Daguang (); Yuille, Alan (); Roth, Holger ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119399508,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
967,pub.1123707006,10.1109/bibe.2019.00182,,,Automatic Detection and Segmentation of Lung Lesions using Deep Residual CNNs,"Early detection of lung cancer has shown to significantly improve patient survival. Apart from lesion detection, tumour segmentation is critical for developing radiomics signatures. In this work, we propose a novel hybrid approach for lung lesion detection and segmentation on CT scans, where the segmentation task is assisted by prior detection of regions containing lesions. For the detection task, we introduce a 2.5D residual deep CNN working in a sliding-window fashion, whereas segmentation is tackled by a modified residual U-Net with a weighted-dice plus cross-entropy loss. Experimental results on the LIDC-IDRI dataset and on the lung tumour task dataset within the Medical Segmentation Decathlon show competitive detection performance of the proposed approach (0.902 recall) and superior segmentation capabilities (0.709 dice score). These results confirm the high potential of simpler models, with lower hardware requirements, thus of more general applicability.",,,,2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE),,2019-10-30,2019,,2019-10-30,0,,977-983,Closed,Proceeding,"S., Carvalho JoÃ£o. B.; Moreira, JosÃ©-Maria; T., Figueiredo MÃ¡rio A.; Papanikolaou, Nickolas","S., Carvalho JoÃ£o. B. (Champalimaud Foundation, Instituto Superior TÃ©cnico, ULisboa, Lisbon, Portugal); Moreira, JosÃ©-Maria (Champalimaud Foundation, Instituto Superior TÃ©cnico, ULisboa, Lisbon, Portugal); T., Figueiredo MÃ¡rio A. (Instituto de TelecomunicaÃ§Ãµes Instituto Superior Tecnico, ULisboa, Lisbon, Portugal); Papanikolaou, Nickolas (Champalimaud Foundation, Lisbon, Portugal)","S., Carvalho JoÃ£o. B. (University of Lisbon)","S., Carvalho JoÃ£o. B. (University of Lisbon); Moreira, JosÃ©-Maria (University of Lisbon); T., Figueiredo MÃ¡rio A. (University of Lisbon); Papanikolaou, Nickolas (Champalimaud Foundation)",3,2,,0.9,,https://app.dimensions.ai/details/publication/pub.1123707006,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences,3 Good Health and Well Being,,,,,,,,,
903,pub.1119380541,10.48550/arxiv.1903.09097,,,Dilated deeply supervised networks for hippocampus segmentation in MRI,"Tissue loss in the hippocampi has been heavily correlated with the
progression of Alzheimer's Disease (AD). The shape and structure of the
hippocampus are important factors in terms of early AD diagnosis and prognosis
by clinicians. However, manual segmentation of such subcortical structures in
MR studies is a challenging and subjective task. In this paper, we investigate
variants of the well known 3D U-Net, a type of convolution neural network (CNN)
for semantic segmentation tasks. We propose an alternative form of the 3D
U-Net, which uses dilated convolutions and deep supervision to incorporate
multi-scale information into the model. The proposed method is evaluated on the
task of hippocampus head and body segmentation in an MRI dataset, provided as
part of the MICCAI 2018 segmentation decathlon challenge. The experimental
results show that our approach outperforms other conventional methods in terms
of different segmentation accuracy metrics.",,,arXiv,,,2019-03-20,2019,,,,,,All OA, Green,Preprint,"Folle, Lukas; Vesal, Sulaiman; Ravikumar, Nishant; Maier, Andreas","Folle, Lukas (); Vesal, Sulaiman (); Ravikumar, Nishant (); Maier, Andreas ()",,"Folle, Lukas (); Vesal, Sulaiman (); Ravikumar, Nishant (); Maier, Andreas ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119380541,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
903,pub.1111948723,10.1007/978-3-658-25326-4_18,,,Dilated Deeply Supervised Networks for Hippocampus Segmentation in MRI,"Tissue loss in the hippocampi has been heavily correlated with the progression of Alzheimerâs Disease (AD). The shape and structure of the hippocampus are important factors in terms of early AD diagnosis and prognosis by clinicians. However, manual segmentation of such subcortical structures in MR studies is a challenging and subjective task. In this paper, we investigate variants of the well known 3D U-Net, a type of convolution neural network (CNN) for semantic segmentation tasks.We propose an alternative form of the 3D U-Net, which uses dilated convolutions and deep supervision to incorporate multi-scale information into the model. The proposed method is evaluated on the task of hippocampus head and body segmentation in an MRI dataset, provided as part of the MICCAI 2018 segmentation decathlon challenge. The experimental results show that our approach outperforms other conventional methods in terms of different segmentation accuracy metrics.",,,Informatik aktuell,Bildverarbeitung fÃ¼r die Medizin 2019,,2019-02-07,2019,2019-02-07,2019,,,68-73,All OA, Green,Chapter,"Folle, Lukas; Vesal, Sulaiman; Ravikumar, Nishant; Maier, Andreas","Folle, Lukas (Pattern Recognition Lab, Friedrich-Alexander-UniversitÃ¤t Erlangen-NÃ¼rnberg, Erlangen, Deutschland); Vesal, Sulaiman (Pattern Recognition Lab, Friedrich-Alexander-UniversitÃ¤t Erlangen-NÃ¼rnberg, Erlangen, Deutschland); Ravikumar, Nishant (Pattern Recognition Lab, Friedrich-Alexander-UniversitÃ¤t Erlangen-NÃ¼rnberg, Erlangen, Deutschland); Maier, Andreas (Pattern Recognition Lab, Friedrich-Alexander-UniversitÃ¤t Erlangen-NÃ¼rnberg, Erlangen, Deutschland)","Folle, Lukas (University of Erlangen-Nuremberg)","Folle, Lukas (University of Erlangen-Nuremberg); Vesal, Sulaiman (University of Erlangen-Nuremberg); Ravikumar, Nishant (University of Erlangen-Nuremberg); Maier, Andreas (University of Erlangen-Nuremberg)",8,3,,3.07,http://eprints.whiterose.ac.uk/149274/7/Hippocampus_Segmentation_Using_Multiscale_Deep_Learning_Approach_Revised.pdf,https://app.dimensions.ai/details/publication/pub.1111948723,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
856,pub.1119455410,10.48550/arxiv.1903.08205,,,Interactive segmentation of medical images through fully convolutional  neural networks,"Image segmentation plays an essential role in medicine for both diagnostic
and interventional tasks. Segmentation approaches are either manual,
semi-automated or fully-automated. Manual segmentation offers full control over
the quality of the results, but is tedious, time consuming and prone to
operator bias. Fully automated methods require no human effort, but often
deliver sub-optimal results without providing users with the means to make
corrections. Semi-automated approaches keep users in control of the results by
providing means for interaction, but the main challenge is to offer a good
trade-off between precision and required interaction. In this paper we present
a deep learning (DL) based semi-automated segmentation approach that aims to be
a ""smart"" interactive tool for region of interest delineation in medical
images. We demonstrate its use for segmenting multiple organs on computed
tomography (CT) of the abdomen. Our approach solves some of the most pressing
clinical challenges: (i) it requires only one to a few user clicks to deliver
excellent 2D segmentations in a fast and reliable fashion; (ii) it can
generalize to previously unseen structures and ""corner cases""; (iii) it
delivers results that can be corrected quickly in a smart and intuitive way up
to an arbitrary degree of precision chosen by the user and (iv) ensures high
accuracy. We present our approach and compare it to other techniques and
previous work to show the advantages brought by our method.",,,arXiv,,,2019-03-19,2019,,,,,,All OA, Green,Preprint,"Sakinis, Tomas; Milletari, Fausto; Roth, Holger; Korfiatis, Panagiotis; Kostandy, Petro; Philbrick, Kenneth; Akkus, Zeynettin; Xu, Ziyue; Xu, Daguang; Erickson, Bradley J.","Sakinis, Tomas (); Milletari, Fausto (); Roth, Holger (); Korfiatis, Panagiotis (); Kostandy, Petro (); Philbrick, Kenneth (); Akkus, Zeynettin (); Xu, Ziyue (); Xu, Daguang (); Erickson, Bradley J. ()",,"Sakinis, Tomas (); Milletari, Fausto (); Roth, Holger (); Korfiatis, Panagiotis (); Kostandy, Petro (); Philbrick, Kenneth (); Akkus, Zeynettin (); Xu, Ziyue (); Xu, Daguang (); Erickson, Bradley J. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119455410,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
806,pub.1123244795,10.48550/arxiv.1912.03681,,,Voxel2Mesh: 3D Mesh Model Generation from Volumetric Data,"CNN-based volumetric methods that label individual voxels now dominate the
field of biomedical segmentation. However, 3D surface representations are often
required for proper analysis. They can be obtained by post-processing the
labeled volumes which typically introduces artifacts and prevents end-to-end
training. In this paper, we therefore introduce a novel architecture that goes
directly from 3D image volumes to 3D surfaces without post-processing and with
better accuracy than current methods. We evaluate it on Electron Microscopy and
MRI brain images as well as CT liver scans. We will show that it outperforms
state-of-the-art segmentation methods.",,,arXiv,,,2019-12-08,2019,,,,,,All OA, Green,Preprint,"Wickramasinghe, Udaranga; Remelli, Edoardo; Knott, Graham; Fua, Pascal","Wickramasinghe, Udaranga (); Remelli, Edoardo (); Knott, Graham (); Fua, Pascal ()",,"Wickramasinghe, Udaranga (); Remelli, Edoardo (); Knott, Graham (); Fua, Pascal ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1123244795,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
790,pub.1125458762,10.1109/macs48846.2019.9024793,,,A Study on Heart Segmentation Using Deep Learning Algorithm for MRI Scans,"Among all body organs heart is a one of the most vital of organs of human body. Dysfunction of heart function even for a couple of moments can be fatal, therefore, efficient monitoring of its physiological state is essential for the patients with cardiovascular diseases. In the recent past, various computer assisted medical imaging systems have been proposed for the segmentation of the organ of interest. However, for the segmentation of heart using MRI, only few methods have been proposed each with its own merits and demerits. For further advancement in this area of research, we analyze automated heart segmentation methods for magnetic resonance images. The analysis are based on deep learning methods that processes a full MR scan in a slice by slice fashion to predict desired mask for heart region. We design two encoder-decoder type fully convolutional neural network models (1) Multi-Channel input scheme (also known as 2.5D method), (2) a single channel input scheme with relatively large size network. Both models are evaluated on real MRI dataset and their performances are analysed for different test samples on standard measures such as Jaccard score, Youden's index and Dice score etc. Python implementation of our code is made publicly available at https://github.com/Shak97/iceest2019 for performance evaluation.",,,,"2019 13th International Conference on Mathematics, Actuarial Science, Computer Science and Statistics (MACS)",,2019-12-14,2019,,2019-12-14,0,,1-5,Closed,Proceeding,"Ibrahim, Shakeel Muhammad; Ibrahim, Muhammad Sohail; Usman, Muhammad; Naseem, Imran; Moinuddin, Muhammad","Ibrahim, Shakeel Muhammad (Department of Computer Science, University of Karachi, Karachi, 75270, Pakistan); Ibrahim, Muhammad Sohail (College of Electrical Engineering, Zhejiang University, Hangzhou, Zhejiang, China, 310027); Usman, Muhammad (Department of Computer Engineering, Chosun University, Gwangju, Republic of Korea); Naseem, Imran (College of Engineering, Karachi Institute of Economics and Technology, Karachi, 75190, Pakistan); Moinuddin, Muhammad (ICEIES, King Abdulaziz University, Saudi Arabia)",,"Ibrahim, Shakeel Muhammad (University of Karachi); Ibrahim, Muhammad Sohail (Zhejiang University); Usman, Muhammad (Chosun University); Naseem, Imran (Karachi Institute of Economics and Technology); Moinuddin, Muhammad (King Abdulaziz University)",3,2,,1.15,,https://app.dimensions.ai/details/publication/pub.1125458762,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
789,pub.1122816179,10.48550/arxiv.1911.09332,,,Heart Segmentation From MRI Scans Using Convolutional Neural Network,"Heart is one of the vital organs of human body. A minor dysfunction of heart
even for a short time interval can be fatal, therefore, efficient monitoring of
its physiological state is essential for the patients with cardiovascular
diseases. In the recent past, various computer assisted medical imaging systems
have been proposed for the segmentation of the organ of interest. However, for
the segmentation of heart using MRI, only few methods have been proposed each
with its own merits and demerits. For further advancement in this area of
research, we analyze automated heart segmentation methods for magnetic
resonance images. The analysis are based on deep learning methods that
processes a full MR scan in a slice by slice fashion to predict desired mask
for heart region. We design two encoder decoder type fully convolutional neural
network models",,,arXiv,,,2019-11-21,2019,,,,,,All OA, Green,Preprint,"Ibrahim, Shakeel Muhammad; Ibrahim, Muhammad Sohail; Usman, Muhammad; Naseem, Imran; Moinuddin, Muhammad","Ibrahim, Shakeel Muhammad (); Ibrahim, Muhammad Sohail (); Usman, Muhammad (); Naseem, Imran (); Moinuddin, Muhammad ()",,"Ibrahim, Shakeel Muhammad (); Ibrahim, Muhammad Sohail (); Usman, Muhammad (); Naseem, Imran (); Moinuddin, Muhammad ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1122816179,32 Biomedical and Clinical Sciences, 40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences,,,,,,,,
787,pub.1121502169,10.48550/arxiv.1910.01236,,,Weakly supervised segmentation from extreme points,"Annotation of medical images has been a major bottleneck for the development
of accurate and robust machine learning models. Annotation is costly and
time-consuming and typically requires expert knowledge, especially in the
medical domain. Here, we propose to use minimal user interaction in the form of
extreme point clicks in order to train a segmentation model that can, in turn,
be used to speed up the annotation of medical images. We use extreme points in
each dimension of a 3D medical image to constrain an initial segmentation based
on the random walker algorithm. This segmentation is then used as a weak
supervisory signal to train a fully convolutional network that can segment the
organ of interest based on the provided user clicks. We show that the network's
predictions can be refined through several iterations of training and
prediction using the same weakly annotated data. Ultimately, our method has the
potential to speed up the generation process of new training datasets for the
development of new machine learning and deep learning-based models for, but not
exclusively, medical image analysis.",,,arXiv,,,2019-10-02,2019,,,,,,All OA, Green,Preprint,"Roth, Holger; Zhang, Ling; Yang, Dong; Milletari, Fausto; Xu, Ziyue; Wang, Xiaosong; Xu, Daguang","Roth, Holger (); Zhang, Ling (); Yang, Dong (); Milletari, Fausto (); Xu, Ziyue (); Wang, Xiaosong (); Xu, Daguang ()",,"Roth, Holger (); Zhang, Ling (); Yang, Dong (); Milletari, Fausto (); Xu, Ziyue (); Wang, Xiaosong (); Xu, Daguang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1121502169,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
787,pub.1119395926,10.48550/arxiv.1906.05956,,,Scalable Neural Architecture Search for 3D Medical Image Segmentation,"In this paper, a neural architecture search (NAS) framework is proposed for
3D medical image segmentation, to automatically optimize a neural architecture
from a large design space. Our NAS framework searches the structure of each
layer including neural connectivities and operation types in both of the
encoder and decoder. Since optimizing over a large discrete architecture space
is difficult due to high-resolution 3D medical images, a novel stochastic
sampling algorithm based on a continuous relaxation is also proposed for
scalable gradient based optimization. On the 3D medical image segmentation
tasks with a benchmark dataset, an automatically designed architecture by the
proposed NAS framework outperforms the human-designed 3D U-Net, and moreover
this optimized architecture is well suited to be transferred for different
tasks.",,,arXiv,,,2019-06-13,2019,,,,,,All OA, Green,Preprint,"Kim, Sungwoong; Kim, Ildoo; Lim, Sungbin; Baek, Woonhyuk; Kim, Chiheon; Cho, Hyungjoo; Yoon, Boogeon; Kim, Taesup","Kim, Sungwoong (); Kim, Ildoo (); Lim, Sungbin (); Baek, Woonhyuk (); Kim, Chiheon (); Cho, Hyungjoo (); Yoon, Boogeon (); Kim, Taesup ()",,"Kim, Sungwoong (); Kim, Ildoo (); Lim, Sungbin (); Baek, Woonhyuk (); Kim, Chiheon (); Cho, Hyungjoo (); Yoon, Boogeon (); Kim, Taesup ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119395926,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
773,pub.1117944774,10.1109/isbi.2019.8759563,,,Accurate Segmentation of Dental Panoramic Radiographs with U-NETS,"Fully convolutional neural networks (FCNs) have proven to be powerful tools for medical image segmentation. We apply an FCN based on the U-Net architecture for the challenging task of semantic segmentation of dental panoramic radiographs and discuss general tricks for improving segmentation performance. Among those are network ensembling, test-time augmentation, data symmetry exploitation and bootstrapping of low quality annotations. The performance of our approach was tested on a highly variable dataset of 1500 dental panoramic radiographs. A single network reached the Dice score of 0.934 where 1201 images were used for training, forming an ensemble increased the score to 0.936.",,,,2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),,2019-04-11,2019,,2019-04-11,0,,15-19,Closed,Proceeding,"Koch, ThorbjÃ¸rn Louring; Perslev, Mathis; Igel, Christian; Brandt, Sami Sebastian","Koch, ThorbjÃ¸rn Louring (Department of Computer Science, University of Copenhagen; Department of Computer Science, IT University of Copenhagen; X1 So.ware Development, 3Shape Medical A/S); Perslev, Mathis (Department of Computer Science, University of Copenhagen); Igel, Christian (Department of Computer Science, University of Copenhagen); Brandt, Sami Sebastian (Department of Computer Science, University of Copenhagen; Department of Computer Science, IT University of Copenhagen)","Koch, ThorbjÃ¸rn Louring (University of Copenhagen; ; )","Koch, ThorbjÃ¸rn Louring (University of Copenhagen); Perslev, Mathis (University of Copenhagen); Igel, Christian (University of Copenhagen); Brandt, Sami Sebastian (University of Copenhagen)",48,40,,25.15,,https://app.dimensions.ai/details/publication/pub.1117944774,32 Biomedical and Clinical Sciences, 3203 Dentistry, 46 Information and Computing Sciences,,,,,,,,,,
773,pub.1121618226,10.1007/978-3-030-32248-9_25,,,Scalable Neural Architecture Search for 3D Medical Image Segmentation,"Abstract
In this paper, a neural architecture search (NAS) framework is proposed for 3D medical image segmentation, to automatically optimize a neural architecture from a large design space. Our NAS framework searches the structure of each layer including neural connectivities and operation types in both of the encoder and decoder. Since optimizing over a large discrete architecture space is difficult due to high-resolution 3D medical images, a novel stochastic sampling algorithm based on a continuous relaxation is also proposed for scalable gradient based optimization. On the 3D medical image segmentation tasks with a benchmark dataset, an automatically designed architecture by the proposed NAS framework outperforms the human-designed 3D U-Net, and moreover this optimized architecture is well suited to be transferred for different tasks.",We thank the Kakao Brain Cloud team for supporting to efficiently use GPU clusters for large-scale experiments.,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2019,,2019-10-10,2019,2019-10-10,2019,11766,,220-228,All OA, Green,Chapter,"Kim, Sungwoong; Kim, Ildoo; Lim, Sungbin; Baek, Woonhyuk; Kim, Chiheon; Cho, Hyungjoo; Yoon, Boogeon; Kim, Taesup","Kim, Sungwoong (Kakao Brain, Pangyo, Seongnam, Gyeonggi, South Korea); Kim, Ildoo (Kakao Brain, Pangyo, Seongnam, Gyeonggi, South Korea); Lim, Sungbin (Kakao Brain, Pangyo, Seongnam, Gyeonggi, South Korea); Baek, Woonhyuk (Kakao Brain, Pangyo, Seongnam, Gyeonggi, South Korea); Kim, Chiheon (Kakao Brain, Pangyo, Seongnam, Gyeonggi, South Korea); Cho, Hyungjoo (Department of Transdisciplinary Studies, Seoul National University, Seoul, South Korea); Yoon, Boogeon (Kakao Brain, Pangyo, Seongnam, Gyeonggi, South Korea); Kim, Taesup (Kakao Brain, Pangyo, Seongnam, Gyeonggi, South Korea; MILA, UniversitÃ© de MontrÃ©al, Montreal, Canada)","Kim, Sungwoong ","Kim, Sungwoong (); Kim, Ildoo (); Lim, Sungbin (); Baek, Woonhyuk (); Kim, Chiheon (); Cho, Hyungjoo (Seoul National University); Yoon, Boogeon (); Kim, Taesup (University of Montreal)",52,36,,19.93,http://arxiv.org/pdf/1906.05956,https://app.dimensions.ai/details/publication/pub.1121618226,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
772,pub.1121619244,10.1007/978-3-030-32245-8_4,,,"One Network to Segment Them All: A General, Lightweight System for Accurate 3D Medical Image Segmentation","Many recent medical segmentation systems rely on powerful deep learning models to solve highly specific tasks. To maximize performance, it is standard practice to evaluate numerous pipelines with varying model topologies, optimization parameters, pre- & postprocessing steps, and even model cascades. It is often not clear how the resulting pipeline transfers to different tasks.We propose a simple and thoroughly evaluated deep learning framework for segmentation of arbitrary medical image volumes. The system requires no task-specific information, no human interaction and is based on a fixed model topology and a fixed hyperparameter set, eliminating the process of model selection and its inherent tendency to cause method-level over-fitting. The system is available in open source and does not require deep learning expertise to use. Without task-specific modifications, the system performed better than or similar to highly specialized deep learning methods across 3 separate segmentation tasks. In addition, it ranked 5-th and 6-th in the first and second round of the 2018 Medical Segmentation Decathlon comprising another 10 tasks.The system relies on multi-planar data augmentation which facilitates the application of a single 2D architecture based on the familiar U-Net. Multi-planar training combines the parameter efficiency of a 2D fully convolutional neural network with a systematic train- and test-time augmentation scheme, which allows the 2D model to learn a representation of the 3D image volume that fosters generalization.",We would like to thank both Microsoft and NVIDIA for providing computational resources on the Azure platform for this project.,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2019,,2019-10-10,2019,2019-10-10,2019,11765,,30-38,All OA, Green,Chapter,"Perslev, Mathias; Dam, Erik BjÃ¸rnager; Pai, Akshay; Igel, Christian","Perslev, Mathias (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark); Dam, Erik BjÃ¸rnager (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Cerebriu A/S, Copenhagen, Denmark); Pai, Akshay (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Cerebriu A/S, Copenhagen, Denmark); Igel, Christian (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark)","Perslev, Mathias (University of Copenhagen)","Perslev, Mathias (University of Copenhagen); Dam, Erik BjÃ¸rnager (University of Copenhagen); Pai, Akshay (University of Copenhagen); Igel, Christian (University of Copenhagen)",56,48,,21.46,http://arxiv.org/pdf/1911.01764,https://app.dimensions.ai/details/publication/pub.1121619244,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
772,pub.1121015402,10.48550/arxiv.1909.06012,,,3D U$^2$-Net: A 3D Universal U-Net for Multi-Domain Medical Image  Segmentation,"Fully convolutional neural networks like U-Net have been the state-of-the-art
methods in medical image segmentation. Practically, a network is highly
specialized and trained separately for each segmentation task. Instead of a
collection of multiple models, it is highly desirable to learn a universal data
representation for different tasks, ideally a single model with the addition of
a minimal number of parameters steered to each task. Inspired by the recent
success of multi-domain learning in image classification, for the first time we
explore a promising universal architecture that handles multiple medical
segmentation tasks and is extendable for new tasks, regardless of different
organs and imaging modalities. Our 3D Universal U-Net (3D U$^2$-Net) is built
upon separable convolution, assuming that {\it images from different domains
have domain-specific spatial correlations which can be probed with channel-wise
convolution while also share cross-channel correlations which can be modeled
with pointwise convolution}. We evaluate the 3D U$^2$-Net on five organ
segmentation datasets. Experimental results show that this universal network is
capable of competing with traditional models in terms of segmentation accuracy,
while requiring only about $1\%$ of the parameters. Additionally, we observe
that the architecture can be easily and effectively adapted to a new domain
without sacrificing performance in the domains used to learn the shared
parameterization of the universal network. We put the code of 3D U$^2$-Net into
public domain. \url{https://github.com/huangmozhilv/u2net_torch/}",,,arXiv,,,2019-09-04,2019,,,,,,All OA, Green,Preprint,"Huang, Chao; Han, Hu; Yao, Qingsong; Zhu, Shankuan; Zhou, S. Kevin","Huang, Chao (); Han, Hu (); Yao, Qingsong (); Zhu, Shankuan (); Zhou, S. Kevin ()",,"Huang, Chao (); Han, Hu (); Yao, Qingsong (); Zhu, Shankuan (); Zhou, S. Kevin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1121015402,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
771,pub.1121611342,10.1007/978-3-030-32245-8_33,,,3D U2-Net: A 3D Universal U-Net for Multi-domain Medical Image Segmentation,"Fully convolutional neural networks like U-Net have been the state-of-the-art methods in medical image segmentation. Practically, a network is highly specialized and trained separately for each segmentation task. Instead of a collection of multiple models, it is highly desirable to learn a universal data representation for different tasks, ideally a single model with the addition of a minimal number of parameters steered to each task. Inspired by the recent success of multi-domain learning in image classification, for the first time we explore a promising universal architecture that handles multiple medical segmentation tasks and is extendable for new tasks, regardless of different organs and imaging modalities. Our 3D Universal U-Net (3D U2\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^2$$\end{document}-Net) is built upon separable convolution, assuming that images from different domains have domain-specific spatial correlations which can be probed with channel-wise convolution while also share cross-channel correlations which can be modeled with pointwise convolution. We evaluate the 3D U2\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^2$$\end{document}-Net on five organ segmentation datasets. Experimental results show that this universal network is capable of competing with traditional models in terms of segmentation accuracy, while requiring only about 1%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\%$$\end{document} of the parameters. Additionally, we observe that the architecture can be easily and effectively adapted to a new domain without sacrificing performance in the domains used to learn the shared parameterization of the universal network. We put the code of 3D U2\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^2$$\end{document}-Net into public domain (https://github.com/huangmozhilv/u2net_torch/).",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2019,,2019-10-10,2019,2019-10-10,2019,11765,,291-299,All OA, Green,Chapter,"Huang, Chao; Han, Hu; Yao, Qingsong; Zhu, Shankuan; Zhou, S. Kevin","Huang, Chao (Chronic Disease Research Institute and Department of Nutrition and Food Hygiene, School of Public Health, and Womenâs Hospital, School of Medicine, Zhejiang University, 310058, Hangzhou, China; Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, 100190, Beijing, China); Han, Hu (Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, 100190, Beijing, China; Peng Cheng Laboratory, Shenzhen, China); Yao, Qingsong (Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, 100190, Beijing, China); Zhu, Shankuan (Chronic Disease Research Institute and Department of Nutrition and Food Hygiene, School of Public Health, and Womenâs Hospital, School of Medicine, Zhejiang University, 310058, Hangzhou, China); Zhou, S. Kevin (Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, 100190, Beijing, China; Peng Cheng Laboratory, Shenzhen, China)","Zhu, Shankuan (Zhejiang University); Zhou, S. Kevin (Institute of Computing Technology; Peng Cheng Laboratory)","Huang, Chao (Zhejiang University; Institute of Computing Technology); Han, Hu (Institute of Computing Technology; Peng Cheng Laboratory); Yao, Qingsong (Institute of Computing Technology); Zhu, Shankuan (Zhejiang University); Zhou, S. Kevin (Institute of Computing Technology; Peng Cheng Laboratory)",60,49,,22.99,http://arxiv.org/pdf/1909.06012,https://app.dimensions.ai/details/publication/pub.1121611342,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
766,pub.1119390589,10.48550/arxiv.1903.11233,,,Deep Co-Training for Semi-Supervised Image Segmentation,"In this paper, we aim to improve the performance of semantic image
segmentation in a semi-supervised setting in which training is effectuated with
a reduced set of annotated images and additional non-annotated images. We
present a method based on an ensemble of deep segmentation models. Each model
is trained on a subset of the annotated data, and uses the non-annotated images
to exchange information with the other models, similar to co-training. Even if
each model learns on the same non-annotated images, diversity is preserved with
the use of adversarial samples. Our results show that this ability to
simultaneously train models, which exchange knowledge while preserving
diversity, leads to state-of-the-art results on two challenging medical image
datasets.",,,arXiv,,,2019-03-26,2019,,,,,,All OA, Green,Preprint,"Peng, Jizong; Estrada, Guillermo; Pedersoli, Marco; Desrosiers, Christian","Peng, Jizong (); Estrada, Guillermo (); Pedersoli, Marco (); Desrosiers, Christian ()",,"Peng, Jizong (); Estrada, Guillermo (); Pedersoli, Marco (); Desrosiers, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119390589,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,15 Life on Land,,,,,,,,,
740,pub.1111721781,10.1007/978-3-030-11726-9_21,,,No New-Net,"In this paper we demonstrate the effectiveness of a well trained U-Net in the context of the BraTS 2018 challenge. This endeavour is particularly interesting given that researchers are currently besting each other with architectural modifications that are intended to improve the segmentation performance. We instead focus on the training process arguing that a well trained U-Net is hard to beat. Our baseline U-Net, which has only minor modifications and is trained with a large patch size and a Dice loss function indeed achieved competitive Dice scores on the BraTS2018 validation data. By incorporating additional measures such as region based training, additional training data, a simple postprocessing technique and a combination of loss functions, we obtain Dice scores of 77.88, 87.81 and 80.62, and Hausdorff Distances (95th percentile) of 2.90, 6.03 and 5.08 for the enhancing tumor, whole tumor and tumor core, respectively on the test data. This setup achieved rank two in BraTS2018, with more than 60 teams participating in the challenge.",,,Lecture Notes in Computer Science,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",,2019-01-26,2019,2019-01-26,2019,11384,,234-244,Closed,Chapter,"Isensee, Fabian; Kickingereder, Philipp; Wick, Wolfgang; Bendszus, Martin; Maier-Hein, Klaus H.","Isensee, Fabian (Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany); Kickingereder, Philipp (Department of Neuroradiology, University of Heidelberg Medical Center, Heidelberg, Germany); Wick, Wolfgang (Neurology Clinic, University of Heidelberg Medical Center, Heidelberg, Germany); Bendszus, Martin (Department of Neuroradiology, University of Heidelberg Medical Center, Heidelberg, Germany); Maier-Hein, Klaus H. (Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany)","Isensee, Fabian (German Cancer Research Center)","Isensee, Fabian (German Cancer Research Center); Kickingereder, Philipp (University Hospital Heidelberg); Wick, Wolfgang (University Hospital Heidelberg); Bendszus, Martin (University Hospital Heidelberg); Maier-Hein, Klaus H. (German Cancer Research Center)",300,198,,,,https://app.dimensions.ai/details/publication/pub.1111721781,46 Information and Computing Sciences,,,,,,,,,,,,
738,pub.1119307935,10.48550/arxiv.1809.10483,,,No New-Net,"In this paper we demonstrate the effectiveness of a well trained U-Net in the
context of the BraTS 2018 challenge. This endeavour is particularly interesting
given that researchers are currently besting each other with architectural
modifications that are intended to improve the segmentation performance. We
instead focus on the training process arguing that a well trained U-Net is hard
to beat. Our baseline U-Net, which has only minor modifications and is trained
with a large patch size and a Dice loss function indeed achieved competitive
Dice scores on the BraTS2018 validation data. By incorporating additional
measures such as region based training, additional training data, a simple
postprocessing technique and a combination of loss functions, we obtain Dice
scores of 77.88, 87.81 and 80.62, and Hausdorff Distances (95th percentile) of
2.90, 6.03 and 5.08 for the enhancing tumor, whole tumor and tumor core,
respectively on the test data. This setup achieved rank two in BraTS2018, with
more than 60 teams participating in the challenge.",,,arXiv,,,2018-09-27,2018,,,,,,All OA, Green,Preprint,"Isensee, Fabian; Kickingereder, Philipp; Wick, Wolfgang; Bendszus, Martin; Maier-Hein, Klaus H.","Isensee, Fabian (); Kickingereder, Philipp (); Wick, Wolfgang (); Bendszus, Martin (); Maier-Hein, Klaus H. ()",,"Isensee, Fabian (); Kickingereder, Philipp (); Wick, Wolfgang (); Bendszus, Martin (); Maier-Hein, Klaus H. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119307935,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
706,pub.1119386227,10.48550/arxiv.1904.10030,,,Reducing the Hausdorff Distance in Medical Image Segmentation with  Convolutional Neural Networks,"The Hausdorff Distance (HD) is widely used in evaluating medical image
segmentation methods. However, existing segmentation methods do not attempt to
reduce HD directly. In this paper, we present novel loss functions for training
convolutional neural network (CNN)-based segmentation methods with the goal of
reducing HD directly. We propose three methods to estimate HD from the
segmentation probability map produced by a CNN. One method makes use of the
distance transform of the segmentation boundary. Another method is based on
applying morphological erosion on the difference between the true and estimated
segmentation maps. The third method works by applying circular/spherical
convolution kernels of different radii on the segmentation probability maps.
Based on these three methods for estimating HD, we suggest three loss functions
that can be used for training to reduce HD. We use these loss functions to
train CNNs for segmentation of the prostate, liver, and pancreas in ultrasound,
magnetic resonance, and computed tomography images and compare the results with
commonly-used loss functions. Our results show that the proposed loss functions
can lead to approximately 18-45 % reduction in HD without degrading other
segmentation performance criteria such as the Dice similarity coefficient. The
proposed loss functions can be used for training medical image segmentation
methods in order to reduce the large segmentation errors.",,,arXiv,,,2019-04-22,2019,,,,,,All OA, Green,Preprint,"Karimi, Davood; Salcudean, Septimiu E.","Karimi, Davood (); Salcudean, Septimiu E. ()",,"Karimi, Davood (); Salcudean, Septimiu E. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119386227,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
638,pub.1121558963,10.48550/arxiv.1910.01763,,,NeurReg: Neural Registration and Its Application to Image Segmentation,"Registration is a fundamental task in medical image analysis which can be
applied to several tasks including image segmentation, intra-operative
tracking, multi-modal image alignment, and motion analysis. Popular
registration tools such as ANTs and NiftyReg optimize an objective function for
each pair of images from scratch which is time-consuming for large images with
complicated deformation. Facilitated by the rapid progress of deep learning,
learning-based approaches such as VoxelMorph have been emerging for image
registration. These approaches can achieve competitive performance in a
fraction of a second on advanced GPUs. In this work, we construct a neural
registration framework, called NeurReg, with a hybrid loss of displacement
fields and data similarity, which substantially improves the current
state-of-the-art of registrations. Within the framework, we simulate various
transformations by a registration simulator which generates fixed image and
displacement field ground truth for training. Furthermore, we design three
segmentation frameworks based on the proposed registration framework: 1)
atlas-based segmentation, 2) joint learning of both segmentation and
registration tasks, and 3) multi-task learning with atlas-based segmentation as
an intermediate feature. Extensive experimental results validate the
effectiveness of the proposed NeurReg framework based on various metrics: the
endpoint error (EPE) of the predicted displacement field, mean square error
(MSE), normalized local cross-correlation (NLCC), mutual information (MI), Dice
coefficient, uncertainty estimation, and the interpretability of the
segmentation. The proposed NeurReg improves registration accuracy with fast
inference speed, which can greatly accelerate related medical image analysis
tasks.",,,arXiv,,,2019-10-03,2019,,,,,,All OA, Green,Preprint,"Zhu, Wentao; Myronenko, Andriy; Xu, Ziyue; Li, Wenqi; Roth, Holger; Huang, Yufang; Milletari, Fausto; Xu, Daguang","Zhu, Wentao (); Myronenko, Andriy (); Xu, Ziyue (); Li, Wenqi (); Roth, Holger (); Huang, Yufang (); Milletari, Fausto (); Xu, Daguang ()",,"Zhu, Wentao (); Myronenko, Andriy (); Xu, Ziyue (); Li, Wenqi (); Roth, Holger (); Huang, Yufang (); Milletari, Fausto (); Xu, Daguang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1121558963,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
618,pub.1119364023,10.48550/arxiv.1901.07419,,,Simultaneous lesion and neuroanatomy segmentation in Multiple Sclerosis  using deep neural networks,"Segmentation of white matter lesions and deep grey matter structures is an
important task in the quantification of magnetic resonance imaging in multiple
sclerosis. In this paper we explore segmentation solutions based on
convolutional neural networks (CNNs) for providing fast, reliable segmentations
of lesions and grey-matter structures in multi-modal MR imaging, and the
performance of these methods when applied to out-of-centre data.
  We trained two state-of-the-art fully convolutional CNN architectures on the
2016 MSSEG training dataset, which was annotated by seven independent human
raters: a reference implementation of a 3D Unet, and a more recently proposed
3D-to-2D architecture (DeepSCAN). We then retrained those methods on a larger
dataset from a single centre, with and without labels for other brain
structures. We quantified changes in performance owing to dataset shift, and
changes in performance by adding the additional brain-structure labels. We also
compared performance with freely available reference methods.
  Both fully-convolutional CNN methods substantially outperform other
approaches in the literature when trained and evaluated in cross-validation on
the MSSEG dataset, showing agreement with human raters in the range of human
inter-rater variability. Both architectures showed drops in performance when
trained on single-centre data and tested on the MSSEG dataset. When trained
with the addition of weak anatomical labels derived from Freesurfer, the
performance of the 3D Unet degraded, while the performance of the DeepSCAN net
improved. Overall, the DeepSCAN network predicting both lesion and anatomical
labels was the best-performing network examined.",,,arXiv,,,2019-01-22,2019,,,,,,All OA, Green,Preprint,"McKinley, Richard; Wepfer, Rik; Aschwanden, Fabian; Grunder, Lorenz; Muri, Raphaela; Rummel, Christian; Verma, Rajeev; Weisstanner, Christian; Reyes, Mauricio; Salmen, Anke; Chan, Andrew; Wagner, Franca; Wiest, Roland","McKinley, Richard (); Wepfer, Rik (); Aschwanden, Fabian (); Grunder, Lorenz (); Muri, Raphaela (); Rummel, Christian (); Verma, Rajeev (); Weisstanner, Christian (); Reyes, Mauricio (); Salmen, Anke (); Chan, Andrew (); Wagner, Franca (); Wiest, Roland ()",,"McKinley, Richard (); Wepfer, Rik (); Aschwanden, Fabian (); Grunder, Lorenz (); Muri, Raphaela (); Rummel, Christian (); Verma, Rajeev (); Weisstanner, Christian (); Reyes, Mauricio (); Salmen, Anke (); Chan, Andrew (); Wagner, Franca (); Wiest, Roland ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119364023,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
598,pub.1119398434,10.48550/arxiv.1904.01150,,,Thickened 2D Networks for Efficient 3D Medical Image Segmentation,"There has been a debate in 3D medical image segmentation on whether to use 2D
or 3D networks, where both pipelines have advantages and disadvantages. 2D
methods enjoy a low inference time and greater transfer-ability while 3D
methods are superior in performance for hard targets requiring contextual
information. This paper investigates efficient 3D segmentation from another
perspective, which uses 2D networks to mimic 3D segmentation. To compensate the
lack of contextual information in 2D manner, we propose to thicken the 2D
network inputs by feeding multiple slices as multiple channels into 2D networks
and thus 3D contextual information is incorporated. We also put forward to use
early-stage multiplexing and slice sensitive attention to solve the confusion
problem of information loss which occurs when 2D networks face thickened
inputs. With this design, we achieve a higher performance while maintaining a
lower inference latency on a few abdominal organs from CT scans, in particular
when the organ has a peculiar 3D shape and thus strongly requires contextual
information, demonstrating our method's effectiveness and ability in capturing
3D information. We also point out that ""thickened"" 2D inputs pave a new method
of 3D segmentation, and look forward to more efforts in this direction.
Experiments on segmenting a few abdominal targets in particular blood vessels
which require strong 3D contexts demonstrate the advantages of our approach.",,,arXiv,,,2019-04-01,2019,,,,,,All OA, Green,Preprint,"Yu, Qihang; Xia, Yingda; Xie, Lingxi; Fishman, Elliot K.; Yuille, Alan L.","Yu, Qihang (); Xia, Yingda (); Xie, Lingxi (); Fishman, Elliot K. (); Yuille, Alan L. ()",,"Yu, Qihang (); Xia, Yingda (); Xie, Lingxi (); Fishman, Elliot K. (); Yuille, Alan L. ()",1,1,,0.44,,https://app.dimensions.ai/details/publication/pub.1119398434,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
595,pub.1121788204,10.48550/arxiv.1910.05121,,,Methods and open-source toolkit for analyzing and visualizing challenge  results,"Biomedical challenges have become the de facto standard for benchmarking
biomedical image analysis algorithms. While the number of challenges is
steadily increasing, surprisingly little effort has been invested in ensuring
high quality design, execution and reporting for these international
competitions. Specifically, results analysis and visualization in the event of
uncertainties have been given almost no attention in the literature. Given
these shortcomings, the contribution of this paper is two-fold: (1) We present
a set of methods to comprehensively analyze and visualize the results of
single-task and multi-task challenges and apply them to a number of simulated
and real-life challenges to demonstrate their specific strengths and
weaknesses; (2) We release the open-source framework challengeR as part of this
work to enable fast and wide adoption of the methodology proposed in this
paper. Our approach offers an intuitive way to gain important insights into the
relative and absolute performance of algorithms, which cannot be revealed by
commonly applied visualization techniques. This is demonstrated by the
experiments performed within this work. Our framework could thus become an
important tool for analyzing and visualizing challenge results in the field of
biomedical image analysis and beyond.",,,arXiv,,,2019-10-11,2019,,,,,,All OA, Green,Preprint,"Wiesenfarth, Manuel; Reinke, Annika; Landman, Bennett A.; Cardoso, Manuel Jorge; Maier-Hein, Lena; Kopp-Schneider, Annette","Wiesenfarth, Manuel (); Reinke, Annika (); Landman, Bennett A. (); Cardoso, Manuel Jorge (); Maier-Hein, Lena (); Kopp-Schneider, Annette ()",,"Wiesenfarth, Manuel (); Reinke, Annika (); Landman, Bennett A. (); Cardoso, Manuel Jorge (); Maier-Hein, Lena (); Kopp-Schneider, Annette ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1121788204,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
579,pub.1123987699,10.1109/cvpr.2019.00221,,,Elastic Boundary Projection for 3D Medical Image Segmentation,"We focus on an important yet challenging problem: using a 2D deep network to deal with 3D segmentation for medical image analysis. Existing approaches either applied multi-view planar (2D) networks or directly used volumetric (3D) networks for this purpose, but both of them are not ideal: 2D networks cannot capture 3D contexts effectively, and 3D networks are both memory-consuming and less stable arguably due to the lack of pre-trained models. In this paper, we bridge the gap between 2D and 3D using a novel approach named Elastic Boundary Projection (EBP). The key observation is that, although the object is a 3D volume, what we really need in segmentation is to find its boundary which is a 2D surface. Therefore, we place a number of pivot points in the 3D space, and for each pivot, we determine its distance to the object boundary along a dense set of directions. This creates an elastic shell around each pivot which is initialized as a perfect sphere. We train a 2D deep network to determine whether each ending point falls within the object, and gradually adjust the shell so that it gradually converges to the actual shape of the boundary and thus achieves the goal of segmentation. EBP allows boundary-based segmentation without cutting a 3D volume into slices or patches, which stands out from conventional 2D and 3D approaches. EBP achieves promising accuracy in abdominal organ segmentation. Our code will be released on https://github.com/twni2016/EBP.","This paper was supported by the Lustgarten Foundation for Pancreatic Cancer Research. We thank Prof. Zhouchen Lin for supporting our research. We thank Prof. Wei Shen, Dr. Yan Wang, Weichao Qiu, Zhuotun Zhu, Yuyin Zhou, Yingda Xia, Qihang Yu, Runtao Liu and Angtian Wang for instructive discussions.",,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2019-06-20,2019,,2019-06-20,0,,2104-2113,All OA, Green,Proceeding,"Ni, Tianwei; Xie, Lingxi; Zheng, Huangjie; Fishman, Elliot K.; Yuille, Alan L.","Ni, Tianwei (Peking University); Xie, Lingxi (Johns Hopkins University; Noahâs Ark Lab, Huawei Inc.); Zheng, Huangjie (Shanghai Jiao Tong University); Fishman, Elliot K. (Johns Hopkins Medical Institute); Yuille, Alan L. (Johns Hopkins University)","Ni, Tianwei (Peking University)","Ni, Tianwei (Peking University); Xie, Lingxi (Johns Hopkins University); Zheng, Huangjie (Shanghai Jiao Tong University); Fishman, Elliot K. (); Yuille, Alan L. (Johns Hopkins University)",24,18,,10.56,http://arxiv.org/pdf/1812.00518,https://app.dimensions.ai/details/publication/pub.1123987699,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
578,pub.1119402770,10.48550/arxiv.1812.00518,,,Elastic Boundary Projection for 3D Medical Image Segmentation,"We focus on an important yet challenging problem: using a 2D deep network to
deal with 3D segmentation for medical image analysis. Existing approaches
either applied multi-view planar (2D) networks or directly used volumetric (3D)
networks for this purpose, but both of them are not ideal: 2D networks cannot
capture 3D contexts effectively, and 3D networks are both memory-consuming and
less stable arguably due to the lack of pre-trained models.
  In this paper, we bridge the gap between 2D and 3D using a novel approach
named Elastic Boundary Projection (EBP). The key observation is that, although
the object is a 3D volume, what we really need in segmentation is to find its
boundary which is a 2D surface. Therefore, we place a number of pivot points in
the 3D space, and for each pivot, we determine its distance to the object
boundary along a dense set of directions. This creates an elastic shell around
each pivot which is initialized as a perfect sphere. We train a 2D deep network
to determine whether each ending point falls within the object, and gradually
adjust the shell so that it gradually converges to the actual shape of the
boundary and thus achieves the goal of segmentation. EBP allows boundary-based
segmentation without cutting a 3D volume into slices or patches, which stands
out from conventional 2D and 3D approaches. EBP achieves promising accuracy in
abdominal organ segmentation. Our code has been open-sourced
https://github.com/twni2016/Elastic-Boundary-Projection.",,,arXiv,,,2018-12-02,2018,,,,,,All OA, Green,Preprint,"Ni, Tianwei; Xie, Lingxi; Zheng, Huangjie; Fishman, Elliot K.; Yuille, Alan L.","Ni, Tianwei (); Xie, Lingxi (); Zheng, Huangjie (); Fishman, Elliot K. (); Yuille, Alan L. ()",,"Ni, Tianwei (); Xie, Lingxi (); Zheng, Huangjie (); Fishman, Elliot K. (); Yuille, Alan L. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119402770,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
542,pub.1121015134,10.48550/arxiv.1909.05773,,,PILOT: Physics-Informed Learned Optimized Trajectories for Accelerated  MRI,"Magnetic Resonance Imaging (MRI) has long been considered to be among ""the
gold standards"" of diagnostic medical imaging. The long acquisition times,
however, render MRI prone to motion artifacts, let alone their adverse
contribution to the relative high costs of MRI examination. Over the last few
decades, multiple studies have focused on the development of both physical and
post-processing methods for accelerated acquisition of MRI scans. These two
approaches, however, have so far been addressed separately. On the other hand,
recent works in optical computational imaging have demonstrated growing success
of concurrent learning-based design of data acquisition and image
reconstruction schemes. Such schemes have already demonstrated substantial
effectiveness, leading to considerably shorter acquisition times and improved
quality of image reconstruction. Inspired by this initial success, in this
work, we propose a novel approach to the learning of optimal schemes for
conjoint acquisition and reconstruction of MRI scans, with the optimization
carried out simultaneously with respect to the time-efficiency of data
acquisition and the quality of resulting reconstructions. To be of a practical
value, the schemes are encoded in the form of general k-space trajectories,
whose associated magnetic gradients are constrained to obey a set of predefined
hardware requirements (as defined in terms of, e.g., peak currents and maximum
slew rates of magnetic gradients). With this proviso in mind, we propose a
novel algorithm for the end-to-end training of a combined
acquisition-reconstruction pipeline using a deep neural network with
differentiable forward- and back-propagation operators. We demonstrate its
effectiveness on image reconstruction and image segmentation tasks, reporting
substantial improvements in terms of acceleration factors as well as the
quality of these tasks.",,,arXiv,,,2019-09-12,2019,,,,,,All OA, Green,Preprint,"Weiss, Tomer; Senouf, Ortal; Vedula, Sanketh; Michailovich, Oleg; Zibulevsky, Michael; Bronstein, Alex","Weiss, Tomer (); Senouf, Ortal (); Vedula, Sanketh (); Michailovich, Oleg (); Zibulevsky, Michael (); Bronstein, Alex ()",,"Weiss, Tomer (); Senouf, Ortal (); Vedula, Sanketh (); Michailovich, Oleg (); Zibulevsky, Michael (); Bronstein, Alex ()",1,1,,0.19,,https://app.dimensions.ai/details/publication/pub.1121015134,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences,,,,,,,,,
514,pub.1119413933,10.48550/arxiv.1906.02849,,,Multi-scale self-guided attention for medical image segmentation,"Even though convolutional neural networks (CNNs) are driving progress in
medical image segmentation, standard models still have some drawbacks. First,
the use of multi-scale approaches, i.e., encoder-decoder architectures, leads
to a redundant use of information, where similar low-level features are
extracted multiple times at multiple scales. Second, long-range feature
dependencies are not efficiently modeled, resulting in non-optimal
discriminative feature representations associated with each semantic class. In
this paper we attempt to overcome these limitations with the proposed
architecture, by capturing richer contextual dependencies based on the use of
guided self-attention mechanisms. This approach is able to integrate local
features with their corresponding global dependencies, as well as highlight
interdependent channel maps in an adaptive manner. Further, the additional loss
between different modules guides the attention mechanisms to neglect irrelevant
information and focus on more discriminant regions of the image by emphasizing
relevant feature associations. We evaluate the proposed model in the context of
semantic segmentation on three different datasets: abdominal organs,
cardiovascular structures and brain tumors. A series of ablation experiments
support the importance of these attention modules in the proposed architecture.
In addition, compared to other state-of-the-art segmentation networks our model
yields better segmentation performance, increasing the accuracy of the
predictions while reducing the standard deviation. This demonstrates the
efficiency of our approach to generate precise and reliable automatic
segmentations of medical images. Our code is made publicly available at
https://github.com/sinAshish/Multi-Scale-Attention",,,arXiv,,,2019-06-06,2019,,,,,,All OA, Green,Preprint,"Sinha, Ashish; Dolz, Jose","Sinha, Ashish (); Dolz, Jose ()",,"Sinha, Ashish (); Dolz, Jose ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119413933,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
421,pub.1123311564,10.48550/arxiv.1912.05396,,,Multimodal Self-Supervised Learning for Medical Image Analysis,"Self-supervised learning approaches leverage unlabeled samples to acquire
generic knowledge about different concepts, hence allowing for
annotation-efficient downstream task learning. In this paper, we propose a
novel self-supervised method that leverages multiple imaging modalities. We
introduce the multimodal puzzle task, which facilitates rich representation
learning from multiple image modalities. The learned representations allow for
subsequent fine-tuning on different downstream tasks. To achieve that, we learn
a modality-agnostic feature embedding by confusing image modalities at the
data-level. Together with the Sinkhorn operator, with which we formulate the
puzzle solving optimization as permutation matrix inference instead of
classification, they allow for efficient solving of multimodal puzzles with
varying levels of complexity. In addition, we also propose to utilize
cross-modal generation techniques for multimodal data augmentation used for
training self-supervised tasks. In other words, we exploit synthetic images for
self-supervised pretraining, instead of downstream tasks directly, in order to
circumvent quality issues associated with synthetic images, while improving
data-efficiency and representations quality. Our experimental results, which
assess the gains in downstream performance and data-efficiency, show that
solving our multimodal puzzles yields better semantic representations, compared
to treating each modality independently. Our results also highlight the
benefits of exploiting synthetic images for self-supervised pretraining. We
showcase our approach on four downstream tasks: Brain tumor segmentation and
survival days prediction using four MRI modalities, Prostate segmentation using
two MRI modalities, and Liver segmentation using unregistered CT and MRI
modalities. We outperform many previous solutions, and achieve results
competitive to state-of-the-art.",,,arXiv,,,2019-12-11,2019,,,,,,All OA, Green,Preprint,"Taleb, Aiham; Lippert, Christoph; Klein, Tassilo; Nabi, Moin","Taleb, Aiham (); Lippert, Christoph (); Klein, Tassilo (); Nabi, Moin ()",,"Taleb, Aiham (); Lippert, Christoph (); Klein, Tassilo (); Nabi, Moin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1123311564,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
356,pub.1121893297,10.48550/arxiv.1910.07655,,,Deep Semantic Segmentation of Natural and Medical Images: A Review,"The semantic image segmentation task consists of classifying each pixel of an
image into an instance, where each instance corresponds to a class. This task
is a part of the concept of scene understanding or better explaining the global
context of an image. In the medical image analysis domain, image segmentation
can be used for image-guided interventions, radiotherapy, or improved
radiological diagnostics. In this review, we categorize the leading deep
learning-based medical and non-medical image segmentation solutions into six
main groups of deep architectural, data synthesis-based, loss function-based,
sequenced models, weakly supervised, and multi-task methods and provide a
comprehensive review of the contributions in each of these groups. Further, for
each group, we analyze each variant of these groups and discuss the limitations
of the current approaches and present potential future research directions for
semantic image segmentation.",,,arXiv,,,2019-10-16,2019,,,,,,All OA, Green,Preprint,"Taghanaki, Saeid Asgari; Abhishek, Kumar; Cohen, Joseph Paul; Cohen-Adad, Julien; Hamarneh, Ghassan","Taghanaki, Saeid Asgari (); Abhishek, Kumar (); Cohen, Joseph Paul (); Cohen-Adad, Julien (); Hamarneh, Ghassan ()",,"Taghanaki, Saeid Asgari (); Abhishek, Kumar (); Cohen, Joseph Paul (); Cohen-Adad, Julien (); Hamarneh, Ghassan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1121893297,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
187,pub.1111948713,10.1007/978-3-658-25326-4,,,"Bildverarbeitung fÃ¼r die Medizin 2019, Algorithmen â Systeme â Anwendungen. Proceedings des Workshops vom 17. bis 19. MÃ¤rz 2019 in LÃ¼beck","In den letzten Jahren hat sich der Workshop ""Bildverarbeitung fÃ¼r die Medizin"" durch erfolgreiche Veranstaltungen etabliert. Ziel ist auch 2019 wieder die Darstellung aktueller Forschungsergebnisse und die Vertiefung der GesprÃ¤che zwischen Wissenschaftlern, Industrie und Anwendern. Die BeitrÃ¤ge dieses Bandes - einige davon in englischer Sprache - umfassen alle Bereiche der medizinischen Bildverarbeitung, insbesondere Bildgebung und -akquisition, Maschinelles Lernen, Bildsegmentierung und Bildanalyse, Visualisierung und Animation, Zeitreihenanalyse, ComputerunterstÃ¼tzte Diagnose, Biomechanische Modellierung, Validierung und QualitÃ¤tssicherung, Bildverarbeitung in der Telemedizin u.v.m.",,,Informatik aktuell,,,2019,2019,,2019,,,,All OA, Green,Edited Book,,,,,2,2,,,https://link.springer.com/content/pdf/bfm:978-3-658-25326-4/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1111948713,,,,,,,,,,,,
107,pub.1121620256,10.1007/978-3-030-32245-8,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2019, 22nd International Conference, Shenzhen, China, October 13â17, 2019, Proceedings, Part II","The six-volume set LNCS 11764, 11765, 11766, 11767, 11768, and 11769 constitutes the refereed proceedings of the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019, held in Shenzhen, China, in October 2019. The 539 revised full papers presented were carefully reviewed and selected from 1730 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: optical imaging; endoscopy; microscopy. Part II: image segmentation; image registration; cardiovascular imaging; growth, development, atrophy and progression. Part III: neuroimage reconstruction and synthesis; neuroimage segmentation; diffusion weighted magnetic resonance imaging; functional neuroimaging (fMRI); miscellaneous neuroimaging. Part IV: shape; prediction; detection and localization; machine learning; computer-aided diagnosis; image reconstruction and synthesis. Part V: computer assisted interventions; MIC meets CAI. Part VI: computed tomography; X-ray imaging.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11765,,,All OA, Green,Edited Book,,,,,7,7,,,https://link.springer.com/content/pdf/bfm:978-3-030-32245-8/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1121620256,46 Information and Computing Sciences,,,,,,,,,,,
68,pub.1121614483,10.1007/978-3-030-32692-0,,,"Machine Learning in Medical Imaging, 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings","This book constitutes the proceedings of the 10th International Workshop on Machine Learning in Medical Imaging, MLMI 2019, held in conjunction with MICCAI 2019, in Shenzhen, China, in October 2019. The 78 papers presented in this volume were carefully reviewed and selected from 158 submissions. They focus on major trends and challenges in the area, aiming to identify new-cutting-edge techniques and their uses in medical imaging. Topics dealt with are: deep learning, generative adversarial learning, ensemble learning, sparse learning, multi-task learning, multi-view learning, manifold learning, and reinforcement learning, with their applications to medical image analysis, computer-aided detection and diagnosis, multi-modality fusion, image reconstruction, image retrieval, cellular image analysis, molecular imaging, digital pathology, etc.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11861,,,All OA, Green,Edited Book,,,,,10,10,,3.83,https://link.springer.com/content/pdf/bfm:978-3-030-32692-0/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1121614483,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,
50,pub.1111721767,10.1007/978-3-030-11726-9,,,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries, 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part II","This two-volume set LNCS 11383 and 11384 constitutes revised selected papers from the 4th International MICCAI Brainlesion Workshop, BrainLes 2018, as well as the International Multimodal Brain Tumor Segmentation, BraTS, Ischemic Stroke Lesion Segmentation, ISLES, MR Brain Image Segmentation, MRBrainS18, Computational Precision Medicine, CPM, and Stroke Workshop on Imaging and Treatment Challenges, SWITCH, which were held jointly at the Medical Image Computing for Computer Assisted Intervention Conference, MICCAI, in Granada, Spain, in September 2018. The 92 papers presented in this volume were carefully reviewed and selected from 95 submissions. They were organized in topical sections named: brain lesion image analysis; brain tumor image segmentation; ischemic stroke lesion image segmentation; grand challenge on MR brain segmentation; computational precision medicine; stroke workshop on imaging and treatment challenges.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11384,,,All OA, Green,Edited Book,,,,,10,10,,,https://library.oapen.org/bitstream/20.500.12657/57925/1/978-3-031-08999-2.pdf,https://app.dimensions.ai/details/publication/pub.1111721767,46 Information and Computing Sciences,3 Good Health and Well Being,,,,,,,,,,
28,pub.1121619956,10.1007/978-3-030-32248-9,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2019, 22nd International Conference, Shenzhen, China, October 13â17, 2019, Proceedings, Part III","The six-volume set LNCS 11764, 11765, 11766, 11767, 11768, and 11769 constitutes the refereed proceedings of the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019, held in Shenzhen, China, in October 2019. The 539 revised full papers presented were carefully reviewed and selected from 1730 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: optical imaging; endoscopy; microscopy. Part II: image segmentation; image registration; cardiovascular imaging; growth, development, atrophy and progression. Part III: neuroimage reconstruction and synthesis; neuroimage segmentation; diffusion weighted magnetic resonance imaging; functional neuroimaging (fMRI); miscellaneous neuroimaging. Part IV: shape; prediction; detection and localization; machine learning; computer-aided diagnosis; image reconstruction and synthesis. Part V: computer assisted interventions; MIC meets CAI. Part VI: computed tomography; X-ray imaging.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11766,,,All OA, Green,Edited Book,,,,,11,10,,,https://univoak.eu/islandora/object/islandora:138456/datastream/PDF/download/citation.pdf,https://app.dimensions.ai/details/publication/pub.1121619956,46 Information and Computing Sciences,,,,,,,,,,,
6777,pub.1154379601,10.1088/1361-6560/acb199,36623320,,SDA-UNet: a hepatic vein segmentation network based on the spatial distribution and density awareness of blood vessels,"Objective.Hepatic vein segmentation is a fundamental task for liver diagnosis and surgical navigation planning. Unlike other organs, the liver is the only organ with two sets of venous systems. Meanwhile, the segmentation target distribution in the hepatic vein scene is extremely unbalanced. The hepatic veins occupy a small area in abdominal CT slices. The morphology of each person's hepatic vein is different, which also makes segmentation difficult. The purpose of this study is to develop an automated hepatic vein segmentation model that guides clinical diagnosis.Approach.We introduce the 3D spatial distribution and density awareness (SDA) of hepatic veins and propose an automatic segmentation network based on 3D U-Net which includes a multi-axial squeeze and excitation module (MASE) and a distribution correction module (DCM). The MASE restrict the activation area to the area with hepatic veins. The DCM improves the awareness of the sparse spatial distribution of the hepatic veins. To obtain global axial information and spatial information at the same time, we study the effect of different training strategies on hepatic vein segmentation. Our method was evaluated by a public dataset and a private dataset. The Dice coefficient achieves 71.37% and 69.58%, improving 3.60% and 3.30% compared to the other SOTA models, respectively. Furthermore, metrics based on distance and volume also show the superiority of our method.Significance.The proposed method greatly reduced false positive areas and improved the segmentation performance of the hepatic vein in CT images. It will assist doctors in making accurate diagnoses and surgical navigation planning.",This work was supported by the Natural Science Foundation of Liaoning Province (No. 2021-YGJC-07) and the National Natural Science Foundation of China (No. 61872075).,,Physics in Medicine and Biology,,"Humans; Hepatic Veins; Liver; Abdomen; Image Processing, Computer-Assisted",2023-01-23,2023,2023-01-23,2023-02-07,68,3,35009,Closed,Article,"Tong, Guoyu; Jiang, Huiyan; Yao, Yu-Dong","Tong, Guoyu (Software College, Northeastern University, Shenyang 110819, Peopleâs Republic of China); Jiang, Huiyan (Software College, Northeastern University, Shenyang 110819, Peopleâs Republic of China; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang 110819, Peopleâs Republic of China); Yao, Yu-Dong (Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ 07030, United States of America)","Jiang, Huiyan (Northeastern University; Northeastern University)","Tong, Guoyu (Northeastern University); Jiang, Huiyan (Northeastern University; Northeastern University); Yao, Yu-Dong (Stevens Institute of Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154379601,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
6017,pub.1143889379,10.3390/tomography7040078,34941650,PMC8704906,Evaluation of a Deep Learning Algorithm for Automated Spleen Segmentation in Patients with Conditions Directly or Indirectly Affecting the Spleen,"The aim of this study was to develop a deep learning-based algorithm for fully automated spleen segmentation using CT images and to evaluate the performance in conditions directly or indirectly affecting the spleen (e.g., splenomegaly, ascites). For this, a 3D U-Net was trained on an in-house dataset (n = 61) including diseases with and without splenic involvement (in-house U-Net), and an open-source dataset from the Medical Segmentation Decathlon (open dataset, n = 61) without splenic abnormalities (open U-Net). Both datasets were split into a training (n = 32.52%), a validation (n = 9.15%) and a testing dataset (n = 20.33%). The segmentation performances of the two models were measured using four established metrics, including the Dice Similarity Coefficient (DSC). On the open test dataset, the in-house and open U-Net achieved a mean DSC of 0.906 and 0.897 respectively (p = 0.526). On the in-house test dataset, the in-house U-Net achieved a mean DSC of 0.941, whereas the open U-Net obtained a mean DSC of 0.648 (p < 0.001), showing very poor segmentation results in patients with abnormalities in or surrounding the spleen. Thus, for reliable, fully automated spleen segmentation in clinical routine, the training dataset of a deep learning-based algorithm should include conditions that directly or indirectly affect the spleen.",,,Tomography,,Algorithms, Deep Learning, Humans, Spleen,2021-12-13,2021,2021-12-13,,7,4,950-960,All OA, Gold,Article,"Meddeb, Aymen; Kossen, Tabea; Bressem, Keno K.; Hamm, Bernd; Nagel, Sebastian N.","Meddeb, Aymen (CharitÃ©âUniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt-UniversitÃ¤t zu Berlin, Klinik fÃ¼r Radiologie, Hindenburgdamm 30, 12203 Berlin, Germany;, keno-kyrill.bressem@charite.de, (K.K.B.);, bernd.hamm@charite.de, (B.H.);, sebastian.nagel@charite.de, (S.N.N.)); Kossen, Tabea (CLAIMâCharitÃ© Lab for AI in Medicine, CharitÃ©âUniversitÃ¤tsmedizin Berlin, Augustenburger Platz 1, 13353 Berlin, Germany;, tabea.kossen@charite.de); Bressem, Keno K. (CharitÃ©âUniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt-UniversitÃ¤t zu Berlin, Klinik fÃ¼r Radiologie, Hindenburgdamm 30, 12203 Berlin, Germany;, keno-kyrill.bressem@charite.de, (K.K.B.);, bernd.hamm@charite.de, (B.H.);, sebastian.nagel@charite.de, (S.N.N.); Berlin Institute of Health, CharitÃ©âUniversitÃ¤tsmedizin Berlin, CharitÃ©platz 1, 10117 Berlin, Germany); Hamm, Bernd (CharitÃ©âUniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt-UniversitÃ¤t zu Berlin, Klinik fÃ¼r Radiologie, Hindenburgdamm 30, 12203 Berlin, Germany;, keno-kyrill.bressem@charite.de, (K.K.B.);, bernd.hamm@charite.de, (B.H.);, sebastian.nagel@charite.de, (S.N.N.)); Nagel, Sebastian N. (CharitÃ©âUniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt-UniversitÃ¤t zu Berlin, Klinik fÃ¼r Radiologie, Hindenburgdamm 30, 12203 Berlin, Germany;, keno-kyrill.bressem@charite.de, (K.K.B.);, bernd.hamm@charite.de, (B.H.);, sebastian.nagel@charite.de, (S.N.N.))","Meddeb, Aymen (CharitÃ© - University Medicine Berlin; )","Meddeb, Aymen (CharitÃ© - University Medicine Berlin); Kossen, Tabea (CharitÃ© - University Medicine Berlin); Bressem, Keno K. (CharitÃ© - University Medicine Berlin; Berlin Institute of Health at CharitÃ© - UniversitÃ¤tsmedizin Berlin); Hamm, Bernd (CharitÃ© - University Medicine Berlin); Nagel, Sebastian N. (CharitÃ© - University Medicine Berlin)",2,2,0.41,2.5,https://www.mdpi.com/2379-139X/7/4/78/pdf?version=1640167984,https://app.dimensions.ai/details/publication/pub.1143889379,32 Biomedical and Clinical Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,
5791,pub.1153642149,10.1007/978-3-031-21014-3_18,36780251,PMC9911329,Dynamic Linear Transformer for 3D Biomedical Image Segmentation,"Transformer-based neural networks have surpassed promising performance on many biomedical image segmentation tasks due to a better global information modeling from the self-attention mechanism. However, most methods are still designed for 2D medical images while ignoring the essential 3D volume information. The main challenge for 3D Transformer-based segmentation methods is the quadratic complexity introduced by the self-attention mechanism [17]. In this paper, we are addressing these two research gaps, lack of 3D methods and computational complexity in Transformers, by proposing a novel Transformer architecture that has an encoder-decoder style architecture with linear complexity. Furthermore, we newly introduce a dynamic token concept to further reduce the token numbers for self-attention calculation. Taking advantage of the global information modeling, we provide uncertainty maps from different hierarchy stages. We evaluate this method on multiple challenging CT pancreas segmentation datasets. Our results show that our novel 3D Transformer-based segmentor could provide promising highly feasible segmentation performance and accurate uncertainty quantification using single annotation. Code is available https://github.com/freshman97/LinTransUNet.",This project is funded by the NIH grants: R01-CA246704 and R01-CA240639.,,Lecture Notes in Computer Science,Machine Learning in Medical Imaging,,2022-12-16,2022,2022-12-16,2022,13583,,171-180,All OA, Green,Chapter,"Zhang, Zheyuan; Bagci, Ulas","Zhang, Zheyuan (Northwestern University, 60201, Evanston, IL, USA); Bagci, Ulas (Northwestern University, 60201, Evanston, IL, USA)","Bagci, Ulas (Northwestern University)","Zhang, Zheyuan (Northwestern University); Bagci, Ulas (Northwestern University)",0,0,,,http://arxiv.org/pdf/2206.00771,https://app.dimensions.ai/details/publication/pub.1153642149,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5720,pub.1135535563,10.3390/jimaging7020031,34460630,PMC8321260,Domain Adaptation for Medical Image Segmentation: A Meta-Learning Method,"Convolutional neural networks (CNNs) have demonstrated great achievement in increasing the accuracy and stability of medical image segmentation. However, existing CNNs are limited by the problem of dependency on the availability of training data owing to high manual annotation costs and privacy issues. To counter this limitation, domain adaptation (DA) and few-shot learning have been extensively studied. Inspired by these two categories of approaches, we propose an optimization-based meta-learning method for segmentation tasks. Even though existing meta-learning methods use prior knowledge to choose parameters that generalize well from few examples, these methods limit the diversity of the task distribution that they can learn from in medical image segmentation. In this paper, we propose a meta-learning algorithm to augment the existing algorithms with the capability to learn from diverse segmentation tasks across the entire task distribution. Specifically, our algorithm aims to learn from the diversity of image features which characterize a specific tissue type while showing diverse signal intensities. To demonstrate the effectiveness of the proposed algorithm, we conducted experiments using a diverse set of segmentation tasks from the Medical Segmentation Decathlon and two meta-learning benchmarks: model-agnostic meta-learning (MAML) and Reptile. U-Net and Dice similarity coefficient (DSC) were selected as the baseline model and the main performance metric, respectively. The experimental results show that our algorithm maximally surpasses MAML and Reptile by 2% and 2.4% respectively, in terms of the DSC. By showing a consistent improvement in subjective measures, we can also infer that our algorithm can produce a better generalization of a target task that has few examples.",,"This research was partially supported by the National Natural Science Foundation of China, 2019, grant number 81873891, in part supported by the Major International (Regional) Joint Research Project of China, 2021, grant number 82020108018, and in part supported by China International Medical Foundation SKY Image Research Fund Project, 2019, grant number Z-2014-07-1912-01.",Journal of Imaging,,,2021-02-10,2021,2021-02-10,,7,2,31,All OA, Gold,Article,"Zhang, Penghao; Li, Jiayue; Wang, Yining; Pan, Judong","Zhang, Penghao (School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China;, zphbupt@bupt.edu.cn); Li, Jiayue (School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281, USA); Wang, Yining (Department of Radiology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing 100730, China;, WangYiNing@pumch.cn); Pan, Judong (Department of Radiology and Biomedical Imaging, University of California San Francisco, San Francisco, CA 94143, USA;, judong.pan@ucsf.edu)","Li, Jiayue (Arizona State University)","Zhang, Penghao (Beijing University of Posts and Telecommunications); Li, Jiayue (Arizona State University); Wang, Yining (Chinese Academy of Medical Sciences & Peking Union Medical College; Peking Union Medical College Hospital); Pan, Judong (University of California, San Francisco)",9,9,1.2,7.07,https://www.mdpi.com/2313-433X/7/2/31/pdf?version=1612953930,https://app.dimensions.ai/details/publication/pub.1135535563,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
5697,pub.1149497161,10.1038/s41467-022-30695-9,35840566,PMC9287542,The Medical Segmentation Decathlon,"International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon (MSD)âa biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. MSD results confirmed this hypothesis, moreover, MSD winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate AI segmentation models is now commoditized to scientists that are not versed in AI model training.","This work was supported by the UK Research and Innovation London Medical Imaging &amp; Artificial Intelligence Center for Value-Based Healthcare. Investigators received support from the Wellcome/EPSRC Center for Medical Engineering (WT203148), Wellcome Flagship Program (WT213038). The research was also supported by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation and by the Helmholtz Imaging Platform (HIP), a platform of the Helmholtz Incubator on Information and Data Science. Team CerebriuDIKU gratefully acknowledges support from the Independent Research Fund Denmark through the project U-Sleep (project number 9131-00099B). R.M.S. is supported by the Intramural Research Program of the National Institutes of Health Clinical Center G.L. reported research grants from the Dutch Cancer Society, the Netherlands Organization for Scientific Research (NWO), and HealthHolland during the conduct of the study, and grants from Philips Digital Pathology Solutions, and consultancy fees from Novartis and Vital Imaging, outside the submitted work. Research reported in this publication was partly supported by the National Institutes of Health (NIH) under award numbers NCI:U01CA242871, NCI:U24CA189523, NINDS:R01NS042645. The content of this publication is solely the responsibility of the authors and does not represent the official views of the NIH.Henkjan Huisman is receiving grant support from Siemens Healthineers. James Meakin received grant funding from AWS. The method presented by BCVUniandes was made in collaboration with Silvana Castillo, from Universidad de los Andes. We would like to thank Minu D. Tizabi for proof-reading the paper.",,Nature Communications,,"Algorithms; Image Processing, Computer-Assisted",2022-07-15,2022,2022-07-15,,13,1,4128,All OA, Gold,Article,"Antonelli, Michela; Reinke, Annika; Bakas, Spyridon; Farahani, Keyvan; Kopp-Schneider, Annette; Landman, Bennett A.; Litjens, Geert; Menze, Bjoern; Ronneberger, Olaf; Summers, Ronald M.; van Ginneken, Bram; Bilello, Michel; Bilic, Patrick; Christ, Patrick F.; Do, Richard K. G.; Gollub, Marc J.; Heckers, Stephan H.; Huisman, Henkjan; Jarnagin, William R.; McHugo, Maureen K.; Napel, Sandy; Pernicka, Jennifer S. Golia; Rhode, Kawal; Tobon-Gomez, Catalina; Vorontsov, Eugene; Meakin, James A.; Ourselin, Sebastien; Wiesenfarth, Manuel; ArbelÃ¡ez, Pablo; Bae, Byeonguk; Chen, Sihong; Daza, Laura; Feng, Jianjiang; He, Baochun; Isensee, Fabian; Ji, Yuanfeng; Jia, Fucang; Kim, Ildoo; Maier-Hein, Klaus; Merhof, Dorit; Pai, Akshay; Park, Beomhee; Perslev, Mathias; Rezaiifar, Ramin; Rippel, Oliver; Sarasua, Ignacio; Shen, Wei; Son, Jaemin; Wachinger, Christian; Wang, Liansheng; Wang, Yan; Xia, Yingda; Xu, Daguang; Xu, Zhanwei; Zheng, Yefeng; Simpson, Amber L.; Maier-Hein, Lena; Cardoso, M. Jorge","Antonelli, Michela (School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Reinke, Annika (Div. Computer Assisted Medical Interventions, German Cancer Research Center (DKFZ), Heidelberg, Germany; HI Helmholtz Imaging, German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Mathematics and Computer Science, University of Heidelberg, Heidelberg, Germany); Bakas, Spyridon (Center for Biomedical Image Computing and Analytics (CBICA), University of Pennsylvania, Philadelphia, PA, USA; Department of Radiology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA; Department of Pathology and Laboratory Medicine, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA); Farahani, Keyvan (Center for Biomedical Informatics and Information Technology, National Cancer Institute (NIH), Bethesda, MD, USA); Kopp-Schneider, Annette (Div. Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany); Landman, Bennett A. (Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA); Litjens, Geert (Radboud University Medical Center, Radboud Institute for Health Sciences, Nijmegen, The Netherlands); Menze, Bjoern (Quantitative Biomedicine, University of Zurich, Zurich, Switzerland); Ronneberger, Olaf (DeepMind, London, UK); Summers, Ronald M. (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Department of Radiology and Imaging Sciences, National Institutes of Health Clinical Center (NIH), Bethesda, MD, USA); van Ginneken, Bram (Radboud University Medical Center, Radboud Institute for Health Sciences, Nijmegen, The Netherlands); Bilello, Michel (Center for Biomedical Image Computing and Analytics (CBICA), University of Pennsylvania, Philadelphia, PA, USA); Bilic, Patrick (Department of Informatics, Technische UniversitÃ¤t MÃ¼nchen, MÃ¼nchen, Germany); Christ, Patrick F. (Department of Informatics, Technische UniversitÃ¤t MÃ¼nchen, MÃ¼nchen, Germany); Do, Richard K. G. (Department of Radiology, Memorial Sloan Kettering Cancer Center, New York, NY, USA); Gollub, Marc J. (Department of Radiology, Memorial Sloan Kettering Cancer Center, New York, NY, USA); Heckers, Stephan H. (Department of Psychiatry & Behavioral Sciences, Vanderbilt University Medical Center, Nashville, TN, USA); Huisman, Henkjan (Radboud University Medical Center, Radboud Institute for Health Sciences, Nijmegen, The Netherlands); Jarnagin, William R. (Department of Surgery, Memorial Sloan Kettering Cancer Center, New York, NY, USA); McHugo, Maureen K. (Department of Psychiatry & Behavioral Sciences, Vanderbilt University Medical Center, Nashville, TN, USA); Napel, Sandy (Department of Radiology, Stanford University, Stanford, CA, USA); Pernicka, Jennifer S. Golia (Department of Radiology, Memorial Sloan Kettering Cancer Center, New York, NY, USA); Rhode, Kawal (School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Tobon-Gomez, Catalina (School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Vorontsov, Eugene (Department of Computer Science and Software Engineering, Ãcole Polytechnique de MontrÃ©al, MontrÃ©al, QC, Canada); Meakin, James A. (Radboud University Medical Center, Radboud Institute for Health Sciences, Nijmegen, The Netherlands); Ourselin, Sebastien (School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Wiesenfarth, Manuel (Div. Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany); ArbelÃ¡ez, Pablo (Universidad de los Andes, Bogota, Colombia); Bae, Byeonguk (VUNO Inc., Seoul, Korea); Chen, Sihong (Tencent Jarvis Lab, Shenzhen, China); Daza, Laura (Universidad de los Andes, Bogota, Colombia); Feng, Jianjiang (Department of Automation, Tsinghua University, Beijing, China); He, Baochun (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China); Isensee, Fabian (HI Applied Computer Vision Lab, Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany); Ji, Yuanfeng (Department of Computer Science, Xiamen University, Xiamen, China); Jia, Fucang (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China); Kim, Ildoo (Kakao Brain, Seongnam-si, Republic of Korea); Maier-Hein, Klaus (Cerebriu A/S, Copenhagen, Denmark; Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany); Merhof, Dorit (Institute of Imaging & Computer Vision, RWTH Aachen University, Aachen, Germany; Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany); Pai, Akshay (Cerebriu A/S, Copenhagen, Denmark; Department of Computer Science, University of Copenhagen, Copenhagen, Denmark); Park, Beomhee (VUNO Inc., Seoul, Korea); Perslev, Mathias (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark); Rezaiifar, Ramin (MaaDoTaa.com, San Diego, CA, USA); Rippel, Oliver (Institute of Imaging & Computer Vision, RWTH Aachen University, Aachen, Germany); Sarasua, Ignacio (Lab for Artificial Intelligence in Medical Imaging (AI-Med), Department of Child and Adolescent Psychiatry, University Hospital, LMU MÃ¼nchen, Germany); Shen, Wei (MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China); Son, Jaemin (VUNO Inc., Seoul, Korea); Wachinger, Christian (Lab for Artificial Intelligence in Medical Imaging (AI-Med), Department of Child and Adolescent Psychiatry, University Hospital, LMU MÃ¼nchen, Germany); Wang, Liansheng (Department of Computer Science, Xiamen University, Xiamen, China); Wang, Yan (Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China); Xia, Yingda (Johns Hopkins University, Baltimore, MD, USA); Xu, Daguang (NVIDIA, Santa Clara, CA, USA); Xu, Zhanwei (Department of Automation, Tsinghua University, Beijing, China); Zheng, Yefeng (Tencent Jarvis Lab, Shenzhen, China); Simpson, Amber L. (School of Computing/Department of Biomedical and Molecular Sciences, Queenâs University, Kingston, ON, Canada); Maier-Hein, Lena (Div. Computer Assisted Medical Interventions, German Cancer Research Center (DKFZ), Heidelberg, Germany; HI Helmholtz Imaging, German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Mathematics and Computer Science, University of Heidelberg, Heidelberg, Germany; Medical Faculty, University of Heidelberg, Heidelberg, Germany); Cardoso, M. Jorge (School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK)","Antonelli, Michela (King's College London)","Antonelli, Michela (King's College London); Reinke, Annika (German Cancer Research Center; German Cancer Research Center; Heidelberg University); Bakas, Spyridon (University of Pennsylvania; University of Pennsylvania Health System; University of Pennsylvania; University of Pennsylvania); Farahani, Keyvan (National Cancer Institute); Kopp-Schneider, Annette (German Cancer Research Center); Landman, Bennett A. (Vanderbilt University); Litjens, Geert (Radboud University Nijmegen Medical Centre); Menze, Bjoern (University of Zurich); Ronneberger, Olaf (DeepMind (United Kingdom)); Summers, Ronald M. (National Institutes of Health Clinical Center); van Ginneken, Bram (Radboud University Nijmegen Medical Centre); Bilello, Michel (University of Pennsylvania; University of Pennsylvania Health System); Bilic, Patrick (Technical University of Munich); Christ, Patrick F. (Technical University of Munich); Do, Richard K. G. (Memorial Sloan Kettering Cancer Center); Gollub, Marc J. (Memorial Sloan Kettering Cancer Center); Heckers, Stephan H. (Vanderbilt University Medical Center); Huisman, Henkjan (Radboud University Nijmegen Medical Centre); Jarnagin, William R. (Memorial Sloan Kettering Cancer Center); McHugo, Maureen K. (Vanderbilt University Medical Center); Napel, Sandy (Stanford University); Pernicka, Jennifer S. Golia (Memorial Sloan Kettering Cancer Center); Rhode, Kawal (King's College London); Tobon-Gomez, Catalina (King's College London); Vorontsov, Eugene (Polytechnique MontrÃ©al); Meakin, James A. (Radboud University Nijmegen Medical Centre); Ourselin, Sebastien (King's College London); Wiesenfarth, Manuel (German Cancer Research Center); ArbelÃ¡ez, Pablo (Universidad de Los Andes); Bae, Byeonguk (); Chen, Sihong (); Daza, Laura (Universidad de Los Andes); Feng, Jianjiang (Tsinghua University); He, Baochun (Shenzhen Institutes of Advanced Technology); Isensee, Fabian (German Cancer Research Center); Ji, Yuanfeng (Xiamen University); Jia, Fucang (Shenzhen Institutes of Advanced Technology); Kim, Ildoo (); Maier-Hein, Klaus (University Hospital Heidelberg); Merhof, Dorit (RWTH Aachen University; Fraunhofer Institute for Digital Medicine); Pai, Akshay (University of Copenhagen); Park, Beomhee (); Perslev, Mathias (University of Copenhagen); Rezaiifar, Ramin (); Rippel, Oliver (RWTH Aachen University); Sarasua, Ignacio (Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen); Shen, Wei (Shanghai Jiao Tong University); Son, Jaemin (); Wachinger, Christian (Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen); Wang, Liansheng (Xiamen University); Wang, Yan (East China Normal University); Xia, Yingda (Johns Hopkins University); Xu, Daguang (Nvidia (United States)); Xu, Zhanwei (Tsinghua University); Zheng, Yefeng (); Simpson, Amber L. (Queen's University); Maier-Hein, Lena (German Cancer Research Center; German Cancer Research Center; Heidelberg University; Heidelberg University); Cardoso, M. Jorge (King's College London)",64,64,,,https://www.nature.com/articles/s41467-022-30695-9.pdf,https://app.dimensions.ai/details/publication/pub.1149497161,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
5693,pub.1149407319,10.1016/j.compbiomed.2022.105817,35841780,,Prostate158 - An expert-annotated 3T MRI dataset and algorithm for prostate cancer detection,"BACKGROUND: The development of deep learning (DL) models for prostate segmentation on magnetic resonance imaging (MRI) depends on expert-annotated data and reliable baselines, which are often not publicly available. This limits both reproducibility and comparability.
METHODS: Prostate158 consists of 158 expert annotated biparametric 3T prostate MRIs comprising T2w sequences and diffusion-weighted sequences with apparent diffusion coefficient maps. Two U-ResNets trained for segmentation of anatomy (central gland, peripheral zone) and suspicious lesions for prostate cancer (PCa) with a PI-RADS score of â¥4 served as baseline algorithms. Segmentation performance was evaluated using the Dice similarity coefficient (DSC), the Hausdorff distance (HD), and the average surface distance (ASD). The Wilcoxon test with Bonferroni correction was used to evaluate differences in performance. The generalizability of the baseline model was assessed using the open datasets Medical Segmentation Decathlon and PROSTATEx.
RESULTS: Compared to Reader 1, the models achieved a DSC/HD/ASD of 0.88/18.3/2.2 for the central gland, 0.75/22.8/1.9 for the peripheral zone, and 0.45/36.7/17.4 for PCa. Compared with Reader 2, the DSC/HD/ASD were 0.88/17.5/2.6 for the central gland, 0.73/33.2/1.9 for the peripheral zone, and 0.4/39.5/19.1 for PCa. Interrater agreement measured in DSC/HD/ASD was 0.87/11.1/1.0 for the central gland, 0.75/15.8/0.74 for the peripheral zone, and 0.6/18.8/5.5 for PCa. Segmentation performances on the Medical Segmentation Decathlon and PROSTATEx were 0.82/22.5/3.4; 0.86/18.6/2.5 for the central gland, and 0.64/29.2/4.7; 0.71/26.3/2.2 for the peripheral zone.
CONCLUSIONS: We provide an openly accessible, expert-annotated 3T dataset of prostate MRI and a reproducible benchmark to foster the development of prostate segmentation algorithms.","LCA is grateful for her participation in the BIH CharitÃ©âJunior Clinician and Clinician Scientist Program and KKB is grateful for his participation in the BIH CharitÃ© Digital Clinician Scientist Program, all funded by the CharitÃ©âUniversitÃ¤tsmedizin Berlin and the Berlin Institute of Health.",,Computers in Biology and Medicine,,Algorithms, Humans, Magnetic Resonance Imaging, Male, Prostate, Prostatic Neoplasms, Reproducibility of Results, Retrospective Studies,2022-07-11,2022,2022-07-11,2022-09,148,,105817,Closed,Article,"Adams, Lisa C; Makowski, Marcus R; Engel, GÃ¼nther; Rattunde, Maximilian; Busch, Felix; Asbach, Patrick; Niehues, Stefan M; Vinayahalingam, Shankeeth; van Ginneken, Bram; Litjens, Geert; Bressem, Keno K","Adams, Lisa C (CharitÃ© - UniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt UniversitÃ¤t zu Berlin, Institute for Radiology, LuisenstraÃe 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany; Berlin Institute of Health at CharitÃ© - UniversitÃ¤tsmedizin Berlin, CharitÃ©platz 1, 10117, Berlin, Germany. Electronic address: lisa.christine.adams@gmail.com.); Makowski, Marcus R (Technical University of Munich, Department of Diagnostic and Interventional Radiology, Faculty of Medicine, Ismaninger Str. 22, 81675, Munich, Germany.); Engel, GÃ¼nther (CharitÃ© - UniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt UniversitÃ¤t zu Berlin, Institute for Radiology, LuisenstraÃe 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany; Institute for Diagnostic and Interventional Radiology, Georg-August University, GÃ¶ttingen, Germany.); Rattunde, Maximilian (CharitÃ© - UniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt UniversitÃ¤t zu Berlin, Institute for Radiology, LuisenstraÃe 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany.); Busch, Felix (CharitÃ© - UniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt UniversitÃ¤t zu Berlin, Institute for Radiology, LuisenstraÃe 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany.); Asbach, Patrick (CharitÃ© - UniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt UniversitÃ¤t zu Berlin, Institute for Radiology, LuisenstraÃe 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany.); Niehues, Stefan M (CharitÃ© - UniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt UniversitÃ¤t zu Berlin, Institute for Radiology, LuisenstraÃe 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany.); Vinayahalingam, Shankeeth (Department of Oral and Maxillofacial Surgery, Radboud University Medical Center, Nijmegen, GA, the Netherlands.); van Ginneken, Bram (Radboud University Medical Center, Nijmegen, GA, the Netherlands.); Litjens, Geert (Radboud University Medical Center, Nijmegen, GA, the Netherlands.); Bressem, Keno K (CharitÃ© - UniversitÃ¤tsmedizin Berlin, Corporate Member of Freie UniversitÃ¤t Berlin and Humboldt UniversitÃ¤t zu Berlin, Institute for Radiology, LuisenstraÃe 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany; Berlin Institute of Health at CharitÃ© - UniversitÃ¤tsmedizin Berlin, CharitÃ©platz 1, 10117, Berlin, Germany.)","Adams, Lisa C (CharitÃ© - University Medicine Berlin; Berlin Institute of Health at CharitÃ© - UniversitÃ¤tsmedizin Berlin)","Adams, Lisa C (CharitÃ© - University Medicine Berlin; Berlin Institute of Health at CharitÃ© - UniversitÃ¤tsmedizin Berlin); Makowski, Marcus R (Technical University of Munich); Engel, GÃ¼nther (CharitÃ© - University Medicine Berlin; University of GÃ¶ttingen); Rattunde, Maximilian (CharitÃ© - University Medicine Berlin); Busch, Felix (CharitÃ© - University Medicine Berlin); Asbach, Patrick (CharitÃ© - University Medicine Berlin); Niehues, Stefan M (CharitÃ© - University Medicine Berlin); Vinayahalingam, Shankeeth (Radboud University Nijmegen Medical Centre); van Ginneken, Bram (Radboud University Nijmegen Medical Centre); Litjens, Geert (Radboud University Nijmegen Medical Centre); Bressem, Keno K (CharitÃ© - University Medicine Berlin; Berlin Institute of Health at CharitÃ© - UniversitÃ¤tsmedizin Berlin)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1149407319,31 Biological Sciences, 3102 Bioinformatics and Computational Biology, 42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences, 4601 Applied Computing
5442,pub.1125081196,10.3390/diagnostics10020110,32085469,PMC7167802,SD-UNet: Stripping down U-Net for Segmentation of Biomedical Images on Platforms with Low Computational Budgets,"During image segmentation tasks in computer vision, achieving high accuracy performance while requiring fewer computations and faster inference is a big challenge. This is especially important in medical imaging tasks but one metric is usually compromised for the other. To address this problem, this paper presents an extremely fast, small and computationally effective deep neural network called Stripped-Down UNet (SD-UNet), designed for the segmentation of biomedical data on devices with limited computational resources. By making use of depthwise separable convolutions in the entire network, we design a lightweight deep convolutional neural network architecture inspired by the widely adapted U-Net model. In order to recover the expected performance degradation in the process, we introduce a weight standardization algorithm with the group normalization method. We demonstrate that SD-UNet has three major advantages including: (i) smaller model size (23x smaller than U-Net); (ii) 8x fewer parameters; and (iii) faster inference time with a computational complexity lower than 8M floating point operations (FLOPs). Experiments on the benchmark dataset of the Internatioanl Symposium on Biomedical Imaging (ISBI) challenge for segmentation of neuronal structures in electron microscopic (EM) stacks and the Medical Segmentation Decathlon (MSD) challenge brain tumor segmentation (BRATs) dataset show that the proposed model achieves comparable and sometimes better results compared to the current state-of-the-art.",,"This work was supported by the National Natural Science Foundation of China (61876010, 61806013, 61906005); Chaoyang Postdoctoral Foundation of Beijing (2019zz-35).",Diagnostics,,,2020-02-18,2020,2020-02-18,,10,2,110,All OA, Gold,Article,"Gadosey, Pius Kwao; Li, Yujian; Agyekum, Enock Adjei; Zhang, Ting; Liu, Zhaoying; Yamak, Peter T.; Essaf, Firdaous","Gadosey, Pius Kwao (Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China;, zhangting@bjut.edu.cn, (T.Z.);, zhaoying.liu@bjut.edu.cn, (Z.L.);, peteryamak@emails.bjut.edu.cn, (P.T.Y.);, firdaous.essaf@emails.bjut.edu.cn, (F.E.)); Li, Yujian (School of Artificial Intelligence, Guilin University of Electronic Technology, Guilin 541004, China;, liyujian@guet.edu.cn); Agyekum, Enock Adjei (College of Life Science and Bioengineering, Beijing University of Technology, Beijing 100124, China;, enockagyekum3@emails.bjut.edu.cn); Zhang, Ting (Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China;, zhangting@bjut.edu.cn, (T.Z.);, zhaoying.liu@bjut.edu.cn, (Z.L.);, peteryamak@emails.bjut.edu.cn, (P.T.Y.);, firdaous.essaf@emails.bjut.edu.cn, (F.E.)); Liu, Zhaoying (Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China;, zhangting@bjut.edu.cn, (T.Z.);, zhaoying.liu@bjut.edu.cn, (Z.L.);, peteryamak@emails.bjut.edu.cn, (P.T.Y.);, firdaous.essaf@emails.bjut.edu.cn, (F.E.)); Yamak, Peter T. (Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China;, zhangting@bjut.edu.cn, (T.Z.);, zhaoying.liu@bjut.edu.cn, (Z.L.);, peteryamak@emails.bjut.edu.cn, (P.T.Y.);, firdaous.essaf@emails.bjut.edu.cn, (F.E.)); Essaf, Firdaous (Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China;, zhangting@bjut.edu.cn, (T.Z.);, zhaoying.liu@bjut.edu.cn, (Z.L.);, peteryamak@emails.bjut.edu.cn, (P.T.Y.);, firdaous.essaf@emails.bjut.edu.cn, (F.E.))","Gadosey, Pius Kwao (Beijing University of Technology; )","Gadosey, Pius Kwao (Beijing University of Technology); Li, Yujian (Guilin University of Electronic Technology); Agyekum, Enock Adjei (Beijing University of Technology); Zhang, Ting (Beijing University of Technology); Liu, Zhaoying (Beijing University of Technology); Yamak, Peter T. (Beijing University of Technology); Essaf, Firdaous (Beijing University of Technology)",44,40,1.49,21.57,https://www.mdpi.com/2075-4418/10/2/110/pdf?version=1583040200,https://app.dimensions.ai/details/publication/pub.1125081196,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
5425,pub.1150454946,10.1016/j.media.2022.102596,36084564,PMC9400372,Distance-based detection of out-of-distribution silent failures for Covid-19 lung lesion segmentation,"Automatic segmentation of ground glass opacities and consolidations in chest computer tomography (CT) scans can potentially ease the burden of radiologists during times of high resource utilisation. However, deep learning models are not trusted in the clinical routine due to failing silently on out-of-distribution (OOD) data. We propose a lightweight OOD detection method that leverages the Mahalanobis distance in the feature space and seamlessly integrates into state-of-the-art segmentation pipelines. The simple approach can even augment pre-trained models with clinically relevant uncertainty quantification. We validate our method across four chest CT distribution shifts and two magnetic resonance imaging applications, namely segmentation of the hippocampus and the prostate. Our results show that the proposed method effectively detects far- and near-OOD samples across all explored scenarios.","This work was supported by the RACOON network under BMBF, Germany grant number [01KX2021]; and the Bundesministerium fÃ¼r Gesundheit (BMG), Germany with grant [ZMVI1-2520DAT03A].",,Medical Image Analysis,,"Humans; Male; COVID-19; Tomography, X-Ray Computed; Magnetic Resonance Imaging; Lung Diseases; Lung",2022-08-24,2022,2022-08-24,2022-11,82,,102596,All OA, Bronze,Article,"GonzÃ¡lez, Camila; Gotkowski, Karol; Fuchs, Moritz; Bucher, Andreas; Dadras, Armin; Fischbach, Ricarda; Kaltenborn, Isabel Jasmin; Mukhopadhyay, Anirban","GonzÃ¡lez, Camila (Darmstadt University of Technology, Karolinenplatz 5, 64289 Darmstadt, Germany. Electronic address: camila.gonzalez@gris.tu-darmstadt.de.); Gotkowski, Karol (Darmstadt University of Technology, Karolinenplatz 5, 64289 Darmstadt, Germany.); Fuchs, Moritz (Darmstadt University of Technology, Karolinenplatz 5, 64289 Darmstadt, Germany.); Bucher, Andreas (Uniklinik Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.); Dadras, Armin (Uniklinik Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.); Fischbach, Ricarda (Uniklinik Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.); Kaltenborn, Isabel Jasmin (Uniklinik Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.); Mukhopadhyay, Anirban (Darmstadt University of Technology, Karolinenplatz 5, 64289 Darmstadt, Germany.)","GonzÃ¡lez, Camila (TU Darmstadt)","GonzÃ¡lez, Camila (TU Darmstadt); Gotkowski, Karol (TU Darmstadt); Fuchs, Moritz (TU Darmstadt); Bucher, Andreas (); Dadras, Armin (); Fischbach, Ricarda (); Kaltenborn, Isabel Jasmin (); Mukhopadhyay, Anirban (TU Darmstadt)",1,1,,,https://doi.org/10.1016/j.media.2022.102596,https://app.dimensions.ai/details/publication/pub.1150454946,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
5367,pub.1134913082,10.1038/s41598-021-82017-6,33504883,PMC7841186,Methods and open-source toolkit for analyzing and visualizing challenge results,"Grand challenges have become the de facto standard for benchmarking image analysis algorithms. While the number of these international competitions is steadily increasing, surprisingly little effort has been invested in ensuring high quality design, execution and reporting for these international competitions. Specifically, results analysis and visualization in the event of uncertainties have been given almost no attention in the literature. Given these shortcomings, the contribution of this paper is two-fold: (1) we present a set of methods to comprehensively analyze and visualize the results of single-task and multi-task challenges and apply them to a number of simulated and real-life challenges to demonstrate their specific strengths and weaknesses; (2) we release the open-source framework challengeR as part of this work to enable fast and wide adoption of the methodology proposed in this paper. Our approach offers an intuitive way to gain important insights into the relative and absolute performance of algorithms, which cannot be revealed by commonly applied visualization techniques. This is demonstrated by the experiments performed in the specific context of biomedical image analysis challenges. Our framework could thus become an important tool for analyzing and visualizing challenge results in the field of biomedical image analysis and beyond.",Open Access funding enabled and organized by Projekt DEAL. We thank Dr. Jorge Bernal for constructive comments on an earlier version.,Open Access funding enabled and organized by Projekt DEAL. This work was supported by the Surgical Oncology Program of the National Center for Tumor Diseases (NCT) and the Helmholtz Association of German Research Centres in the scope of the Helmholtz Imaging Platform (HIP).,Scientific Reports,,,2021-01-27,2021,2021-01-27,,11,1,2369,All OA, Gold,Article,"Wiesenfarth, Manuel; Reinke, Annika; Landman, Bennett A.; Eisenmann, Matthias; Saiz, Laura Aguilera; Cardoso, M. Jorge; Maier-Hein, Lena; Kopp-Schneider, Annette","Wiesenfarth, Manuel (Division of Biostatistics, German Cancer Research Center (DKFZ), Im Neuenheimer Feld 581, 69120, Heidelberg, Germany); Reinke, Annika (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Im Neuenheimer Feld 223, 69120, Heidelberg, Germany); Landman, Bennett A. (Electrical Engineering, Vanderbilt University, 37235-1679, Nashville, TN, USA); Eisenmann, Matthias (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Im Neuenheimer Feld 223, 69120, Heidelberg, Germany); Saiz, Laura Aguilera (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Im Neuenheimer Feld 223, 69120, Heidelberg, Germany); Cardoso, M. Jorge (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, WC2R 2LS, London, UK); Maier-Hein, Lena (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Im Neuenheimer Feld 223, 69120, Heidelberg, Germany); Kopp-Schneider, Annette (Division of Biostatistics, German Cancer Research Center (DKFZ), Im Neuenheimer Feld 581, 69120, Heidelberg, Germany)","Wiesenfarth, Manuel (German Cancer Research Center); Maier-Hein, Lena (German Cancer Research Center)","Wiesenfarth, Manuel (German Cancer Research Center); Reinke, Annika (German Cancer Research Center); Landman, Bennett A. (Vanderbilt University); Eisenmann, Matthias (German Cancer Research Center); Saiz, Laura Aguilera (German Cancer Research Center); Cardoso, M. Jorge (King's College London); Maier-Hein, Lena (German Cancer Research Center); Kopp-Schneider, Annette (German Cancer Research Center)",34,31,6.16,31.21,https://www.nature.com/articles/s41598-021-82017-6.pdf,https://app.dimensions.ai/details/publication/pub.1134913082,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
5199,pub.1139216682,10.1038/s41598-021-93030-0,34188157,PMC8242021,Medical imaging deep learning with differential privacy,"The successful training of deep learning models for diagnostic deployment in medical imaging applications requires large volumes of data. Such data cannot be procured without consideration for patient privacy, mandated both by legal regulations and ethical requirements of the medical profession. Differential privacy (DP) enables the provision of information-theoretic privacy guarantees to patients and can be implemented in the setting of deep neural network training through the differentially private stochastic gradient descent (DP-SGD) algorithm. We here present deepee, a free-and-open-source framework for differentially private deep learning for use with the PyTorch deep learning framework. Our framework is based on parallelised execution of neural network operations to obtain and modify the per-sample gradients. The process is efficiently abstracted via a data structure maintaining shared memory references to neural network weights to maintain memory efficiency. We furthermore offer specialised data loading procedures and privacy budget accounting based on the Gaussian Differential Privacy framework, as well as automated modification of the user-supplied neural network architectures to ensure DP-conformity of its layers. We benchmark our frameworkâs computational performance against other open-source DP frameworks and evaluate its application on the paediatric pneumonia dataset, an image classification task and on the Medical Segmentation Decathlon Liver dataset in the task of medical image segmentation. We find that neural network training with rigorous privacy guarantees is possible while maintaining acceptable classification performance and excellent segmentation performance. Our framework compares favourably to related work with respect to memory consumption and computational performance. Our work presents an open-source software framework for differentially private deep learning, which we demonstrate in medical imaging analysis tasks. It serves to further the utilisation of privacy-enhancing techniques in medicine and beyond in order to assist researchers and practitioners in addressing the numerous outstanding challenges towards their widespread implementation.",This work was supported by the UK Research and Innovation London Medical Imaging &amp, Artificial Intelligence Centre for Value Based Healthcare (G.K. and D.R.) and the Technical University Munich/Imperial College London Joint Academy of Doctoral Studies (D.U.).,Open Access funding enabled and organized by Projekt DEAL.,Scientific Reports,,,2021-06-29,2021,2021-06-29,,11,1,13524,All OA, Gold,Article,"Ziller, Alexander; Usynin, Dmitrii; Braren, Rickmer; Makowski, Marcus; Rueckert, Daniel; Kaissis, Georgios","Ziller, Alexander (Institute for Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; Artificial Intelligence in Medicine and Healthcare, Technical University of Munich, Munich, Germany; OpenMined, Oxford, UK); Usynin, Dmitrii (Institute for Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; Artificial Intelligence in Medicine and Healthcare, Technical University of Munich, Munich, Germany; Department of Computing, Imperial College, London, UK; OpenMined, Oxford, UK); Braren, Rickmer (Institute for Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany); Makowski, Marcus (Institute for Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany); Rueckert, Daniel (Artificial Intelligence in Medicine and Healthcare, Technical University of Munich, Munich, Germany; Department of Computing, Imperial College, London, UK); Kaissis, Georgios (Institute for Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; Artificial Intelligence in Medicine and Healthcare, Technical University of Munich, Munich, Germany; Department of Computing, Imperial College, London, UK; OpenMined, Oxford, UK)","Kaissis, Georgios (Rechts der Isar Hospital; Technical University of Munich; Technical University of Munich; Imperial College London; )","Ziller, Alexander (Rechts der Isar Hospital; Technical University of Munich; Technical University of Munich); Usynin, Dmitrii (Rechts der Isar Hospital; Technical University of Munich; Technical University of Munich; Imperial College London); Braren, Rickmer (Rechts der Isar Hospital; Technical University of Munich); Makowski, Marcus (Rechts der Isar Hospital; Technical University of Munich); Rueckert, Daniel (Technical University of Munich; Imperial College London); Kaissis, Georgios (Rechts der Isar Hospital; Technical University of Munich; Technical University of Munich; Imperial College London)",28,28,1.25,22.92,https://www.nature.com/articles/s41598-021-93030-0.pdf,https://app.dimensions.ai/details/publication/pub.1139216682,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,
5100,pub.1148458143,10.1109/tmi.2022.3180435,35666795,,Benchmarking of Deep Architectures for Segmentation of Medical Images,"In recent years, there were many suggestions regarding modifications of the well-known U-Net architecture in order to improve its performance. The central motivation of this work is to provide a fair comparison of U-Net and its five extensions using identical conditions to disentangle the influence of model architecture, model training, and parameter settings on the performance of a trained model. For this purpose each of these six segmentation architectures is trained on the same nine data sets. The data sets are selected to cover various imaging modalities (X-rays, computed tomography, magnetic resonance imaging), single- and multi-class segmentation problems, and single- and multi-modal inputs. During the training, it is ensured that the data preprocessing, data set split into training, validation, and testing subsets, optimizer, learning rate change strategy, architecture depth, loss function, supervision and inference are exactly the same for all the architectures compared. Performance is evaluated in terms of Dice coefficient, surface Dice coefficient, average surface distance, Hausdorff distance, training, and prediction time. The main contribution of this experimental study is demonstrating that the architecture variants do not improve the quality of inference related to the basic U-Net architecture while resource demand rises.","The authors acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI Database used in this study.","This work was supported by the PLGrid Infrastructure [Academic Computer Centre (ACC) Cyfronet Akademia GÃ³rniczo-Hutnicza (AGH)]. The work of Daniel Gut was supported by the National Center for Research and Development (NCBR), Poland, under Grant POIR.01.01.01-00-1666/20.",IEEE Transactions on Medical Imaging,,"Deep Learning; Benchmarking; Tomography, X-Ray Computed; Magnetic Resonance Imaging; Image Processing, Computer-Assisted",2022-10-27,2022,2022-10-27,2022-11,41,11,3231-3241,All OA, Hybrid,Article,"Gut, Daniel; Tabor, ZbisÅaw; Szymkowski, Mateusz; Rozynek, MiÅosz; KucybaÅa, Iwona; Wojciechowski, Wadim","Gut, Daniel (Department of Biocybernetics and Biomedical Engineering, AGH University of Science and Technology, 30-059, KrakÃ³w, Poland); Tabor, ZbisÅaw (Department of Biocybernetics and Biomedical Engineering, AGH University of Science and Technology, 30-059, KrakÃ³w, Poland); Szymkowski, Mateusz (Department of Biocybernetics and Biomedical Engineering, AGH University of Science and Technology, 30-059, KrakÃ³w, Poland); Rozynek, MiÅosz (Department of Radiology, Jagiellonian University Medical College, 31-501, KrakÃ³w, Poland); KucybaÅa, Iwona (Department of Radiology, Jagiellonian University Medical College, 31-501, KrakÃ³w, Poland); Wojciechowski, Wadim (Department of Radiology, Jagiellonian University Medical College, 31-501, KrakÃ³w, Poland)","Gut, Daniel (AGH University of Science and Technology)","Gut, Daniel (AGH University of Science and Technology); Tabor, ZbisÅaw (AGH University of Science and Technology); Szymkowski, Mateusz (AGH University of Science and Technology); Rozynek, MiÅosz (Jagiellonian University); KucybaÅa, Iwona (Jagiellonian University); Wojciechowski, Wadim (Jagiellonian University)",0,0,,,https://ieeexplore.ieee.org/ielx7/42/9931396/09789098.pdf,https://app.dimensions.ai/details/publication/pub.1148458143,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5096,pub.1127896065,10.1016/s2589-7500(20)30078-9,33328124,,Deep learning to distinguish pancreatic cancer tissue from non-cancerous pancreatic tissue: a retrospective study with cross-racial external validation,"BACKGROUND: The diagnostic performance of CT for pancreatic cancer is interpreter-dependent, and approximately 40% of tumours smaller than 2 cm evade detection. Convolutional neural networks (CNNs) have shown promise in image analysis, but the networks' potential for pancreatic cancer detection and diagnosis is unclear. We aimed to investigate whether CNN could distinguish individuals with and without pancreatic cancer on CT, compared with radiologist interpretation.
METHODS: In this retrospective, diagnostic study, contrast-enhanced CT images of 370 patients with pancreatic cancer and 320 controls from a Taiwanese centre were manually labelled and randomly divided for training and validation (295 patients with pancreatic cancer and 256 controls) and testing (75 patients with pancreatic cancer and 64 controls; local test set 1). Images were preprocessed into patches, and a CNN was trained to classify patches as cancerous or non-cancerous. Individuals were classified as with or without pancreatic cancer on the basis of the proportion of patches diagnosed as cancerous by the CNN, using a cutoff determined using the training and validation set. The CNN was further tested with another local test set (101 patients with pancreatic cancers and 88 controls; local test set 2) and a US dataset (281 pancreatic cancers and 82 controls). Radiologist reports of pancreatic cancer images in the local test sets were retrieved for comparison.
FINDINGS: Between Jan 1, 2006, and Dec 31, 2018, we obtained CT images. In local test set 1, CNN-based analysis had a sensitivity of 0Â·973, specificity of 1Â·000, and accuracy of 0Â·986 (area under the curve [AUC] 0Â·997 (95% CI 0Â·992-1Â·000). In local test set 2, CNN-based analysis had a sensitivity of 0Â·990, specificity of 0Â·989, and accuracy of 0Â·989 (AUC 0Â·999 [0Â·998-1Â·000]). In the US test set, CNN-based analysis had a sensitivity of 0Â·790, specificity of 0Â·976, and accuracy of 0Â·832 (AUC 0Â·920 [0Â·891-0Â·948)]. CNN-based analysis achieved higher sensitivity than radiologists did (0Â·983 vs 0Â·929, difference 0Â·054 [95% CI 0Â·011-0Â·098]; p=0Â·014) in the two local test sets combined. CNN missed three (1Â·7%) of 176 pancreatic cancers (1Â·1-1Â·2 cm). Radiologists missed 12 (7%) of 168 pancreatic cancers (1Â·0-3Â·3 cm), of which 11 (92%) were correctly classified using CNN. The sensitivity of CNN for tumours smaller than 2 cm was 92Â·1% in the local test sets and 63Â·1% in the US test set.
INTERPRETATION: CNN could accurately distinguish pancreatic cancer on CT, with acceptable generalisability to images of patients from various races and ethnicities. CNN could supplement radiologist interpretation.
FUNDING: Taiwan Ministry of Science and Technology.","Acknowledgments This study was supported by Ministry of Science and Technology (MOST; MOST 107-2634-F-002-014-, MOST 107-2634-F-002-016-, MOST 108-2634-F-002-011-, MOST 108-2634-F-002-013-, and MOST 109-2634-F-002-028-), MOST All vista Healthcare Sub-center, and National Center for Theoretical Sciences Mathematics Division. The authors thank Nico Wu for assistance in data management and preprocessing, and Huihsuan Yen, Yenting Lin, and Pochuan Wang for technical support.","The funder of this study had no role in study design, data collection, data analysis, data interpretation, or writing of the report. All authors had full access to all the data in the study, and W-CL and WW had final responsibility for the decision to submit for publication.",The Lancet Digital Health,,"Aged; Contrast Media; Deep Learning; Diagnosis, Differential; Female; Humans; Male; Middle Aged; Pancreas; Pancreatic Neoplasms; Racial Groups; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Retrospective Studies; Sensitivity and Specificity; Taiwan; Tomography, X-Ray Computed",2020-06,2020,,2020-06,2,6,e303-e313,All OA, Gold,Article,"Liu, Kao-Lang; Wu, Tinghui; Chen, Po-Ting; Tsai, Yuhsiang M; Roth, Holger; Wu, Ming-Shiang; Liao, Wei-Chih; Wang, Weichung","Liu, Kao-Lang (Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan; Department of Medical Imaging, National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan); Wu, Tinghui (Institute of Applied Mathematical Sciences, National Taiwan University, Taipei, Taiwan); Chen, Po-Ting (Department of Medical Imaging, National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan); Tsai, Yuhsiang M (Institute of Applied Mathematical Sciences, National Taiwan University, Taipei, Taiwan); Roth, Holger (NVIDIA, Bethesda, MD, USA); Wu, Ming-Shiang (Division of Gastroenterology and Hepatology, Department of Internal Medicine, National Taiwan University Hospital, National Taiwan University, Taipei, Taiwan; Internal Medicine, College of Medicine, National Taiwan University, Taipei, Taiwan); Liao, Wei-Chih (Division of Gastroenterology and Hepatology, Department of Internal Medicine, National Taiwan University Hospital, National Taiwan University, Taipei, Taiwan; Internal Medicine, College of Medicine, National Taiwan University, Taipei, Taiwan); Wang, Weichung (Institute of Applied Mathematical Sciences, National Taiwan University, Taipei, Taiwan)","Liao, Wei-Chih (National Taiwan University; National Taiwan University Hospital; National Taiwan University); Wang, Weichung (National Taiwan University)","Liu, Kao-Lang (National Taiwan University; National Taiwan University Hospital; National Taiwan University); Wu, Tinghui (National Taiwan University); Chen, Po-Ting (National Taiwan University Hospital; National Taiwan University); Tsai, Yuhsiang M (National Taiwan University); Roth, Holger (Nvidia (United States)); Wu, Ming-Shiang (National Taiwan University; National Taiwan University Hospital; National Taiwan University); Liao, Wei-Chih (National Taiwan University; National Taiwan University Hospital; National Taiwan University); Wang, Weichung (National Taiwan University)",81,75,6.83,41.44,http://www.thelancet.com/article/S2589750020300789/pdf,https://app.dimensions.ai/details/publication/pub.1127896065,42 Health Sciences, 4203 Health Services and Systems,,,,,,,,,,
4912,pub.1127526799,10.1109/jbhi.2020.2994114,32406848,,Discriminative Feature Network Based on a Hierarchical Attention Mechanism for Semantic Hippocampus Segmentation,"The morphological analysis of hippocampus is vital to various neurological studies including brain disorders and brain anatomy. To assist doctors in analyzing the shape and volume of the hippocampus, an accurate and automatic hippocampus segmentation method is highly demanded in the clinical practice. Given that fully convolutional networks (FCNs) have made significant contributions in biomedical image segmentation applications, we propose a notably discriminative feature network based on a hierarchical attention mechanism in hippocampal segmentation. First, considering the problem that the hippocampus is a rather small part in MR images, we design a context-aware high-level feature extraction module (CHFEM) to extract high-level features of scale invariance in the encoder stage. Further, we introduce a hierarchical attention mechanism into our segmentation framework. The mechanism is divided into three parts: a low-level feature spatial attention module (LFSAM) is developed to learn the spatial relationship between different pixels on each channel in the low-level stage of the encoder, a high-level feature channel attention module (HFCAM) is to model the semantic information relationship on different channel images in the high-level stage of the encoder, and a cross-connected attention module (CCAM) is designed in the decoder part to further suppress the noisy boundaries of hippocampus and simultaneously utilize the attentional low-level features from the encoder to better guide the high-level hippocampus edge segmentation in the decoder phase. The proposed approach achieves outstanding performance on the ADNI dataset and the Decathlon dataset compared with other semantic segmentation models and existing hippocampal segmentation approaches. Source code is available at https://github.com/LannyShi/Hippocampal-segmentation.","This work was supported in part by the Zhejiang Provincial Public Welfare Technology Research Project (No. LGF18F020007), in part by the National Natural Science Foundation of China (No. 61762078), and in part by the Ningbo Municipal Natural Science Foundation of China under Grants 2018A610057 and 2018A610163.",,IEEE Journal of Biomedical and Health Informatics,,"Brain; Hippocampus; Image Processing, Computer-Assisted; Neural Networks, Computer; Semantics",2021-02-05,2021,2021-02-05,2021-02,25,2,504-513,Closed,Article,"Shi, Jiali; Zhang, Rong; Guo, Lijun; Gao, Linlin; Ma, Huifang; Wang, Jianhua","Shi, Jiali (Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo Zhejiang, 315000, China); Zhang, Rong (Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo Zhejiang, 315000, China); Guo, Lijun (Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo Zhejiang, 315000, China); Gao, Linlin (Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo Zhejiang, 315000, China); Ma, Huifang (College of Computer Science and Engineering, Northwest Normal University, Lanzhou Gansu, 730000, China); Wang, Jianhua (Department of Radiology, The Affiliated Hospital of Medicine School of Ningbo University, Ningbo Zhejiang, 315000, China)","Zhang, Rong (Ningbo University)","Shi, Jiali (Ningbo University); Zhang, Rong (Ningbo University); Guo, Lijun (Ningbo University); Gao, Linlin (Ningbo University); Ma, Huifang (Northwest Normal University); Wang, Jianhua (Ningbo University)",7,7,0.9,5.42,,https://app.dimensions.ai/details/publication/pub.1127526799,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
4846,pub.1151028153,10.3389/fnins.2022.1009581,36188458,PMC9521364,A medical image segmentation method based on multi-dimensional statistical features,"Medical image segmentation has important auxiliary significance for clinical diagnosis and treatment. Most of existing medical image segmentation solutions adopt convolutional neural networks (CNNs). Althought these existing solutions can achieve good image segmentation performance, CNNs focus on local information and ignore global image information. Since Transformer can encode the whole image, it has good global modeling ability and is effective for the extraction of global information. Therefore, this paper proposes a hybrid feature extraction network, into which CNNs and Transformer are integrated to utilize their advantages in feature extraction. To enhance low-dimensional texture features, this paper also proposes a multi-dimensional statistical feature extraction module to fully fuse the features extracted by CNNs and Transformer and enhance the segmentation performance of medical images. The experimental results confirm that the proposed method achieves better results in brain tumor segmentation and ventricle segmentation than state-of-the-art solutions.",,"This research was sponsored by the China Postdoctoral Science Foundation (2020M670111ZX), Chongqing medical scientific research project (Joint project of Chongqing Health Commission and Science and Technology Bureau, 2020GDRC019 and 2022MSXM184), Natural Science Foundation of Chongqing (cstc2020jcyj-bshX0068), and Special Fund for Young and Middle-aged Medical Top Talents of Chongqing (ZQNYXGDRCGZS2019005).",Frontiers in Neuroscience,,,2022-09-15,2022,2022-09-15,,16,,1009581,All OA, Gold,Article,"Xu, Yang; He, Xianyu; Xu, Guofeng; Qi, Guanqiu; Yu, Kun; Yin, Li; Yang, Pan; Yin, Yuehui; Chen, Hao","Xu, Yang (College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China); He, Xianyu (College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China); Xu, Guofeng (College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China); Qi, Guanqiu (Department of Computer Information Systems, Buffalo State College, Buffalo, NY, United States); Yu, Kun (College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China); Yin, Li (Chongqing Key Laboratory of Translational Research of Cancer Metastasis and Individualized Treatment, Chongqing University Cancer Hospital, Chongqing, China); Yang, Pan (Department of Cardiovascular Surgery, Chongqing General Hospital, University of Chinese Academy of Sciences, Chongqing, China; Department of Emergency, The Second Affiliated Hospital of Chongqing Medical University, Chongqing, China); Yin, Yuehui (Department of Cardiology, The Second Affiliated Hospital of Chongqing Medical University, Chongqing, China); Chen, Hao (Department of Cardiovascular Surgery, Chongqing General Hospital, University of Chinese Academy of Sciences, Chongqing, China)","Yin, Li (Chongqing University); Yin, Yuehui (Second Affiliated Hospital of Chongqing Medical University); Chen, Hao (Chongqing General Hospital)","Xu, Yang (Chongqing University of Posts and Telecommunications); He, Xianyu (Chongqing University of Posts and Telecommunications); Xu, Guofeng (Chongqing University of Posts and Telecommunications); Qi, Guanqiu (Buffalo State College); Yu, Kun (Chongqing University of Posts and Telecommunications); Yin, Li (Chongqing University); Yang, Pan (Chongqing General Hospital; Second Affiliated Hospital of Chongqing Medical University); Yin, Yuehui (Second Affiliated Hospital of Chongqing Medical University); Chen, Hao (Chongqing General Hospital)",4,4,,,https://www.frontiersin.org/articles/10.3389/fnins.2022.1009581/pdf,https://app.dimensions.ai/details/publication/pub.1151028153,32 Biomedical and Clinical Sciences, 3209 Neurosciences, 52 Psychology, 5202 Biological Psychology,,,,,,,,
4724,pub.1149390782,10.3390/diagnostics12071690,35885594,PMC9324146,Exploiting the Dixon Method for a Robust Breast and Fibro-Glandular Tissue Segmentation in Breast MRI,"Automatic breast and fibro-glandular tissue (FGT) segmentation in breast MRI allows for the efficient and accurate calculation of breast density. The U-Net architecture, either 2D or 3D, has already been shown to be effective at addressing the segmentation problem in breast MRI. However, the lack of publicly available datasets for this task has forced several authors to rely on internal datasets composed of either acquisitions without fat suppression (WOFS) or with fat suppression (FS), limiting the generalization of the approach. To solve this problem, we propose a data-centric approach, efficiently using the data available. By collecting a dataset of T1-weighted breast MRI acquisitions acquired with the use of the Dixon method, we train a network on both T1 WOFS and FS acquisitions while utilizing the same ground truth segmentation. Using the ""plug-and-play"" framework nnUNet, we achieve, on our internal test set, a Dice Similarity Coefficient (DSC) of 0.96 and 0.91 for WOFS breast and FGT segmentation and 0.95 and 0.86 for FS breast and FGT segmentation, respectively. On an external, publicly available dataset, a panel of breast radiologists rated the quality of our automatic segmentation with an average of 3.73 on a four-point scale, with an average percentage agreement of 67.5%.",The authors would like to kindly thank Linda Appelman for taking part in the reader study conducted for this manuscript.,"This research was funded by EU EFRO OP-Oost Nederland, project name: MARBLE.",Diagnostics,,,2022-07-11,2022,2022-07-11,,12,7,1690,All OA, Gold,Article,"Samperna, Riccardo; Moriakov, Nikita; Karssemeijer, Nico; Teuwen, Jonas; Mann, Ritse M.","Samperna, Riccardo (Department of Medical Imaging, Radboudumc, 6525 GA Nijmegen, The Netherlands;, n.moriakov@nki.nl, (N.M.);, nico.karssemeijer@gmail.com, (N.K.);, j.teuwen@nki.nl, (J.T.);, ritse.mann@radboudumc.nl, (R.M.M.); Department of Radiology, The Netherlands Cancer Institute (NKI), 1066 CX Amsterdam, The Netherlands); Moriakov, Nikita (Department of Medical Imaging, Radboudumc, 6525 GA Nijmegen, The Netherlands;, n.moriakov@nki.nl, (N.M.);, nico.karssemeijer@gmail.com, (N.K.);, j.teuwen@nki.nl, (J.T.);, ritse.mann@radboudumc.nl, (R.M.M.); Department of Radiation Oncology, The Netherlands Cancer Institute (NKI), 1066 CX Amsterdam, The Netherlands); Karssemeijer, Nico (Department of Medical Imaging, Radboudumc, 6525 GA Nijmegen, The Netherlands;, n.moriakov@nki.nl, (N.M.);, nico.karssemeijer@gmail.com, (N.K.);, j.teuwen@nki.nl, (J.T.);, ritse.mann@radboudumc.nl, (R.M.M.); ScreenPoint Medical BV, 6525 EC Nijmegen, The Netherlands); Teuwen, Jonas (Department of Medical Imaging, Radboudumc, 6525 GA Nijmegen, The Netherlands;, n.moriakov@nki.nl, (N.M.);, nico.karssemeijer@gmail.com, (N.K.);, j.teuwen@nki.nl, (J.T.);, ritse.mann@radboudumc.nl, (R.M.M.); Department of Radiation Oncology, The Netherlands Cancer Institute (NKI), 1066 CX Amsterdam, The Netherlands); Mann, Ritse M. (Department of Medical Imaging, Radboudumc, 6525 GA Nijmegen, The Netherlands;, n.moriakov@nki.nl, (N.M.);, nico.karssemeijer@gmail.com, (N.K.);, j.teuwen@nki.nl, (J.T.);, ritse.mann@radboudumc.nl, (R.M.M.); Department of Radiology, The Netherlands Cancer Institute (NKI), 1066 CX Amsterdam, The Netherlands)","Samperna, Riccardo (Radboud University Nijmegen Medical Centre; ; Antoni van Leeuwenhoek Hospital)","Samperna, Riccardo (Radboud University Nijmegen Medical Centre; Antoni van Leeuwenhoek Hospital); Moriakov, Nikita (Radboud University Nijmegen Medical Centre; Antoni van Leeuwenhoek Hospital); Karssemeijer, Nico (Radboud University Nijmegen Medical Centre); Teuwen, Jonas (Radboud University Nijmegen Medical Centre; Antoni van Leeuwenhoek Hospital); Mann, Ritse M. (Radboud University Nijmegen Medical Centre; Antoni van Leeuwenhoek Hospital)",0,0,,,https://www.mdpi.com/2075-4418/12/7/1690/pdf?version=1658742556,https://app.dimensions.ai/details/publication/pub.1149390782,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
4696,pub.1134408868,10.3390/healthcare9010054,33419018,PMC7825313,TMD-Unet: Triple-Unet with Multi-Scale Input Features and Dense Skip Connection for Medical Image Segmentation,"Deep learning is one of the most effective approaches to medical image processing applications. Network models are being studied more and more for medical image segmentation challenges. The encoder-decoder structure is achieving great success, in particular the Unet architecture, which is used as a baseline architecture for the medical image segmentation networks. Traditional Unet and Unet-based networks still have a limitation that is not able to fully exploit the output features of the convolutional units in the node. In this study, we proposed a new network model named TMD-Unet, which had three main enhancements in comparison with Unet: (1) modifying the interconnection of the network node, (2) using dilated convolution instead of the standard convolution, and (3) integrating the multi-scale input features on the input side of the model and applying a dense skip connection instead of a regular skip connection. Our experiments were performed on seven datasets, including many different medical image modalities such as colonoscopy, electron microscopy (EM), dermoscopy, computed tomography (CT), and magnetic resonance imaging (MRI). The segmentation applications implemented in the paper include EM, nuclei, polyp, skin lesion, left atrium, spleen, and liver segmentation. The dice score of our proposed models achieved 96.43% for liver segmentation, 95.51% for spleen segmentation, 92.65% for polyp segmentation, 94.11% for EM segmentation, 92.49% for nuclei segmentation, 91.81% for left atrium segmentation, and 87.27% for skin lesion segmentation. The experimental results showed that the proposed model was superior to the popular models for all seven applications, which demonstrates the high generality of the proposed model.",The technical supports of the computer center of Feng Chia University on GPU resources is acknowledged.,This research received no external funding.,Healthcare,,,2021-01-06,2021,2021-01-06,,9,1,54,All OA, Gold,Article,"Tran, Song-Toan; Cheng, Ching-Hwa; Nguyen, Thanh-Tuan; Le, Minh-Hai; Liu, Don-Gey","Tran, Song-Toan (Program of Electrical and Communications Engineering, Feng Chia University, Taichung 40724, Taiwan;, nttuan@kgc.edu.vn, (T.-T.N.);, lmhai@tvu.edu.vn, (M.-H.L.);, dgliu@fcu.edu.tw, (D.-G.L.); Department of Electrical and Electronics, Tra Vinh University, Tra Vinh 87000, Vietnam); Cheng, Ching-Hwa (Department of Electronic Engineering, Feng Chia University, Taichung 40724, Taiwan;, chengch@fcu.edu.tw); Nguyen, Thanh-Tuan (Program of Electrical and Communications Engineering, Feng Chia University, Taichung 40724, Taiwan;, nttuan@kgc.edu.vn, (T.-T.N.);, lmhai@tvu.edu.vn, (M.-H.L.);, dgliu@fcu.edu.tw, (D.-G.L.)); Le, Minh-Hai (Program of Electrical and Communications Engineering, Feng Chia University, Taichung 40724, Taiwan;, nttuan@kgc.edu.vn, (T.-T.N.);, lmhai@tvu.edu.vn, (M.-H.L.);, dgliu@fcu.edu.tw, (D.-G.L.); Department of Electrical and Electronics, Tra Vinh University, Tra Vinh 87000, Vietnam); Liu, Don-Gey (Program of Electrical and Communications Engineering, Feng Chia University, Taichung 40724, Taiwan;, nttuan@kgc.edu.vn, (T.-T.N.);, lmhai@tvu.edu.vn, (M.-H.L.);, dgliu@fcu.edu.tw, (D.-G.L.); Department of Electronic Engineering, Feng Chia University, Taichung 40724, Taiwan;, chengch@fcu.edu.tw)","Tran, Song-Toan (Feng Chia University; Tra Vinh University)","Tran, Song-Toan (Feng Chia University; Tra Vinh University); Cheng, Ching-Hwa (Feng Chia University); Nguyen, Thanh-Tuan (Feng Chia University); Le, Minh-Hai (Feng Chia University; Tra Vinh University); Liu, Don-Gey (Feng Chia University; Feng Chia University)",30,30,3.86,,https://www.mdpi.com/2227-9032/9/1/54/pdf?version=1609928914,https://app.dimensions.ai/details/publication/pub.1134408868,32 Biomedical and Clinical Sciences, 42 Health Sciences,,,,,,,,,,
4610,pub.1150855167,10.1007/s00330-022-09113-7,36074262,PMC9889463,Visual ensemble selection of deep convolutional neural networks for 3D segmentation of breast tumors on dynamic contrast enhanced MRI,"ObjectivesTo develop a visual ensemble selection of deep convolutional neural networks (CNN) for 3D segmentation of breast tumors using T1-weighted dynamic contrast-enhanced (T1-DCE) MRI.MethodsMulti-center 3D T1-DCE MRI (n = 141) were acquired for a cohort of patients diagnosed with locally advanced or aggressive breast cancer. Tumor lesions of 111 scans were equally divided between two radiologists and segmented for training. The additional 30 scans were segmented independently by both radiologists for testing. Three 3D U-Net models were trained using either post-contrast images or a combination of post-contrast and subtraction images fused at either the image or the feature level. Segmentation accuracy was evaluated quantitatively using the Dice similarity coefficient (DSC) and the Hausdorff distance (HD95) and scored qualitatively by a radiologist as excellent, useful, helpful, or unacceptable. Based on this score, a visual ensemble approach selecting the best segmentation among these three models was proposed.ResultsThe mean and standard deviation of DSC and HD95 between the two radiologists were equal to 77.8 Â± 10.0% and 5.2 Â± 5.9 mm. Using the visual ensemble selection, a DSC and HD95 equal to 78.1 Â± 16.2% and 14.1 Â± 40.8 mm was reached. The qualitative assessment was excellent (resp. excellent or useful) in 50% (resp. 77%).ConclusionUsing subtraction images in addition to post-contrast images provided complementary information for 3D segmentation of breast lesions by CNN. A visual ensemble selection allowing the radiologist to select the most optimal segmentation obtained by the three 3D U-Net models achieved comparable results to inter-radiologist agreement, yielding 77% segmented volumes considered excellent or useful.Key Pointsâ¢ Deep convolutional neural networks were developed using T1-weighted post-contrast and subtraction MRI to perform automated 3D segmentation of breast tumors.â¢ A visual ensemble selection allowing the radiologist to choose the best segmentation among the three 3D U-Net models outperformed each of the three models.â¢ The visual ensemble selection provided clinically useful segmentations in 77% of cases, potentially allowing for a valuable reduction of the manual 3D segmentation workload for the radiologist and greatly facilitating quantitative studies on non-invasive biomarker in breast MRI.",,"This study has received funding from the European Unionâs Horizon 2020 Research and Innovation Programme under the Marie Sklodowska-Curie grant agreement No 764458. The resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation - Flanders (FWO) and the Flemish Government. The authors thank NVIDIA Corporation for donating a Titan X GPU. Pia Akl thanks the Institut Curie for supporting her financially during her MSc (Bourse Curie M2 2018)",European Radiology,,"Humans; Female; Image Processing, Computer-Assisted; Neural Networks, Computer; Breast; Breast Neoplasms; Magnetic Resonance Imaging",2022-09-08,2022,2022-09-08,2023-02,33,2,959-969,All OA, Hybrid,Article,"Rahimpour, Masoomeh; Saint Martin, Marie-Judith; Frouin, FrÃ©dÃ©rique; Akl, Pia; Orlhac, Fanny; Koole, Michel; Malhaire, Caroline","Rahimpour, Masoomeh (Department of Imaging and Pathology, KU Leuven, Leuven, Belgium); Saint Martin, Marie-Judith (Laboratoire dâImagerie Translationnelle en Oncologie (LITO), U1288 Inserm, UniversitÃ© Paris-Saclay, Centre de Recherche de lâInstitut Curie, BÃ¢timent 101B Rue de la Chaufferie, 91400, Orsay, France); Frouin, FrÃ©dÃ©rique (Laboratoire dâImagerie Translationnelle en Oncologie (LITO), U1288 Inserm, UniversitÃ© Paris-Saclay, Centre de Recherche de lâInstitut Curie, BÃ¢timent 101B Rue de la Chaufferie, 91400, Orsay, France); Akl, Pia (Department of Radiology, HÃ´pital Femme MÃ¨re Enfant, Hospices civils de Lyon, Lyon, France); Orlhac, Fanny (Laboratoire dâImagerie Translationnelle en Oncologie (LITO), U1288 Inserm, UniversitÃ© Paris-Saclay, Centre de Recherche de lâInstitut Curie, BÃ¢timent 101B Rue de la Chaufferie, 91400, Orsay, France); Koole, Michel (Department of Imaging and Pathology, KU Leuven, Leuven, Belgium); Malhaire, Caroline (Laboratoire dâImagerie Translationnelle en Oncologie (LITO), U1288 Inserm, UniversitÃ© Paris-Saclay, Centre de Recherche de lâInstitut Curie, BÃ¢timent 101B Rue de la Chaufferie, 91400, Orsay, France; Department of Radiology, Ensemble Hospitalier de lâInstitut Curie, Paris, France)","Frouin, FrÃ©dÃ©rique (University of Paris-Saclay)","Rahimpour, Masoomeh (KU Leuven); Saint Martin, Marie-Judith (University of Paris-Saclay); Frouin, FrÃ©dÃ©rique (University of Paris-Saclay); Akl, Pia (Hospices Civils de Lyon); Orlhac, Fanny (University of Paris-Saclay); Koole, Michel (KU Leuven); Malhaire, Caroline (University of Paris-Saclay; Institute Curie)",2,2,,,https://link.springer.com/content/pdf/10.1007/s00330-022-09113-7.pdf,https://app.dimensions.ai/details/publication/pub.1150855167,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
4597,pub.1128801387,10.1016/j.media.2020.101766,32623276,,Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation,"Although having achieved great success in medical image segmentation, deep learning-based approaches usually require large amounts of well-annotated data, which can be extremely expensive in the field of medical image analysis. Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised learning and unsupervised domain adaptation both take the advantage of unlabeled data, and they are closely related to each other. In this paper, we propose uncertainty-aware multi-view co-training (UMCT), a unified framework that addresses these two tasks for volumetric medical image segmentation. Our framework is capable of efficiently utilizing unlabeled data for better performance. We firstly rotate and permute the 3D volumes into multiple views and train a 3D deep network on each view. We then apply co-training by enforcing multi-view consistency on unlabeled data, where an uncertainty estimation of each view is utilized to achieve accurate labeling. Experiments on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset show state-of-the-art performance of the proposed framework on semi-supervised medical image segmentation. Under unsupervised domain adaptation settings, we validate the effectiveness of this work by adapting our multi-organ segmentation model to two pathological organs from the Medical Segmentation Decathlon Datasets. Additionally, we show that our UMCT-DA model can even effectively handle the challenging situation where labeled source data is inaccessible, demonstrating strong potentials for real-world applications.",,,Medical Image Analysis,,Humans, Supervised Machine Learning, Uncertainty,2020-06-27,2020,2020-06-27,2020-10,65,,101766,All OA, Green,Article,"Xia, Yingda; Yang, Dong; Yu, Zhiding; Liu, Fengze; Cai, Jinzheng; Yu, Lequan; Zhu, Zhuotun; Xu, Daguang; Yuille, Alan; Roth, Holger","Xia, Yingda (Johns Hopkins Unversity, Baltimore, MD, 21218, USA.); Yang, Dong (NVIDIA Corporation, Bethesda, MD, 20814, USA.); Yu, Zhiding (NVIDIA Corporation, Bethesda, MD, 20814, USA.); Liu, Fengze (Johns Hopkins Unversity, Baltimore, MD, 21218, USA.); Cai, Jinzheng (University of Florida, Gainesville, FL, 32611, USA.); Yu, Lequan (The Chinese University of Hong Kong, Hong Kong, China.); Zhu, Zhuotun (Johns Hopkins Unversity, Baltimore, MD, 21218, USA.); Xu, Daguang (NVIDIA Corporation, Bethesda, MD, 20814, USA.); Yuille, Alan (Johns Hopkins Unversity, Baltimore, MD, 21218, USA.); Roth, Holger (NVIDIA Corporation, Bethesda, MD, 20814, USA. Electronic address: hroth@nvidia.com.)","Roth, Holger (Nvidia (United States))","Xia, Yingda (); Yang, Dong (Nvidia (United States)); Yu, Zhiding (Nvidia (United States)); Liu, Fengze (); Cai, Jinzheng (University of Florida); Yu, Lequan (Chinese University of Hong Kong); Zhu, Zhuotun (); Xu, Daguang (Nvidia (United States)); Yuille, Alan (); Roth, Holger (Nvidia (United States))",82,81,5.04,,http://arxiv.org/pdf/2006.16806,https://app.dimensions.ai/details/publication/pub.1128801387,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,
4589,pub.1140592380,10.1016/j.artmed.2021.102154,34531013,,EMONAS-Net: Efficient multiobjective neural architecture search using surrogate-assisted evolutionary algorithm for 3D medical image segmentation,"Deep learning plays a critical role in medical image segmentation. Nevertheless, manually designing a neural network for a specific segmentation problem is a very difficult and time-consuming task due to the massive hyperparameter search space, long training time and large volumetric data. Therefore, most designed networks are highly complex, task specific and over-parametrized. Recently, multiobjective neural architecture search (NAS) methods have been proposed to automate the design of accurate and efficient segmentation architectures. However, they only search for either the micro- or macro-structure of the architecture, do not use the information produced during the optimization process to increase the efficiency of the search, or do not consider the volumetric nature of medical images. In this work, we present EMONAS-Net, an Efficient MultiObjective NAS framework for 3D medical image segmentation that optimizes both the segmentation accuracy and size of the network. EMONAS-Net has two key components, a novel search space that considers the configuration of the micro- and macro-structure of the architecture and a Surrogate-assisted Multiobjective Evolutionary based Algorithm (SaMEA algorithm) that efficiently searches for the best hyperparameter values. The SaMEA algorithm uses the information collected during the initial generations of the evolutionary process to identify the most promising subproblems and select the best performing hyperparameter values during mutation to improve the convergence speed. Furthermore, a Random Forest surrogate model is incorporated to accelerate the fitness evaluation of the candidate architectures. EMONAS-Net is tested on the tasks of prostate segmentation from the MICCAI PROMISE12 challenge, hippocampus segmentation from the Medical Segmentation Decathlon challenge, and cardiac segmentation from the MICCAI ACDC challenge. In all the benchmarks, the proposed framework finds architectures that perform better or comparable with competing state-of-the-art NAS methods while being considerably smaller and reducing the architecture search time by more than 50%.",,"This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.",Artificial Intelligence in Medicine,,"Algorithms; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Male; Neural Networks, Computer",2021-08-24,2021,2021-08-24,2021-09,119,,102154,Closed,Article,"Baldeon Calisto, Maria; Lai-Yuen, Susana K","Baldeon Calisto, Maria (Departamento de IngenierÃ­a Industrial, Instituto de InnovaciÃ³n en Productividad y LogÃ­stica CATENA-USFQ, Colegio de Ciencias e IngenierÃ­a, Universidad San Francisco de Quito, Diego de Robles s/n y VÃ­a InteroceÃ¡nica, Quito 170901, Ecuador.); Lai-Yuen, Susana K (Department of Industrial and Management Systems Engineering, University of South Florida, 4202 E. Fowler Avenue, Tampa, FL 33620, USA. Electronic address: laiyuen@usf.edu.)","Lai-Yuen, Susana K (University of South Florida)","Baldeon Calisto, Maria (Universidad San Francisco de Quito); Lai-Yuen, Susana K (University of South Florida)",11,11,0.17,8.71,,https://app.dimensions.ai/details/publication/pub.1140592380,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,,
4387,pub.1132334859,10.1016/j.cmpb.2020.105818,33218708,,Fully-automated functional region annotation of liver via a 2.5D class-aware deep neural network with spatial adaptation,"BACKGROUND AND OBJECTIVE: Automatic functional region annotation of liver should be very useful for preoperative planning of liver resection in the clinical domain. However, many traditional computer-aided annotation methods based on anatomical landmarks or the vascular tree often fail to extract accurate liver segments. Furthermore, these methods are difficult to fully automate and thus remain time-consuming. To address these issues, in this study we aim to develop a fully-automated approach for functional region annotation of liver using deep learning based on 2.5D class-aware deep neural networks with spatial adaptation.
METHODS: 112 CT scans were fed into our 2.5D class-aware deep neural network with spatial adaptation for automatic functional region annotation of liver. The proposed model was built upon the ResU-net architecture, which adaptively selected a stack of adjacent CT slices as input and, generating masks corresponding to the center slice, automatically annotated the liver functional region from abdominal CT images. Furthermore, to minimize the problem of class-level ambiguity between different slices, the anatomy class-specific information was used.
RESULTS: The final algorithm performance for automatic functional region annotation of liver showed large overlap with that of manual reference segmentation. The dice similarity coefficient of hepatic segments achieved high scores and an average dice score of 0.882. The entire calculation time was quite fast (~5 s) compared to manual annotation (~2.5 hours).
CONCLUSION: The proposed models described in this paper offer a feasible solution for fully-automated functional region annotation of liver from CT images. The experimental results demonstrated that the proposed method can attain a high average dice score and low computational time. Therefore, this work should allow for improved liver surgical resection planning by our precise segmentation and simple fully-automated method.",This work was supported by the National Natural Science Foundation of China (No. 61901463 and 61871374) and Shenzhen Science and Technology Program of China grant JCYJ20170818160306270, Guangdong province key research and development areas grant 2020B1111140001.,,Computer Methods and Programs in Biomedicine,,"Algorithms; Image Processing, Computer-Assisted; Liver; Neural Networks, Computer; Tomography, X-Ray Computed",2020-11-04,2020,2020-11-04,2021-03,200,,105818,Closed,Article,"Tian, Yinli; Xue, Fei; Lambo, Ricardo; He, Jiahui; An, Chao; Xie, Yaoqin; Cao, Hailin; Qin, Wenjian","Tian, Yinli (School of Microelectronics and Communication Engineering, Chongqing University, Chongqing 400044, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China.); Xue, Fei (Osaka University, 1-1 Yamadaoka, Suita, Osaka 5650871, Japan.); Lambo, Ricardo (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China.); He, Jiahui (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China.); An, Chao (Department of Minimal invasive intervention, Sun Yat-sen University Cancer Center, Guangzhou 510060, China.); Xie, Yaoqin (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China.); Cao, Hailin (School of Microelectronics and Communication Engineering, Chongqing University, Chongqing 400044, China. Electronic address: hailincao@cqu.edu.cn.); Qin, Wenjian (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China. Electronic address: wj.qin@siat.ac.cn.)","Cao, Hailin (Chongqing University); Qin, Wenjian (Shenzhen Institutes of Advanced Technology)","Tian, Yinli (Chongqing University; Shenzhen Institutes of Advanced Technology); Xue, Fei (Osaka University); Lambo, Ricardo (Shenzhen Institutes of Advanced Technology); He, Jiahui (Shenzhen Institutes of Advanced Technology); An, Chao (Sun Yat-sen University Cancer Center); Xie, Yaoqin (Shenzhen Institutes of Advanced Technology); Cao, Hailin (Chongqing University); Qin, Wenjian (Shenzhen Institutes of Advanced Technology)",3,3,0.41,1.55,,https://app.dimensions.ai/details/publication/pub.1132334859,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
4378,pub.1152702900,10.1007/s10140-022-02099-1,36371579,,Toward automated interpretable AAST grading for blunt splenic injury,"BackgroundThe American Association for the Surgery of Trauma (AAST) splenic organ injury scale (OIS) is the most frequently used CT-based grading system for blunt splenic trauma. However, reported inter-rater agreement is modest, and an algorithm that objectively automates grading based on transparent and verifiable criteria could serve as a high-trust diagnostic aid.PurposeTo pilot the development of an automated interpretable multi-stage deep learning-based system to predict AAST grade from admission trauma CT.MethodsOur pipeline includes 4 parts: (1) automated splenic localization, (2) Faster R-CNN-based detection of pseudoaneurysms (PSA) and active bleeds (AB), (3) nnU-Net segmentation and quantification of splenic parenchymal disruption (SPD), and (4) a directed graph that infers AAST grades from detection and segmentation results. Training and validation is performed on a dataset of adult patients (ageââ¥â18) with voxelwise labeling, consensus AAST grading, and hemorrhage-related outcome data (nâ=â174).ResultsAAST classification agreement (weighted Îº) between automated and consensus AAST grades was substantial (0.79). High-grade (IV and V) injuries were predicted with accuracy, positive predictive value, and negative predictive value of 92%, 95%, and 89%. The area under the curve for predicting hemorrhage control intervention was comparable between expert consensus and automated AAST grading (0.83 vs 0.88). The mean combined inference time for the pipeline was 96.9Â s.ConclusionsThe results of our method were rapid and verifiable, with high agreement between automated and expert consensus grades. Diagnosis of high-grade lesions and prediction of hemorrhage control intervention produced accurate results in adult patients.",,"NIH K08 EB027141-01A1 (PI: David Dreizin, MD).",Emergency Radiology,,"Adult; Humans; United States; Tomography, X-Ray Computed; Predictive Value of Tests; Wounds, Nonpenetrating; Spleen; Hemorrhage; Retrospective Studies",2022-11-12,2022,2022-11-12,2023-02,30,1,41-50,Closed,Article,"Chen, Haomin; Unberath, Mathias; Dreizin, David","Chen, Haomin (Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA); Unberath, Mathias (Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA); Dreizin, David (Emergency and Trauma Imaging, Department of Diagnostic Radiology and Nuclear Medicine, R Adams Cowley Shock Trauma Center, University of Maryland School of Medicine, Baltimore, MD, USA)","Dreizin, David (University of Maryland, Baltimore)","Chen, Haomin (Johns Hopkins University); Unberath, Mathias (Johns Hopkins University); Dreizin, David (University of Maryland, Baltimore)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152702900,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,3 Good Health and Well Being,,,,,,,,,,
4317,pub.1146308140,10.1016/j.media.2022.102420,35334445,,Towards bi-directional skip connections in encoder-decoder architectures and beyond,"U-Net, as an encoder-decoder architecture with forward skip connections, has achieved promising results in various medical image analysis tasks. Many recent approaches have also extended U-Net with more complex building blocks, which typically increase the number of network parameters considerably. Such complexity makes the inference stage highly inefficient for clinical applications. Towards an effective yet economic segmentation network design, in this work, we propose backward skip connections that bring decoded features back to the encoder. Our design can be jointly adopted with forward skip connections in any encoder-decoder architecture forming a recurrence structure without introducing extra parameters. With the backward skip connections, we propose a U-Net based network family, namely Bi-directional O-shape networks, which set new benchmarks on multiple public medical imaging segmentation datasets. On the other hand, with the most plain architecture (BiO-Net), network computations inevitably increase along with the pre-set recurrence time. We have thus studied the deficiency bottleneck of such recurrent design and propose a novel two-phase Neural Architecture Search (NAS) algorithm, namely BiX-NAS, to search for the best multi-scale bi-directional skip connections. The ineffective skip connections are then discarded to reduce computational costs and speed up network inference. The finally searched BiX-Net yields the least network complexity and outperforms other state-of-the-art counterparts by large margins. We evaluate our methods on both 2D and 3D segmentation tasks in a total of six datasets. Extensive ablation studies have also been conducted to provide a comprehensive analysis for our proposed methods.",,,Medical Image Analysis,,"Algorithms; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer",2022-03-16,2022,2022-03-16,2022-05,78,,102420,All OA, Green,Article,"Xiang, Tiange; Zhang, Chaoyi; Wang, Xinyi; Song, Yang; Liu, Dongnan; Huang, Heng; Cai, Weidong","Xiang, Tiange (School of Computer Science, University of Sydney, Australia. Electronic address: txia7609@uni.sydney.edu.au.); Zhang, Chaoyi (School of Computer Science, University of Sydney, Australia.); Wang, Xinyi (School of Computer Science, University of Sydney, Australia.); Song, Yang (School of Computer Science and Engineering, University of New South Wales, Australia.); Liu, Dongnan (School of Computer Science, University of Sydney, Australia.); Huang, Heng (Department of Electrical and Computer Engineering, University of Pittsburg, USA.); Cai, Weidong (School of Computer Science, University of Sydney, Australia. Electronic address: tom.cai@sydney.edu.au.)","Cai, Weidong (The University of Sydney)","Xiang, Tiange (The University of Sydney); Zhang, Chaoyi (The University of Sydney); Wang, Xinyi (The University of Sydney); Song, Yang (UNSW Sydney); Liu, Dongnan (The University of Sydney); Huang, Heng (University of Pittsburgh); Cai, Weidong (The University of Sydney)",0,0,,,http://arxiv.org/pdf/2203.05709,https://app.dimensions.ai/details/publication/pub.1146308140,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
4195,pub.1146074047,10.1016/j.pacs.2022.100341,35371919,PMC8968659,Semantic segmentation of multispectral photoacoustic images using deep learning,"Photoacoustic (PA) imaging has the potential to revolutionize functional medical imaging in healthcare due to the valuable information on tissue physiology contained in multispectral photoacoustic measurements. Clinical translation of the technology requires conversion of the high-dimensional acquired data into clinically relevant and interpretable information. In this work, we present a deep learning-based approach to semantic segmentation of multispectral photoacoustic images to facilitate image interpretability. Manually annotated photoacoustic and ultrasound imaging data are used as reference and enable the training of a deep learning-based segmentation algorithm in a supervised manner. Based on a validation study with experimentally acquired data from 16 healthy human volunteers, we show that automatic tissue segmentation can be used to create powerful analyses and visualizations of multispectral photoacoustic images. Due to the intuitive representation of high-dimensional information, such a preprocessing algorithm could be a valuable means to facilitate the clinical translation of photoacoustic imaging.","This project was funded by the European Research Council (ERC) under the European Unionâs Horizon 2020 research and innovation programme through the ERC starting grant COMBIOSCOPY (grant agreement No. ERC-2015-StG-37960) and consolidator grant NEURAL SPICING (grant agreement No. [101002198]) and the Surgical Oncology Program of the National Center for Tumor Diseases (NCT) Heidelberg . Part of this work was funded by Helmholtz Imaging (HI), a platform of the Helmholtz Incubator on Information and Data Science. Additional information The healthy human volunteer experiments were approved by the ethics committee of the medical faculty of Heidelberg University under reference number S-451/2020 and the study is registered with the German Clinical Trials Register under reference number DRKS00023205.",,Photoacoustics,,,2022-03-05,2022,2022-03-05,2022-06,26,,100341,All OA, Gold,Article,"Schellenberg, Melanie; Dreher, Kris K.; Holzwarth, Niklas; Isensee, Fabian; Reinke, Annika; Schreck, Nicholas; Seitel, Alexander; Tizabi, Minu D.; Maier-Hein, Lena; GrÃ¶hl, Janek","Schellenberg, Melanie (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany; HIDSS4Health - Helmholtz Information and Data Science School for Health, Heidelberg, Germany); Dreher, Kris K. (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany); Holzwarth, Niklas (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany); Isensee, Fabian (HI Applied Computer Vision Lab, Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany); Reinke, Annika (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany; HI Applied Computer Vision Lab, Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany); Schreck, Nicholas (Division of Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany); Seitel, Alexander (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany); Tizabi, Minu D. (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany); Maier-Hein, Lena (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany; HIDSS4Health - Helmholtz Information and Data Science School for Health, Heidelberg, Germany; HI Applied Computer Vision Lab, Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany); GrÃ¶hl, Janek (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany)","Schellenberg, Melanie (German Cancer Research Center; Heidelberg University; ); Maier-Hein, Lena (German Cancer Research Center; Heidelberg University; ; German Cancer Research Center; Heidelberg University)","Schellenberg, Melanie (German Cancer Research Center; Heidelberg University); Dreher, Kris K. (German Cancer Research Center; Heidelberg University); Holzwarth, Niklas (German Cancer Research Center); Isensee, Fabian (German Cancer Research Center); Reinke, Annika (German Cancer Research Center; Heidelberg University; German Cancer Research Center); Schreck, Nicholas (German Cancer Research Center); Seitel, Alexander (German Cancer Research Center); Tizabi, Minu D. (German Cancer Research Center); Maier-Hein, Lena (German Cancer Research Center; Heidelberg University; German Cancer Research Center; Heidelberg University); GrÃ¶hl, Janek (German Cancer Research Center)",8,8,,,https://doi.org/10.1016/j.pacs.2022.100341,https://app.dimensions.ai/details/publication/pub.1146074047,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 51 Physical Sciences,,,,,,,,,
4191,pub.1139833089,10.1016/j.compbiomed.2021.104658,34311262,,Multiorgan segmentation from partially labeled datasets with conditional nnU-Net,"Accurate and robust multiorgan abdominal CT segmentation plays a significant role in numerous clinical applications, such as therapy treatment planning and treatment delivery. Almost all existing segmentation networks rely on fully annotated data with strong supervision. However, annotating fully annotated multiorgan data in CT images is both laborious and time-consuming. In comparison, massive partially labeled datasets are usually easily accessible. In this paper, we propose conditional nnU-Net trained on the union of partially labeled datasets for multiorgan segmentation. The deep model employs the state-of-the-art nnU-Net as the backbone and introduces a conditioning strategy by feeding auxiliary information into the decoder architecture as an additional input layer. This model leverages the prior conditional information to identify the organ class at the pixel-wise level and encourages organs' spatial information recovery. Furthermore, we adopt a deep supervision mechanism to refine the outputs at different scales and apply the combination of Dice loss and Focal loss to optimize the training model. Our proposed method is evaluated on seven publicly available datasets of the liver, pancreas, spleen and kidney, in which promising segmentation performance has been achieved. The proposed conditional nnU-Net breaks down the barriers between nonoverlapping labeled datasets and further alleviates the problem of data hunger in multiorgan segmentation.","This research was supported by the National Natural Science Foundation of China (Grant No. 81871457), the National Natural Science Foundation of China (Grant No. 51775368) and the National Natural Science Foundation of China (Grant No. 51811530310).",,Computers in Biology and Medicine,,,2021-07-21,2021,2021-07-21,2021-09,136,,104658,Closed,Article,"Zhang, Guobin; Yang, Zhiyong; Huo, Bin; Chai, Shude; Jiang, Shan","Zhang, Guobin (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China.); Yang, Zhiyong (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China.); Huo, Bin (Department of Oncology, Tianjin Medical University Second Hospital, Tianjin, 300211, China.); Chai, Shude (Department of Oncology, Tianjin Medical University Second Hospital, Tianjin, 300211, China.); Jiang, Shan (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China. Electronic address: shanjmri@tju.edu.cn.)","Jiang, Shan (Tianjin University)","Zhang, Guobin (Tianjin University); Yang, Zhiyong (Tianjin University); Huo, Bin (Tianjin Medical University); Chai, Shude (Tianjin Medical University); Jiang, Shan (Tianjin University)",6,6,0.7,4.23,,https://app.dimensions.ai/details/publication/pub.1139833089,31 Biological Sciences, 3102 Bioinformatics and Computational Biology, 42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences, 4601 Applied Computing,,,,,,,
4183,pub.1128192865,10.1016/j.media.2020.101731,32544841,,Unified generative adversarial networks for multimodal segmentation from unpaired 3D medical images,"To fully define the target objects of interest in clinical diagnosis, many deep convolution neural networks (CNNs) use multimodal paired registered images as inputs for segmentation tasks. However, these paired images are difficult to obtain in some cases. Furthermore, the CNNs trained on one specific modality may fail on others for images acquired with different imaging protocols and scanners. Therefore, developing a unified model that can segment the target objects from unpaired multiple modalities is significant for many clinical applications. In this work, we propose a 3D unified generative adversarial network, which unifies the any-to-any modality translation and multimodal segmentation in a single network. Since the anatomical structure is preserved during modality translation, the auxiliary translation task is used to extract the modality-invariant features and generate the additional training data implicitly. To fully utilize the segmentation-related features, we add a cross-task skip connection with feature recalibration from the translation decoder to the segmentation decoder. Experiments on abdominal organ segmentation and brain tumor segmentation indicate that our method outperforms the existing unified methods.","This work is supported by the Natural Science Foundation of Guangdong Province (2017A030313358, 2017A030313355, 2020A1515010717), the Science and Technology Planning Project of Guangdong Province (2016A040403046), the Guangzhou Science and Technology Planning Project (201704030051), the Fundamental Research Funds for the Central Universities (2019MS073).",,Medical Image Analysis,,"Brain Neoplasms; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Neural Networks, Computer",2020-06-04,2020,2020-06-04,2020-08,64,,101731,Closed,Article,"Yuan, Wenguang; Wei, Jia; Wang, Jiabing; Ma, Qianli; Tasdizen, Tolga","Yuan, Wenguang (School of Computer Science and Engineering, South China University of Technology, Guangzhou, China.); Wei, Jia (School of Computer Science and Engineering, South China University of Technology, Guangzhou, China. Electronic address: csjwei@scut.edu.cn.); Wang, Jiabing (School of Computer Science and Engineering, South China University of Technology, Guangzhou, China.); Ma, Qianli (School of Computer Science and Engineering, South China University of Technology, Guangzhou, China.); Tasdizen, Tolga (Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, USA.)","Wei, Jia (South China University of Technology)","Yuan, Wenguang (South China University of Technology); Wei, Jia (South China University of Technology); Wang, Jiabing (South China University of Technology); Ma, Qianli (South China University of Technology); Tasdizen, Tolga (University of Utah)",18,16,0.97,,,https://app.dimensions.ai/details/publication/pub.1128192865,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,,
4180,pub.1142562450,10.3390/s21217018,34770324,PMC8587013,Medical Augmentation (Med-Aug) for Optimal Data Augmentation in Medical Deep Learning Networks,"Deep learning (DL) algorithms have become an increasingly popular choice for image classification and segmentation tasks; however, their range of applications can be limited. Their limitation stems from them requiring ample data to achieve high performance and adequate generalizability. In the case of clinical imaging data, images are not always available in large quantities. This issue can be alleviated by using data augmentation (DA) techniques. The choice of DA is important because poor selection can possibly hinder the performance of a DL algorithm. We propose a DA policy search algorithm that offers an extended set of transformations that accommodate the variations in biomedical imaging datasets. The algorithm makes use of the efficient and high-dimensional optimizer Bi-Population Covariance Matrix Adaptation Evolution Strategy (BIPOP-CMA-ES) and returns an optimal DA policy based on any input imaging dataset and a DL algorithm. Our proposed algorithm, Medical Augmentation (Med-Aug), can be implemented by other researchers in related medical DL applications to improve their model's performance. Furthermore, we present our found optimal DA policies for a variety of medical datasets and popular segmentation networks for other researchers to use in related tasks.",,,Sensors,,"Algorithms; Deep Learning; Diagnostic Imaging; Education, Medical; Neural Networks, Computer",2021-10-22,2021,2021-10-22,,21,21,7018,All OA, Gold,Article,"Lo, Justin; Cardinell, Jillian; Costanzo, Alejo; Sussman, Dafna","Lo, Justin (Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, ON M5B 2K3, Canada;, justin1.lo@ryerson.ca, (J.L.);, jillian.cardinell@ryerson.ca, (J.C.);, acostanzo@ryerson.ca, (A.C.); Institute for Biomedical Engineering, Science and Technology (iBEST) at Ryerson University & St. Michaelâs Hospital, Toronto, ON M5B 1T8, Canada); Cardinell, Jillian (Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, ON M5B 2K3, Canada;, justin1.lo@ryerson.ca, (J.L.);, jillian.cardinell@ryerson.ca, (J.C.);, acostanzo@ryerson.ca, (A.C.); Institute for Biomedical Engineering, Science and Technology (iBEST) at Ryerson University & St. Michaelâs Hospital, Toronto, ON M5B 1T8, Canada); Costanzo, Alejo (Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, ON M5B 2K3, Canada;, justin1.lo@ryerson.ca, (J.L.);, jillian.cardinell@ryerson.ca, (J.C.);, acostanzo@ryerson.ca, (A.C.); Institute for Biomedical Engineering, Science and Technology (iBEST) at Ryerson University & St. Michaelâs Hospital, Toronto, ON M5B 1T8, Canada); Sussman, Dafna (Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, ON M5B 2K3, Canada;, justin1.lo@ryerson.ca, (J.L.);, jillian.cardinell@ryerson.ca, (J.C.);, acostanzo@ryerson.ca, (A.C.); Institute for Biomedical Engineering, Science and Technology (iBEST) at Ryerson University & St. Michaelâs Hospital, Toronto, ON M5B 1T8, Canada; The Keenan Research Centre for Biomedical Science, St. Michaelâs Hospital, Toronto, ON M5B 1T8, Canada; Department of Obstetrics and Gynecology, Faculty of Medicine, University of Toronto, Toronto, ON M5G 1E2, Canada)","Sussman, Dafna (Ryerson University; ; St. Michael's Hospital; St. Michael's Hospital; University of Toronto)","Lo, Justin (Ryerson University; St. Michael's Hospital); Cardinell, Jillian (Ryerson University; St. Michael's Hospital); Costanzo, Alejo (Ryerson University; St. Michael's Hospital); Sussman, Dafna (Ryerson University; St. Michael's Hospital; St. Michael's Hospital; University of Toronto)",2,2,,1.65,https://www.mdpi.com/1424-8220/21/21/7018/pdf?version=1635398920,https://app.dimensions.ai/details/publication/pub.1142562450,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4611 Machine Learning,,,,,,,,,
4170,pub.1144579946,10.3390/cancers14010101,35008265,PMC8750371,Automatic Segmentation of Metastatic Breast Cancer Lesions on 18F-FDG PET/CT Longitudinal Acquisitions for Treatment Response Assessment,"Metastatic breast cancer patients receive lifelong medication and are regularly monitored for disease progression. The aim of this work was to (1) propose networks to segment breast cancer metastatic lesions on longitudinal whole-body PET/CT and (2) extract imaging biomarkers from the segmentations and evaluate their potential to determine treatment response. Baseline and follow-up PET/CT images of 60 patients from the EPICUREseinmeta study were used to train two deep-learning models to segment breast cancer metastatic lesions: One for baseline images and one for follow-up images. From the automatic segmentations, four imaging biomarkers were computed and evaluated: SULpeak, Total Lesion Glycolysis (TLG), PET Bone Index (PBI) and PET Liver Index (PLI). The first network obtained a mean Dice score of 0.66 on baseline acquisitions. The second network obtained a mean Dice score of 0.58 on follow-up acquisitions. SULpeak, with a 32% decrease between baseline and follow-up, was the biomarker best able to assess patients' response (sensitivity 87%, specificity 87%), followed by TLG (43% decrease, sensitivity 73%, specificity 81%) and PBI (8% decrease, sensitivity 69%, specificity 69%). Our networks constitute promising tools for the automatic segmentation of lesions in patients with metastatic breast cancer allowing treatment response assessment with several biomarkers.",,,Cancers,,,2021-12-26,2021,2021-12-26,,14,1,101,All OA, Gold,Article,"Moreau, NoÃ©mie; Rousseau, Caroline; Fourcade, Constance; Santini, Gianmarco; Brennan, Aislinn; Ferrer, Ludovic; Lacombe, Marie; Guillerminet, Camille; ColombiÃ©, Mathilde; JÃ©zÃ©quel, Pascal; Campone, Mario; Normand, Nicolas; Rubeaux, Mathieu","Moreau, NoÃ©mie (LS2N, University of Nantes, CNRS, 44000 Nantes, France;, constance.fourcade@keosys.com, (C.F.);, Nicolas.Normand@univ-nantes.fr, (N.N.); Keosys Medical Imaging, 13 Imp. Serge Reggiani, 44815 Saint-Herblain, France;, gianmarco.santini@keosys.com, (G.S.);, aislinn.brennan@keosys.com, (A.B.);, mathieu.rubeaux@keosys.com, (M.R.)); Rousseau, Caroline (CRCINA, University of Nantes, INSERM UMR1232, CNRS-ERL6001, 44000 Nantes, France;, Caroline.Rousseau@ico.unicancer.fr, (C.R.);, Pascal.Jezequel@ico.unicancer.fr, (P.J.); ICO Cancer Center, 49000 Angers, France;, Ludovic.Ferrer@ico.unicancer.fr, (L.F.);, Marie.Lacombe@ico.unicancer.fr, (M.L.);, camille.guillerminet@ico.unicancer.fr, (C.G.);, mathilde.colombie@ico.unicancer.fr, (M.C.);, mario.campone@ico.unicancer.fr, (M.C.)); Fourcade, Constance (LS2N, University of Nantes, CNRS, 44000 Nantes, France;, constance.fourcade@keosys.com, (C.F.);, Nicolas.Normand@univ-nantes.fr, (N.N.); Keosys Medical Imaging, 13 Imp. Serge Reggiani, 44815 Saint-Herblain, France;, gianmarco.santini@keosys.com, (G.S.);, aislinn.brennan@keosys.com, (A.B.);, mathieu.rubeaux@keosys.com, (M.R.)); Santini, Gianmarco (Keosys Medical Imaging, 13 Imp. Serge Reggiani, 44815 Saint-Herblain, France;, gianmarco.santini@keosys.com, (G.S.);, aislinn.brennan@keosys.com, (A.B.);, mathieu.rubeaux@keosys.com, (M.R.)); Brennan, Aislinn (Keosys Medical Imaging, 13 Imp. Serge Reggiani, 44815 Saint-Herblain, France;, gianmarco.santini@keosys.com, (G.S.);, aislinn.brennan@keosys.com, (A.B.);, mathieu.rubeaux@keosys.com, (M.R.)); Ferrer, Ludovic (ICO Cancer Center, 49000 Angers, France;, Ludovic.Ferrer@ico.unicancer.fr, (L.F.);, Marie.Lacombe@ico.unicancer.fr, (M.L.);, camille.guillerminet@ico.unicancer.fr, (C.G.);, mathilde.colombie@ico.unicancer.fr, (M.C.);, mario.campone@ico.unicancer.fr, (M.C.); CRCINA, University of Angers, INSERM UMR1232, CNRS-ERL6001, 49000 Angers, France); Lacombe, Marie (ICO Cancer Center, 49000 Angers, France;, Ludovic.Ferrer@ico.unicancer.fr, (L.F.);, Marie.Lacombe@ico.unicancer.fr, (M.L.);, camille.guillerminet@ico.unicancer.fr, (C.G.);, mathilde.colombie@ico.unicancer.fr, (M.C.);, mario.campone@ico.unicancer.fr, (M.C.)); Guillerminet, Camille (ICO Cancer Center, 49000 Angers, France;, Ludovic.Ferrer@ico.unicancer.fr, (L.F.);, Marie.Lacombe@ico.unicancer.fr, (M.L.);, camille.guillerminet@ico.unicancer.fr, (C.G.);, mathilde.colombie@ico.unicancer.fr, (M.C.);, mario.campone@ico.unicancer.fr, (M.C.)); ColombiÃ©, Mathilde (ICO Cancer Center, 49000 Angers, France;, Ludovic.Ferrer@ico.unicancer.fr, (L.F.);, Marie.Lacombe@ico.unicancer.fr, (M.L.);, camille.guillerminet@ico.unicancer.fr, (C.G.);, mathilde.colombie@ico.unicancer.fr, (M.C.);, mario.campone@ico.unicancer.fr, (M.C.)); JÃ©zÃ©quel, Pascal (CRCINA, University of Nantes, INSERM UMR1232, CNRS-ERL6001, 44000 Nantes, France;, Caroline.Rousseau@ico.unicancer.fr, (C.R.);, Pascal.Jezequel@ico.unicancer.fr, (P.J.); ICO Cancer Center, 49000 Angers, France;, Ludovic.Ferrer@ico.unicancer.fr, (L.F.);, Marie.Lacombe@ico.unicancer.fr, (M.L.);, camille.guillerminet@ico.unicancer.fr, (C.G.);, mathilde.colombie@ico.unicancer.fr, (M.C.);, mario.campone@ico.unicancer.fr, (M.C.)); Campone, Mario (ICO Cancer Center, 49000 Angers, France;, Ludovic.Ferrer@ico.unicancer.fr, (L.F.);, Marie.Lacombe@ico.unicancer.fr, (M.L.);, camille.guillerminet@ico.unicancer.fr, (C.G.);, mathilde.colombie@ico.unicancer.fr, (M.C.);, mario.campone@ico.unicancer.fr, (M.C.); CRCINA, University of Angers, INSERM UMR1232, CNRS-ERL6001, 49000 Angers, France); Normand, Nicolas (LS2N, University of Nantes, CNRS, 44000 Nantes, France;, constance.fourcade@keosys.com, (C.F.);, Nicolas.Normand@univ-nantes.fr, (N.N.)); Rubeaux, Mathieu (Keosys Medical Imaging, 13 Imp. Serge Reggiani, 44815 Saint-Herblain, France;, gianmarco.santini@keosys.com, (G.S.);, aislinn.brennan@keosys.com, (A.B.);, mathieu.rubeaux@keosys.com, (M.R.))","Moreau, NoÃ©mie (Laboratoire des Sciences du NumÃ©rique de Nantes; )","Moreau, NoÃ©mie (Laboratoire des Sciences du NumÃ©rique de Nantes); Rousseau, Caroline (University of Nantes; Institut de CancÃ©rologie de l'Ouest); Fourcade, Constance (Laboratoire des Sciences du NumÃ©rique de Nantes); Santini, Gianmarco (); Brennan, Aislinn (); Ferrer, Ludovic (Institut de CancÃ©rologie de l'Ouest; University of Angers); Lacombe, Marie (Institut de CancÃ©rologie de l'Ouest); Guillerminet, Camille (Institut de CancÃ©rologie de l'Ouest); ColombiÃ©, Mathilde (Institut de CancÃ©rologie de l'Ouest); JÃ©zÃ©quel, Pascal (University of Nantes; Institut de CancÃ©rologie de l'Ouest); Campone, Mario (Institut de CancÃ©rologie de l'Ouest; University of Angers); Normand, Nicolas (Laboratoire des Sciences du NumÃ©rique de Nantes); Rubeaux, Mathieu ()",4,4,0.64,2.9,https://www.mdpi.com/2072-6694/14/1/101/pdf?version=1640668642,https://app.dimensions.ai/details/publication/pub.1144579946,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4010,pub.1149638972,10.1109/tmi.2022.3193146,35862336,,AADG: Automatic Augmentation for Domain Generalization on Retinal Image Segmentation,"Convolutional neural networks have been widely applied to medical image segmentation and have achieved considerable performance. However, the performance may be significantly affected by the domain gap between training data (source domain) and testing data (target domain). To address this issue, we propose a data manipulation based domain generalization method, called Automated Augmentation for Domain Generalization (AADG). Our AADG framework can effectively sample data augmentation policies that generate novel domains and diversify the training set from an appropriate search space. Specifically, we introduce a novel proxy task maximizing the diversity among multiple augmented novel domains as measured by the Sinkhorn distance in a unit sphere space, making automated augmentation tractable. Adversarial training and deep reinforcement learning are employed to efficiently search the objectives. Quantitative and qualitative experiments on 11 publicly-accessible fundus image datasets (four for retinal vessel segmentation, four for optic disc and cup (OD/OC) segmentation and three for retinal lesion segmentation) are comprehensively performed. Two OCTA datasets for retinal vasculature segmentation are further involved to validate cross-modality generalization. Our proposed AADG exhibits state-of-the-art generalization performance and outperforms existing approaches by considerable margins on retinal vessel, OD/OC and lesion segmentation tasks. The learned policies are empirically validated to be model-agnostic and can transfer well to other models. The source code is available at https://github.com/CRazorback/AADG.",,"This work was supported in part by the Shenzhen Basic Research Program under Grant JCYJ20190809120205578, in part by the National Natural Science Foundation of China under Grant 62071210, in part by the Shenzhen Basic Research Program under Grant JCYJ20200925153847004, and in part by the High-Level University Fund under Grant G02236002.",IEEE Transactions on Medical Imaging,,"Humans; Fundus Oculi; Glaucoma; Image Processing, Computer-Assisted; Neural Networks, Computer; Optic Disk",2022-12-02,2022,2022-12-02,2022-12,41,12,3699-3711,All OA, Green,Article,"Lyu, Junyan; Zhang, Yiqi; Huang, Yijin; Lin, Li; Cheng, Pujin; Tang, Xiaoying","Lyu, Junyan (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, 518055, China); Zhang, Yiqi (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, 518055, China); Huang, Yijin (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, 518055, China); Lin, Li (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, 518055, China); Cheng, Pujin (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, 518055, China); Tang, Xiaoying (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, 518055, China)","Tang, Xiaoying (Southern University of Science and Technology)","Lyu, Junyan (Southern University of Science and Technology); Zhang, Yiqi (Southern University of Science and Technology); Huang, Yijin (Southern University of Science and Technology); Lin, Li (Southern University of Science and Technology); Cheng, Pujin (Southern University of Science and Technology); Tang, Xiaoying (Southern University of Science and Technology)",0,0,,,http://arxiv.org/pdf/2207.13249,https://app.dimensions.ai/details/publication/pub.1149638972,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4611 Machine Learning,,,,,,,,,
4007,pub.1150454966,10.1016/j.compbiomed.2022.106034,36058068,,Uncertainty teacher with dense focal loss for semi-supervised medical image segmentation,"In medical scenarios, obtaining pixel-level annotations for medical images is expensive and time-consuming, even if considering its importance for automating segmentation tasks. Due to the scarcity of labels in the training phase, semi-supervised methods are widely applied for various medical tasks. To better utilize the unlabeled data, several works have explored the method of uncertainty estimation and exhibited huge success. Despite their impressive performance, we believe that the underlying information of the unlabeled data has been largely unexplored. Meanwhile, there is an extreme foreground-background class imbalance during the training phase of semantic segmentation, which may cause a vast number of easily classified samples to overwhelm the loss during training and lead to a model collapse. In this paper, we proposed uncertainty teacher with dense focal loss, a method that can take good advantage of unlabeled data simultaneously and address the class imbalance problem, based on Deep Co-Training. On one hand, the uncertainty teacher framework is presented to better utilize the unlabeled data by introducing a novel method to regularize uncertainty in the right direction, and the uncertainty is estimated by Monte Carlo Sampling. On the other hand, the dense focal loss is proposed to help solve the class imbalance problem between different classes of samples in medical image segmentation and effectively convert the multi-variate entropy into a multiple binary entropy. We implemented our method on three challenging public medical datasets and experimental results have shown desirable improvements to state-of-the-art.","This research is supported by the National Natural Science Foundation of China (No. 62032013), the Fundamental Research Funds for the Central Universities, China (Nos. N2224001-7 and N2116020) and the Natural Science Foundation of Liaoning Province, China (No. 2021-YGJC-24).",,Computers in Biology and Medicine,,"Deep Learning; Entropy; Image Processing, Computer-Assisted; Neural Networks, Computer; Uncertainty",2022-08-24,2022,2022-08-24,2022-10,149,,106034,Closed,Article,"Chen, Jialei; Fu, Chong; Xie, Haoyu; Zheng, Xu; Geng, Rong; Sham, Chiu-Wing","Chen, Jialei (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China.); Fu, Chong (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China; Engineering Research Center of Security Technology of Complex Network System, Ministry of Education, China; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang 110819, China. Electronic address: fuchong@mail.neu.edu.cn.); Xie, Haoyu (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China.); Zheng, Xu (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China.); Geng, Rong (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China.); Sham, Chiu-Wing (School of Computer Science, The University of Auckland, New Zealand.)","Fu, Chong (Ministry of Education of the People's Republic of China; Northeastern University)","Chen, Jialei (Northeastern University); Fu, Chong (Ministry of Education of the People's Republic of China; Northeastern University); Xie, Haoyu (Northeastern University); Zheng, Xu (Northeastern University); Geng, Rong (Northeastern University); Sham, Chiu-Wing (University of Auckland)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150454966,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
4001,pub.1150461210,10.1016/j.compbiomed.2022.106051,36055155,,Uncertainty-aware deep co-training for semi-supervised medical image segmentation,"Semi-supervised learning has made significant strides in the medical domain since it alleviates the heavy burden of collecting abundant pixel-wise annotated data for semantic segmentation tasks. Existing semi-supervised approaches enhance the ability to extract features from unlabeled data with prior knowledge obtained from limited labeled data. However, due to the scarcity of labeled data, the features extracted by the models are limited in supervised learning, and the quality of predictions for unlabeled data also cannot be guaranteed. Both will impede consistency training. To this end, we proposed a novel uncertainty-aware scheme to make models learn regions purposefully. Specifically, we employ Monte Carlo Sampling as an estimation method to attain an uncertainty map, which can serve as a weight for losses to force the models to focus on the valuable region according to the characteristics of supervised learning and unsupervised learning. Simultaneously, in the backward process, we joint unsupervised and supervised losses to accelerate the convergence of the network via enhancing the gradient flow between different tasks. Quantitatively, we conduct extensive experiments on three challenging medical datasets. Experimental results show desirable improvements to state-of-the-art counterparts.","This work was supported by the National Natural Science Foundation of China (No. 62032013), the Fundamental Research Funds for the Central Universities, China (Nos. N2224001-7 and N2116020) and the Natural Science Foundation of Liaoning Province, China (No. 2021-YGJC-24).",,Computers in Biology and Medicine,,"Image Processing, Computer-Assisted; Supervised Machine Learning; Uncertainty",2022-08-24,2022,2022-08-24,2022-10,149,,106051,All OA, Green,Article,"Zheng, Xu; Fu, Chong; Xie, Haoyu; Chen, Jialei; Wang, Xingwei; Sham, Chiu-Wing","Zheng, Xu (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China.); Fu, Chong (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China; Engineering Research Center of Security Technology of Complex Network System, Ministry of Education, China; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang 110819, China. Electronic address: fuchong@mail.neu.edu.cn.); Xie, Haoyu (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China.); Chen, Jialei (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China.); Wang, Xingwei (School of Computer Science and Engineering, Northeastern University, Shenyang 110819, China.); Sham, Chiu-Wing (School of Computer Science, The University of Auckland, New Zealand.)","Fu, Chong (Ministry of Education of the People's Republic of China; Northeastern University)","Zheng, Xu (Northeastern University); Fu, Chong (Ministry of Education of the People's Republic of China; Northeastern University); Xie, Haoyu (Northeastern University); Chen, Jialei (Northeastern University); Wang, Xingwei (Northeastern University); Sham, Chiu-Wing (University of Auckland)",1,1,,,http://arxiv.org/pdf/2111.11629,https://app.dimensions.ai/details/publication/pub.1150461210,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
4001,pub.1149884999,10.1186/s41747-022-00288-8,35909214,PMC9339427,"Artificial intelligence for prostate MRI: open datasets, available applications, and grand challenges","Artificial intelligence (AI) for prostate magnetic resonance imaging (MRI) is starting to play a clinical role for prostate cancer (PCa) patients. AI-assisted reading is feasible, allowing workflow reduction. A total of 3,369 multi-vendor prostate MRI cases are available in open datasets, acquired from 2003 to 2021 in Europe or USA at 3 T (n = 3,018; 89.6%) or 1.5 T (n = 296; 8.8%), 346 cases scanned with endorectal coil (10.3%), 3,023 (89.7%) with phased-array surface coils; 412 collected for anatomical segmentation tasks, 3,096 for PCa detection/classification; for 2,240 cases lesions delineation is available and 56 cases have matching histopathologic images; for 2,620 cases the PSA level is provided; the total size of all open datasets amounts to approximately 253 GB. Of note, quality of annotations provided per dataset highly differ and attention must be paid when using these datasets (e.g., data overlap). Seven grand challenges and commercial applications from eleven vendors are here considered. Few small studies provided prospective validation. More work is needed, in particular validation on large-scale multi-institutional, well-curated public datasets to test general applicability. Moreover, AI needs to be explored for clinical stages other than detection/characterization (e.g., follow-up, prognosis, interventions, and focal treatment).",,"This work was financed by the Research Council of Norway (Grant Number 295013), the Norwegian Cancer Society and Prostatakreftforeningen (Grant Number 215951), the Liaison Committee between the Central Norway Regional Health Authority and the Norwegian University of Science and Technology (Grant Numbers 90265300 and 90793700), EU H2020 ProCAncer-I (Grant Number 952159), EU H2020 PANCAIM (Grant Number 101016851), and EU IMI2 PIONEER (Grant Number 777492). Open access funding provided by Norwegian University of Science and Technology.",European Radiology Experimental,,Artificial Intelligence, Humans, Magnetic Resonance Imaging, Male, Prostate, Prostatic Neoplasms, Sensitivity and Specificity,2022-08-01,2022,2022-08-01,,6,1,35,All OA, Gold,Article,"Sunoqrot, Mohammed R. S.; Saha, Anindo; Hosseinzadeh, Matin; Elschot, Mattijs; Huisman, Henkjan","Sunoqrot, Mohammed R. S. (Department of Circulation and Medical Imaging, NTNUâNorwegian University of Science and Technology, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030, Trondheim, Norway); Saha, Anindo (Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6525 GA, Nijmegen, The Netherlands); Hosseinzadeh, Matin (Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6525 GA, Nijmegen, The Netherlands); Elschot, Mattijs (Department of Circulation and Medical Imaging, NTNUâNorwegian University of Science and Technology, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030, Trondheim, Norway); Huisman, Henkjan (Department of Circulation and Medical Imaging, NTNUâNorwegian University of Science and Technology, 7030, Trondheim, Norway; Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6525 GA, Nijmegen, The Netherlands)","Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology; St Olav's University Hospital)","Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology; St Olav's University Hospital); Saha, Anindo (Radboud University Nijmegen Medical Centre); Hosseinzadeh, Matin (Radboud University Nijmegen Medical Centre); Elschot, Mattijs (Norwegian University of Science and Technology; St Olav's University Hospital); Huisman, Henkjan (Norwegian University of Science and Technology; Radboud University Nijmegen Medical Centre)",6,6,,,https://eurradiolexp.springeropen.com/counter/pdf/10.1186/s41747-022-00288-8,https://app.dimensions.ai/details/publication/pub.1149884999,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,
3941,pub.1143817480,10.1186/s40658-021-00426-y,34897550,PMC8665861,Artificial intelligence with deep learning in nuclear medicine and radiology,"The use of deep learning in medical imaging has increased rapidly over the past few years, finding applications throughout the entire radiology pipeline, from improved scanner performance to automatic disease detection and diagnosis. These advancements have resulted in a wide variety of deep learning approaches being developed, solving unique challenges for various imaging modalities. This paper provides a review on these developments from a technical point of view, categorizing the different methodologies and summarizing their implementation. We provide an introduction to the design of neural networks and their training procedure, after which we take an extended look at their uses in medical imaging. We cover the different sections of the radiology pipeline, highlighting some influential works and discussing the merits and limitations of deep learning approaches compared to other traditional methods. As such, this review is intended to provide a broad yet concise overview for the interested reader, facilitating adoption and interdisciplinary research of deep learning in the field of medical imaging.",The authors would like to thank Roland Hustinx from the University of Liege for reading the manuscript and making useful suggestions for further improvements.,,EJNMMI Physics,,,2021-12-11,2021,2021-12-11,2021-12,8,1,81,All OA, Gold,Article,"Decuyper, Milan; Maebe, Jens; Van Holen, Roel; Vandenberghe, Stefaan","Decuyper, Milan (Department of Electronics and Information Systems, Ghent University, Ghent, Belgium); Maebe, Jens (Department of Electronics and Information Systems, Ghent University, Ghent, Belgium); Van Holen, Roel (Department of Electronics and Information Systems, Ghent University, Ghent, Belgium); Vandenberghe, Stefaan (Department of Electronics and Information Systems, Ghent University, Ghent, Belgium)","Maebe, Jens (Ghent University)","Decuyper, Milan (Ghent University); Maebe, Jens (Ghent University); Van Holen, Roel (Ghent University); Vandenberghe, Stefaan (Ghent University)",10,10,0.85,7.41,https://ejnmmiphys.springeropen.com/counter/pdf/10.1186/s40658-021-00426-y,https://app.dimensions.ai/details/publication/pub.1143817480,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,
3856,pub.1150859951,10.1109/embc48229.2022.9871257,36085959,,A Comparative Study on the Potential of Unsupervised Deep Learning-based Feature Selection in Radiomics**This work was not supported by any organization,"In Radiomics, deep learning-based systems for medical image analysis play an increasing role. However, due to the better explainability, feature-based systems are still preferred, especially by physicians. Often, high-dimensional data and low sample size pose different challenges (e.g. increased risk of overfitting) to machine learning systems. By removing irrelevant and redundant features from the data, feature selection is an effective way of pre-processing. The research in this study is focused on unsupervised deep learning-based methods for feature selection. Five recently proposed algorithms are compared regarding their applicability and efficiency on seven data sets in three different sample applications. It was found that deep learning-based feature selection leads to improved classification results compared to conventional methods, especially for small feature subsets. Clinical Relevance - The exploration of distinctive features and the ability to rank their importance without the need for outcome information is a potential field of application for unsupervised feature selection methods. Especially in multiparametric radiology, the number of features is increasing. The identification of new potential biomarkers is important both for treatment and prevention.",,*This work was not supported by any organization.,Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),Algorithms, Deep Learning, Machine Learning, Sample Size,2022-07,2022,,2022-07,0,,541-544,Closed,Proceeding,"Haueise, Tobias; Liebgott, Annika; Yang, Bin","Haueise, Tobias (Section on Experimental Radiology, University Hospital Tuebingen, Tuebingen, Germany; Institute of Diabetes Research and Metabolic Diseases at Helmholtz Munich, Tuebingen, Germany); Liebgott, Annika (Institute of Signal Processing and System Theory, University of Stuttgart, Stuttgart, Germany); Yang, Bin (Institute of Signal Processing and System Theory, University of Stuttgart, Stuttgart, Germany)",,"Haueise, Tobias (UniversitÃ¤tsklinikum TÃ¼bingen; Helmholtz Zentrum MÃ¼nchen); Liebgott, Annika (University of Stuttgart); Yang, Bin (University of Stuttgart)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150859951,46 Information and Computing Sciences, 4605 Data Management and Data Science,3 Good Health and Well Being,,,,,,,
3843,pub.1150157554,10.1109/tcbb.2022.3198284,35951571,,DFTNet: Dual-Path Feature Transfer Network for Weakly Supervised Medical Image Segmentation,"Medical image segmentation has long suffered from the problem of expensive labels. Acquiring pixel-level annotations is time-consuming, labor-intensive, and relies on extensive expert knowledge. Bounding box annotations, in contrast, are relatively easy to acquire. Thus, in this paper, we explore to segment images through a novel Dual-path Feature Transfer design with only bounding box annotations. Specifically, a Target-aware Reconstructor is proposed to extract target-related features by reconstructing the pixels within the bounding box through the channel and spatial attention module. Then, a sliding Feature Fusion and Transfer Module (FFTM) fuses the extracted features from Reconstructor and transfers them to guide the Segmentor for segmentation. Finally, we present the Confidence Ranking Loss (CRLoss) which dynamically assigns weights to the loss of each pixel based on the network's confidence. CRLoss mitigates the impact of inaccurate pseudo-labels on performance. Extensive experiments demonstrate that our proposed model achieves state-of-the-art performance on the Medical Segmentation Decathlon (MSD) Brain Tumour and PROMISE12 datasets.",,,IEEE/ACM Transactions on Computational Biology and Bioinformatics,,,2022-08-11,2022,2022-08-11,2022-08-11,PP,99,1-12,Closed,Article,"Cai, Wentian; Xie, Linsen; Yang, Weixian; Li, Yijiang; Gao, Ying; Wang, Tingting","Cai, Wentian (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Xie, Linsen (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Yang, Weixian (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Li, Yijiang (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Gao, Ying (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Wang, Tingting (Faculty of Information Technology, Macau University of Science and Technology, Taipa, Macao)",,"Cai, Wentian (South China University of Technology); Xie, Linsen (South China University of Technology); Yang, Weixian (South China University of Technology); Li, Yijiang (South China University of Technology); Gao, Ying (South China University of Technology); Wang, Tingting (Macau University of Science and Technology)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1150157554,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,,
3835,pub.1147207344,10.1016/j.cmpb.2022.106818,35483271,,ClinicaDL: An open-source deep learning software for reproducible neuroimaging processing,"BACKGROUND AND OBJECTIVE: As deep learning faces a reproducibility crisis and studies on deep learning applied to neuroimaging are contaminated by methodological flaws, there is an urgent need to provide a safe environment for deep learning users to help them avoid common pitfalls that will bias and discredit their results. Several tools have been proposed to help deep learning users design their framework for neuroimaging data sets. Software overview: We present here ClinicaDL, one of these software tools. ClinicaDL interacts with BIDS, a standard format in the neuroimaging field, and its derivatives, so it can be used with a large variety of data sets. Moreover, it checks the absence of data leakage when inferring the results of new data with trained networks, and saves all necessary information to guarantee the reproducibility of results. The combination of ClinicaDL and its companion project Clinica allows performing an end-to-end neuroimaging analysis, from the download of raw data sets to the interpretation of trained networks, including neuroimaging preprocessing, quality check, label definition, architecture search, and network training and evaluation.
CONCLUSIONS: We implemented ClinicaDL to bring answers to three common issues encountered by deep learning users who are not always familiar with neuroimaging data: (1) the format and preprocessing of neuroimaging data sets, (2) the contamination of the evaluation procedure by data leakage and (3) a lack of reproducibility. We hope that its use by researchers will allow producing more reliable and thus valuable scientific studies in our field.","The authors would like to thank Junhao Wen for his initial contribution to what became ClinicaDL, Simona Bottani and Omar El Rifai for their contributions and feedback, and Igor Koval for his soothing support.","The research leading to these results has received funding from the French government under management of Agence Nationale de la Recherche as part of the âInvestissements dâavenirâ program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute) and reference ANR-10-IAIHU-06 (Agence Nationale de la Recherche-10-IA Institut Hospitalo-Universitaire-6).",Computer Methods and Programs in Biomedicine,,"Deep Learning; Image Processing, Computer-Assisted; Neuroimaging; Reproducibility of Results; Software",2022-04-19,2022,2022-04-19,2022-06,220,,106818,All OA, Green,Article,"Thibeau-Sutre, Elina; DÃ­az, Mauricio; Hassanaly, Ravi; Routier, Alexandre; Dormont, Didier; Colliot, Olivier; Burgos, Ninon","Thibeau-Sutre, Elina (Sorbonne UniversitÃ©, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP, HÃ´pital de la PitiÃ© SalpÃªtriÃ¨re, Paris, F-75013, France.); DÃ­az, Mauricio (Sorbonne UniversitÃ©, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP, HÃ´pital de la PitiÃ© SalpÃªtriÃ¨re, Paris, F-75013, France.); Hassanaly, Ravi (Sorbonne UniversitÃ©, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP, HÃ´pital de la PitiÃ© SalpÃªtriÃ¨re, Paris, F-75013, France.); Routier, Alexandre (Sorbonne UniversitÃ©, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP, HÃ´pital de la PitiÃ© SalpÃªtriÃ¨re, Paris, F-75013, France.); Dormont, Didier (Sorbonne UniversitÃ©, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP, HÃ´pital de la PitiÃ© SalpÃªtriÃ¨re, DMU DIAMENT, Paris, F-75013, France.); Colliot, Olivier (Sorbonne UniversitÃ©, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP, HÃ´pital de la PitiÃ© SalpÃªtriÃ¨re, Paris, F-75013, France.); Burgos, Ninon (Sorbonne UniversitÃ©, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP, HÃ´pital de la PitiÃ© SalpÃªtriÃ¨re, Paris, F-75013, France. Electronic address: ninon.burgos@cnrs.fr.)","Burgos, Ninon (Institut du Cerveau)","Thibeau-Sutre, Elina (Institut du Cerveau); DÃ­az, Mauricio (Institut du Cerveau); Hassanaly, Ravi (Institut du Cerveau); Routier, Alexandre (Institut du Cerveau); Dormont, Didier (Institut du Cerveau); Colliot, Olivier (Institut du Cerveau); Burgos, Ninon (Institut du Cerveau)",4,4,,,https://hal.archives-ouvertes.fr/hal-03351976v1/file/2021_Thibeau-Sutre_ClinicaDL_manuscript.pdf,https://app.dimensions.ai/details/publication/pub.1147207344,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
3832,pub.1148374761,10.3389/fonc.2022.894970,35719964,PMC9202000,AX-Unet: A Deep Learning Framework for Image Segmentation to Assist Pancreatic Tumor Diagnosis,"Image segmentation plays an essential role in medical imaging analysis such as tumor boundary extraction. Recently, deep learning techniques have dramatically improved performance for image segmentation. However, an important factor preventing deep neural networks from going further is the information loss during the information propagation process. In this article, we present AX-Unet, a deep learning framework incorporating a modified atrous spatial pyramid pooling module to learn the location information and to extract multi-level contextual information to reduce information loss during downsampling. We also introduce a special group convolution operation on the feature map at each level to achieve information decoupling between channels. In addition, we propose an explicit boundary-aware loss function to tackle the blurry boundary problem. We evaluate our model on two public Pancreas-CT datasets, NIH Pancreas-CT dataset, and the pancreas part in medical segmentation decathlon (MSD) medical dataset. The experimental results validate that our model can outperform the state-of-the-art methods in pancreas CT image segmentation. By comparing the extracted feature output of our model, we find that the pancreatic region of normal people and patients with pancreatic tumors shows significant differences. This could provide a promising and reliable way to assist physicians for the screening of pancreatic tumors.",,"This work was supported in part by the National Key Research and Development Program of China (Grant No. 2019YFA0706200), in part by the National Natural Science Foundation of China (Grant No.61632014, No.61627808).",Frontiers in Oncology,,,2022-06-02,2022,2022-06-02,,12,,894970,All OA, Gold,Article,"Yang, Minqiang; Zhang, Yuhong; Chen, Haoning; Wang, Wei; Ni, Haixu; Chen, Xinlong; Li, Zhuoheng; Mao, Chengsheng","Yang, Minqiang (School of Information Science Engineering, Lanzhou University, Lanzhou, China); Zhang, Yuhong (School of Information Science Engineering, Lanzhou University, Lanzhou, China); Chen, Haoning (School of Statistics and Data Science, Nankai University, Tianjin, China); Wang, Wei (School of Intelligent Systems Engineering, Sun Yat-sen University, Shenzhen, China); Ni, Haixu (Department of General Surgery, First Hospital of Lanzhou University, Lanzhou, China); Chen, Xinlong (First Clinical Medical College, Lanzhou University, Lanzhou, China); Li, Zhuoheng (School of Information Science Engineering, Lanzhou University, Lanzhou, China); Mao, Chengsheng (Department of Preventive Medicine, Feinberg School of Medicine, Northwestern University, Chicago, IL, United States)","Ni, Haixu (First Hospital of Lanzhou University); Mao, Chengsheng (Northwestern University)","Yang, Minqiang (Lanzhou University); Zhang, Yuhong (Lanzhou University); Chen, Haoning (Nankai University); Wang, Wei (Sun Yat-sen University); Ni, Haixu (First Hospital of Lanzhou University); Chen, Xinlong (First Hospital of Lanzhou University); Li, Zhuoheng (Lanzhou University); Mao, Chengsheng (Northwestern University)",0,0,,,https://www.frontiersin.org/articles/10.3389/fonc.2022.894970/pdf,https://app.dimensions.ai/details/publication/pub.1148374761,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
3701,pub.1151421894,10.1148/ryai.220096,36523645,PMC9745441,"Semantic Segmentation of Spontaneous Intracerebral Hemorrhage, Intraventricular Hemorrhage, and Associated Edema on CT Images Using Deep Learning","This study evaluated deep learning algorithms for semantic segmentation and quantification of intracerebral hemorrhage (ICH), perihematomal edema (PHE), and intraventricular hemorrhage (IVH) on noncontrast CT scans of patients with spontaneous ICH. Models were assessed on 1732 annotated baseline noncontrast CT scans obtained from the Tranexamic Acid for Hyperacute Primary Intracerebral Haemorrhage (ie, TICH-2) international multicenter trial (ISRCTN93732214), and different loss functions using a three-dimensional no-new-U-Net (nnU-Net) were examined to address class imbalance (30% of participants with IVH in dataset). On the test cohort (n = 174, 10% of dataset), the top-performing models achieved median Dice similarity coefficients of 0.92 (IQR, 0.89-0.94), 0.66 (0.58-0.71), and 1.00 (0.87-1.00), respectively, for ICH, PHE, and IVH segmentation. U-Net-based networks showed comparable, satisfactory performances on ICH and PHE segmentations (P > .05), but all nnU-Net variants achieved higher accuracy than the Brain Lesion Analysis and Segmentation Tool for CT (BLAST-CT) and DeepLabv3+ for all labels (P < .05). The Focal model showed improved performance in IVH segmentation compared with the Tversky, two-dimensional nnU-Net, U-Net, BLAST-CT, and DeepLabv3+ models (P < .05). Focal achieved concordance values of 0.98, 0.88, and 0.99 for ICH, PHE, and ICH volumes, respectively. The mean volumetric differences between the ground truth and prediction were 0.32 mL (95% CI: -8.35, 9.00), 1.14 mL (-9.53, 11.8), and 0.06 mL (-1.71, 1.84), respectively. In conclusion, U-Net-based networks provide accurate segmentation on CT images of spontaneous ICH, and Focal loss can address class imbalance. International Clinical Trials Registry Platform (ICTRP) no. ISRCTN93732214 Supplemental material is available for this article. Â© RSNA, 2022 Keywords: Head/Neck, Brain/Brain Stem, Hemorrhage, Segmentation, Quantification, Convolutional Neural Network (CNN), Deep Learning Algorithms, Machine Learning Algorithms.","We thank all the participants, staff, and centers who took part in the Tranexamic Acid for Intracerebral Haemorrhage (TICH-2) trial.",,Radiology Artificial Intelligence,,,2022-09-28,2022,2022-09-28,2022-11-01,4,6,e220096,All OA, Green,Article,"Kok, Yong En; Pszczolkowski, Stefan; Law, Zhe Kang; Ali, Azlinawati; Krishnan, Kailash; Bath, Philip M; Sprigg, Nikola; Dineen, Robert A; French, Andrew P","Kok, Yong En (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).); Pszczolkowski, Stefan (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).); Law, Zhe Kang (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).); Ali, Azlinawati (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).); Krishnan, Kailash (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).); Bath, Philip M (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).); Sprigg, Nikola (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).); Dineen, Robert A (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).); French, Andrew P (Computer Vision Laboratory, School of Computer Science (Y.E.K., A.P.F.), Department of Radiological Sciences, Mental Health & Clinical Neuroscience (S.P., R.A.D.), Stroke Trials Unit, Mental Health & Clinical Neuroscience (Z.K.L., K.K., P.M.B., N.S.), and Sir Peter Mansfield Imaging Centre (R.A.D.), University of Nottingham, Jubilee Campus, 7301 Wollaton Rd, Lenton, Nottingham NG8â1BB, England; NIHR Nottingham Biomedical Research Centre, Nottingham, England (S.P., R.A.D.); Department of Medicine, National University of Malaysia, Kuala Lumpur, Malaysia (Z.K.L.); School of Medical Imaging, Universiti Sultan Zainal Abidin, Terengganu, Malaysia (A.A.); and Stroke, Nottingham University Hospitals NHS Trust, Nottingham, England (K.K., P.M.B., N.S.).)",,"Kok, Yong En (University of Nottingham); Pszczolkowski, Stefan (University of Nottingham); Law, Zhe Kang (University of Nottingham); Ali, Azlinawati (University of Nottingham); Krishnan, Kailash (University of Nottingham); Bath, Philip M (University of Nottingham); Sprigg, Nikola (University of Nottingham); Dineen, Robert A (University of Nottingham); French, Andrew P (University of Nottingham)",0,0,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9745441,https://app.dimensions.ai/details/publication/pub.1151421894,32 Biomedical and Clinical Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
3679,pub.1147209806,10.1038/s41598-022-10285-x,35440793,PMC9018750,A novel 2-phase residual U-net algorithm combined with optimal mass transportation for 3D brain tumor detection and segmentation,"Utilizing the optimal mass transportation (OMT) technique to convert an irregular 3D brain image into a cube, a required input format for a U-net algorithm, is a brand new idea for medical imaging research. We develop a cubic volume-measure-preserving OMT (V-OMT) model for the implementation of this conversion. The contrast-enhanced histogram equalization grayscale of fluid-attenuated inversion recovery (FLAIR) in a brain image creates the corresponding density function. We then propose an effective two-phase residual U-net algorithm combined with the V-OMT algorithm for training and validation. First, we use the residual U-net and V-OMT algorithms to precisely predict the whole tumor (WT) region. Second, we expand this predicted WT region with dilation and create a smooth function by convolving the step-like function associated with the WT region in the brain image with a 5Ã5Ã5\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5\times 5\times 5$$\end{document} blur tensor. Then, a new V-OMT algorithm with mesh refinement is constructed to allow the residual U-net algorithm to effectively train Net1âNet3 models. Finally, we propose ensemble voting postprocessing to validate the final labels of brain images. We randomly chose 1000 and 251 brain samples from the Brain Tumor Segmentation (BraTS) 2021 training dataset, which contains 1251 samples, for training and validation, respectively. The Dice scores of the WT, tumor core (TC) and enhanced tumor (ET) regions for validation computed by Net1âNet3 were 0.93705, 0.90617 and 0.87470, respectively. A significant improvement in brain tumor detection and segmentation with higher accuracy is achieved.","This work was partially supported by the Ministry of Science and Technology (MoST), the National Center for Theoretical Sciences, the ST Yau Center in Taiwan, and the Big Data Computing Center of Southeast University. W.-W. Lin and T.-M. Huang were partially supported by MoST 110-2115-M-A49-004- and 110-2115-M-003-012-MY3, respectively, and M.-H. Yueh was partially supported by MoST 109-2115-M-003-010-MY2 and 110-2115-M-003-014- . T. Li was supported in part by the National Natural Science Foundation of China (NSFC) 11971105.",,Scientific Reports,,"Algorithms; Brain; Brain Neoplasms; Disease Progression; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer",2022-04-19,2022,2022-04-19,,12,1,6452,All OA, Gold,Article,"Lin, Wen-Wei; Lin, Jia-Wei; Huang, Tsung-Ming; Li, Tiexiang; Yueh, Mei-Heng; Yau, Shing-Tung","Lin, Wen-Wei (Department of Applied Mathematics, National Yang Ming Chiao Tung University, 300, Hsinchu, Taiwan); Lin, Jia-Wei (Department of Applied Mathematics, National Yang Ming Chiao Tung University, 300, Hsinchu, Taiwan); Huang, Tsung-Ming (Department of Mathematics, National Taiwan Normal University, 116, Taipei, Taiwan); Li, Tiexiang (Nanjing Center for Applied Mathematics, 211135, Nanjing, Peopleâs Republic of China; School of Mathematics and Shing-Tung Yau Center, Southeast University, 210096, Nanjing, Peopleâs Republic of China); Yueh, Mei-Heng (Department of Mathematics, National Taiwan Normal University, 116, Taipei, Taiwan); Yau, Shing-Tung (Department of Mathematics, Harvard University, Cambridge, USA)","Huang, Tsung-Ming (National Taiwan Normal University); Li, Tiexiang (; Southeast University)","Lin, Wen-Wei (National Yang Ming Chiao Tung University); Lin, Jia-Wei (National Yang Ming Chiao Tung University); Huang, Tsung-Ming (National Taiwan Normal University); Li, Tiexiang (Southeast University); Yueh, Mei-Heng (National Taiwan Normal University); Yau, Shing-Tung (Harvard University)",1,1,,,https://www.nature.com/articles/s41598-022-10285-x.pdf,https://app.dimensions.ai/details/publication/pub.1147209806,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
3672,pub.1133288818,10.1038/s41592-020-01008-z,33288961,,nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation,"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.","This work was co-funded by the National Center for Tumor Diseases (NCT) in Heidelberg and the Helmholtz Imaging Platform (HIP). We thank our colleagues at DKFZ who were involved in the various challenge contributions, especially A. Klein, D. Zimmerer, J. Wasserthal, G. Koehler, T. Norajitra and S. Wirkert, who contributed to the Decathlon submission. We also thank the MITK team, which supported us in producing all medical dataset visualizations. We are also thankful to all the challenge organizers, who provided an important basis for our work. We want to especially mention N. Heller, who enabled the collection of all the details from the KiTS challenge through excellent challenge design, E. Kavur from the CHAOS team, who generated comprehensive leaderboard information for us, C. Petitjean, who provided detailed leaderboard information of the SegTHOR entries from ISBI 2019 and M. MaÅ¡ka, who patiently supported us during our Cell Tracking Challenge submission. We thank M. Wiesenfarth for his helpful advice concerning the ranking of methods and the visualization of rankings. We further thank C. Pape and T. Wollman for their crucial introductions to the CREMI and Cell Tracking Challenges, respectively. Last but not least, we thank O. Ronneberger and L. Maier-Hein for their important feedback on this manuscript.",,Nature Methods,,"Algorithms; Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer",2020-12-07,2020,2020-12-07,2021-02,18,2,203-211,All OA, Green,Article,"Isensee, Fabian; Jaeger, Paul F.; Kohl, Simon A. A.; Petersen, Jens; Maier-Hein, Klaus H.","Isensee, Fabian (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Faculty of Biosciences, University of Heidelberg, Heidelberg, Germany); Jaeger, Paul F. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany); Kohl, Simon A. A. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; DeepMind, London, UK); Petersen, Jens (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Faculty of Physics & Astronomy, University of Heidelberg, Heidelberg, Germany); Maier-Hein, Klaus H. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany)","Maier-Hein, Klaus H. (German Cancer Research Center; University Hospital Heidelberg)","Isensee, Fabian (German Cancer Research Center; Heidelberg University); Jaeger, Paul F. (German Cancer Research Center); Kohl, Simon A. A. (German Cancer Research Center; DeepMind (United Kingdom)); Petersen, Jens (German Cancer Research Center; Heidelberg University); Maier-Hein, Klaus H. (German Cancer Research Center; University Hospital Heidelberg)",1224,1222,142.07,,http://arxiv.org/pdf/1904.08128,https://app.dimensions.ai/details/publication/pub.1133288818,31 Biological Sciences,,,,,,,,,,,
3546,pub.1149468651,10.1002/mp.15827,35833617,,Multiscale unsupervised domain adaptation for automatic pancreas segmentation in CT volumes using adversarial learning,"PURPOSE: Computer-aided automatic pancreas segmentation is essential for early diagnosis and treatment of pancreatic diseases. However, the annotation of pancreas images requires professional doctors and considerable expenditure. Due to imaging differences among various institution population, scanning devices, imaging protocols, and so on, significant degradation in the performance of model inference results is prone to occur when models trained with domain-specific (usually institution-specific) datasets are directly applied to new (other centers/institutions) domain data. In this paper, we propose a novel unsupervised domain adaptation method based on adversarial learning to address pancreas segmentation challenges with the lack of annotations and domain shift interference.
METHODS: A 3D semantic segmentation model with attention module and residual module is designed as the backbone pancreas segmentation model. In both segmentation model and domain adaptation discriminator network, a multiscale progressively weighted structure is introduced to acquire different field of views. Features of labeled data and unlabeled data are fed in pairs into the proposed multiscale discriminator to learn domain-specific characteristics. Then the unlabeled data features with pseudodomain label are fed to the discriminator to acquire domain-ambiguous information. With this adversarial learning strategy, the performance of the segmentation network is enhanced to segment unseen unlabeledÂ data.
RESULTS: Experiments were conducted on two public annotated datasets as source datasets, respectively, and one private dataset as target dataset, where annotations were not used for the training process but only for evaluation. The 3D segmentation model achieves comparative performance with state-of-the-art pancreas segmentation methods on source domain. After implementing our domain adaptation architecture, the average dice similarity coefficient (DSC) of the segmentation model trained on the NIH-TCIA source dataset increases from 58.79% to 72.73% on the local hospital dataset, while the performance of the target domain segmentation model transferred from the medical segmentation decathlon (MSD) source dataset rises from 62.34% to 71.17%.
CONCLUSIONS: Correlations of features across data domains are utilized to train the pancreas segmentation model on unlabeled data domain, improving the generalization of the model. Our results demonstrate that the proposed method enables the segmentation model to make meaningful segmentation for unseen data of the training set. In the future, the proposed method has the potential to apply segmentation model trained on public dataset to clinical unannotated CT images from local hospital, effectively assisting radiologists in clinicalÂ practice.","This work was supported in part by the National Natural Science Foundation of China (No. 12101571, No. 82172069, No. 81702332), the Key Research Project of Zhejiang Laboratory (No. 2022ND0AC01), the Zhejiang Provincial Natural Science Foundation of China (No. LQ20H180001), the Zhejiang Provincial Key Research and Development Program (No. 2020C03117), and the Selected Project Funded by Zhejiang Province Postdoctoral Research Projects (No. 2020ND2UA01).",,Medical Physics,,"Deep Learning; Image Processing, Computer-Assisted; Pancreas; Tomography, X-Ray Computed",2022-07-27,2022,2022-07-27,2022-09,49,9,5799-5818,Closed,Article,"Zhu, Yan; Hu, Peijun; Li, Xiang; Tian, Yu; Bai, Xueli; Liang, Tingbo; Li, Jingsong","Zhu, Yan (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China); Hu, Peijun (Research Center for Healthcare Data Science, Zhejiang Lab, Hangzhou, China); Li, Xiang (Department of Hepatobiliary and Pancreatic Surgery, the First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou, China); Tian, Yu (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China); Bai, Xueli (Department of Hepatobiliary and Pancreatic Surgery, the First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou, China); Liang, Tingbo (Department of Hepatobiliary and Pancreatic Surgery, the First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou, China); Li, Jingsong (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China; Research Center for Healthcare Data Science, Zhejiang Lab, Hangzhou, China)","Li, Jingsong (Zhejiang University; Zhejiang Lab)","Zhu, Yan (Zhejiang University); Hu, Peijun (Zhejiang Lab); Li, Xiang (Zhejiang University; First Affiliated Hospital Zhejiang University); Tian, Yu (Zhejiang University); Bai, Xueli (Zhejiang University; First Affiliated Hospital Zhejiang University); Liang, Tingbo (Zhejiang University; First Affiliated Hospital Zhejiang University); Li, Jingsong (Zhejiang University; Zhejiang Lab)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149468651,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
3437,pub.1140515641,10.1371/journal.pone.0255397,34411138,PMC8375977,"Efficient, high-performance semantic segmentation using multi-scale feature extraction","The success of deep learning in recent years has arguably been driven by the availability of large datasets for training powerful predictive algorithms. In medical applications however, the sensitive nature of the data limits the collection and exchange of large-scale datasets. Privacy-preserving and collaborative learning systems can enable the successful application of machine learning in medicine. However, collaborative protocols such as federated learning require the frequent transfer of parameter updates over a network. To enable the deployment of such protocols to a wide range of systems with varying computational performance, efficient deep learning architectures for resource-constrained environments are required. Here we present MoNet, a small, highly optimized neural-network-based segmentation algorithm leveraging efficient multi-scale image features. MoNet is a shallow, U-Net-like architecture based on repeated, dilated convolutions with decreasing dilation rates. We apply and test our architecture on the challenging clinical tasks of pancreatic segmentation in computed tomography (CT) images as well as brain tumor segmentation in magnetic resonance imaging (MRI) data. We assess our model's segmentation performance and demonstrate that it provides performance on par with compared architectures while providing superior out-of-sample generalization performance, outperforming larger architectures on an independent validation set, while utilizing significantly fewer parameters. We furthermore confirm the suitability of our architecture for federated learning applications by demonstrating a substantial reduction in serialized model storage requirement as a surrogate for network data transfer. Finally, we evaluate MoNet's inference latency on the central processing unit (CPU) to determine its utility in environments without access to graphics processing units. Our implementation is publicly available as free and open-source software.","Rickmer Braren received funding from: German Research Foundation, Priority Programme SPP2177 Radiomics: Next Generation of Biomedical Imaging German Cancer Consortium Joint Funding UPGRADE Programme: Subtyping of Pancreatic Cancer based on radiographic and pathological Features Bavarian Research Foundation Deutsches Konsortium fÃ¼r Translationale Krebsforschung, SUBPAN Georgios Kaissis received funding from: The Technical University of Munich Clinician Scientist Programme, Grant Reference H-14. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. The authors wish to thank Alexander Ziller and Nicolas Remerscheid for their scientific input, Novi Quadrianto for his support in supervising the bachelor thesis that this work originated from and lastly, Karl Schulze for the excellent graphic design of the figures.","Rickmer Braren received funding from: German Research Foundation, Priority Programme SPP2177 Radiomics: Next Generation of Biomedical Imaging German Cancer Consortium Joint Funding UPGRADE Programme: Subtyping of Pancreatic Cancer based on radiographic and pathological Features Bavarian Research Foundation Deutsches Konsortium fÃ¼r Translationale Krebsforschung, SUBPAN Georgios Kaissis received funding from: The Technical University of Munich Clinician Scientist Programme, Grant Reference H-14. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.",PLOS ONE,,"Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer; Semantics; Tomography, X-Ray Computed",2021-08-19,2021,2021-08-19,,16,8,e0255397,All OA, Gold,Article,"Knolle, Moritz; Kaissis, Georgios; Jungmann, Friederike; Ziegelmayer, Sebastian; Sasse, Daniel; Makowski, Marcus; Rueckert, Daniel; Braren, Rickmer","Knolle, Moritz (Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; Institute for Artificial Intelligence and Informatics in Medicine, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany); Kaissis, Georgios (Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; Institute for Artificial Intelligence and Informatics in Medicine, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; OpenMined; Department of Computing, Imperial College London, London, United Kingdom); Jungmann, Friederike (Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany); Ziegelmayer, Sebastian (Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany); Sasse, Daniel (Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany); Makowski, Marcus (Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany); Rueckert, Daniel (Institute for Artificial Intelligence and Informatics in Medicine, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; Department of Computing, Imperial College London, London, United Kingdom); Braren, Rickmer (Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany)","Braren, Rickmer (Rechts der Isar Hospital; Technical University of Munich)","Knolle, Moritz (Rechts der Isar Hospital; Technical University of Munich; Rechts der Isar Hospital; Technical University of Munich); Kaissis, Georgios (Rechts der Isar Hospital; Technical University of Munich; Rechts der Isar Hospital; Technical University of Munich; Imperial College London); Jungmann, Friederike (Rechts der Isar Hospital; Technical University of Munich); Ziegelmayer, Sebastian (Rechts der Isar Hospital; Technical University of Munich); Sasse, Daniel (Rechts der Isar Hospital; Technical University of Munich); Makowski, Marcus (Rechts der Isar Hospital; Technical University of Munich); Rueckert, Daniel (Rechts der Isar Hospital; Technical University of Munich; Imperial College London); Braren, Rickmer (Rechts der Isar Hospital; Technical University of Munich)",6,6,0.37,4.91,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0255397&type=printable,https://app.dimensions.ai/details/publication/pub.1140515641,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
3314,pub.1149770442,10.1155/2022/8544337,35928919,PMC9345701,Detection of Pancreatic Cancer in CT Scan Images Using PSO SVM and Image Processing,"A diagnosis of pancreatic cancer is one of the worst cancers that may be received anywhere in the world; the five-year survival rate is very less. The majority of cases of this condition may be traced back to pancreatic cancer. Due to medical image scans, a significant number of cancer patients are able to identify abnormalities at an earlier stage. The expensive cost of the necessary gear and infrastructure makes it difficult to disseminate the technology, putting it out of the reach of a lot of people. This article presents detection of pancreatic cancer in CT scan images using machine PSO SVM and image processing. The Gaussian elimination filter is utilized during the image preprocessing stage of the removal of noise from images. The K means algorithm uses a partitioning technique to separate the image into its component parts. The process of identifying objects in an image and determining the regions of interest is aided by image segmentation. The PCA method is used to extract important information from digital photographs. PSO SVM, naive Bayes, and AdaBoost are the algorithms that are used to perform the classification. Accuracy, sensitivity, and specificity of the PSO SVM algorithm are better.",,,BioMed Research International,,"Algorithms; Bayes Theorem; Humans; Image Processing, Computer-Assisted; Pancreatic Neoplasms; Support Vector Machine; Tomography, X-Ray Computed",2022-07-26,2022,2022-07-26,2022-07-26,2022,,8544337,All OA, Gold,Article,"Ansari, Arshiya S; Zamani, Abu Sarwar; Mohammadi, Mohammad Sajid; Meenakshi; Ritonga, Mahyudin; Ahmed, Syed Sohail; Pounraj, Devabalan; Kaliyaperumal, Karthikeyan","Ansari, Arshiya S (Department of Information Technology, College of Computer and Information Sciences, Majmaah University, Al-Majmaah 11952, Saudi Arabia.); Zamani, Abu Sarwar (Department of Computer and Self Development, Preparatory Year Deanship, Prince Sattam Bin Abdulaziz University, Al-Kharj, Saudi Arabia.); Mohammadi, Mohammad Sajid (Department of Information Technology, College of Computer, Qassim University, Buraydah, Saudi Arabia.); Meenakshi (GD Goenka University Sohna Haryana, India.); Ritonga, Mahyudin (Universitas Muhammadiyah Sumatera Barat, Indonesia.); Ahmed, Syed Sohail (Department of Computer Engineering, Qassim University, Buraydah, Saudi Arabia.); Pounraj, Devabalan (BVC Engineering College (Autonomous), Odalarevu, Allavaram Mandal, East-GodhavariDistrict, Andhra Pradesh, India.); Kaliyaperumal, Karthikeyan (IT @ IoT-HH Campus, Ambo University, Ethiopia.)","Kaliyaperumal, Karthikeyan (Ambo University)","Ansari, Arshiya S (Majmaah University); Zamani, Abu Sarwar (Prince Sattam Bin Abdulaziz University); Mohammadi, Mohammad Sajid (Qassim University); Meenakshi (GD Goenka University); Ritonga, Mahyudin (Universitas Muhammadiyah Sumatera Barat); Ahmed, Syed Sohail (Qassim University); Pounraj, Devabalan (); Kaliyaperumal, Karthikeyan (Ambo University)",1,1,,,https://downloads.hindawi.com/journals/bmri/2022/8544337.pdf,https://app.dimensions.ai/details/publication/pub.1149770442,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
3304,pub.1152059323,10.1109/tmi.2022.3213983,36264729,,"Learn2Reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning","Image registration is a fundamental medical image analysis task, and a wide variety of approaches have been proposed. However, only a few studies have comprehensively compared medical image registration approaches on a wide range of clinically relevant tasks. This limits the development of registration methods, the adoption of research advances into practice, and a fair benchmark across competing approaches. The Learn2Reg challenge addresses these limitations by providing a multi-task medical image registration data set for comprehensive characterisation of deformable registration algorithms. A continuous evaluation will be possible at https:// learn2reg.grand-challenge.org. Learn2Reg covers a wide range of anatomies (brain, abdomen, and thorax), modalities (ultrasound, CT, MR), availability of annotations, as well as intra- and inter-patient registration evaluation. We established an easily accessible framework for training and validation of 3D registration methods, which enabled the compilation of results of over 65 individual method submissions from more than 20 unique teams. We used a complementary set of metrics, including robustness, accuracy, plausibility, and runtime, enabling unique insight into the current state-of-the-art of medical image registration. This paper describes datasets, tasks, evaluation methods and results of the challenge, as well as results of further analysis of transferability to new datasets, the importance of label supervision, and resulting bias. While no single approach worked best across all tasks, many methodological aspects could be identified that push the performance of medical image registration to new state-of-the-art performance. Furthermore, we demystified the common belief that conventional registration methods have to be much slower than deep-learning-based methods.",,,IEEE Transactions on Medical Imaging,,,2022-10-20,2022,2022-10-20,2022-10-20,PP,99,1-1,All OA, Hybrid,Article,"Hering, Alessa; Hansen, Lasse; Mok, Tony C. W.; Chung, Albert C. S.; Siebert, Hanna; Hager, Stephanie; Lange, Annkristin; Kuckertz, Sven; Heldmann, Stefan; Shao, Wei; Vesal, Sulaiman; Rusu, Mirabela; Sonn, Geoffrey; Estienne, Theo; Vakalopoulou, Maria; Han, Luyi; Huang, Yunzhi; Yap, Pew-Thian; Brudfors, Mikael; Balbastre, Yael; Joutard, Samuel; Modat, Marc; Lifshitz, Gal; Raviv, Dan; Lv, Jinxin; Li, Qiang; Jaouen, Vincent; Visvikis, Dimitris; Fourcade, Constance; Rubeaux, Mathieu; Pan, Wentao; Xu, Zhe; Jian, Bailiang; De Benetti, Francesca; Wodzinski, Marek; Gunnarsson, Niklas; Sjolund, Jens; Grzech, Daniel; Qiu, Huaqi; Li, Zeju; Thorley, Alexander; Duan, Jinming; Grossbrohmer, Christoph; Hoopes, Andrew; Reinertsen, Ingerid; Xiao, Yiming; Landman, Bennett; Huo, Yuankai; Murphy, Keelin; Lessmann, Nikolas; Van Ginneken, Bram; Dalca, Adrian V.; Heinrich, Mattias P.","Hering, Alessa (Dept. of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, GA, NL); Hansen, Lasse (Institute of Medical Informatics, Universit&#x00E4;t zu L&#x00FC;beck, L&#x00FC;beck, DE, Germany); Mok, Tony C. W. (Dept. of Computer Science and Engineering, The Hong Kong University of Science and Technology, HK, China); Chung, Albert C. S. (Dept. of Computer Science and Engineering, The Hong Kong University of Science and Technology, HK, China); Siebert, Hanna (Institute of Medical Informatics, Universit&#x00E4;t zu L&#x00FC;beck, L&#x00FC;beck, DE, Germany); Hager, Stephanie (Fraunhofer MEVIS, Institute for Digital Medicine, L&#x00FC;beck, DE, Germany); Lange, Annkristin (Fraunhofer MEVIS, Institute for Digital Medicine, L&#x00FC;beck, DE, Germany); Kuckertz, Sven (Fraunhofer MEVIS, Institute for Digital Medicine, L&#x00FC;beck, DE, Germany); Heldmann, Stefan (Fraunhofer MEVIS, Institute for Digital Medicine, L&#x00FC;beck, DE, Germany); Shao, Wei (Dept. of Radiology, Stanford University, Stanford, US); Vesal, Sulaiman (Dept. of Urology, Stanford University, Stanford, US); Rusu, Mirabela (Dept. of Radiology, Stanford University, Stanford, US); Sonn, Geoffrey (Dept. of Urology, Stanford University, Stanford, US); Estienne, Theo (Math&#x00E9;matiques et Informatique pour la Complexit&#x00E9; et les Syst&#x00E8;mes, Inria Saclay, Universit&#x00E9; Paris-Saclay, CentraleSup&#x00E9;lec, Gif-sur-Yvette, FR); Vakalopoulou, Maria (Math&#x00E9;matiques et Informatique pour la Complexit&#x00E9; et les Syst&#x00E8;mes, Inria Saclay, Universit&#x00E9; Paris-Saclay, CentraleSup&#x00E9;lec, Gif-sur-Yvette, FR); Han, Luyi (Dept. of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, NL); Huang, Yunzhi (School of Automation, Nanjing University of Information Science and Technology, Nanjing, CN); Yap, Pew-Thian (Dept. of Radiology and Biomedical Research Imaging Center, University of North Carolina, Chapel Hill, US); Brudfors, Mikael (King&#x2019;s College, London, UK); Balbastre, Yael (Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, US); Joutard, Samuel (King&#x2019;s College, London, UK); Modat, Marc (King&#x2019;s College, London, UK); Lifshitz, Gal (Tel Aviv University, IL, USA); Raviv, Dan (Tel Aviv University, IL, USA); Lv, Jinxin (Wuhan National Laboratory for Optoelectronics-Huazhong University of Science and Technology, Wuhan, CN); Li, Qiang (Wuhan National Laboratory for Optoelectronics-Huazhong University of Science and Technology, Wuhan, CN); Jaouen, Vincent (UMR 1101 LaTIM, IMT Atlantique, Inserm, Brest, FR); Visvikis, Dimitris (UMR 1101 LaTIM, IMT Atlantique, Inserm, Brest, FR); Fourcade, Constance (Ecole Centrale de Nantes, LS2N, UMR CNRS, Nantes, FR); Rubeaux, Mathieu (Keosys Medical Imaging, Saint Herblain, FR); Pan, Wentao (Shenzhen International Graduate School, Tsinghua University, CN); Xu, Zhe (Dept. of Biomedical Engineering, The Chinese University of Hong Kong, HK, China); Jian, Bailiang (Chair for Computer Aided Medical Procedures and Augmented Reality, TUM, Garching, DE, Germany); De Benetti, Francesca (Chair for Computer Aided Medical Procedures and Augmented Reality, TUM, Garching, DE, Germany); Wodzinski, Marek (Dept. of Measurement and Electronics, AGH University of Science and Technology, Krakow, PL); Gunnarsson, Niklas (Dept. of Information Technology, Uppsala University, Uppsala, SE); Sjolund, Jens (Dept. of Information Technology, Uppsala University, Uppsala, SE); Grzech, Daniel (Dept. of Computing, Imperial College London, UK); Qiu, Huaqi (Dept. of Computing, Imperial College London, UK); Li, Zeju (Dept. of Computing, Imperial College London, UK); Thorley, Alexander (University of Birmingham, UK); Duan, Jinming (University of Birmingham, UK); Grossbrohmer, Christoph (Institute of Medical Informatics, Universit&#x00E4;t zu L&#x00FC;beck, L&#x00FC;beck, DE, Germany); Hoopes, Andrew (Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, US); Reinertsen, Ingerid (Dept. Health Research, SINTEF Digital, Trondheim, NO); Xiao, Yiming (Western University, London, CA); Landman, Bennett (Dept. of Electrical and Computer Engineering, Vanderbilt University, Nashville, US); Huo, Yuankai (Dept. of Electrical and Computer Engineering, Vanderbilt University, Nashville, US); Murphy, Keelin (Dept. of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, NL); Lessmann, Nikolas (Dept. of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, NL); Van Ginneken, Bram (Dept. of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, GA, NL); Dalca, Adrian V. (Harvard Medical School, Boston, US); Heinrich, Mattias P. (Institute of Medical Informatics, Universit&#x00E4;t zu L&#x00FC;beck, L&#x00FC;beck, DE, Germany)",,"Hering, Alessa (Radboud University Nijmegen Medical Centre); Hansen, Lasse (); Mok, Tony C. W. (Hong Kong University of Science and Technology); Chung, Albert C. S. (Hong Kong University of Science and Technology); Siebert, Hanna (); Hager, Stephanie (Fraunhofer Institute for Digital Medicine); Lange, Annkristin (Fraunhofer Institute for Digital Medicine); Kuckertz, Sven (Fraunhofer Institute for Digital Medicine); Heldmann, Stefan (Fraunhofer Institute for Digital Medicine); Shao, Wei (Stanford University); Vesal, Sulaiman (Stanford University); Rusu, Mirabela (Stanford University); Sonn, Geoffrey (Stanford University); Estienne, Theo (Inria Saclay - Ãle-de-France Research Centre); Vakalopoulou, Maria (Inria Saclay - Ãle-de-France Research Centre); Han, Luyi (Radboud University Nijmegen Medical Centre); Huang, Yunzhi (Nanjing University of Information Science and Technology); Yap, Pew-Thian (University of North Carolina System); Brudfors, Mikael (); Balbastre, Yael (Massachusetts General Hospital); Joutard, Samuel (); Modat, Marc (); Lifshitz, Gal (Tel Aviv University); Raviv, Dan (Tel Aviv University); Lv, Jinxin (Huazhong University of Science and Technology); Li, Qiang (Huazhong University of Science and Technology); Jaouen, Vincent (IMT Atlantique); Visvikis, Dimitris (IMT Atlantique); Fourcade, Constance (Laboratoire des Sciences du NumÃ©rique de Nantes); Rubeaux, Mathieu (); Pan, Wentao (Tsinghua University); Xu, Zhe (Chinese University of Hong Kong); Jian, Bailiang (); De Benetti, Francesca (); Wodzinski, Marek (AGH University of Science and Technology); Gunnarsson, Niklas (Uppsala University); Sjolund, Jens (Uppsala University); Grzech, Daniel (Imperial College London); Qiu, Huaqi (Imperial College London); Li, Zeju (Imperial College London); Thorley, Alexander (University of Birmingham); Duan, Jinming (University of Birmingham); Grossbrohmer, Christoph (); Hoopes, Andrew (Massachusetts General Hospital); Reinertsen, Ingerid (SINTEF); Xiao, Yiming (Western University); Landman, Bennett (Vanderbilt University); Huo, Yuankai (Vanderbilt University); Murphy, Keelin (Radboud University Nijmegen Medical Centre); Lessmann, Nikolas (Radboud University Nijmegen Medical Centre); Van Ginneken, Bram (Radboud University Nijmegen Medical Centre); Dalca, Adrian V. (Harvard University); Heinrich, Mattias P. ()",12,12,,,https://ieeexplore.ieee.org/ielx7/42/4359023/09925717.pdf,https://app.dimensions.ai/details/publication/pub.1152059323,40 Engineering, 46 Information and Computing Sciences,,,,,,,,,,
3304,pub.1150555625,10.1016/j.media.2022.102597,36095907,,Enhancing MR image segmentation with realistic adversarial data augmentation,"The success of neural networks on medical image segmentation tasks typically relies on large labeled datasets for model training. However, acquiring and manually labeling a large medical image set is resource-intensive, expensive, and sometimes impractical due to data sharing and privacy issues. To address this challenge, we propose AdvChain, a generic adversarial data augmentation framework, aiming at improving both the diversity and effectiveness of training data for medical image segmentation tasks. AdvChainÂ augments data with dynamic data augmentation, generating randomly chained photo-metric and geometric transformations to resemble realistic yet challenging imaging variations to expand training data. By jointly optimizing the data augmentation model and a segmentation network during training, challenging examples are generated to enhance network generalizability for the downstream task. The proposed adversarial data augmentation does not rely on generative networks and can be used as a plug-in module in general segmentation networks. It is computationally efficient and applicable for both low-shot supervised and semi-supervised learning. We analyze and evaluate the method on two MR image segmentation tasks: cardiac segmentation and prostate segmentation with limited labeled data. Results show that the proposed approach can alleviate the need for labeled data while improving model generalization ability, indicating its practical value in medical imaging applications.","This work was supported by two EPSRC Grants (EP/P001009/1, EP/R005982/1) and the ERC Grant (884622). W. Bai was supported by EPSRC DeepGeM Grant (EP/W01842X/1). S. Wang was supported by the Shanghai Sailing Programs of Shanghai Municipal Science and Technology Committee, China (22YF1409300).",,Medical Image Analysis,,"Humans; Male; Image Processing, Computer-Assisted; Neural Networks, Computer; Supervised Machine Learning",2022-08-28,2022,2022-08-28,2022-11,82,,102597,All OA, Hybrid,Article,"Chen, Chen; Qin, Chen; Ouyang, Cheng; Li, Zeju; Wang, Shuo; Qiu, Huaqi; Chen, Liang; Tarroni, Giacomo; Bai, Wenjia; Rueckert, Daniel","Chen, Chen (Department of Computing, Imperial College London, UK. Electronic address: chen.chen15@imperial.ac.uk.); Qin, Chen (Institute for Digital Communications, School of Engineering, University of Edinburgh, UK; Department of Electronics and Electrical Engineering, Imperial College London, UK.); Ouyang, Cheng (Department of Computing, Imperial College London, UK.); Li, Zeju (Department of Computing, Imperial College London, UK.); Wang, Shuo (Digital Medicine Research Centre, School of Basic Medical Sciences, Fudan University, China; Shanghai Key Laboratory of MICCAI, Shanghai, China.); Qiu, Huaqi (Department of Computing, Imperial College London, UK.); Chen, Liang (Department of Computing, Imperial College London, UK.); Tarroni, Giacomo (Department of Computing, Imperial College London, UK; CitAI Research Centre, Department of Computer Science, City, University of London, UK.); Bai, Wenjia (Department of Computing, Imperial College London, UK; Department of Brain Sciences, Imperial College London, UK; Data Science Institute, Imperial College London, UK.); Rueckert, Daniel (Department of Computing, Imperial College London, UK; Klinikum rechts der Isar, Technical University of Munich, Germany.)","Chen, Chen (Imperial College London)","Chen, Chen (Imperial College London); Qin, Chen (University of Edinburgh; Imperial College London); Ouyang, Cheng (Imperial College London); Li, Zeju (Imperial College London); Wang, Shuo (Fudan University); Qiu, Huaqi (Imperial College London); Chen, Liang (Imperial College London); Tarroni, Giacomo (Imperial College London; University of London); Bai, Wenjia (Imperial College London); Rueckert, Daniel (Imperial College London; Rechts der Isar Hospital; Technical University of Munich)",7,7,,,https://doi.org/10.1016/j.media.2022.102597,https://app.dimensions.ai/details/publication/pub.1150555625,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
3204,pub.1151067507,10.1142/s0129065722500599,36328969,,Mixture 2D Convolutions for 3D Medical Image Segmentation,"Three-dimensional (3D) medical image segmentation plays a crucial role in medical care applications. Although various two-dimensional (2D) and 3D neural network models have been applied to 3D medical image segmentation and achieved impressive results, a trade-off remains between efficiency and accuracy. To address this issue, a novel mixture convolutional network (MixConvNet) is proposed, in which traditional 2D/3D convolutional blocks are replaced with novel MixConv blocks. In the MixConv block, 3D convolution is decomposed into a mixture of 2D convolutions from different views. Therefore, the MixConv block fully utilizes the advantages of 2D convolution and maintains the learning ability of 3D convolution. It acts as 3D convolutions and thus can process volumetric input directly and learn intra-slice features, which are absent in the traditional 2D convolutional block. By contrast, the proposed MixConv block only contains 2D convolutions; hence, it has significantly fewer trainable parameters and less computation budget than a block containing 3D convolutions. Furthermore, the proposed MixConvNet is pre-trained with small input patches and fine-tuned with large input patches to improve segmentation performance further. In experiments on the Decathlon Heart dataset and Sliver07 dataset, the proposed MixConvNet outperformed the state-of-the-art methods such as UNet3D, VNet, and nnUnet.",,,International Journal of Neural Systems,,"Imaging, Three-Dimensional; Neural Networks, Computer; Image Processing, Computer-Assisted",2022-11-04,2022,2022-11-04,2023-01,33,1,2250059,Closed,Article,"Wang, Jianyong; Zhang, Lei; Zhang, Yi","Wang, Jianyong (Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu, Sichuan, P. R. China.); Zhang, Lei (Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu, Sichuan, P. R. China.); Zhang, Yi (Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu, Sichuan, P. R. China.)","Zhang, Yi (Sichuan University)","Wang, Jianyong (Sichuan University); Zhang, Lei (Sichuan University); Zhang, Yi (Sichuan University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151067507,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
3198,pub.1142363642,10.1088/1361-6560/ac35c7,34727529,,Liver tumor detection based on objects as points,"The automatic detection of liver tumors by computed tomography is challenging, owing to their wide variations in size and location, as well as to their irregular shapes. Existing detection methods largely rely on two-stage detectors and use CT images marked with bounding boxes for training and detection. In this study, we propose a single-stage detector method designed to accurately detect multiple tumors simultaneously, and provide results demonstrating its increased speed and efficiency compared to prior methods. The proposed model divides CT images into multiple channels to obtain continuity information and implements a bounding box attention mechanism to overcome the limitation of inaccurate prediction of tumor center points and decrease redundant bounding boxes. The model integrates information from various channels using an effective Squeeze-and-Excitation attention module. The proposed model obtained a mean average precision result of 0.476 on the Decathlon dataset, which was superior to that of the prior methods examined for comparison. This research is expected to enable physicians to diagnose tumors very efficiently; particularly, the prediction of tumor center points is expected to enable physicians to rapidly verify their diagnostic judgments. The proposed method is considered suitable for future adoption in clinical practice in hospitals and resource-poor areas because its superior performance does not increase computational cost; hence, the equipment required is relatively inexpensive.",,,Physics in Medicine and Biology,,"Abdomen; Humans; Liver Neoplasms; Radiopharmaceuticals; Tomography, X-Ray Computed",2021-11-29,2021,2021-11-29,2021-12-07,66,23,235009,Closed,Article,"Peng, Xuefeng; Yang, Xinwu","Peng, Xuefeng (The Faculty of Information, Beijing, University of Technology, Beijing, Peopleâs Republic of China); Yang, Xinwu (The Faculty of Information, Beijing, University of Technology, Beijing, Peopleâs Republic of China)","Yang, Xinwu (Beijing University of Technology)","Peng, Xuefeng (Beijing University of Technology); Yang, Xinwu (Beijing University of Technology)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142363642,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
3189,pub.1150097695,10.1007/s11102-022-01255-7,35943676,,Generating novel pituitary datasets from open-source imaging data and deep volumetric segmentation,"PurposeThe estimated incidence of pituitary adenomas in the general population is 10â30%, yet radiographic diagnosis remains a challenge. Diagnosis is complicated by the heterogeneity of radiographic features in both normal (e.g. complex anatomy, pregnancy) and pathologic states (e.g. primary endocrinopathy, hypophysitis). Clinical symptoms and laboratory testing are often equivocal, which can result in misdiagnosis or unnecessary specialist referrals. Computer vision models can aid in pituitary adenoma diagnosis; however, a major challenge to model development is the lack of dedicated pituitary imaging datasets. We hypothesized that deep volumetric segmentation models trained to extract the sellar and parasellar region from existing whole-brain MRI scans could be used to generate a novel dataset of pituitary imaging.MethodsSix open-source whole-brain MRI datasets, created for research purposes, were included for model development. Deep learning-based volumetric segmentation models were trained using 318 manually annotated MRI scans from a single open-source MRI dataset. Out-of-distribution volumetric segmentation performance was then tested on 418 MRIs from five held-out research datasets.ResultsOn our annotated images, agreement between manual and model volumetric segmentations was high. Dice scores (a measure of overlap) ranged 0.76â0.82 for both in-distribution and out-of-distribution model testing. In total, 6,755 MRIs from six data sources were included in the final generated pituitary dataset.ConclusionsWe present the first and largest dataset of pituitary imaging constructed using existing MRI data and deep volumetric segmentation models trained to identify sellar and parasellar anatomy. The model generalizes well across patient populations and MRI scanner types. We hope our pituitary dataset will be an integral part of future machine learning research on pituitary pathologies.",,This work was supported by the Neurosurgery Research & Education Foundation (2021 Medical Student Summer Research Fellowship to R. Gologorsky).,Pituitary,,Humans, Female, Pregnancy, Pituitary Diseases, Pituitary Gland, Pituitary Neoplasms, Hypophysitis, Neuroimaging,2022-08-09,2022,2022-08-09,2022-12,25,6,842-853,Closed,Article,"Gologorsky, Rachel; Harake, Edward; von Oiste, Grace; Nasir-Moin, Mustafa; Couldwell, William; Oermann, Eric; Hollon, Todd","Gologorsky, Rachel (Department of Medicine, Icahn School of Medicine at Mount Sinai, 1 Gustave L. Levy Pl, 10029, New York, NY, USA); Harake, Edward (Department of Medicine, University of Michigan Medical School, 1500 E Medical Center Dr, 48109, Ann Arbor, MI, USA); von Oiste, Grace (Department of Neurosurgery, NYU Langone Health System, 530 First Ave, 10016, New York, NY, USA); Nasir-Moin, Mustafa (Department of Neurosurgery, NYU Langone Health System, 530 First Ave, 10016, New York, NY, USA); Couldwell, William (Department of Neurosurgery, University of Utah, 201 Presidentsâ Cir, 84132, Salt Lake City, UT, USA); Oermann, Eric (Department of Neurosurgery, NYU Langone Health System, 530 First Ave, 10016, New York, NY, USA; Department of Radiology, NYU Langone Health System, 530 First Ave, 10016, New York, NY, USA; Center for Data Science, New York University, 60 5th Ave, 10011, New York, NY, USA); Hollon, Todd (Machine Learning in Neurosurgery Laboratory, Department of Neurosurgery, University of Michigan, 1500 E Medical Center Dr, 48109, Ann Arbor, MI, USA)","Hollon, Todd (University of MichiganâAnn Arbor)","Gologorsky, Rachel (Icahn School of Medicine at Mount Sinai); Harake, Edward (University of MichiganâAnn Arbor); von Oiste, Grace (New York University); Nasir-Moin, Mustafa (New York University); Couldwell, William (University of Utah); Oermann, Eric (New York University; New York University; New York University); Hollon, Todd (University of MichiganâAnn Arbor)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150097695,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,
3189,pub.1136534521,10.1002/mp.14852,33742454,,Interactive contouring through contextual deep learning,"PURPOSE: To investigate a deep learning approach that enables three-dimensional (3D) segmentation of an arbitrary structure of interest given a user provided two-dimensional (2D) contour for context. Such an approach could decrease delineation times and improve contouring consistency, particularly for anatomical structures for which no automatic segmentation tools exist.
METHODS: A series of deep learning segmentation models using a Recurrent Residual U-Net with attention gates was trained with a successively expanding training set. Contextual information was provided to the models, using a previously contoured slice as an input, in addition to the slice to be contoured. In total, 6 models were developed, and 19 different anatomical structures were used for training and testing. Each of the models was evaluated for all 19 structures, even if they were excluded from the training set, in order to assess the model's ability to segment unseen structures of interest. Each model's performance was evaluated using the Dice similarity coefficient (DSC), Hausdorff distance, and relative added path length (APL).
RESULTS: The segmentation performance for seen and unseen structures improved when the training set was expanded by addition of structures previously excluded from the training set. A model trained exclusively on heart structures achieved a DSC of 0.33, HD of 44Â mm, and relative APL of 0.85 when segmenting the spleen, whereas a model trained on a diverse set of structures, but still excluding the spleen, achieved a DSC of 0.80, HD of 13Â mm, and relative APL of 0.35. Iterative prediction performed better compared to direct prediction when considering unseen structures.
CONCLUSIONS: Training a contextual deep learning model on a diverse set of structures increases the segmentation performance for the structures in the training set, but importantly enables the model to generalize and make predictions even for unseen structures that were not represented in the training set. This shows that user-provided context can be incorporated into deep learning contouring to facilitate semi-automatic segmentation of CT images for any given structure. Such an approach can enable faster de-novo contouring in clinical practice.",This project has received funding from the European Union&#x27,s Horizon 2020 research and innovation programme under the Marie SkÅodowskaâCurie grant agreement No. 766276. KAV acknowledges funding support from CRUK (A15935) and the CRUK Radnet Centre (A28736).,,Medical Physics,,"Deep Learning; Heart; Image Processing, Computer-Assisted; Tomography, X-Ray Computed",2021-05-03,2021,2021-05-03,2021-06,48,6,2951-2959,All OA, Green,Article,"Trimpl, Michael J.; Boukerroui, Djamal; Stride, Eleanor P. J.; Vallis, Katherine A.; Gooding, Mark J.","Trimpl, Michael J. (Mirada Medical Ltd, Oxford, UK; Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, UK; Oxford Institute for Radiation Oncology, University of Oxford, Oxford, UK); Boukerroui, Djamal (Mirada Medical Ltd, Oxford, UK); Stride, Eleanor P. J. (Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, UK); Vallis, Katherine A. (Oxford Institute for Radiation Oncology, University of Oxford, Oxford, UK); Gooding, Mark J. (Mirada Medical Ltd, Oxford, UK)","Trimpl, Michael J. (Mirada Medical (United Kingdom); University of Oxford; University of Oxford)","Trimpl, Michael J. (Mirada Medical (United Kingdom); University of Oxford; University of Oxford); Boukerroui, Djamal (Mirada Medical (United Kingdom)); Stride, Eleanor P. J. (University of Oxford); Vallis, Katherine A. (University of Oxford); Gooding, Mark J. (Mirada Medical (United Kingdom))",6,6,1.56,7.51,https://ora.ox.ac.uk/objects/uuid:52cc5990-81c9-42af-935f-a18ddf3d16db/files/r2j62s513w,https://app.dimensions.ai/details/publication/pub.1136534521,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
3182,pub.1147249273,10.1002/mp.15679,35443086,,MultiâeXpert fusion: An ensemble learning framework to segment 3D TRUS prostate images,"PURPOSE: Prostate segmentation of 3D TRUS images is a prerequisite for several diagnostic and therapeutic applications. Unfortunately, this difficult task suffers from high intra and interobserver variability, even for experienced urologists/radiologists. This is why automatic segmentation algorithms could have a significant clinicalÂ added-value.
METHODS: This paper introduces a new deep segmentation architecture consisting of two main phases: view-specific segmentations of 2D slices and their fusion. The segmentation phase is based on three segmentation networks trained in parallel on specific slice viewing directions: axial, coronal, and sagittal. The proposed fusion network is then fed with the output of the segmentation networks and trained to produce three confidence maps. These maps correspond to the local trust granted by the fusion network to each view-specific segmentation network. Finally, for a given slice, the segmentation is computed by combining these confidence maps with their corresponding segmentations. The 3D segmentation of the prostate is obtained by restacking all the segmented slices to form aÂ volume.
RESULTS: This approach was evaluated on a database of 100 patients with several combinations of network architectures (for both the segmentation phase and the fusion phase) to show the flexibility and reliability of the framework. The proposed approach was also compared to STAPLE, to the majority voting strategy, and to a direct 3D approach tested on the same database. The new method outperforms these three approaches on all evaluation criteria. Finally, the results of the multi-eXpert fusion (MXF) framework compare favorably with other state-of-the-art methods, while these methods typically work on smallerÂ databases.
CONCLUSIONS: We proposed a novel MXF framework to segment 3D TRUS images of the prostate. The main feature of this approach is the fusion of expert network results at the pixel level using computed confidence maps. Experiments conducted on a clinical database have shown the robustness and flexibility of this approach and its superiority over state-of-the-art approaches. Finally, the MXF framework demonstrated its ability to capture and preserve the underlying gland structures, particularly in the base and apexÂ regions.",The authors want to thank the urologists from Grenoble University Hospital for providing the data.,,Medical Physics,,"Algorithms; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Machine Learning; Male; Prostate; Reproducibility of Results",2022-04-29,2022,2022-04-29,2022-08,49,8,5138-5148,All OA, Green,Article,"Beitone, ClÃ©ment; Troccaz, Jocelyne","Beitone, ClÃ©ment (Univ. Grenoble Alpes, CNRS, CHU Grenoble Alpes, Grenoble INP, TIMCâIMAG, Grenoble, France); Troccaz, Jocelyne (Univ. Grenoble Alpes, CNRS, CHU Grenoble Alpes, Grenoble INP, TIMCâIMAG, Grenoble, France)","Beitone, ClÃ©ment (Grenoble Institute of Technology)","Beitone, ClÃ©ment (Grenoble Institute of Technology); Troccaz, Jocelyne (Grenoble Institute of Technology)",0,0,,,https://hal.archives-ouvertes.fr/hal-03654488/file/final-soumis.pdf,https://app.dimensions.ai/details/publication/pub.1147249273,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
3182,pub.1134892452,10.1007/s00234-021-02649-3,33501512,,Application of deep learning for automatic segmentation of brain tumors on magnetic resonance imaging: a heuristic approach in the clinical scenario,"PurposeAccurate brain tumor segmentation on magnetic resonance imaging (MRI) has wide-ranging applications such as radiosurgery planning. Advances in artificial intelligence, especially deep learning (DL), allow development of automatic segmentation that overcome the labor-intensive and operator-dependent manual segmentation. We aimed to evaluate the accuracy of the top-performing DL model from the 2018 Brain Tumor Segmentation (BraTS) challenge, the impact of missing MRI sequences, and whether a model trained on gliomas can accurately segment other brain tumor types.MethodsWe trained the model using Medical Decathlon dataset, applied it to the BraTS 2019 glioma dataset, and developed additional models using individual and multimodal MRI sequences. The Dice score was calculated to assess the modelâs accuracy compared to ground truth labels by neuroradiologists on BraTS dataset. The model was then applied to a local dataset of 105 brain tumors, performance of which was qualitatively evaluated.ResultsThe DL model using pre- and post-gadolinium contrast T1 and T2 FLAIR sequences performed best, with a Dice score 0.878 for whole tumor, 0.732 tumor core, and 0.699 active tumor. Lack of T1 or T2 sequences did not significantly degrade performance, but FLAIR and T1C were important contributors. All segmentations performed by the model in the local dataset, including non-glioma cases, were considered accurate by a pool of specialists.ConclusionThe DL model could use available MRI sequences to optimize glioma segmentation and adopt transfer learning to segment non-glioma tumors, thereby serving as a useful tool to improve treatment planning and personalized surveillance of patients.","Special thanks to the Macquarie Neurosurgery members, above all Prof. Marcus Stoodley, as well as to Prof. Roger Chung, Associate Dean, Faculty of Medicine, Human and Health Sciences, and Prof. Enrico Coiera, Director of the Centre for Health Informatics and Australian Institute of Health Innovation, Macquarie University, for their uninterrupted support.",,Neuroradiology,,"Artificial Intelligence; Brain Neoplasms; Deep Learning; Heuristics; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Magnetic Resonance Spectroscopy",2021-01-26,2021,2021-01-26,2021-08,63,8,1253-1262,Closed,Article,"Di Ieva, Antonio; Russo, Carlo; Liu, Sidong; Jian, Anne; Bai, Michael Y.; Qian, Yi; Magnussen, John S.","Di Ieva, Antonio (Department of Clinical Medicine, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia; Computational NeuroSurgery (CNS) Lab, Department of Clinical Medicine, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia; Macquarie Neurosurgery, Macquarie University, Level 2, Suite 201, 2 Technology Place, 2109, Sydney, NSW, Australia); Russo, Carlo (Computational NeuroSurgery (CNS) Lab, Department of Clinical Medicine, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia); Liu, Sidong (Computational NeuroSurgery (CNS) Lab, Department of Clinical Medicine, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia; Australian Institute of Health Innovation, Centre for Health Informatics, Macquarie University, Sydney, Australia); Jian, Anne (Computational NeuroSurgery (CNS) Lab, Department of Clinical Medicine, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia; Melbourne Medical School, University of Melbourne, Melbourne, Australia); Bai, Michael Y. (Computational NeuroSurgery (CNS) Lab, Department of Clinical Medicine, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia); Qian, Yi (Computational NeuroSurgery (CNS) Lab, Department of Clinical Medicine, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia; Department of Biomedical Sciences, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia); Magnussen, John S. (Computational NeuroSurgery (CNS) Lab, Department of Clinical Medicine, Faculty of Medicine, Human and Health Sciences, Macquarie University, Sydney, Australia; Macquarie Medical Imaging, Macquarie University, Sydney, New South Wales, Australia)","Di Ieva, Antonio (Macquarie University; Macquarie University; Macquarie University)","Di Ieva, Antonio (Macquarie University; Macquarie University; Macquarie University); Russo, Carlo (Macquarie University); Liu, Sidong (Macquarie University; Macquarie University); Jian, Anne (Macquarie University; University of Melbourne); Bai, Michael Y. (Macquarie University); Qian, Yi (Macquarie University; Macquarie University); Magnussen, John S. (Macquarie University; Macquarie University)",15,15,3.91,9.09,,https://app.dimensions.ai/details/publication/pub.1134892452,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,,
3169,pub.1130310761,10.1016/s2589-7500(20)30188-6,33328112,,Convolutional neural network for the detection of pancreatic cancer on CT scans â Authors' reply,,,,The Lancet Digital Health,,"Humans; Neural Networks, Computer; Pancreatic Neoplasms; Tomography, X-Ray Computed",2020-09,2020,,2020-09,2,9,e454,All OA, Gold,Article,"Liao, Wei-Chih; Simpson, Amber L; Wang, Weichung","Liao, Wei-Chih (Division of Gastroenterology and Hepatology, Department of Internal Medicine, National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei 10002, Taiwan; Internal Medicine, College of Medicine, National Taiwan University, Taipei, Taiwan); Simpson, Amber L (School of Computing and Department of Biomedical and Molecular Sciences, Queen's University, Kingston, ON, Canada); Wang, Weichung (Institute of Applied Mathematical Sciences, National Taiwan University, Taipei, Taiwan)",,"Liao, Wei-Chih (National Taiwan University; National Taiwan University Hospital; National Taiwan University); Simpson, Amber L (Queen's University); Wang, Weichung (National Taiwan University)",3,3,0.38,1.53,http://www.thelancet.com/article/S2589750020301886/pdf,https://app.dimensions.ai/details/publication/pub.1130310761,42 Health Sciences, 4203 Health Services and Systems,,,,,,,,,,
3167,pub.1147926563,10.1007/s42979-022-01166-1,35602289,PMC9112642,Survey of Supervised Learning for Medical Image Processing,"Medical image interpretation is an essential task for the correct diagnosis of many diseases. Pathologists, radiologists, physicians, and researchers rely heavily on medical images to perform diagnoses and develop new treatments. However, manual medical image analysis is tedious and time consuming, making it necessary to identify accurate automated methods. Deep learningâespecially supervised deep learningâshows impressive performance in the classification, detection, and segmentation of medical images and has proven comparable in ability to humans. This survey aims to help researchers and practitioners of medical image analysis understand the key concepts and algorithms of supervised learning techniques. Specifically, this survey explains the performance metrics of supervised learning methods; summarizes the available medical datasets; studies the state-of-the-art supervised learning architectures for medical imaging processing, including convolutional neural networks (CNNs) and their corresponding algorithms, region-based CNNs and their variants, fully convolutional networks (FCN) and U-Net architecture; and discusses the trends and challenges in the application of supervised learning methods to medical image analysis. Supervised learning requires large labeled datasets to learn and achieve good performance, and data augmentation, transfer learning, and dropout techniques have widely been employed in medical image processing to overcome the lack of such datasets.",,,SN Computer Science,,,2022-05-17,2022,2022-05-17,2022,3,4,292,All OA, Bronze,Article,"Aljuaid, Abeer; Anwar, Mohd","Aljuaid, Abeer (Department of Computer Science, North Carolina A&T State University, 1601 E Market St, 27411, Greensboro, NC, USA); Anwar, Mohd (Department of Computer Science, North Carolina A&T State University, 1601 E Market St, 27411, Greensboro, NC, USA)","Anwar, Mohd (North Carolina Agricultural and Technical State University)","Aljuaid, Abeer (North Carolina Agricultural and Technical State University); Anwar, Mohd (North Carolina Agricultural and Technical State University)",6,6,,,https://link.springer.com/content/pdf/10.1007/s42979-022-01166-1.pdf,https://app.dimensions.ai/details/publication/pub.1147926563,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
3092,pub.1150961928,10.1148/radiol.220152,36098642,,Pancreatic Cancer Detection on CT Scans with Deep Learning: A Nationwide Population-based Study,"Background Approximately 40% of pancreatic tumors smaller than 2 cm are missed at abdominal CT. Purpose To develop and to validate a deep learning (DL)-based tool able to detect pancreatic cancer at CT. Materials and Methods Retrospectively collected contrast-enhanced CT studies in patients diagnosed with pancreatic cancer between January 2006 and July 2018 were compared with CT studies of individuals with a normal pancreas (control group) obtained between January 2004 and December 2019. An end-to-end tool comprising a segmentation convolutional neural network (CNN) and a classifier ensembling five CNNs was developed and validated in the internal test set and a nationwide real-world validation set. The sensitivities of the computer-aided detection (CAD) tool and radiologist interpretation were compared using the McNemar test. Results A total of 546 patients with pancreatic cancer (mean age, 65 years Â± 12 [SD], 297 men) and 733 control subjects were randomly divided into training, validation, and test sets. In the internal test set, the DL tool achieved 89.9% (98 of 109; 95% CI: 82.7, 94.9) sensitivity and 95.9% (141 of 147; 95% CI: 91.3, 98.5) specificity (area under the receiver operating characteristic curve [AUC], 0.96; 95% CI: 0.94, 0.99), without a significant difference (P = .11) in sensitivity compared with the original radiologist report (96.1% [98 of 102]; 95% CI: 90.3, 98.9). In a test set of 1473 real-world CT studies (669 malignant, 804 control) from institutions throughout Taiwan, the DL tool distinguished between CT malignant and control studies with 89.7% (600 of 669; 95% CI: 87.1, 91.9) sensitivity and 92.8% specificity (746 of 804; 95% CI: 90.8, 94.5) (AUC, 0.95; 95% CI: 0.94, 0.96), with 74.7% (68 of 91; 95% CI: 64.5, 83.3) sensitivity for malignancies smaller than 2 cm. Conclusion The deep learning-based tool enabled accurate detection of pancreatic cancer on CT scans, with reasonable sensitivity for tumors smaller than 2 cm. Â© RSNA, 2022 Online supplemental material is available for this article. See also the editorial by Aisen and Rodrigues in this issue.",The authors thank Nico Wu for data management and preprocessing.,,Radiology,,"Male; Humans; Aged; Deep Learning; Retrospective Studies; Sensitivity and Specificity; Tomography, X-Ray Computed; Pancreas; Pancreatic Neoplasms",2022-09-13,2022,2022-09-13,2023-01,306,1,172-182,Closed,Article,"Chen, Po-Ting; Wu, Tinghui; Wang, Pochuan; Chang, Dawei; Liu, Kao-Lang; Wu, Ming-Shiang; Roth, Holger R; Lee, Po-Chang; Liao, Wei-Chih; Wang, Weichung","Chen, Po-Ting (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Wu, Tinghui (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Wang, Pochuan (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Chang, Dawei (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Liu, Kao-Lang (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Wu, Ming-Shiang (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Roth, Holger R (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Lee, Po-Chang (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Liao, Wei-Chih (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).); Wang, Weichung (From the Department of Medical Imaging (P.T.C., K.L.L.) and Division of Gastroenterology and Hepatology, Department of Internal Medicine (M.S.W., W.C.L.), National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan; Institute of Applied Mathematical Sciences (T.W., D.C., W.W.) and Departments of Computer Science and Information Engineering (P.W.) and Internal Medicine, College of Medicine (M.S.W., W.C.L.), National Taiwan University, No. 1, Section 4, Roosevelt Rd, Taipei 10617, Taiwan; Department of Medical Imaging, National Taiwan University Cancer Center, Taipei, Taiwan (K.L.L.); NVIDIA, Bethesda, Md (H.R.R.); and National Health Insurance Administration, Ministry of Health and Welfare, Taipei, Taiwan (P.C.L.).)",,"Chen, Po-Ting (National Taiwan University Hospital; National Taiwan University); Wu, Tinghui (National Taiwan University Hospital; National Taiwan University); Wang, Pochuan (National Taiwan University Hospital; National Taiwan University); Chang, Dawei (National Taiwan University Hospital; National Taiwan University); Liu, Kao-Lang (National Taiwan University Hospital; National Taiwan University); Wu, Ming-Shiang (National Taiwan University Hospital; National Taiwan University); Roth, Holger R (National Taiwan University Hospital; National Taiwan University); Lee, Po-Chang (National Taiwan University Hospital; National Taiwan University); Liao, Wei-Chih (National Taiwan University Hospital; National Taiwan University); Wang, Weichung (National Taiwan University Hospital; National Taiwan University)",7,7,,,,https://app.dimensions.ai/details/publication/pub.1150961928,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
3089,pub.1149633172,10.1016/j.compbiomed.2022.105891,35932729,PMC9596264,Stacked dilated convolutions and asymmetric architecture for U-Net-based medical image segmentation,"Deep learning has been widely utilized for medical image segmentation. The most commonly used U-Net and its variants often share two common characteristics but lack solid evidence for the effectiveness. First, each block (i.e., consecutive convolutions of feature maps of the same resolution) outputs feature maps from the last convolution, limiting the variety of the receptive fields. Second, the network has a symmetric structure where the encoder and the decoder paths have similar numbers of channels. We explored two novel revisions: a stacked dilated operation that outputs feature maps from multi-scale receptive fields to replace the consecutive convolutions; an asymmetric architecture with fewer channels in the decoder path. Two novel models were developed: U-Net using the stacked dilated operation (SDU-Net) and asymmetric SDU-Net (ASDU-Net). We used both publicly available and private datasets to assess the efficacy of the proposed models. Extensive experiments confirmed SDU-Net outperformed or achieved performance similar to the state-of-the-art while using fewer parameters (40% of U-Net). ASDU-Net further reduced the model parameters to 20% of U-Net with performance comparable to SDU-Net. In conclusion, the stacked dilated operation and the asymmetric structure are promising for improving the performance of U-Net and its variants.","Dr. Samirâs effort on this work was supported by the NIDDK of the National Institutes of Health, United States under award number R01DK119860. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.",,Computers in Biology and Medicine,,"Image Processing, Computer-Assisted; Neural Networks, Computer",2022-07-21,2022,2022-07-21,2022-09,148,,105891,Closed,Article,"Wang, Shuhang; Singh, Vivek Kumar; Cheah, Eugene; Wang, Xiaohong; Li, Qian; Chou, Shinn-Huey; Lehman, Constance D; Kumar, Viksit; Samir, Anthony E","Wang, Shuhang (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA. Electronic address: swang38@mgh.harvard.edu.); Singh, Vivek Kumar (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA.); Cheah, Eugene (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA.); Wang, Xiaohong (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA.); Li, Qian (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA.); Chou, Shinn-Huey (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA.); Lehman, Constance D (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA.); Kumar, Viksit (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA.); Samir, Anthony E (Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, 02114, MA, USA.)","Wang, Shuhang (Harvard University; Massachusetts General Hospital)","Wang, Shuhang (Harvard University; Massachusetts General Hospital); Singh, Vivek Kumar (Harvard University; Massachusetts General Hospital); Cheah, Eugene (Harvard University; Massachusetts General Hospital); Wang, Xiaohong (Harvard University; Massachusetts General Hospital); Li, Qian (Harvard University; Massachusetts General Hospital); Chou, Shinn-Huey (Harvard University; Massachusetts General Hospital); Lehman, Constance D (Harvard University; Massachusetts General Hospital); Kumar, Viksit (Harvard University; Massachusetts General Hospital); Samir, Anthony E (Harvard University; Massachusetts General Hospital)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1149633172,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
3086,pub.1154909695,10.1001/jamanetworkopen.2022.53370,36705919,,Development and Validation of a Deep Learning Algorithm to Differentiate Colon Carcinoma From Acute Diverticulitis in Computed Tomography Images,"Importance: Differentiating between malignant and benign etiology in large-bowel wall thickening on computed tomography (CT) images can be a challenging task. Artificial intelligence (AI) support systems can improve the diagnostic accuracy of radiologists, as shown for a variety of imaging tasks. Improvements in diagnostic performance, in particular the reduction of false-negative findings, may be useful in patient care.
Objective: To develop and evaluate a deep learning algorithm able to differentiate colon carcinoma (CC) and acute diverticulitis (AD) on CT images and analyze the impact of the AI-support system in a reader study.
Design, Setting, and Participants: In this diagnostic study, patients who underwent surgery between July 1, 2005, and October 1, 2020, for CC or AD were included. Three-dimensional (3-D) bounding boxes including the diseased bowel segment and surrounding mesentery were manually delineated and used to develop a 3-D convolutional neural network (CNN). A reader study with 10 observers of different experience levels was conducted. Readers were asked to classify the testing cohort under reading room conditions, first without and then with algorithmic support.
Main Outcomes and Measures: To evaluate the diagnostic performance, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were calculated for all readers and reader groups with and without AI support. Metrics were compared using the McNemar test and relative and absolute predictive value comparisons.
Results: A total of 585 patients (AD: nâ=â267, CC: nâ=â318; mean [SD] age, 63.2 [13.4] years; 341 men [58.3%]) were included. The 3-D CNN reached a sensitivity of 83.3% (95% CI, 70.0%-96.6%) and specificity of 86.6% (95% CI, 74.5%-98.8%) for the test set, compared with the mean reader sensitivity of 77.6% (95% CI, 72.9%-82.3%) and specificity of 81.6% (95% CI, 77.2%-86.1%). The combined group of readers improved significantly with AI support from a sensitivity of 77.6% to 85.6% (95% CI, 81.3%-89.3%; Pâ<â.001) and a specificity of 81.6% to 91.3% (95% CI, 88.1%-94.5%; Pâ<â.001). Artificial intelligence support significantly reduced the number of false-negative and false-positive findings (NPV from 78.5% to 86.4% and PPV from 80.9% to 90.8%; Pâ<â.001).
Conclusions and Relevance: The findings of this study suggest that a deep learning model able to distinguish CC and AD in CT images as a support system may significantly improve the diagnostic performance of radiologists, which may improve patient care.",,,JAMA Network Open,,"Male; Humans; Middle Aged; Artificial Intelligence; Deep Learning; Retrospective Studies; Algorithms; Tomography, X-Ray Computed; Diverticulitis; Carcinoma; Colon",2023-01-03,2023,2023-01-27,2023-01-03,6,1,e2253370,All OA, Gold,Article,"Ziegelmayer, Sebastian; Reischl, Stefan; Havrda, Hannah; Gawlitza, Joshua; Graf, Markus; Lenhart, Nicolas; Nehls, Nadja; Lemke, Tristan; Wilhelm, Dirk; LohÃ¶fer, Fabian; Burian, Egon; Neumann, Philipp-Alexander; Makowski, Marcus; Braren, Rickmer","Ziegelmayer, Sebastian (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Reischl, Stefan (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Havrda, Hannah (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Gawlitza, Joshua (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Graf, Markus (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Lenhart, Nicolas (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Nehls, Nadja (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Lemke, Tristan (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Wilhelm, Dirk (Department of Surgery, Technical University of Munich, School of Medicine, Munich, Germany); LohÃ¶fer, Fabian (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Burian, Egon (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Neumann, Philipp-Alexander (Department of Surgery, Technical University of Munich, School of Medicine, Munich, Germany); Makowski, Marcus (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany); Braren, Rickmer (Institute of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine, Munich, Germany)","Ziegelmayer, Sebastian (Technical University of Munich)","Ziegelmayer, Sebastian (Technical University of Munich); Reischl, Stefan (Technical University of Munich); Havrda, Hannah (Technical University of Munich); Gawlitza, Joshua (Technical University of Munich); Graf, Markus (Technical University of Munich); Lenhart, Nicolas (Technical University of Munich); Nehls, Nadja (Technical University of Munich); Lemke, Tristan (Technical University of Munich); Wilhelm, Dirk (Technical University of Munich); LohÃ¶fer, Fabian (Technical University of Munich); Burian, Egon (Technical University of Munich); Neumann, Philipp-Alexander (Technical University of Munich); Makowski, Marcus (Technical University of Munich); Braren, Rickmer (Technical University of Munich)",0,0,,,https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2800855/ziegelmayer_2023_oi_221509_1674184348.86751.pdf,https://app.dimensions.ai/details/publication/pub.1154909695,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
3082,pub.1144666713,10.3390/cancers14020376,35053538,PMC8774174,Fully Automatic Deep Learning Framework for Pancreatic Ductal Adenocarcinoma Detection on Computed Tomography,"Early detection improves prognosis in pancreatic ductal adenocarcinoma (PDAC), but is challenging as lesions are often small and poorly defined on contrast-enhanced computed tomography scans (CE-CT). Deep learning can facilitate PDAC diagnosis; however, current models still fail to identify small (<2 cm) lesions. In this study, state-of-the-art deep learning models were used to develop an automatic framework for PDAC detection, focusing on small lesions. Additionally, the impact of integrating the surrounding anatomy was investigated. CE-CT scans from a cohort of 119 pathology-proven PDAC patients and a cohort of 123 patients without PDAC were used to train a nnUnet for automatic lesion detection and segmentation (nnUnet_T). Two additional nnUnets were trained to investigate the impact of anatomy integration: (1) segmenting the pancreas and tumor (nnUnet_TP), and (2) segmenting the pancreas, tumor, and multiple surrounding anatomical structures (nnUnet_MS). An external, publicly available test set was used to compare the performance of the three networks. The nnUnet_MS achieved the best performance, with an area under the receiver operating characteristic curve of 0.91 for the whole test set and 0.88 for tumors <2 cm, showing that state-of-the-art deep learning can detect small PDAC and benefits from anatomy information.",,,Cancers,,,2022-01-13,2022,2022-01-13,,14,2,376,All OA, Gold,Article,"Alves, NatÃ¡lia; Schuurmans, Megan; Litjens, Geke; Bosma, Joeran S.; Hermans, John; Huisman, Henkjan","Alves, NatÃ¡lia (Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6500 HB Nijmegen, The Netherlands;, megan.schuurmans@radboudumc.nl, (M.S.);, joeran.bosma@radboudumc.nl, (J.S.B.);, henkjan.huisman@radboudumc.nl, (H.H.)); Schuurmans, Megan (Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6500 HB Nijmegen, The Netherlands;, megan.schuurmans@radboudumc.nl, (M.S.);, joeran.bosma@radboudumc.nl, (J.S.B.);, henkjan.huisman@radboudumc.nl, (H.H.)); Litjens, Geke (Department of Medical Imaging, Radboud Institute for Health Sciences, 6500 HB Nijmegen, The Netherlands;, g.litjens@radboudumc.nl, (G.L.);, John.Hermans@radboudumc.nl, (J.H.)); Bosma, Joeran S. (Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6500 HB Nijmegen, The Netherlands;, megan.schuurmans@radboudumc.nl, (M.S.);, joeran.bosma@radboudumc.nl, (J.S.B.);, henkjan.huisman@radboudumc.nl, (H.H.)); Hermans, John (Department of Medical Imaging, Radboud Institute for Health Sciences, 6500 HB Nijmegen, The Netherlands;, g.litjens@radboudumc.nl, (G.L.);, John.Hermans@radboudumc.nl, (J.H.)); Huisman, Henkjan (Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6500 HB Nijmegen, The Netherlands;, megan.schuurmans@radboudumc.nl, (M.S.);, joeran.bosma@radboudumc.nl, (J.S.B.);, henkjan.huisman@radboudumc.nl, (H.H.))","Alves, NatÃ¡lia (Radboud University Nijmegen Medical Centre; )","Alves, NatÃ¡lia (Radboud University Nijmegen Medical Centre); Schuurmans, Megan (Radboud University Nijmegen Medical Centre); Litjens, Geke (Radboud University Nijmegen Medical Centre); Bosma, Joeran S. (Radboud University Nijmegen Medical Centre); Hermans, John (Radboud University Nijmegen Medical Centre); Huisman, Henkjan (Radboud University Nijmegen Medical Centre)",7,7,,,https://www.mdpi.com/2072-6694/14/2/376/pdf?version=1642052000,https://app.dimensions.ai/details/publication/pub.1144666713,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
3075,pub.1132976842,10.1016/j.media.2020.101920,33676097,,Comparative validation of multi-instance instrument segmentation in endoscopy: Results of the ROBUST-MIS 2019 challenge,"Intraoperative tracking of laparoscopic instruments is often a prerequisite for computer and robotic-assisted interventions. While numerous methods for detecting, segmenting and tracking of medical instruments based on endoscopic video images have been proposed in the literature, key limitations remain to be addressed: Firstly, robustness, that is, the reliable performance of state-of-the-art methods when run on challenging images (e.g. in the presence of blood, smoke or motion artifacts). Secondly, generalization; algorithms trained for a specific intervention in a specific hospital should generalize to other interventions or institutions. In an effort to promote solutions for these limitations, we organized the Robust Medical Instrument Segmentation (ROBUST-MIS) challenge as an international benchmarking competition with a specific focus on the robustness and generalization capabilities of algorithms. For the first time in the field of endoscopic image processing, our challenge included a task on binary segmentation and also addressed multi-instance detection and segmentation. The challenge was based on a surgical data set comprising 10,040 annotated images acquired from a total of 30 surgical procedures from three different types of surgery. The validation of the competing methods for the three tasks (binary segmentation, multi-instance detection and multi-instance segmentation) was performed in three different stages with an increasing domain gap between the training and the test data. The results confirm the initial hypothesis, namely that algorithm performance degrades with an increasing domain gap. While the average detection and segmentation quality of the best-performing algorithms is high, future research should concentrate on detection and segmentation of small, crossing, moving and transparent instrument(s) (parts).","This challenge has been funded by the Surgical Oncology Program of the National Center for Tumor Diseases (NCT) Heidelberg and the project âOP4.1â, funded by the German Federal Ministry of Economic Affairs and Energy (grant number BMWI 01MT17001C). It was further supported by UNDERSTAND.AI 18 18 https://understand.ai. , NVIDIA GmbH 19 19 https://www.nvidia.com. and Digital Surgery 20 20 https://digitalsurgery.co. . The challenge was further supported by the Helmholtz Association under the joint research school HIDSS4Health (Helmholtz Information and Data Science School for Health). Furthermore, the authors wish to thank Tim Adler, Janek GrÃ¶hl, Alexander Seitel and Minu Dietlinde Tizabi for proofreading the paper. L.M.-H., T.R., A.R., S.B. and S.S. worked with device manufacturer Karl Storz GmbH Co. KG in the joint research project âOP4.1â, funded by the German Federal Ministry of Economic Affairs and Energy (grant number BMWI 01MT17001C). M.W., H.G.K. and P.P.M. worked with device manufacturer Karl Storz GmbH Co. KG in the joint research project âInnOPlanâ, funded by the German Federal Ministry of Economic Affairs and Energy (grant number BMWI 01MD15002E). G.-B.B., Z.-G.H., Z.-L.N., Y.-J.Z. and H.-B.C. were supported by the National Key Research and Development Program of China (Grant 2017YFB1302704). D.G., G.W. and L.W. were funded by National Natural Science Fundation of China (81771921, 61901084). P.H., M.A.R. and D.J. were funded by Research Council of Norway projects number 263,248 (Privaton). S.K. and K.S. were funded by the FWF Austrian Science Fund under grant P 32010-N38. All challenge organizers and some members of their institute had access to training and test cases and were therefore not eligible for awards.",,Medical Image Analysis,,"Algorithms; Artifacts; Image Processing, Computer-Assisted; Laparoscopy",2020-11-28,2020,2020-11-28,2021-05,70,,101920,All OA, Hybrid,Article,"RoÃ, Tobias; Reinke, Annika; Full, Peter M; Wagner, Martin; Kenngott, Hannes; Apitz, Martin; Hempe, Hellena; Mindroc-Filimon, Diana; Scholz, Patrick; Tran, Thuy Nuong; Bruno, Pierangela; ArbelÃ¡ez, Pablo; Bian, Gui-Bin; Bodenstedt, Sebastian; Bolmgren, Jon LindstrÃ¶m; Bravo-SÃ¡nchez, Laura; Chen, Hua-Bin; GonzÃ¡lez, Cristina; Guo, Dong; Halvorsen, PÃ¥l; Heng, Pheng-Ann; Hosgor, Enes; Hou, Zeng-Guang; Isensee, Fabian; Jha, Debesh; Jiang, Tingting; Jin, Yueming; Kirtac, Kadir; Kletz, Sabrina; Leger, Stefan; Li, Zhixuan; Maier-Hein, Klaus H; Ni, Zhen-Liang; Riegler, Michael A; Schoeffmann, Klaus; Shi, Ruohua; Speidel, Stefanie; Stenzel, Michael; Twick, Isabell; Wang, Gutai; Wang, Jiacheng; Wang, Liansheng; Wang, Lu; Zhang, Yujie; Zhou, Yan-Jie; Zhu, Lei; Wiesenfarth, Manuel; Kopp-Schneider, Annette; MÃ¼ller-Stich, Beat P; Maier-Hein, Lena","RoÃ, Tobias (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany; University of Heidelberg, Germany, SeminarstraÃe 2, 69117 Heidelberg, Germany. Electronic address: t.ross@dkfz-heidelberg.de.); Reinke, Annika (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany; University of Heidelberg, Germany, SeminarstraÃe 2, 69117 Heidelberg, Germany.); Full, Peter M (University of Heidelberg, Germany, SeminarstraÃe 2, 69117 Heidelberg, Germany; Division of Medical Image Computing (MIC), Im Neuenheimer Feld 223, 69120 Heidelberg, Germany.); Wagner, Martin (Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Im Neuenheimer Feld 110, 69120 Heidelberg, Germany.); Kenngott, Hannes (Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Im Neuenheimer Feld 110, 69120 Heidelberg, Germany.); Apitz, Martin (Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Im Neuenheimer Feld 110, 69120 Heidelberg, Germany.); Hempe, Hellena (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany.); Mindroc-Filimon, Diana (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany.); Scholz, Patrick (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany; HIDSS4Health - Helmholtz Information and Data Science School for Health, Im Neuenheimer Feld 223, 69120 Heidelberg, Germany.); Tran, Thuy Nuong (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany.); Bruno, Pierangela (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany; Department of Mathematics and Computer Science, University of Calabria, 87036 Rende, Italy.); ArbelÃ¡ez, Pablo (Universidad de los Andes, Cra. 1 No 18A - 12, 111711 BogotÃ¡, Colombia.); Bian, Gui-Bin (University of Chinese Academy Sciences, 52 Sanlihe Rd., Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, 100864 Beijing, China.); Bodenstedt, Sebastian (National Center for Tumor Diseases (NCT), Partner Site Dresden, Germany: German Cancer Research Center, Im Neuenheimer Feld 460, 69120 Heidelberg, Germany; Faculty of Medicine and University Hospital Carl Gustav Carus, Technische UniversitÃ¤t Dresden, FetscherstraÃe 74, 01307 Dresden, Germany; Helmholtz Association/Helmholtz-Zentrum Dresden - Rossendorf (HZDR), Bautzner LandstraÃe 400, 01328 Dresden, Germany.); Bolmgren, Jon LindstrÃ¶m (caresyntax, KomturstraÃe 18A, 12099 Berlin, Germany.); Bravo-SÃ¡nchez, Laura (Universidad de los Andes, Cra. 1 No 18A - 12, 111711 BogotÃ¡, Colombia.); Chen, Hua-Bin (University of Chinese Academy Sciences, 52 Sanlihe Rd., Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, 100864 Beijing, China.); GonzÃ¡lez, Cristina (Universidad de los Andes, Cra. 1 No 18A - 12, 111711 BogotÃ¡, Colombia.); Guo, Dong (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Shahe Campus:No.4, Section 2, North Jianshe Road, 610054 | Qingshuihe Campus:No.2006, Xiyuan Ave, West Hi-Tech Zone, 611731, Chengdu, China.); Halvorsen, PÃ¥l (SimulaMet, Pilestredet 52, 0167 Oslo, Norway; Oslo Metropolitan University (OsloMet), Pilestredet 52, 0167 Oslo, Norway.); Heng, Pheng-Ann (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Chung Chi Rd, Ma Liu Shui, Hong Kong, China.); Hosgor, Enes (caresyntax, KomturstraÃe 18A, 12099 Berlin, Germany.); Hou, Zeng-Guang (University of Chinese Academy Sciences, 52 Sanlihe Rd., Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, 100864 Beijing, China.); Isensee, Fabian (University of Heidelberg, Germany, SeminarstraÃe 2, 69117 Heidelberg, Germany; Division of Medical Image Computing (MIC), Im Neuenheimer Feld 223, 69120 Heidelberg, Germany.); Jha, Debesh (SimulaMet, Pilestredet 52, 0167 Oslo, Norway; Department of Informatics, UIT The Arctic University of Norway, Hansine Hansens vei 54, 9037 TromsÃ¸, Norway.); Jiang, Tingting (Institute of Digital Media (NELVT), Peking University, 5 Yiheyuan Rd, Haidian District, 100871 Peking, China.); Jin, Yueming (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Chung Chi Rd, Ma Liu Shui, Hong Kong, China.); Kirtac, Kadir (caresyntax, KomturstraÃe 18A, 12099 Berlin, Germany.); Kletz, Sabrina (Institute of Information Technology, Klagenfurt University, UniversitÃ¤tsstraÃe 65-67, 9020 Klagenfurt, Austria.); Leger, Stefan (National Center for Tumor Diseases (NCT), Partner Site Dresden, Germany: German Cancer Research Center, Im Neuenheimer Feld 460, 69120 Heidelberg, Germany; Faculty of Medicine and University Hospital Carl Gustav Carus, Technische UniversitÃ¤t Dresden, FetscherstraÃe 74, 01307 Dresden, Germany; Helmholtz Association/Helmholtz-Zentrum Dresden - Rossendorf (HZDR), Bautzner LandstraÃe 400, 01328 Dresden, Germany.); Li, Zhixuan (Institute of Digital Media (NELVT), Peking University, 5 Yiheyuan Rd, Haidian District, 100871 Peking, China.); Maier-Hein, Klaus H (Division of Medical Image Computing (MIC), Im Neuenheimer Feld 223, 69120 Heidelberg, Germany.); Ni, Zhen-Liang (University of Chinese Academy Sciences, 52 Sanlihe Rd., Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, 100864 Beijing, China.); Riegler, Michael A (SimulaMet, Pilestredet 52, 0167 Oslo, Norway.); Schoeffmann, Klaus (Institute of Information Technology, Klagenfurt University, UniversitÃ¤tsstraÃe 65-67, 9020 Klagenfurt, Austria.); Shi, Ruohua (Institute of Digital Media (NELVT), Peking University, 5 Yiheyuan Rd, Haidian District, 100871 Peking, China.); Speidel, Stefanie (National Center for Tumor Diseases (NCT), Partner Site Dresden, Germany: German Cancer Research Center, Im Neuenheimer Feld 460, 69120 Heidelberg, Germany; Faculty of Medicine and University Hospital Carl Gustav Carus, Technische UniversitÃ¤t Dresden, FetscherstraÃe 74, 01307 Dresden, Germany; Helmholtz Association/Helmholtz-Zentrum Dresden - Rossendorf (HZDR), Bautzner LandstraÃe 400, 01328 Dresden, Germany.); Stenzel, Michael (caresyntax, KomturstraÃe 18A, 12099 Berlin, Germany.); Twick, Isabell (caresyntax, KomturstraÃe 18A, 12099 Berlin, Germany.); Wang, Gutai (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Shahe Campus:No.4, Section 2, North Jianshe Road, 610054 | Qingshuihe Campus:No.2006, Xiyuan Ave, West Hi-Tech Zone, 611731, Chengdu, China.); Wang, Jiacheng (Department of Computer Science, School of Informatics, Xiamen University, 422 Siming South Road, 361005 Xiamen, China.); Wang, Liansheng (Department of Computer Science, School of Informatics, Xiamen University, 422 Siming South Road, 361005 Xiamen, China.); Wang, Lu (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Shahe Campus:No.4, Section 2, North Jianshe Road, 610054 | Qingshuihe Campus:No.2006, Xiyuan Ave, West Hi-Tech Zone, 611731, Chengdu, China.); Zhang, Yujie (Department of Computer Science, School of Informatics, Xiamen University, 422 Siming South Road, 361005 Xiamen, China.); Zhou, Yan-Jie (University of Chinese Academy Sciences, 52 Sanlihe Rd., Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, 100864 Beijing, China.); Zhu, Lei (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Chung Chi Rd, Ma Liu Shui, Hong Kong, China.); Wiesenfarth, Manuel (Division of Biostatistics, German Cancer Research Center, Im Neuenheimer Feld 581, Heidelberg, Germany.); Kopp-Schneider, Annette (Division of Biostatistics, German Cancer Research Center, Im Neuenheimer Feld 581, Heidelberg, Germany.); MÃ¼ller-Stich, Beat P (Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Im Neuenheimer Feld 110, 69120 Heidelberg, Germany.); Maier-Hein, Lena (Computer Assisted Medical Interventions (CAMI), German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany.)","RoÃ, Tobias (German Cancer Research Center; Heidelberg University)","RoÃ, Tobias (German Cancer Research Center; Heidelberg University); Reinke, Annika (German Cancer Research Center; Heidelberg University); Full, Peter M (Heidelberg University); Wagner, Martin (University Hospital Heidelberg); Kenngott, Hannes (University Hospital Heidelberg); Apitz, Martin (University Hospital Heidelberg); Hempe, Hellena (German Cancer Research Center); Mindroc-Filimon, Diana (German Cancer Research Center); Scholz, Patrick (German Cancer Research Center); Tran, Thuy Nuong (German Cancer Research Center); Bruno, Pierangela (German Cancer Research Center; University of Calabria); ArbelÃ¡ez, Pablo (Universidad de Los Andes); Bian, Gui-Bin (University of Chinese Academy of Sciences; Institute of Automation); Bodenstedt, Sebastian (National Center for Tumor Diseases; TU Dresden; Helmholtz-Zentrum Dresden-Rossendorf); Bolmgren, Jon LindstrÃ¶m (); Bravo-SÃ¡nchez, Laura (Universidad de Los Andes); Chen, Hua-Bin (University of Chinese Academy of Sciences; Institute of Automation); GonzÃ¡lez, Cristina (Universidad de Los Andes); Guo, Dong (University of Electronic Science and Technology of China); Halvorsen, PÃ¥l (Simula Metropolitan Center for Digital Engineering; OsloMet â Oslo Metropolitan University); Heng, Pheng-Ann (Chinese University of Hong Kong); Hosgor, Enes (); Hou, Zeng-Guang (University of Chinese Academy of Sciences; Institute of Automation); Isensee, Fabian (Heidelberg University); Jha, Debesh (Simula Metropolitan Center for Digital Engineering; UiT The Arctic University of Norway); Jiang, Tingting (Peking University); Jin, Yueming (Chinese University of Hong Kong); Kirtac, Kadir (); Kletz, Sabrina (University of Klagenfurt); Leger, Stefan (National Center for Tumor Diseases; TU Dresden; Helmholtz-Zentrum Dresden-Rossendorf); Li, Zhixuan (Peking University); Maier-Hein, Klaus H (); Ni, Zhen-Liang (University of Chinese Academy of Sciences; Institute of Automation); Riegler, Michael A (Simula Metropolitan Center for Digital Engineering); Schoeffmann, Klaus (University of Klagenfurt); Shi, Ruohua (Peking University); Speidel, Stefanie (National Center for Tumor Diseases; TU Dresden; Helmholtz-Zentrum Dresden-Rossendorf); Stenzel, Michael (); Twick, Isabell (); Wang, Gutai (University of Electronic Science and Technology of China); Wang, Jiacheng (Xiamen University); Wang, Liansheng (Xiamen University); Wang, Lu (University of Electronic Science and Technology of China); Zhang, Yujie (Xiamen University); Zhou, Yan-Jie (University of Chinese Academy of Sciences; Institute of Automation); Zhu, Lei (Chinese University of Hong Kong); Wiesenfarth, Manuel (German Cancer Research Center); Kopp-Schneider, Annette (German Cancer Research Center); MÃ¼ller-Stich, Beat P (University Hospital Heidelberg); Maier-Hein, Lena (German Cancer Research Center)",34,31,8.18,,https://doi.org/10.1016/j.media.2020.101920,https://app.dimensions.ai/details/publication/pub.1132976842,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
3008,pub.1144121092,10.1148/rycan.210081,34939849,PMC8830432,Radiomics for Detection of Pancreas Adenocarcinoma on CT Scans: Impact of Biliary Stents,,,,Radiology Imaging Cancer,,"Adenocarcinoma; Humans; Pancreas; Pancreatic Neoplasms; Stents; Tomography, X-Ray Computed",2022-01-01,2022,,2022-01-01,4,1,e210081,All OA, Gold,Article,"Suman, Garima; Patra, Anurima; Mukherjee, Sovanlal; Korffiatis, Panagiotis; Goenka, Ajit H","Suman, Garima (Department of Radiology, Mayo Clinic, 200 First St SW, Charlton 1, Rochester, MN 55905.); Patra, Anurima (Department of Radiology, Mayo Clinic, 200 First St SW, Charlton 1, Rochester, MN 55905.); Mukherjee, Sovanlal (Department of Radiology, Mayo Clinic, 200 First St SW, Charlton 1, Rochester, MN 55905.); Korffiatis, Panagiotis (Department of Radiology, Mayo Clinic, 200 First St SW, Charlton 1, Rochester, MN 55905.); Goenka, Ajit H (Department of Radiology, Mayo Clinic, 200 First St SW, Charlton 1, Rochester, MN 55905.)",,"Suman, Garima (Mayo Clinic); Patra, Anurima (Mayo Clinic); Mukherjee, Sovanlal (Mayo Clinic); Korffiatis, Panagiotis (Mayo Clinic); Goenka, Ajit H (Mayo Clinic)",0,0,,,https://doi.org/10.1148/rycan.210081,https://app.dimensions.ai/details/publication/pub.1144121092,,,,,,,,,,,,
3008,pub.1127902455,10.1016/s2589-7500(20)30105-9,33328118,,Deep learning for pancreatic cancer detection: current challenges and future strategies,,,,The Lancet Digital Health,,"Deep Learning; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Pancreatic Neoplasms; Retrospective Studies",2020-06,2020,,2020-06,2,6,e271-e272,All OA, Gold,Article,"Chu, Linda C; Fishman, Elliot K","Chu, Linda C (The Russell H Morgan Department of Radiology and Radiological Science, Johns Hopkins University School of Medicine, Baltimore, MD 21287, USA); Fishman, Elliot K (The Russell H Morgan Department of Radiology and Radiological Science, Johns Hopkins University School of Medicine, Baltimore, MD 21287, USA)",,"Chu, Linda C (Johns Hopkins University); Fishman, Elliot K (Johns Hopkins University)",3,2,0.2,1.53,http://www.thelancet.com/article/S2589750020301059/pdf,https://app.dimensions.ai/details/publication/pub.1127902455,42 Health Sciences, 4203 Health Services and Systems,,,,,,,,,,
2999,pub.1136859878,10.1016/j.pan.2021.03.016,33840636,,Quality gaps in public pancreas imaging datasets: Implications & challenges for AI applications,"OBJECTIVE: Quality gaps in medical imaging datasets lead to profound errors in experiments. Our objective was to characterize such quality gaps in public pancreas imaging datasets (PPIDs), to evaluate their impact on previously published studies, and to provide post-hoc labels and segmentations as a value-add for these PPIDs.
METHODS: We scored the available PPIDs on the medical imaging data readiness (MIDaR) scale, and evaluated for associated metadata, image quality, acquisition phase, etiology of pancreas lesion, sources of confounders, and biases. Studies utilizing these PPIDs were evaluated for awareness of and any impact of quality gaps on their results. Volumetric pancreatic adenocarcinoma (PDA) segmentations were performed for non-annotated CTs by a junior radiologist (R1) and reviewed by a senior radiologist (R3).
RESULTS: We found three PPIDs with 560 CTs and six MRIs. NIH dataset of normal pancreas CTs (PCT) (nÂ =Â 80 CTs) had optimal image quality and met MIDaR A criteria but parts of pancreas have been excluded in the provided segmentations. TCIA-PDA (nÂ =Â 60 CTs; 6 MRIs) and MSD(nÂ =Â 420 CTs) datasets categorized to MIDaR B due to incomplete annotations, limited metadata, and insufficient documentation. Substantial proportion of CTs from TCIA-PDA and MSD datasets were found unsuitable for AI due to biliary stents [TCIA-PDA:10 (17%); MSD:112 (27%)] or other factors (non-portal venous phase, suboptimal image quality, non-PDA etiology, or post-treatment status) [TCIA-PDA:5 (8.5%); MSD:156 (37.1%)]. These quality gaps were not accounted for in any of the 25 studies that have used these PPIDs (NIH-PCT:20; MSD:1; both: 4). PDA segmentations were done by R1 in 91 eligible CTs (TCIA-PDA:42; MSD:49). Of these, corrections were made by R3 in 16 CTs (18%) (TCIA-PDA:4; MSD:12) [mean (standard deviation) Dice: 0.72(0.21) and 0.63(0.23) respectively].
CONCLUSION: Substantial quality gaps, sources of bias, and high proportion of CTs unsuitable for AI characterize the available limited PPIDs. Published studies on these PPIDs do not account for these quality gaps. We complement these PPIDs through post-hoc labels and segmentations for public release on the TCIA portal. Collaborative efforts leading to large, well-curated PPIDs supported by adequate documentation are critically needed to translate the promise of AI to clinical practice.","Funding Sources: Dr. Goenka acknowledges research grant from the Champions for Hope Pancreatic Cancer Research Program of the Funk-Zitiello Foundation and the Advance the Practice Award from the Department of Radiology, Mayo Clinic, Rochester, Minnesota. Disclosures: Dr. Suman, Dr. Patra, Dr. Korfiatis, Dr. Majumder, Dr. Chari, Dr. Truty, Dr. Fletcher, Dr. Goenka have no conflicts of interest or relevant financial ties to disclose.",,Pancreatology,,Adenocarcinoma, Artificial Intelligence, Humans, Magnetic Resonance Imaging, Pancreas, Pancreatic Neoplasms,2021-04-02,2021,2021-04-02,2021-08,21,5,1001-1008,Closed,Article,"Suman, Garima; Patra, Anurima; Korfiatis, Panagiotis; Majumder, Shounak; Chari, Suresh T; Truty, Mark J; Fletcher, Joel G; Goenka, Ajit H","Suman, Garima (Department of Radiology, Mayo Clinic, 200 First Street SW, Rochester, MN, 55905, USA.); Patra, Anurima (Department of Radiology, Mayo Clinic, 200 First Street SW, Rochester, MN, 55905, USA.); Korfiatis, Panagiotis (Department of Radiology, Mayo Clinic, 200 First Street SW, Rochester, MN, 55905, USA.); Majumder, Shounak (Department of Gastroenterology and Hepatology, Mayo Clinic, 200 First Street SW, Rochester, MN, 55905, USA.); Chari, Suresh T (Department of Gastroenterology, Hepatology and Nutrition, The University of Texas MD Anderson Cancer Center, 1515 Holcombe Blvd, Houston, TX, 77030, USA.); Truty, Mark J (Department of General and Gastrointestinal Surgery, Mayo Clinic, 200 First Street SW, MN, 55905, USA.); Fletcher, Joel G (Department of Radiology, Mayo Clinic, 200 First Street SW, Rochester, MN, 55905, USA.); Goenka, Ajit H (Department of Radiology, Mayo Clinic, 200 First Street SW, Rochester, MN, 55905, USA. Electronic address: goenka.ajit@mayo.edu.)","Goenka, Ajit H (Mayo Clinic)","Suman, Garima (Mayo Clinic); Patra, Anurima (Mayo Clinic); Korfiatis, Panagiotis (Mayo Clinic); Majumder, Shounak (Mayo Clinic); Chari, Suresh T (The University of Texas MD Anderson Cancer Center); Truty, Mark J (Mayo Clinic); Fletcher, Joel G (Mayo Clinic); Goenka, Ajit H (Mayo Clinic)",8,8,1.57,7.01,,https://app.dimensions.ai/details/publication/pub.1136859878,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,
2990,pub.1141117914,10.1016/j.cmpb.2021.106419,34563895,,Automatic segmentation of organs at risk and tumors in CT images of lung cancer from partially labelled datasets with a semi-supervised conditional nnU-Net,"BACKGROUND AND OBJECTIVE: Accurately and reliably defining organs at risk (OARs) and tumors are the cornerstone of radiation therapy (RT) treatment planning for lung cancer. Almost all segmentation networks based on deep learning techniques rely on fully annotated data with strong supervision. However, existing public imaging datasets encountered in the RT domain frequently include singly labelled tumors or partially labelled organs because annotating full OARs and tumors in CT images is both rigorous and tedious. To utilize labelled data from different sources, we proposed a dual-path semi-supervised conditional nnU-Net for OARs and tumor segmentation that is trained on a union of partially labelled datasets.
METHODS: The framework employs the nnU-Net as the base model and introduces a conditioning strategy by incorporating auxiliary information as an additional input layer into the decoder. The conditional nnU-Net efficiently leverages prior conditional information to classify the target class at the pixelwise level. Specifically, we employ the uncertainty-aware mean teacher (UA-MT) framework to assist in OARs segmentation, which can effectively leverage unlabelled data (images from a tumor labelled dataset) by encouraging consistent predictions of the same input under different perturbations. Furthermore, we individually design different combinations of loss functions to optimize the segmentation of OARs (Dice loss and cross-entropy loss) and tumors (Dice loss and focal loss) in a dual path.
RESULTS: The proposed method is evaluated on two publicly available datasets of the spinal cord, left and right lung, heart, esophagus, and lung tumor, in which satisfactory segmentation performance has been achieved in term of both the region-based Dice similarity coefficient (DSC) and the boundary-based Hausdorff distance (HD).
CONCLUSIONS: The proposed semi-supervised conditional nnU-Net breaks down the barriers between nonoverlapping labelled datasets and further alleviates the problem of ""data hunger"" and ""data waste"" in multi-class segmentation. The method has the potential to help radiologists with RT treatment planning in clinical practice.",This research was supported by the National Natural Science Foundation of China (Grant No. 81871457) and the National Natural Science Foundation of China (Grant No. 51775368).,,Computer Methods and Programs in Biomedicine,,"Humans; Image Processing, Computer-Assisted; Lung; Lung Neoplasms; Organs at Risk; Tomography, X-Ray Computed",2021-09-15,2021,2021-09-15,2021-11,211,,106419,Closed,Article,"Zhang, Guobin; Yang, Zhiyong; Huo, Bin; Chai, Shude; Jiang, Shan","Zhang, Guobin (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China.); Yang, Zhiyong (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China.); Huo, Bin (Department of Oncology, Tianjin Medical University Second Hospital, Tianjin, 300211, China.); Chai, Shude (Department of Oncology, Tianjin Medical University Second Hospital, Tianjin, 300211, China.); Jiang, Shan (School of Mechanical Engineering, Tianjin University, Tianjin, 300350, China. Electronic address: shanjmri@tju.edu.cn.)","Jiang, Shan (Tianjin University)","Zhang, Guobin (Tianjin University); Yang, Zhiyong (Tianjin University); Huo, Bin (Tianjin Medical University); Chai, Shude (Tianjin Medical University); Jiang, Shan (Tianjin University)",3,3,0.61,2.02,,https://app.dimensions.ai/details/publication/pub.1141117914,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,
2986,pub.1150427250,10.1148/radiol.220171,35997607,,Deep Learningâbased Detection of Solid and Cystic Pancreatic Neoplasms at Contrast-enhanced CT,"Background Deep learning (DL) may facilitate the diagnosis of various pancreatic lesions at imaging. Purpose To develop and validate a DL-based approach for automatic identification of patients with various solid and cystic pancreatic neoplasms at abdominal CT and compare its diagnostic performance with that of radiologists. Materials and Methods In this retrospective study, a three-dimensional nnU-Net-based DL model was trained using the CT data of patients who underwent resection for pancreatic lesions between January 2014 and March 2015 and a subset of patients without pancreatic abnormality who underwent CT in 2014. Performance of the DL-based approach to identify patients with pancreatic lesions was evaluated in a temporally independent cohort (test set 1) and a temporally and spatially independent cohort (test set 2) and was compared with that of two board-certified radiologists. Performance was assessed using receiver operating characteristic analysis. Results The study included 852 patients in the training set (median age, 60 years [range, 19-85 years]; 462 men), 603 patients in test set 1 (median age, 58 years [range, 18-82 years]; 376 men), and 589 patients in test set 2 (median age, 63 years [range, 18-99 years]; 343 men). In test set 1, the DL-based approach had an area under the receiver operating characteristic curve (AUC) of 0.91 (95% CI: 0.89, 0.94) and showed slightly worse performance in test set 2 (AUC, 0.87 [95% CI: 0.84, 0.89]). The DL-based approach showed high sensitivity in identifying patients with solid lesions of any size (98%-100%) or cystic lesions measuring 1.0 cm or larger (92%-93%), which was comparable with the radiologists (95%-100% for solid lesions [P = .51 to P > .99]; 93%-98% for cystic lesions â¥1.0 cm [P = .38 to P > .99]). Conclusion The deep learning-based approach demonstrated high performance in identifying patients with various solid and cystic pancreatic lesions at CT. Â© RSNA, 2022 Online supplemental material is available for this article.","* H.J.P. and K.S. equally contributed to this work. ** N.K. and H.J.K. are coâsenior authors. Supported by a grant from the National Research Foundation of Korea, funded by the Korean government (Ministry of Science and ICT) (grant 2021R1C1C1010138). area under the receiver operating characteristic curve deep learning intraductal pancreatic mucinous neoplasm Medical Segmentation Decathlon neuroendocrine neoplasm pancreatic ductal adenocarcinoma",,Radiology,,"Male; Humans; Middle Aged; Retrospective Studies; Pancreatic Cyst; Deep Learning; Pancreatic Neoplasms; Tomography, X-Ray Computed",2022-08-23,2022,2022-08-23,2023-01,306,1,140-149,Closed,Article,"Park, Hyo Jung; Shin, Keewon; You, Myung-Won; Kyung, Sung-Gu; Kim, So Yeon; Park, Seong Ho; Byun, Jae Ho; Kim, Namkug; Kim, Hyoung Jung","Park, Hyo Jung (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).); Shin, Keewon (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).); You, Myung-Won (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).); Kyung, Sung-Gu (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).); Kim, So Yeon (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).); Park, Seong Ho (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).); Byun, Jae Ho (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).); Kim, Namkug (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).); Kim, Hyoung Jung (From the Department of Radiology and Research Institute of Radiology (H.J.P., S.Y.K., S.H.P., J.H.B., H.J.K.) and Department of Bioengineering, Asan Medical Institute of Convergence Science and Technology (K.S., S.G.K., N.K.), Asan Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Republic of Korea; and Department of Radiology, Kyung Hee University Hospital, Seoul, Republic of Korea (M.W.Y.).)",,"Park, Hyo Jung (Kyung Hee University Medical Center); Shin, Keewon (Kyung Hee University Medical Center); You, Myung-Won (Kyung Hee University Medical Center); Kyung, Sung-Gu (Kyung Hee University Medical Center); Kim, So Yeon (Kyung Hee University Medical Center); Park, Seong Ho (Kyung Hee University Medical Center); Byun, Jae Ho (Kyung Hee University Medical Center); Kim, Namkug (Kyung Hee University Medical Center); Kim, Hyoung Jung (Kyung Hee University Medical Center)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150427250,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,,
2986,pub.1147881662,10.1016/j.cmpb.2022.106883,35597203,PMC9107178,WVALE: Weak variational autoencoder for localisation and enhancement of COVID-19 lung infections,"BACKGROUND AND OBJECTIVE: The COVID-19 pandemic is a major global health crisis of this century. The use of neural networks with CT imaging can potentially improve clinicians' efficiency in diagnosis. Previous studies in this field have primarily focused on classifying the disease on CT images, while few studies targeted the localisation of disease regions. Developing neural networks for automating the latter task is impeded by limited CT images with pixel-level annotations available to the research community.
METHODS: This paper proposes a weakly-supervised framework named ""Weak Variational Autoencoder for Localisation and Enhancement"" (WVALE) to address this challenge for COVID-19 CT images. This framework includes two components: anomaly localisation with a novel WVAE model and enhancement of supervised segmentation models with WVALE.
RESULTS: The WVAE model have been shown to produce high-quality post-hoc attention maps with fine borders around infection regions, while weak supervision segmentation shows results comparable to conventional supervised segmentation models. The WVALE framework can enhance the performance of a range of supervised segmentation models, including state-of-art models for the segmentation of COVID-19 lung infection.
CONCLUSIONS: Our study provides a proof-of-concept for weakly supervised segmentation and an alternative approach to alleviate the lack of annotation, while its independence from classification & segmentation frameworks makes it easily integrable with existing systems.","This paper is partially supported by Royal Society International Exchanges Cost Share Award, UK (RP202G0230); Medical Research Council Confidence in Concept Award, UK (MC_PC_17171); Hope Foundation for Cancer Research, UK (RM60G0680); British Heart Foundation Accelerator Award, UK (AA/18/3/34220); Sino-UK Industrial Fund (RP202G0289); Global Challenges Research Fund (GCRF), UK (P202PF11); LIAS Pioneering Partnerships award, UK (P202ED10); Data Science Enhancement Fund, UK (P202RE237 ).",,Computer Methods and Programs in Biomedicine,,"COVID-19; Humans; Image Processing, Computer-Assisted; Lung; Pandemics; Supervised Machine Learning",2022-05-14,2022,2022-05-14,2022-06,221,,106883,All OA, Bronze,Article,"Zhou, Qinghua; Wang, Shuihua; Zhang, Xin; Zhang, Yu-Dong","Zhou, Qinghua (School of Computing and Mathematical Sciences, University of Leicester, Leicester, LE1 7RH, UK. Electronic address: qz106@le.ac.uk.); Wang, Shuihua (School of Computing and Mathematical Sciences, University of Leicester, Leicester, LE1 7RH, UK. Electronic address: shuihuawang@ieee.org.); Zhang, Xin (Department of Medical Imaging, The Fourth Peoples Hospital of Huaian, Huaian, Jiangsu Province 223002, China. Electronic address: hasyzx@njmu.edu.cn.); Zhang, Yu-Dong (School of Computing and Mathematical Sciences, University of Leicester, Leicester, LE1 7RH, UK. Electronic address: yudongzhang@ieee.org.)","Zhang, Xin (The Fourth People's Hospital); Zhang, Yu-Dong (University of Leicester)","Zhou, Qinghua (University of Leicester); Wang, Shuihua (University of Leicester); Zhang, Xin (The Fourth People's Hospital); Zhang, Yu-Dong (University of Leicester)",3,3,,,https://doi.org/10.1016/j.cmpb.2022.106883,https://app.dimensions.ai/details/publication/pub.1147881662,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,
2986,pub.1134516398,10.1038/s41598-020-79925-4,33441684,PMC7806997,Simultaneous lesion and brain segmentation in multiple sclerosis using deep neural networks,"Segmentation of white matter lesions and deep grey matter structures is an important task in the quantification of magnetic resonance imaging in multiple sclerosis. In this paper we explore segmentation solutions based on convolutional neural networks (CNNs) for providing fast, reliable segmentations of lesions and grey-matter structures in multi-modal MR imaging, and the performance of these methods when applied to out-of-centre data. We trained two state-of-the-art fully convolutional CNN architectures on the 2016 MSSEG training dataset, which was annotated by seven independent human raters: a reference implementation of a 3D Unet, and a more recently proposed 3D-to-2D architecture (DeepSCAN). We then retrained those methods on a larger dataset from a single centre, with and without labels for other brain structures. We quantified changes in performance owing to dataset shift, and changes in performance by adding the additional brain-structure labels. We also compared performance with freely available reference methods. Both fully-convolutional CNN methods substantially outperform other approaches in the literature when trained and evaluated in cross-validation on the MSSEG dataset, showing agreement with human raters in the range of human inter-rater variability. Both architectures showed drops in performance when trained on single-centre data and tested on the MSSEG dataset. When trained with the addition of weak anatomical labels derived from Freesurfer, the performance of the 3D Unet degraded, while the performance of the DeepSCAN net improved. Overall, the DeepSCAN network predicting both lesion and anatomical labels was the best-performing network examined.","This research was supported by the Swiss Multiple Sclerosis Society (âAutomatic segmentation of MS lesions, brain volumetry and morphometry: Proposal of a diagnostic tool for disease monitoringâ) and a grant from the Novartis Forschungsstiftung (âDeepSCAN-Cortexâ).",,Scientific Reports,,"Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Multiple Sclerosis; Neural Networks, Computer",2021-01-13,2021,2021-01-13,,11,1,1087,All OA, Gold,Article,"McKinley, Richard; Wepfer, Rik; Aschwanden, Fabian; Grunder, Lorenz; Muri, Raphaela; Rummel, Christian; Verma, Rajeev; Weisstanner, Christian; Reyes, Mauricio; Salmen, Anke; Chan, Andrew; Wagner, Franca; Wiest, Roland","McKinley, Richard (Support Center for Advanced Neuroimaging, University Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Bern, Switzerland); Wepfer, Rik (Support Center for Advanced Neuroimaging, University Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Bern, Switzerland); Aschwanden, Fabian (Support Center for Advanced Neuroimaging, University Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Bern, Switzerland); Grunder, Lorenz (Support Center for Advanced Neuroimaging, University Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Bern, Switzerland); Muri, Raphaela (Support Center for Advanced Neuroimaging, University Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Bern, Switzerland); Rummel, Christian (Support Center for Advanced Neuroimaging, University Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Bern, Switzerland); Verma, Rajeev (Swiss Paraplegic Centre, Nottwil, Switzerland); Weisstanner, Christian (Medizinisch Radiologisches Institut, Zurich, Switzerland); Reyes, Mauricio (ARTORG Centre for Biomedical Engineering Research, University of Bern, Bern, Switzerland); Salmen, Anke (University Clinic for Neurology, Inselspital, Bern University Hospital, Bern, Switzerland); Chan, Andrew (University Clinic for Neurology, Inselspital, Bern University Hospital, Bern, Switzerland); Wagner, Franca (Support Center for Advanced Neuroimaging, University Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Bern, Switzerland); Wiest, Roland (Support Center for Advanced Neuroimaging, University Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Bern, Switzerland)","McKinley, Richard (University Hospital of Bern)","McKinley, Richard (University Hospital of Bern); Wepfer, Rik (University Hospital of Bern); Aschwanden, Fabian (University Hospital of Bern); Grunder, Lorenz (University Hospital of Bern); Muri, Raphaela (University Hospital of Bern); Rummel, Christian (University Hospital of Bern); Verma, Rajeev (Swiss Paraplegic Center); Weisstanner, Christian (); Reyes, Mauricio (University of Bern); Salmen, Anke (University Hospital of Bern); Chan, Andrew (University Hospital of Bern); Wagner, Franca (University Hospital of Bern); Wiest, Roland (University Hospital of Bern)",31,29,6.58,25.37,https://www.nature.com/articles/s41598-020-79925-4.pdf,https://app.dimensions.ai/details/publication/pub.1134516398,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2977,pub.1154679032,10.1038/s41597-022-01721-8,36658144,PMC9852451,MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification,"We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into a small size of 28âÃâ28 (2D) or 28âÃâ28âÃâ28 (3D) with the corresponding classification labels so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various dataset scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression, and multi-label). The resulting dataset, consisting of 708,069 2D images and 9,998 3D images in total, could support numerous research/educational purposes in biomedical image analysis, computer vision, and machine learning. We benchmark several baseline methods on MedMNIST v2, including 2D/3D neural networks and open-source/commercial AutoML tools. The data and code are publicly available at https://medmnist.com/.","This work was supported by National Science Foundation of China (U20B200011, 61976137). This work was also supported by Grant YG2021ZD18 from Shanghai Jiao Tong University Medical Engineering Cross Research. We would like to acknowledge all authors of the open datasets used in this study.",,Scientific Data,,"Benchmarking; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Machine Learning; Neural Networks, Computer",2023-01-19,2023,2023-01-19,,10,1,41,All OA, Gold,Article,"Yang, Jiancheng; Shi, Rui; Wei, Donglai; Liu, Zequan; Zhao, Lin; Ke, Bilian; Pfister, Hanspeter; Ni, Bingbing","Yang, Jiancheng (Shanghai Jiao Tong University, Shanghai, China); Shi, Rui (Shanghai Jiao Tong University, Shanghai, China); Wei, Donglai (Boston College, Chestnut Hill, MA, USA); Liu, Zequan (RWTH Aachen University, Aachen, Germany); Zhao, Lin (Department of Endocrinology and Metabolism, Fudan Institute of Metabolic Diseases, Zhongshan Hospital, Fudan University, Shanghai, China); Ke, Bilian (Department of Ophthalmology, Shanghai General Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China); Pfister, Hanspeter (Harvard University, Cambridge, MA, USA); Ni, Bingbing (Shanghai Jiao Tong University, Shanghai, China)","Ni, Bingbing (Shanghai Jiao Tong University)","Yang, Jiancheng (Shanghai Jiao Tong University); Shi, Rui (Shanghai Jiao Tong University); Wei, Donglai (Boston College); Liu, Zequan (RWTH Aachen University); Zhao, Lin (Fudan University; Zhongshan Hospital); Ke, Bilian (Shanghai First People's Hospital; Shanghai Jiao Tong University); Pfister, Hanspeter (Harvard University); Ni, Bingbing (Shanghai Jiao Tong University)",3,3,,,https://www.nature.com/articles/s41597-022-01721-8.pdf,https://app.dimensions.ai/details/publication/pub.1154679032,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2977,pub.1151728444,10.1038/s41598-022-21562-0,36216965,PMC9550798,Deep supervision and atrous inception-based U-Net combining CRF for automatic liver segmentation from CT,"Due to low contrast and the blurred boundary between liver tissue and neighboring organs sharing similar intensity values, the problem of liver segmentation from CT images has not yet achieved satisfactory performance and remains a challenge. To alleviate these problems, we introduce deep supervision (DS) and atrous inception (AI) technologies with conditional random field (CRF) and propose three major improvements that are experimentally shown to have substantive and practical value. First, we replace the encoder's standard convolution with the residual block. Residual blocks can increase the depth of the network. Second, we provide an AI module to connect the encoder and decoder. AI allows us to obtain multi-scale features. Third, we incorporate the DS mechanism into the decoder. This helps to make full use of information of the shallow layers. In addition, we employ the Tversky loss function to balance the segmented and non-segmented regions and perform further refinement with a dense CRF. Finally, we extensively validate the proposed method on three public databases: LiTS17, 3DIRCADb, and SLiver07. Compared to the state-of-the-art methods, the proposed method achieved increased segmentation accuracy for the livers with low contrast and the fuzzy boundary between liver tissue and neighboring organs and is, therefore, more suited for automatic segmentation of these livers.","This work is supported by the National Nature Science Foundation (Nos. 61741106, 61701178).",,Scientific Reports,,"Image Processing, Computer-Assisted; Liver; Tomography, X-Ray Computed",2022-10-10,2022,2022-10-10,,12,1,16995,All OA, Gold,Article,"Lv, Peiqing; Wang, Jinke; Zhang, Xiangyang; Shi, Changfa","Lv, Peiqing (School of Automation, Harbin University of Science and Technology, 150080, Harbin, China); Wang, Jinke (Department of Software Engineering, Harbin University of Science and Technology, 264300, Rongcheng, China; School of Automation, Harbin University of Science and Technology, 150080, Harbin, China); Zhang, Xiangyang (School of Automation, Harbin University of Science and Technology, 150080, Harbin, China); Shi, Changfa (Mobile E-business Collaborative Innovation Center of Hunan Province, Hunan University of Technology and Business, 410205, Changsha, China)","Wang, Jinke (Harbin University of Science and Technology; Harbin University of Science and Technology)","Lv, Peiqing (Harbin University of Science and Technology); Wang, Jinke (Harbin University of Science and Technology; Harbin University of Science and Technology); Zhang, Xiangyang (Harbin University of Science and Technology); Shi, Changfa (Hunan University of Technology)",0,0,,,https://www.nature.com/articles/s41598-022-21562-0.pdf,https://app.dimensions.ai/details/publication/pub.1151728444,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
2905,pub.1147110982,10.1002/mp.15670,35420165,,Uncertaintyâguided symmetric multilevel supervision network for 3D left atrium segmentation in late gadoliniumâenhanced MRI,"PURPOSE: Atrial fibrillation (AF) is a common arrhythmia and requires volumetric imaging to guide the therapy procedure. Late gadolinium-enhanced magnetic resonance imaging (LGE MRI) is an efficient noninvasive technology for imaging the diseased heart. Three-dimensional segmentation of the left atrium (LA) in LGE MRI is a fundamental step for guiding the therapy of patients with AF. However, the low contrast and fuzzy surface of the LA in LGE MRI make accurate and objective LA segmentation a challenge. The purpose of this study is to propose an automatic and efficient LA segmentation model based on a convolutional neural network to obtain a more accurate predicted surface and improve the LA segmentationÂ results.
METHODS: In this study, we proposed an uncertainty-guided symmetric multilevel supervision (SML) network for 3D LA segmentation in LGE MRI. First, we constructed an SML structure to combine the corresponding features from the encoding and decoding stages to learn the multiscale representation of LA. Second, we formulated the discrepancy of predictions of our model as model uncertainty. Then we proposed an uncertainty-guided objective function to further increase the segmentation accuracy on theÂ surface.
RESULTS: We evaluated our proposed model on the public LA segmentation database using four universal metrics. The proposed model achieved Hausdorff Distance (HD) of 11.68 mm, average symmetric surface distance of 0.92 mm, Dice score of 0.92, and Jaccard of 0.85. Compared with state-of-the-art models, our model achieved the best HD that is sensitive to surface accuracy. For the other three metrics, our model also achieved better or comparableÂ performance.
CONCLUSIONS: We proposed an efficient automatic LA segmentation model that consisted of an SML structure and an uncertainty-guided objective function. Compared to other models, we designed an additional supervision branch in the encoding stage to learn more detailed representations of LA while learning global context information through the multilevel structure of each supervision branch. To address the fuzzy surface challenge of LA segmentation in LGE MRI, we leveraged the model uncertainty to enhance the distinguishing ability of the model on the surface, thereby the predicted accuracy of the LA surface can be further increased. We conducted extensive ablation and comparative experiments with state-of-the-art models. The experiment results demonstrated that our proposed model could handle the complex structure of LA and had superior advantages in improving the segmentation performance on theÂ surface.","This work was supported by the National Natural Science Foundation of China under Grants 62001141 and 62001144; China Postdoctoral Science Foundation under Grants 2021T140162, 2021M690574, and 2020M670911; Heilongjiang Postdoctoral Fund under Grant LBHZ20066; Shandong Provincial Natural Science Foundation (ZR2020MF050); Science and Technology Innovation Committee of Shenzhen Municipality under Grant JCYJ20210324131800002; and Interdisciplinary Research Foundation of HIT under Grant IR2021230.",,Medical Physics,,Atrial Fibrillation, Gadolinium, Heart Atria, Humans, Magnetic Resonance Imaging, Uncertainty,2022-04-29,2022,2022-04-29,2022-07,49,7,4554-4565,Closed,Article,"Liu, Yashu; Wang, Wei; Luo, Gongning; Wang, Kuanquan; Liang, Dong; Li, Shuo","Liu, Yashu (School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China); Wang, Wei (School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China); Luo, Gongning (School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China); Wang, Kuanquan (School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China); Liang, Dong (School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China); Li, Shuo (Department of Medical Imaging, Western University, London, Ontario, Canada)","Wang, Kuanquan (Harbin Institute of Technology)","Liu, Yashu (Harbin Institute of Technology); Wang, Wei (Harbin Institute of Technology); Luo, Gongning (Harbin Institute of Technology); Wang, Kuanquan (Harbin Institute of Technology); Liang, Dong (Harbin Institute of Technology); Li, Shuo (Western University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1147110982,40 Engineering, 4003 Biomedical Engineering,,,,,,
2900,pub.1150678142,10.1007/s11548-022-02730-z,36048319,PMC9889459,Efficient contour-based annotation by iterative deep learning for organ segmentation from volumetric medical images,"PurposeTraining deep neural networks usually require a large number of human-annotated data. For organ segmentation from volumetric medical images, human annotation is tedious and inefficient. To save human labour and to accelerate the training process, the strategy of annotation by iterative deep learning recently becomes popular in the research community. However, due to the lack of domain knowledge or efficient human-interaction tools, the current AID methods still suffer from long training time and high annotation burden.MethodsWe develop a contour-based annotation by iterative deep learning (AID) algorithm which uses boundary representation instead of voxel labels to incorporate high-level organ shape knowledge. We propose a contour segmentation network with a multi-scale feature extraction backbone to improve the boundary detection accuracy. We also developed a contour-based human-intervention method to facilitate easy adjustments of organ boundaries. By combining the contour-based segmentation network and the contour-adjustment intervention method, our algorithm achieves fast few-shot learning and efficient human proofreading.ResultsFor validation, two human operators independently annotated four abdominal organs in computed tomography (CT) images using our method and two compared methods, i.e. a traditional contour-interpolation method and a state-of-the-art (SOTA) convolutional network (CNN) method based on voxel label representation. Compared to these methods, our approach considerably saved annotation time and reduced inter-rater variabilities. Our contour detection network also outperforms the SOTA nnU-Net in producing anatomically plausible organ shape with only a small training set.ConclusionTaking advantage of the boundary shape prior and the contour representation, our method is more efficient, more accurate and less prone to inter-operator variability than the SOTA AID methods for organ segmentation from volumetric medical images. The good shape learning ability and flexible boundary adjustment function make it suitable for fast annotation of organ structures with regular shape.","This work was supported in part by the National Key Research and Development Program No. 2020YFB1711500, 2020YFB1711501 and 2020YFB1711503, the general program of National Natural Science Fund of China (No. 81971693, 61971445 and 61971089), Dalian City Science and Technology Innovation Funding (No. 2018J12GX042), the Fundamental Research Funds for the Central Universities (No. DUT19JC01 and DUT20YG122), the funding of Liaoning Key Lab of IC &amp; BME System and Dalian Engineering Research Center for Artificial Intelligence in Medical Imaging.",,International Journal of Computer Assisted Radiology and Surgery,,"Humans; Deep Learning; Neural Networks, Computer; Tomography, X-Ray Computed; Algorithms; Image Processing, Computer-Assisted",2022-09-01,2022,2022-09-01,2023-02,18,2,379-394,All OA, Hybrid,Article,"Zhuang, Mingrui; Chen, Zhonghua; Wang, Hongkai; Tang, Hong; He, Jiang; Qin, Bobo; Yang, Yuxin; Jin, Xiaoxian; Yu, Mengzhu; Jin, Baitao; Li, Taijing; Kettunen, Lauri","Zhuang, Mingrui (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); Chen, Zhonghua (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Information Technology, University of JyvÃ¤skylÃ¤, JyvÃ¤skylÃ¤, Finland); Wang, Hongkai (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Liaoning Key Laboratory of Integrated Circuit and Biomedical Electronic System, Dalian, China); Tang, Hong (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); He, Jiang (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); Qin, Bobo (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); Yang, Yuxin (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); Jin, Xiaoxian (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); Yu, Mengzhu (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); Jin, Baitao (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); Li, Taijing (School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China); Kettunen, Lauri (Faculty of Information Technology, University of JyvÃ¤skylÃ¤, JyvÃ¤skylÃ¤, Finland)","Wang, Hongkai (Dalian University of Technology; )","Zhuang, Mingrui (Dalian University of Technology); Chen, Zhonghua (Dalian University of Technology; University of JyvÃ¤skylÃ¤); Wang, Hongkai (Dalian University of Technology); Tang, Hong (Dalian University of Technology); He, Jiang (Dalian University of Technology); Qin, Bobo (Dalian University of Technology); Yang, Yuxin (Dalian University of Technology); Jin, Xiaoxian (Dalian University of Technology); Yu, Mengzhu (Dalian University of Technology); Jin, Baitao (Dalian University of Technology); Li, Taijing (Dalian University of Technology); Kettunen, Lauri (University of JyvÃ¤skylÃ¤)",0,0,,,https://link.springer.com/content/pdf/10.1007/s11548-022-02730-z.pdf,https://app.dimensions.ai/details/publication/pub.1150678142,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2900,pub.1150354842,10.1038/s41598-022-16828-6,35986015,PMC9391485,A lightweight neural network with multiscale feature enhancement for liver CT segmentation,"Segmentation of abdominal Computed Tomography (CT) scan is essential for analyzing, diagnosing, and treating visceral organ diseases (e.g., hepatocellular carcinoma). This paper proposes a novel neural network (Res-PAC-UNet) that employs a fixed-width residual UNet backbone and Pyramid Atrous Convolutions, providing a low disk utilization method for precise liver CT segmentation. The proposed network is trained on medical segmentation decathlon dataset using a modified surface loss function. Additionally, we evaluate its quantitative and qualitative performance; the Res16-PAC-UNet achieves a Dice coefficient of 0.950 Â± 0.019 with less than half a million parameters. Alternatively, the Res32-PAC-UNet obtains a Dice coefficient of 0.958 Â± 0.015 with an acceptable parameter count of approximately 1.2 million.","This publication was made possible by NPRP-11S-1219-170106 from the Qatar National Research Fund (a member of Qatar Foundation). The findings herein reflect the work, and are solely the responsibility of the authors.",,Scientific Reports,,"Humans; Image Processing, Computer-Assisted; Liver Neoplasms; Neural Networks, Computer; Tomography, X-Ray Computed",2022-08-19,2022,2022-08-19,,12,1,14153,All OA, Gold,Article,"Ansari, Mohammed Yusuf; Yang, Yin; Balakrishnan, Shidin; Abinahed, Julien; Al-Ansari, Abdulla; Warfa, Mohamed; Almokdad, Omran; Barah, Ali; Omer, Ahmed; Singh, Ajay Vikram; Meher, Pramod Kumar; Bhadra, Jolly; Halabi, Osama; Azampour, Mohammad Farid; Navab, Nassir; Wendler, Thomas; Dakua, Sarada Prasad","Ansari, Mohammed Yusuf (Hamad Medical Corporation, Doha, Qatar); Yang, Yin (Hamad Bin Khalifa University, Doha, Qatar); Balakrishnan, Shidin (Hamad Medical Corporation, Doha, Qatar); Abinahed, Julien (Hamad Medical Corporation, Doha, Qatar); Al-Ansari, Abdulla (Hamad Medical Corporation, Doha, Qatar); Warfa, Mohamed (Wake Forest Baptist Medical Center, Winston-Salem, USA); Almokdad, Omran (Hamad Medical Corporation, Doha, Qatar); Barah, Ali (Hamad Medical Corporation, Doha, Qatar); Omer, Ahmed (Hamad Medical Corporation, Doha, Qatar); Singh, Ajay Vikram (German Federal Institute for Risk Assessment (BfR), Berlin, Germany); Meher, Pramod Kumar (C. V. Raman Global University, Bhubaneswar, India); Bhadra, Jolly (Qatar University, Doha, Qatar); Halabi, Osama (Qatar University, Doha, Qatar); Azampour, Mohammad Farid (Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany); Navab, Nassir (Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany); Wendler, Thomas (Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany); Dakua, Sarada Prasad (Hamad Medical Corporation, Doha, Qatar)","Dakua, Sarada Prasad (Hamad Medical Corporation)","Ansari, Mohammed Yusuf (Hamad Medical Corporation); Yang, Yin (Hamad bin Khalifa University); Balakrishnan, Shidin (Hamad Medical Corporation); Abinahed, Julien (Hamad Medical Corporation); Al-Ansari, Abdulla (Hamad Medical Corporation); Warfa, Mohamed (Wake Forest Baptist Medical Center); Almokdad, Omran (Hamad Medical Corporation); Barah, Ali (Hamad Medical Corporation); Omer, Ahmed (Hamad Medical Corporation); Singh, Ajay Vikram (Federal Institute for Risk Assessment); Meher, Pramod Kumar (); Bhadra, Jolly (Qatar University); Halabi, Osama (Qatar University); Azampour, Mohammad Farid (Technical University of Munich); Navab, Nassir (Technical University of Munich); Wendler, Thomas (Technical University of Munich); Dakua, Sarada Prasad (Hamad Medical Corporation)",7,7,,,https://www.nature.com/articles/s41598-022-16828-6.pdf,https://app.dimensions.ai/details/publication/pub.1150354842,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2900,pub.1148951605,10.1186/s12859-022-04794-9,35751030,PMC9229514,A state-of-the-art technique to perform cloud-based semantic segmentation using deep learning 3D U-Net architecture,"Glioma is the most aggressive and dangerous primary brain tumor with a survival time of less than 14Â months. Segmentation of tumors is a necessary task in the image processing of the gliomas and is important for its timely diagnosis and starting a treatment. Using 3D U-net architecture to perform semantic segmentation on brain tumor dataset is at the core of deep learning. In this paper, we present a unique cloud-based 3D U-Net method to perform brain tumor segmentation using BRATS dataset. The system was effectively trained by using Adam optimization solver by utilizing multiple hyper parameters. We got an average dice score of 95% which makes our method the first cloud-based method to achieve maximum accuracy. The dice score is calculated by using SÃ¸rensen-Dice similarity coefficient. We also performed an extensive literature review of the brain tumor segmentation methods implemented in the last five years to get a state-of-the-art picture of well-known methodologies with a higher dice score. In comparison to the already implemented architectures, our method ranks on top in terms of accuracy in using a cloud-based 3D U-Net framework for glioma segmentation.",Not Applicable.,Not Applicable.,BMC Bioinformatics,,"Brain Neoplasms; Cloud Computing; Deep Learning; Glioma; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Semantics",2022-06-24,2022,2022-06-24,2022-12,23,1,251,All OA, Gold,Article,"Shaukat, Zeeshan; Farooq, Qurat ul Ain; Tu, Shanshan; Xiao, Chuangbai; Ali, Saqib","Shaukat, Zeeshan (Faculty of Information Technology, Beijing University of Technology, Beijing, Peopleâs Republic of China; Faculty of Computer Science, University of South Asia, Lahore, Pakistan); Farooq, Qurat ul Ain (Faculty of Environmental and Life Sciences, Beijing University of Technology, Beijing, Peopleâs Republic of China); Tu, Shanshan (Faculty of Information Technology, Beijing University of Technology, Beijing, Peopleâs Republic of China); Xiao, Chuangbai (Faculty of Information Technology, Beijing University of Technology, Beijing, Peopleâs Republic of China); Ali, Saqib (Faculty of Information Technology, Beijing University of Technology, Beijing, Peopleâs Republic of China)","Shaukat, Zeeshan (Beijing University of Technology; University of South Asia); Xiao, Chuangbai (Beijing University of Technology)","Shaukat, Zeeshan (Beijing University of Technology; University of South Asia); Farooq, Qurat ul Ain (Beijing University of Technology); Tu, Shanshan (Beijing University of Technology); Xiao, Chuangbai (Beijing University of Technology); Ali, Saqib (Beijing University of Technology)",4,4,,,https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/s12859-022-04794-9,https://app.dimensions.ai/details/publication/pub.1148951605,46 Information and Computing Sciences,,,,,,,,,,,
2893,pub.1155121148,10.3390/diagnostics13030546,36766655,PMC9914913,Medical Images Segmentation for Lung Cancer Diagnosis Based on Deep Learning Architectures,"Lung cancer presents one of the leading causes of mortalities for people around the world. Lung image analysis and segmentation are one of the primary steps used for early diagnosis of cancer. Handcrafted medical imaging segmentation presents a very time-consuming task for radiation oncologists. To address this problem, we propose in this work to develop a full and entire system used for early diagnosis of lung cancer in CT scan imaging. The proposed lung cancer diagnosis system is composed of two main parts: the first part is used for segmentation developed on top of the UNETR network, and the second part is a classification part used to classify the output segmentation part, either benign or malignant, developed on top of the self-supervised network. The proposed system presents a powerful tool for early diagnosing and combatting lung cancer using 3D-input CT scan data. Extensive experiments have been performed to contribute to better segmentation and classification results. Training and testing experiments have been performed using the Decathlon dataset. Experimental results have been conducted to new state-of-the-art performances: segmentation accuracy of 97.83%, and 98.77% as classification accuracy. The proposed system presents a new powerful tool to use for early diagnosing and combatting lung cancer using 3D-input CT scan data.","This research work was funded by the Institutional Fund Projects under grant no. (IFPIP: 1543-611-1443). The authors gratefully acknowledge the technical and financial support provided by the Ministry of Education and King Abdulaziz University, DSR, Jeddah, Saudi Arabia.","This research work was funded by the Institutional Fund Projects under grant no. (IFPIP: 1543-611-1443). The authors gratefully acknowledge the technical and financial support provided by the Ministry of Education and King Abdulaziz University, DSR, Jeddah, Saudi Arabia.",Diagnostics,,,2023-02-02,2023,2023-02-02,,13,3,546,All OA, Gold,Article,"Said, Yahia; Alsheikhy, Ahmed A.; Shawly, Tawfeeq; Lahza, Husam","Said, Yahia (Department of Electrical Engineering, College of Engineering, Northern Border University, Arar 91431, Saudi Arabia; Laboratory of Electronics and Microelectronics (LR99ES30), University of Monastir, Monastir 5019, Tunisia); Alsheikhy, Ahmed A. (Department of Electrical Engineering, College of Engineering, Northern Border University, Arar 91431, Saudi Arabia); Shawly, Tawfeeq (Department of Electrical Engineering, Faculty of Engineering at Rabigh, King Abdulaziz University, Jeddah 21589, Saudi Arabia); Lahza, Husam (Department of Information Technology, College of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi Arabia)","Said, Yahia (Northern Border University; University of Monastir)","Said, Yahia (Northern Border University; University of Monastir); Alsheikhy, Ahmed A. (Northern Border University); Shawly, Tawfeeq (King Abdulaziz University); Lahza, Husam (King Abdulaziz University)",0,0,,,https://www.mdpi.com/2075-4418/13/3/546/pdf?version=1675334574,https://app.dimensions.ai/details/publication/pub.1155121148,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
2893,pub.1151156692,10.1016/j.cmpb.2022.107147,36206688,,Convolutional bi-directional learning and spatial enhanced attentions for lung tumor segmentation,"BACKGROUND AND OBJECTIVE: Accurate lung tumor segmentation from computed tomography (CT) is complex due to variations in tumor sizes, shapes, patterns and growing locations. Learning semantic and spatial relations between different feature channels, image regions and positions is critical yet challenging.
METHODS: We propose a new segmentation method, PRCS, by learning and integrating multi-channel contextual relations, and spatial and position dependencies across image regions. Firstly, to extract contextual relationships between different deep image feature tensor channels, we propose a new convolutional bi-directional gated recurrent unit based module for forward and backward learning. Secondly, a novel cross-channel region-level attention mechanism is proposed to discriminate the contributions of different local regions and associated features in the global learning process. Finally, spatial and position dependencies are formulated by a new position-enhanced self-attention mechanism. The new attention can measure the diverse contributions of other positions to a target position and obtain an enhanced adaptive feature vector for the target position.
RESULTS: Our model outperformed seven state-of-the-art segmentation methods on both public and in-house lung tumor datasets in terms of spatial overlapping and shape similarity. Ablation study results proved the effectiveness of three technical innovations and generalization ability on different 3D CNN segmentation backbones.
CONCLUSION: The proposed model enhanced the learning and propagation of contextual, spatial and position relations in 3D volumes, improving lung tumours' segmentation performance with large variations and indistinct boundaries. PRCS provides an effective automated approach to support precision diagnosis and treatment planning of lung cancer.","This work is supported by the Natural Science Foundation of China (61972135, 62172143, 8217102892), the Natural Science Foundation of Heilongjiang Province (LH2019A029), the China Postdoctoral Science Foundation (2019M650069, 2020M670939), and Natural Science Foundation of Shandong Province (ZR2019LZL012).",,Computer Methods and Programs in Biomedicine,,"Humans; Neural Networks, Computer; Deep Learning; Tomography, X-Ray Computed; Lung Neoplasms; Image Processing, Computer-Assisted",2022-09-20,2022,2022-09-20,2022-11,226,,107147,Closed,Article,"Xuan, Ping; Jiang, Bin; Cui, Hui; Jin, Qiangguo; Cheng, Peng; Nakaguchi, Toshiya; Zhang, Tiangang; Li, Changyang; Ning, Zhiyu; Guo, Menghan; Wang, Linlin","Xuan, Ping (School of Computer Science and Technology, Heilongjiang University, Harbin, China; Department of Computer Science, School of Engineering, Shantou University, Shantou, China.); Jiang, Bin (School of Computer Science and Technology, Heilongjiang University, Harbin, China.); Cui, Hui (Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia.); Jin, Qiangguo (School of Software, Northwestern Polytechnical University, Xi' an, China.); Cheng, Peng (Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia.); Nakaguchi, Toshiya (Center for Frontier Medical Engineering, Chiba University, Chiba, Japan.); Zhang, Tiangang (School of Mathematical Science, Heilongjiang University, Harbin, China. Electronic address: zhang@hlju.edu.cn.); Li, Changyang (Rudder Technology Pty Ltd, Sydney, Australia.); Ning, Zhiyu (Sydney Polytechnic Institute, Sydney, Australia.); Guo, Menghan (GMH Technology Pty Ltd, Sydney, Australia.); Wang, Linlin (Department of Radiation Oncology, Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences, Jinan, China.)","Zhang, Tiangang (Heilongjiang University)","Xuan, Ping (Heilongjiang University; Shantou University); Jiang, Bin (Heilongjiang University); Cui, Hui (La Trobe University); Jin, Qiangguo (Northwestern Polytechnical University); Cheng, Peng (La Trobe University); Nakaguchi, Toshiya (Chiba University); Zhang, Tiangang (Heilongjiang University); Li, Changyang (); Ning, Zhiyu (); Guo, Menghan (); Wang, Linlin (Shandong Tumor Hospital)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151156692,46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
2893,pub.1126682708,10.1109/jbhi.2020.2986926,32305947,,Multi-Scale Self-Guided Attention for Medical Image Segmentation,"Even though convolutional neural networks (CNNs) are driving progress in medical image segmentation, standard models still have some drawbacks. First, the use of multi-scale approaches, i.e., encoder-decoder architectures, leads to a redundant use of information, where similar low-level features are extracted multiple times at multiple scales. Second, long-range feature dependencies are not efficiently modeled, resulting in non-optimal discriminative feature representations associated with each semantic class. In this paper we attempt to overcome these limitations with the proposed architecture, by capturing richer contextual dependencies based on the use of guided self-attention mechanisms. This approach is able to integrate local features with their corresponding global dependencies, as well as highlight interdependent channel maps in an adaptive manner. Further, the additional loss between different modules guides the attention mechanisms to neglect irrelevant information and focus on more discriminant regions of the image by emphasizing relevant feature associations. We evaluate the proposed model in the context of semantic segmentation on three different datasets: abdominal organs, cardiovascular structures and brain tumors. A series of ablation experiments support the importance of these attention modules in the proposed architecture. In addition, compared to other state-of-the-art segmentation networks our model yields better segmentation performance, increasing the accuracy of the predictions while reducing the standard deviation. This demonstrates the efficiency of our approach to generate precise and reliable automatic segmentations of medical images. Our code is made publicly available at: https://github.com/sinAshish/Multi-Scale-Attention.",This work was supported by the Startup Professor Funding from ETS Montreal. The authors wish to thank NVIDIA for its kind donation of the Titan V GPU used in this work.,,IEEE Journal of Biomedical and Health Informatics,,"Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Semantics",2021-01-05,2021,2021-01-05,2021-01,25,1,121-130,All OA, Green,Article,"Sinha, Ashish; Dolz, Jose","Sinha, Ashish (Indian Institute of Technology Roorkee, Roorkee, 247667, India); Dolz, Jose (Superieure, Montreal, QC, H3C 1K3, Canada)","Dolz, Jose ","Sinha, Ashish (Indian Institute of Technology Roorkee); Dolz, Jose ()",199,190,17.09,162.87,http://arxiv.org/pdf/1906.02849,https://app.dimensions.ai/details/publication/pub.1126682708,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2889,pub.1151714092,10.3390/life12101570,36295005,PMC9604839,nn-TransUNet: An Automatic Deep Learning Pipeline for Heart MRI Segmentation,"Cardiovascular disease (CVD) is a disease with high mortality in modern times. The segmentation task for MRI to extract the related organs for CVD is essential for diagnosis. Currently, a large number of deep learning methods are designed for medical image segmentation tasks. However, the design of segmentation algorithms tends to have more focus on deepening the network architectures and tuning the parameters and hyperparameters manually, which not only leads to a high time and effort consumption, but also causes the problem that the architectures and setting designed for a single task only performs well in a single dataset, but have low performance in other cases. In this paper, nn-TransUNet, an automatic deep learning pipeline for MRI segmentation of the heart is proposed to combine the experiment planning of nnU-net and the network architecture of TransUNet. nn-TransUNet uses vision transformers and convolution layers in the design of the encoder and takes up convolution layers as decoder. With the adaptive preprocessing and network training plan generated by the proposed automatic experiment planning pipeline, nn-TransUNet is able to fulfill the target of medical image segmentation in heart MRI tasks. nn-TransUNet achieved state-of-the-art level in heart MRI segmentation task on Automatic Cardiac Diagnosis Challenge (ACDC) Dataset. It also saves the effort and time to manually tune the parameters and hyperparameters, which can reduce the burden on researchers.","Thanks for Bernard et al., by Antonelli et al. and Zhuang for opening source ACDC, MSD02 and MyoPS 2020 datasets, which are the experimental datasets of this research.","This work was supported by the National Natural Science Foundation of China under Grants 62066047, 61966037.",Life,,,2022-10-09,2022,2022-10-09,,12,10,1570,All OA, Gold,Article,"Zhao, Li; Zhou, Dongming; Jin, Xin; Zhu, Weina","Zhao, Li (School of Information Science and Engineering, Yunnan University, Kunming 650504, China); Zhou, Dongming (School of Information Science and Engineering, Yunnan University, Kunming 650504, China); Jin, Xin (School of Software, Yunnan University, Kunming 650504, China); Zhu, Weina (School of Information Science and Engineering, Yunnan University, Kunming 650504, China)","Zhu, Weina (Yunnan University)","Zhao, Li (Yunnan University); Zhou, Dongming (Yunnan University); Jin, Xin (Yunnan University); Zhu, Weina (Yunnan University)",1,1,,,https://www.mdpi.com/2075-1729/12/10/1570/pdf?version=1665365822,https://app.dimensions.ai/details/publication/pub.1151714092,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,
2889,pub.1150769701,10.1109/tmi.2022.3204551,36063521,,MsVRL: Self-Supervised Multiscale Visual Representation Learning via Cross-Level Consistency for Medical Image Segmentation,"Automated medical image segmentation for organs or lesions plays an essential role in clinical diagnoses and treatment plannings. However, training an accurate and robust segmentation model is still a long-standing challenge due to the time-consuming and expertise-intensive annotations for training data, especially 3-D medical images. Recently, self-supervised learning emerges as a promising approach for unsupervised visual representation learning, showing great potential to alleviate the expertise annotations for medical images. Although global representation learning has attained remarkable results on iconic datasets, such as ImageNet, it can not be applied directly to medical image segmentation, because the segmentation task is non-iconic, and the targets always vary in physical scales. To address these problems, we propose a Multi-scale Visual Representation self-supervised Learning (MsVRL) model, to perform finer-grained representation and deal with different target scales. Specifically, a multi-scale representation conception, a canvas matching method, an embedding pre-sampling module, a center-ness branch, and a cross-level consistent loss are introduced to improve the performance. After pre-trained on unlabeled datasets (RibFrac and part of MSD), MsVRL performs downstream segmentation tasks on labeled datasets (BCV, spleen of MSD, and KiTS). Results of the experiments show that MsVRL outperforms other state-of-the-art works on these medical image segmentation tasks.",,This work was supported by the National Key Research and Development Program of China under Grant 2020AAA0109002.,IEEE Transactions on Medical Imaging,,,2022-09-05,2022,2022-09-05,2022-09-05,42,1,91-102,Closed,Article,"Zheng, Ruifeng; Zhong, Ying; Yan, Senxiang; Sun, Hongcheng; Shen, Haibin; Huang, Kejie","Zheng, Ruifeng (College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, 310027, China); Zhong, Ying (College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, 310027, China); Yan, Senxiang (Department of Radiation Oncology, the First Affiliated Hospital, College of Medicine, Zhejiang University, Hangzhou, Zhejiang, 310003, China); Sun, Hongcheng (Department of General Surgery, Shanghai General Hospital, Shanghai Jiao Tong University, Shanghai, 200080, China); Shen, Haibin (College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, 310027, China); Huang, Kejie (College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, 310027, China)","Huang, Kejie (Zhejiang University)","Zheng, Ruifeng (Zhejiang University); Zhong, Ying (Zhejiang University); Yan, Senxiang (Zhejiang University); Sun, Hongcheng (Shanghai First People's Hospital; Shanghai Jiao Tong University); Shen, Haibin (Zhejiang University); Huang, Kejie (Zhejiang University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1150769701,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
2889,pub.1145018652,10.1007/s10278-021-00535-1,35083618,PMC8921448,A Deep Learning-Based and Fully Automated Pipeline for Thoracic Aorta Geometric Analysis and Planning for Endovascular Repair from Computed Tomography,"Feasibility assessment and planning of thoracic endovascular aortic repair (TEVAR) require computed tomography (CT)-based analysis of geometric aortic features to identify adequate landing zones (LZs) for endograft deployment. However, no consensus exists on how to take the necessary measurements from CT image data. We trained and applied a fully automated pipeline embedding a convolutional neural network (CNN), which feeds on 3D CT images to automatically segment the thoracic aorta, detects proximal landing zones (PLZs), and quantifies geometric features that are relevant for TEVAR planning. For 465 CT scans, the thoracic aorta and pulmonary arteries were manually segmented; 395 randomly selected scans with the corresponding ground truth segmentations were used to train a CNN with a 3D U-Net architecture. The remaining 70 scans were used for testing. The trained CNN was embedded within computational geometry processing pipeline which provides aortic metrics of interest for TEVAR planning. The resulting metrics included aortic arch centerline radius of curvature, proximal landing zones (PLZs) maximum diameters, angulation, and tortuosity. These parameters were statistically analyzed to compare standard arches vs. arches with a common origin of the innominate and left carotid artery (CILCA). The trained CNN yielded a mean Dice score of 0.95 and was able to generalize to 9 pathological cases of thoracic aortic aneurysm, providing accurate segmentations. CILCA arches were characterized by significantly greater angulation (pâ=â0.015) and tortuosity (pâ=â0.048) in PLZ 3 vs. standard arches. For both arch configurations, comparisons among PLZs revealed statistically significant differences in maximum zone diameters (pâ<â0.0001), angulation (pâ<â0.0001), and tortuosity (pâ<â0.0001). Our tool allows clinicians to obtain objective and repeatable PLZs mapping, and a range of automatically derived complex aortic metrics.",,"âThis work was supported by âRicerca Correnteâ and 5xmilleâ grants from IRCCS Policlinico San Donato, a clinical research hospital partially funded by the Italian Ministry of Health.",Journal of Digital Imaging,,"Aorta, Thoracic; Aortography; Blood Vessel Prosthesis; Blood Vessel Prosthesis Implantation; Computed Tomography Angiography; Deep Learning; Endovascular Procedures; Humans; Retrospective Studies; Tomography, X-Ray Computed; Treatment Outcome",2022-01-26,2022,2022-01-26,2022-04,35,2,226-239,Closed,Article,"Saitta, Simone; Sturla, Francesco; Caimi, Alessandro; Riva, Alessandra; Palumbo, Maria Chiara; Nano, Giovanni; Votta, Emiliano; Corte, Alessandro Della; Glauber, Mattia; Chiappino, Dante; Marrocco-Trischitta, Massimiliano M.; Redaelli, Alberto","Saitta, Simone (Department of Electronics Information and Bioengineering, Politecnico Di Milano, Milan, Italy); Sturla, Francesco (3D and Computer Simulation Laboratory, IRCCS Policlinico San Donato, San Donato Milanese, Italy; Department of Electronics Information and Bioengineering, Politecnico Di Milano, Milan, Italy); Caimi, Alessandro (Department of Electronics Information and Bioengineering, Politecnico Di Milano, Milan, Italy); Riva, Alessandra (Department of Electronics Information and Bioengineering, Politecnico Di Milano, Milan, Italy; 3D and Computer Simulation Laboratory, IRCCS Policlinico San Donato, San Donato Milanese, Italy); Palumbo, Maria Chiara (Department of Electronics Information and Bioengineering, Politecnico Di Milano, Milan, Italy); Nano, Giovanni (Clinical Research Unit and Division of Vascular Surgery, IRCCS Policlinico San Donato, Via Morandi 30, 20097, San Donato Milanese, Italy); Votta, Emiliano (Department of Electronics Information and Bioengineering, Politecnico Di Milano, Milan, Italy; 3D and Computer Simulation Laboratory, IRCCS Policlinico San Donato, San Donato Milanese, Italy); Corte, Alessandro Della (Department of Translational Medical Sciences, University of Campania âL. Vanvitelliâ, Unit of Cardiac Surgery, V. Monaldi Hospital, Naples, Italy); Glauber, Mattia (Minimally Invasive Cardiac Surgery Unit, Istituto Clinico SantâAmbrogio, Milan, Italy); Chiappino, Dante (Department of Radiology, CNR (National Council of Research), Tuscany Region âGabriele Monasterioâ Foundation (FTGM), Massa, Italy); Marrocco-Trischitta, Massimiliano M. (Clinical Research Unit and Division of Vascular Surgery, IRCCS Policlinico San Donato, Via Morandi 30, 20097, San Donato Milanese, Italy); Redaelli, Alberto (Department of Electronics Information and Bioengineering, Politecnico Di Milano, Milan, Italy)","Marrocco-Trischitta, Massimiliano M. (IRCCS Policlinico San Donato)","Saitta, Simone (Politecnico di Milano); Sturla, Francesco (IRCCS Policlinico San Donato; Politecnico di Milano); Caimi, Alessandro (Politecnico di Milano); Riva, Alessandra (Politecnico di Milano; IRCCS Policlinico San Donato); Palumbo, Maria Chiara (Politecnico di Milano); Nano, Giovanni (IRCCS Policlinico San Donato); Votta, Emiliano (Politecnico di Milano; IRCCS Policlinico San Donato); Corte, Alessandro Della (Ospedale Monaldi); Glauber, Mattia (Istituto Clinico Sant'Ambrogio); Chiappino, Dante (); Marrocco-Trischitta, Massimiliano M. (IRCCS Policlinico San Donato); Redaelli, Alberto (Politecnico di Milano)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1145018652,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,,
2887,pub.1136403113,10.1016/j.compmedimag.2021.101896,33752079,,Dual-task ultrasound spine transverse vertebrae segmentation network with contour regularization,"3D ultrasound imaging has become one of the common diagnosis ways to assess scoliosis since it is radiation-free, real-time, and low-cost. Spine curvature angle measurement is an important step to assess scoliosis precisely. One way to calculate the angle is using the vertebrae features of the 2-D coronal images to identify the most tilted vertebrae. To do the measurement, the segmentation of the transverse vertebrae is an important step. In this paper, we propose a dual-task ultrasound transverse vertebrae segmentation network (D-TVNet) based on U-Net. First, we arrange an auxiliary shape regularization network to learn the contour segmentation of the bones. It improves the boundary segmentation and anti-interference ability of the U-Net by fusing some of the features of the auxiliary task and the main task. Then, we introduce the atrous spatial pyramid pooling (ASPP) module to the end of the down-sampling stage of the main task stream to improve the relative feature extraction ability. To further improve the boundary segmentation, we extendedly fuse the down-sampling output features of the auxiliary network in the ASPP. The experiment results show that the proposed D-TVNet achieves the best dice score of 86.68% and the mean dice score of 86.17% based on cross-validation, which is an improvement of 5.17% over the baseline U-Net. An automatic ultrasound spine bone segmentation network with promising results has been achieved.",The project is partially supported by Hong Kong Research Grant Council Research Impact Fund (R5017-18). Conflict of interest: No benefits in any form have been or will be received from a commercial party related directly or indirectly to the subject of this manuscript.,,Computerized Medical Imaging and Graphics,,"Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Scoliosis; Spine; Ultrasonography",2021-03-15,2021,2021-03-15,2021-04,89,,101896,Closed,Article,"Lyu, Juan; Bi, Xiaojun; Banerjee, Sunetra; Huang, Zixun; Leung, Frank H F; Lee, Timothy Tin-Yan; Yang, De-De; Zheng, Yong-Ping; Ling, Sai Ho","Lyu, Juan (College of Information and Communication Engineering, Harbin Engineering University, Harbin, China.); Bi, Xiaojun (College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; College of Information Engineering, Minzu University of China, Beijing, China.); Banerjee, Sunetra (School of Biomedical Engineering, University of Technology Sydney, Ultimo, NSW 2007, Australia.); Huang, Zixun (Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hung Hum, Hong Kong.); Leung, Frank H F (Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hung Hum, Hong Kong.); Lee, Timothy Tin-Yan (Department of Biomedical Engineering, The Hong Kong Polytechnic University, Hung Hum, Hong Kong.); Yang, De-De (Department of Biomedical Engineering, The Hong Kong Polytechnic University, Hung Hum, Hong Kong.); Zheng, Yong-Ping (Department of Biomedical Engineering, The Hong Kong Polytechnic University, Hung Hum, Hong Kong.); Ling, Sai Ho (School of Biomedical Engineering, University of Technology Sydney, Ultimo, NSW 2007, Australia. Electronic address: Steve.Ling@uts.edu.au.)","Ling, Sai Ho (University of Technology Sydney)","Lyu, Juan (Harbin Engineering University); Bi, Xiaojun (Harbin Engineering University; Minzu University of China); Banerjee, Sunetra (University of Technology Sydney); Huang, Zixun (Hong Kong Polytechnic University); Leung, Frank H F (Hong Kong Polytechnic University); Lee, Timothy Tin-Yan (Hong Kong Polytechnic University); Yang, De-De (Hong Kong Polytechnic University); Zheng, Yong-Ping (Hong Kong Polytechnic University); Ling, Sai Ho (University of Technology Sydney)",7,7,1.51,5.36,,https://app.dimensions.ai/details/publication/pub.1136403113,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,,
2753,pub.1123813569,10.1007/s00330-019-06593-y,31900702,,Automated volumetric assessment with artificial neural networks might enable a more accurate assessment of disease burden in patients with multiple sclerosis,"ObjectivesPatients with multiple sclerosis (MS) regularly undergo MRI for assessment of disease burden. However, interpretation may be time consuming and prone to intra- and interobserver variability. Here, we evaluate the potential of artificial neural networks (ANN) for automated volumetric assessment of MS disease burden and activity on MRI.MethodsA single-institutional dataset with 334 MS patients (334 MRI exams) was used to develop and train an ANN for automated identification and volumetric segmentation of T2/FLAIR-hyperintense and contrast-enhancing (CE) lesions. Independent testing was performed in a single-institutional longitudinal dataset with 82 patients (266 MRI exams). We evaluated lesion detection performance (F1 scores), lesion segmentation agreement (DICE coefficients), and lesion volume agreement (concordance correlation coefficients [CCC]). Independent evaluation was performed on the public ISBI-2015 challenge dataset.ResultsThe F1 score was maximized in the training set at a detection threshold of 7Â mm3 for T2/FLAIR lesions and 14Â mm3 for CE lesions. In the training set, mean F1 scores were 0.867 for T2/FLAIR lesions and 0.636 for CE lesions, as compared to 0.878 for T2/FLAIR lesions and 0.715 for CE lesions in the test set. Using these thresholds, the ANN yielded mean DICE coefficients of 0.834 and 0.878 for segmentation of T2/FLAIR and CE lesions in the training set (fivefold cross-validation). Corresponding DICE coefficients in the test set were 0.846 for T2/FLAIR lesions and 0.908 for CE lesions, and the CCC was â¥â0.960 in each dataset.ConclusionsOur results highlight the capability of ANN for quantitative state-of-the-art assessment of volumetric lesion load on MRI and potentially enable a more accurate assessment of disease burden in patients with MS.Key Pointsâ¢ Artificial neural networks (ANN) can accurately detect and segment both T2/FLAIR and contrast-enhancing MS lesions in MRI data.â¢ Performance of the ANN was consistent in a clinically derived dataset, with patients presenting all possible disease stages in MRI scans acquired from standard clinical routine rather than with high-quality research sequences.â¢ Computer-aided evaluation of MS with ANN could streamline both clinical and research procedures in the volumetric assessment of MS disease burden as well as in lesion detection.",,"Dr. Kickingereder, MBA was supported by the Medical Faculty Heidelberg Postdoc-Program and the Else KrÃ¶ner-Fresenius Foundation (Else-KrÃ¶ner Memorial Scholarship).",European Radiology,,"Adult; Brain; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Multiple Sclerosis; Neural Networks, Computer; Reproducibility of Results",2020-01-03,2020,2020-01-03,2020-04,30,4,2356-2364,Closed,Article,"Brugnara, Gianluca; Isensee, Fabian; Neuberger, Ulf; Bonekamp, David; Petersen, Jens; Diem, Ricarda; Wildemann, Brigitte; Heiland, Sabine; Wick, Wolfgang; Bendszus, Martin; Maier-Hein, Klaus; Kickingereder, Philipp","Brugnara, Gianluca (Department of Neuroradiology, University of Heidelberg Medical Center, Heidelberg, Germany); Isensee, Fabian (Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany); Neuberger, Ulf (Department of Neuroradiology, University of Heidelberg Medical Center, Heidelberg, Germany); Bonekamp, David (Department of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany); Petersen, Jens (Department of Neuroradiology, University of Heidelberg Medical Center, Heidelberg, Germany; Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany); Diem, Ricarda (Department of Neurology, University of Heidelberg Medical Center, Heidelberg, Germany); Wildemann, Brigitte (Department of Neurology, University of Heidelberg Medical Center, Heidelberg, Germany); Heiland, Sabine (Department of Neuroradiology, University of Heidelberg Medical Center, Heidelberg, Germany); Wick, Wolfgang (Department of Neurology, University of Heidelberg Medical Center, Heidelberg, Germany; Clinical Cooperation Unit Neurooncology, German Cancer Consortium (DKTK), DKFZ, Heidelberg, Germany); Bendszus, Martin (Department of Neuroradiology, University of Heidelberg Medical Center, Heidelberg, Germany); Maier-Hein, Klaus (Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany); Kickingereder, Philipp (Department of Neuroradiology, University of Heidelberg Medical Center, Heidelberg, Germany)","Kickingereder, Philipp (University Hospital Heidelberg)","Brugnara, Gianluca (University Hospital Heidelberg); Isensee, Fabian (German Cancer Research Center); Neuberger, Ulf (University Hospital Heidelberg); Bonekamp, David (German Cancer Research Center); Petersen, Jens (University Hospital Heidelberg; German Cancer Research Center); Diem, Ricarda (University Hospital Heidelberg); Wildemann, Brigitte (University Hospital Heidelberg); Heiland, Sabine (University Hospital Heidelberg); Wick, Wolfgang (University Hospital Heidelberg; German Cancer Research Center); Bendszus, Martin (University Hospital Heidelberg); Maier-Hein, Klaus (German Cancer Research Center); Kickingereder, Philipp (University Hospital Heidelberg)",9,8,1.12,4.41,,https://app.dimensions.ai/details/publication/pub.1123813569,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,,
2735,pub.1154340585,10.1016/j.compmedimag.2022.102174,36640485,,Self-supervised contrastive learning with random walks for medical image segmentation with limited annotations,"Medical image segmentation has seen significant progress through the use of supervised deep learning. Hereby, large annotated datasets were employed to reliably segment anatomical structures. To reduce the requirement for annotated training data, self-supervised pre-training strategies on non-annotated data were designed. Especially contrastive learning schemes operating on dense pixel-wise representations have been introduced as an effective tool. In this work, we expand on this strategy and leverage inherent anatomical similarities in medical imaging data. We apply our approach to the task of semantic segmentation in a semi-supervised setting with limited amounts of annotated volumes. Trained alongside a segmentation loss in one single training stage, a contrastive loss aids to differentiate between salient anatomical regions that conform to the available annotations. Our approach builds upon the work of Jabri et al. (2020), who proposed cyclical contrastive random walks (CCRW) for self-supervision on palindromes of video frames. We adapt this scheme to operate on entries of paired embedded image slices. Using paths of cyclical random walks bypasses the need for negative samples, as commonly used in contrastive approaches, enabling the algorithm to discriminate among relevant salient (anatomical) regions implicitly. Further, a multi-level supervision strategy is employed, ensuring adequate representations of local and global characteristics of anatomical structures. The effectiveness of reducing the amount of required annotations is shown on three MRI datasets. A median increase of 8.01 and 5.90 pp in the Dice Similarity Coefficient (DSC) compared to our baseline could be achieved across all three datasets in the case of one and two available annotated examples per dataset.",,"This work was supported in part by Germanyâs Excellence Strategy [EXC-Number 2064/1, Project number 390727645].",Computerized Medical Imaging and Graphics,,"Algorithms; Image Processing, Computer-Assisted; Supervised Machine Learning",2023-01-09,2023,2023-01-09,2023-03,104,,102174,Closed,Article,"Fischer, Marc; Hepp, Tobias; Gatidis, Sergios; Yang, Bin","Fischer, Marc (Institute of Signal Processing and System Theory, University of Stuttgart, 70550 Stuttgart, Germany. Electronic address: marc.fischer@iss.uni-stuttgart.de.); Hepp, Tobias (Max Planck Institute for Intelligent Systems, 72076 TÃ¼bingen, Germany.); Gatidis, Sergios (Max Planck Institute for Intelligent Systems, 72076 TÃ¼bingen, Germany.); Yang, Bin (Institute of Signal Processing and System Theory, University of Stuttgart, 70550 Stuttgart, Germany.)","Fischer, Marc (University of Stuttgart)","Fischer, Marc (University of Stuttgart); Hepp, Tobias (Max Planck Institute for Intelligent Systems); Gatidis, Sergios (Max Planck Institute for Intelligent Systems); Yang, Bin (University of Stuttgart)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154340585,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
2735,pub.1152168304,10.1109/jbhi.2022.3217174,36282819,,CCS-Net: Cascade Detection Network With the Convolution Kernel Switch Block and Statistics Optimal Anchors Block in Hypopharyngeal Cancer MRI,"Magnetic resonance imaging (MRI) is a common diagnostic method for hypopharyngeal cancer (HPC). It is a challenge to automatically detect HPC tumors and swollen lymph nodes (HPC risk areas) from MRI slices because of the small size and irregular shape of HPC risk areas. Herein, we propose a cascade detection network with Convolution Kernel Switch (CKS) Block and Statistics Optimal Anchors (SOA) Block in HPC MRI (CCS-Net). CKS Block can adaptively switch standard convolution to deformable convolution in some appropriate layers to detect irregular objects more efficiently without taking up too much computing resources. SOA Block can automatically generate the optimal anchors based on the size distribution of objects. Compared with other methods, our method achieves splendid detection performance and outperforms other methods on the HPC dataset (more than 1800 T2 MRI slices), achieving the highest AP50 of 78.90%. Experiments show that the proposed network can be the basis of a computer aided diagnosis utility that helps achieve faster and more accurate diagnostic decisions for HPC. The code is available at: https://github.com/zhongqiu1245/CCS-Net.",,"This work was supported in part by the National Natural Science Foundation of China under Grants 51975011 and U1501253, in part by the Research Funds for Leading Talents Program under Grant 048000514120530, and in part by the Non-Profit Central Research Institute Fund of the Chinese Academy of Medical Science under Grant 2019-RC-HL-004.",IEEE Journal of Biomedical and Health Informatics,,,2022-10-25,2022,2022-10-25,2022-10-25,27,1,433-444,Closed,Article,"Zhang, Shuo; Miao, Yang; Chen, Jun; Zhang, Xiwei; Han, Lei; Huang, Zehao; Pei, Ning; Liu, Haibin; An, Changming","Zhang, Shuo (Beijing University of Technology, Beijing, 100124, China); Miao, Yang (Beijing University of Technology, Beijing, 100124, China; Beijing Key Laboratory of Advanced Manufacturing Technology, Beijing University of Technology, Beijing, 100124, China); Chen, Jun (Beijing Engineering Research Center of Pediatric Surgery, Engineering and Transformation Center, Beijing Children's Hospital, National Center for Children's Health, Capital Medical University, Beijing, 100045, China); Zhang, Xiwei (Department of Head and Neck Surgery, National Cancer Center, National Clinical Research Center for Cancer, Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 100021, China); Han, Lei (Beijing University of Posts and Telecommunications, Beijing, 100191, China); Huang, Zehao (Department of Head and Neck Surgery, National Cancer Center, National Clinical Research Center for Cancer, Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 100021, China); Pei, Ning (School of Mechanical Engineering, Beijing Institute of Technology, Beijing, 100081, China); Liu, Haibin (Beijing University of Technology, Beijing, 100124, China); An, Changming (Department of Head and Neck Surgery, National Cancer Center, National Clinical Research Center for Cancer, Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 100021, China)","Han, Lei (Beijing University of Posts and Telecommunications)","Zhang, Shuo (Beijing University of Technology); Miao, Yang (Beijing University of Technology; Beijing University of Technology); Chen, Jun (Capital Medical University; Beijing Childrenâs Hospital); Zhang, Xiwei (Chinese Academy of Medical Sciences & Peking Union Medical College); Han, Lei (Beijing University of Posts and Telecommunications); Huang, Zehao (Chinese Academy of Medical Sciences & Peking Union Medical College); Pei, Ning (Beijing Institute of Technology); Liu, Haibin (Beijing University of Technology); An, Changming (Chinese Academy of Medical Sciences & Peking Union Medical College)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1152168304,46 Information and Computing Sciences, 4601 Applied Computing,,,,,,,,,,,
2735,pub.1132063197,10.1016/j.media.2020.101884,33246228,,A deep learning framework for pancreas segmentation with multi-atlas registration and 3D level-set,"In this paper, we propose and validate a deep learning framework that incorporates both multi-atlas registration and level-set for segmenting pancreas from CT volume images. The proposed segmentation pipeline consists of three stages, namely coarse, fine, and refine stages. Firstly, a coarse segmentation is obtained through multi-atlas based 3D diffeomorphic registration and fusion. After that, to learn the connection feature, a 3D patch-based convolutional neural network (CNN) and three 2D slice-based CNNs are jointly used to predict a fine segmentation based on a bounding box determined from the coarse segmentation. Finally, a 3D level-set method is used, with the fine segmentation being one of its constraints, to integrate information of the original image and the CNN-derived probability map to achieve a refine segmentation. In other words, we jointly utilize global 3D location information (registration), contextual information (patch-based 3D CNN), shape information (slice-based 2.5D CNN) and edge information (3D level-set) in the proposed framework. These components form our cascaded coarse-fine-refine segmentation framework. We test the proposed framework on three different datasets with varying intensity ranges obtained from different resources, respectively containing 36, 82 and 281 CT volume images. In each dataset, we achieve an average Dice score over 82%, being superior or comparable to other existing state-of-the-art pancreas segmentation algorithms.","The authors would like to thank Benxiang Jiang and Junyan Lyu from Southern University of Science and Technology as well as Mengye Lyu from the University of Hong Kong for their help on this work. This study was supported by the National Natural Science Foundation of China (62071210), the Shenzhen Basic Research Program (JCYJ20190809120205578), the National Key RD Program of China (2017YFC0112404), the High-level University Fund (G02236002), and the National Natural Science Foundation of China (81501546).",,Medical Image Analysis,,"Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer; Pancreas; Tomography, X-Ray Computed",2020-10-28,2020,2020-10-28,2021-02,68,,101884,Closed,Article,"Zhang, Yue; Wu, Jiong; Liu, Yilong; Chen, Yifan; Chen, Wei; Wu, Ed X; Li, Chunming; Tang, Xiaoying","Zhang, Yue (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, China.); Wu, Jiong (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; School of Computer and Electrical Engineering, Hunan University of Arts and Science, Hunan, China.); Liu, Yilong (Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, China.); Chen, Yifan (School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China.); Chen, Wei (Department of Radiology, Third Military Medical University Southwest Hospital, Chongqing, China.); Wu, Ed X (Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, China.); Li, Chunming (Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China.); Tang, Xiaoying (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China. Electronic address: tangxy@sustech.edu.cn.)","Tang, Xiaoying (Southern University of Science and Technology)","Zhang, Yue (Southern University of Science and Technology; University of Hong Kong); Wu, Jiong (Southern University of Science and Technology; Hunan University of Arts and Science); Liu, Yilong (University of Hong Kong); Chen, Yifan (University of Electronic Science and Technology of China); Chen, Wei (Southwest Hospital); Wu, Ed X (University of Hong Kong); Li, Chunming (University of Electronic Science and Technology of China); Tang, Xiaoying (Southern University of Science and Technology)",33,31,4.51,,,https://app.dimensions.ai/details/publication/pub.1132063197,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,,
2730,pub.1149779127,10.3390/bioengineering9080343,35892756,PMC9394419,A Fusion Biopsy Framework for Prostate Cancer Based on Deformable Superellipses and nnU-Net,"In prostate cancer, fusion biopsy, which couples magnetic resonance imaging (MRI) with transrectal ultrasound (TRUS), poses the basis for targeted biopsy by allowing the comparison of information coming from both imaging modalities at the same time. Compared with the standard clinical procedure, it provides a less invasive option for the patients and increases the likelihood of sampling cancerous tissue regions for the subsequent pathology analyses. As a prerequisite to image fusion, segmentation must be achieved from both MRI and TRUS domains. The automatic contour delineation of the prostate gland from TRUS images is a challenging task due to several factors including unclear boundaries, speckle noise, and the variety of prostate anatomical shapes. Automatic methodologies, such as those based on deep learning, require a huge quantity of training data to achieve satisfactory results. In this paper, the authors propose a novel optimization formulation to find the best superellipse, a deformable model that can accurately represent the prostate shape. The advantage of the proposed approach is that it does not require extensive annotations, and can be used independently of the specific transducer employed during prostate biopsies. Moreover, in order to show the clinical applicability of the method, this study also presents a module for the automatic segmentation of the prostate gland from MRI, exploiting the nnU-Net framework. Lastly, segmented contours from both imaging domains are fused with a customized registration algorithm in order to create a tool that can help the physician to perform a targeted prostate biopsy by interacting with the graphical user interface.",,This research received no external funding.,Bioengineering,,,2022-07-26,2022,2022-07-26,,9,8,343,All OA, Gold,Article,"Altini, Nicola; Brunetti, Antonio; Napoletano, Valeria Pia; Girardi, Francesca; Allegretti, Emanuela; Hussain, Sardar Mehboob; Brunetti, Gioacchino; Triggiani, Vito; Bevilacqua, Vitoantonio; Buongiorno, Domenico","Altini, Nicola (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Brunetti, Antonio (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.); Apulian Bioengineering s.r.l., Via delle Violette n.14, 70026 Modugno, BA, Italy); Napoletano, Valeria Pia (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Girardi, Francesca (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Allegretti, Emanuela (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Hussain, Sardar Mehboob (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Brunetti, Gioacchino (Masmec Biomed SpA, Via delle Violette n.14, 70026 Modugno, BA, Italy;, gioacchino.brunetti@masmecbiomed.com, (G.B.);, vito.triggiani@masmecbiomed.com, (V.T.)); Triggiani, Vito (Masmec Biomed SpA, Via delle Violette n.14, 70026 Modugno, BA, Italy;, gioacchino.brunetti@masmecbiomed.com, (G.B.);, vito.triggiani@masmecbiomed.com, (V.T.)); Bevilacqua, Vitoantonio (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.); Apulian Bioengineering s.r.l., Via delle Violette n.14, 70026 Modugno, BA, Italy); Buongiorno, Domenico (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.); Apulian Bioengineering s.r.l., Via delle Violette n.14, 70026 Modugno, BA, Italy)","Altini, Nicola (Polytechnic University of Bari; )","Altini, Nicola (Polytechnic University of Bari); Brunetti, Antonio (Polytechnic University of Bari); Napoletano, Valeria Pia (Polytechnic University of Bari); Girardi, Francesca (Polytechnic University of Bari); Allegretti, Emanuela (Polytechnic University of Bari); Hussain, Sardar Mehboob (Polytechnic University of Bari); Brunetti, Gioacchino (); Triggiani, Vito (); Bevilacqua, Vitoantonio (Polytechnic University of Bari); Buongiorno, Domenico (Polytechnic University of Bari)",1,1,,,https://www.mdpi.com/2306-5354/9/8/343/pdf?version=1659416933,https://app.dimensions.ai/details/publication/pub.1149779127,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,
2728,pub.1146053440,10.1109/access.2022.3156894,35656515,PMC9159704,Medical Image Segmentation Using Transformer Networks,"Deep learning models represent the state of the art in medical image segmentation. Most of these models are fully-convolutional networks (FCNs), namely each layer processes the output of the preceding layer with convolution operations. The convolution operation enjoys several important properties such as sparse interactions, parameter sharing, and translation equivariance. Because of these properties, FCNs possess a strong and useful inductive bias for image modeling and analysis. However, they also have certain important shortcomings, such as performing a fixed and pre-determined operation on a test image regardless of its content and difficulty in modeling long-range interactions. In this work we show that a different deep neural network architecture, based entirely on self-attention between neighboring image patches and without any convolution operations, can achieve more accurate segmentations than FCNs. Our proposed model is based directly on the transformer network architecture. Given a 3D image block, our network divides it into non-overlapping 3D patches and computes a 1D embedding for each patch. The network predicts the segmentation map for the block based on the self-attention between these patch embeddings. Furthermore, in order to address the common problem of scarcity of labeled medical images, we propose methods for pre-training this model on large corpora of unlabeled images. Our experiments show that the proposed model can achieve segmentation accuracies that are better than several state of the art FCN architectures on two datasets. Our proposed network can be trained using only tens of labeled images. Moreover, with the proposed pre-training strategies, our network outperforms FCNs when labeled training data is small.","This work was supported in part by the National Institute of Biomedical Imaging and Bioengineering and the National Institute of Neurological Disorders and Stroke of the National Institutes of Health (NIH) under Award R01EB031849, Award R01NS106030, and Award R01EB032366; in part by the Office of the Director of the NIH under Award S10OD0250111; and in part by a Technological Innovations in Neuroscience Award from the McKnight Foundation. The work of Haoran Dou was supported by the EPSRC Doctoral Training Partnership (DTP) Studentship. The content of this publication is solely the responsibility of the authors and does not necessarily represent the official views of the NIH or the McKnight Foundation.","This work was supported in part by the National Institute of Biomedical Imaging and Bioengineering and the National Institute of Neurological Disorders and Stroke of the National Institutes of Health (NIH) under Award R01EB031849, Award R01NS106030, and Award R01EB032366; in part by the Office of the Director of the NIH under Award S10OD0250111; and in part by a Technological Innovations in Neuroscience Award from the McKnight Foundation. The work of Haoran Dou was supported by the EPSRC Doctoral Training Partnership (DTP) Studentship. The content of this publication is solely the responsibility of the authors and does not necessarily represent the official views of the NIH or the McKnight Foundation.",IEEE Access,,,2022-03-04,2022,2022-03-04,2022,10,,29322-29332,All OA, Gold,Article,"Karimi, Davood; Dou, Haoran; Gholipour, Ali","Karimi, Davood (Department of Radiology, Boston Childrenâs Hospital, Harvard Medical School, Boston, MA, 02115, USA); Dou, Haoran (Centre for Computational Imaging & Simulation Technologies in Biomedicine (CISTIB), School of Computing, University of Leeds, Leeds, LS2 9JT, U.K.); Gholipour, Ali (Department of Radiology, Boston Childrenâs Hospital, Harvard Medical School, Boston, MA, 02115, USA)","Karimi, Davood (Boston Children's Hospital; Harvard University)","Karimi, Davood (Boston Children's Hospital; Harvard University); Dou, Haoran (University of Leeds); Gholipour, Ali (Boston Children's Hospital; Harvard University)",4,4,,,https://ieeexplore.ieee.org/ielx7/6287639/9668973/09729189.pdf,https://app.dimensions.ai/details/publication/pub.1146053440,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2728,pub.1142340126,10.1007/s11517-021-02464-1,34729681,,Spherical coordinates transformation pre-processing in Deep Convolution Neural Networks for brain tumor segmentation in MRI,"Magnetic Resonance Imaging (MRI) is used in everyday clinical practice to assess brain tumors. Deep Convolutional Neural Networks (DCNN) have recently shown very promising results in brain tumor segmentation tasks; however, DCNN models fail the task when applied to volumes that are different from the training dataset. One of the reasons is due to the lack of data standardization to adjust for different models and MR machines. In this work, a 3D spherical coordinates transform during the pre-processing phase has been hypothesized to improve DCNN modelsâ accuracy and to allow more generalizable results even when the model is trained on small and heterogeneous datasets and translated into different domains. Indeed, the spherical coordinate system avoids several standardization issues since it works independently of resolution and imaging settings. The model trained on spherical transform pre-processed inputs resulted in superior performance over the Cartesian-input trained model on predicting gliomasâ segmentation on Tumor Core and Enhancing Tumor classes, achieving a further improvement in accuracy by merging the two models together. The proposed model is not resolution-dependent, thus improving segmentation accuracy and theoretically solving some transfer learning problems related to the domain shifting, at least in terms of image resolution in the datasets.Graphical abstract",,,Medical & Biological Engineering & Computing,,"Brain Neoplasms; Glioma; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer",2021-11-02,2021,2021-11-02,2022-01,60,1,121-134,All OA, Green,Article,"Russo, Carlo; Liu, Sidong; Di Ieva, Antonio","Russo, Carlo (Computational NeuroSurgery (CNS) Lab, Macquarie Medical School, Faculty of Medicine, Health and Human Science, Macquarie University, 1st floor, 75 Talavera Rd, Macquarie Park, 2109, Sydney, NSW, Australia); Liu, Sidong (Computational NeuroSurgery (CNS) Lab, Macquarie Medical School, Faculty of Medicine, Health and Human Science, Macquarie University, 1st floor, 75 Talavera Rd, Macquarie Park, 2109, Sydney, NSW, Australia; Australian Institute of Health Innovation, Centre for Health Informatics, Macquarie University, Sydney, Australia); Di Ieva, Antonio (Computational NeuroSurgery (CNS) Lab, Macquarie Medical School, Faculty of Medicine, Health and Human Science, Macquarie University, 1st floor, 75 Talavera Rd, Macquarie Park, 2109, Sydney, NSW, Australia)","Russo, Carlo (Macquarie University)","Russo, Carlo (Macquarie University); Liu, Sidong (Macquarie University; Macquarie University); Di Ieva, Antonio (Macquarie University)",7,6,,5.73,http://arxiv.org/pdf/2008.07090,https://app.dimensions.ai/details/publication/pub.1142340126,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2728,pub.1135646427,10.1016/j.cmpb.2021.106004,33662804,PMC7899930,Does non-COVID-19 lung lesion help? investigating transferability in COVID-19 CT image segmentation,"BACKGROUND AND OBJECTIVE: Coronavirus disease 2019 (COVID-19) is a highly contagious virus spreading all around the world. Deep learning has been adopted as an effective technique to aid COVID-19 detection and segmentation from computed tomography (CT) images. The major challenge lies in the inadequate public COVID-19 datasets. Recently, transfer learning has become a widely used technique that leverages the knowledge gained while solving one problem and applying it to a different but related problem. However, it remains unclear whether various non-COVID19 lung lesions could contribute to segmenting COVID-19 infection areas and how to better conduct this transfer procedure. This paper provides a way to understand the transferability of non-COVID19 lung lesions and a better strategy to train a robust deep learning model for COVID-19 infection segmentation.
METHODS: Based on a publicly available COVID-19 CT dataset and three public non-COVID19 datasets, we evaluate four transfer learning methods using 3D U-Net as a standard encoder-decoder method. i) We introduce the multi-task learning method to get a multi-lesion pre-trained model for COVID-19 infection. ii) We propose and compare four transfer learning strategies with various performance gains and training time costs. Our proposed Hybrid-encoder Learning strategy introduces a Dedicated-encoder and an Adapted-encoder to extract COVID-19 infection features and general lung lesion features, respectively. An attention-based Selective Fusion unit is designed for dynamic feature selection and aggregation.
RESULTS: Experiments show that trained with limited data, proposed Hybrid-encoder strategy based on multi-lesion pre-trained model achieves a mean DSC, NSD, Sensitivity, F1-score, Accuracy and MCC of 0.704, 0.735, 0.682, 0.707, 0.994 and 0.716, respectively, with better genetalization and lower over-fitting risks for segmenting COVID-19 infection.
CONCLUSIONS: The results reveal the benefits of transferring knowledge from non-COVID19 lung lesions, and learning from multiple lung lesion datasets can extract more general features, leading to accurate and robust pre-trained models. We further show the capability of the encoder to learn feature representations of lung lesions, which improves segmentation accuracy and facilitates training convergence. In addition, our proposed Hybrid-encoder learning method incorporates transferred lung lesion features from non-COVID19 datasets effectively and achieves significant improvement. These findings promote new insights into transfer learning for COVID-19 CT image segmentation, which can also be further generalized to other medical tasks.",,,Computer Methods and Programs in Biomedicine,,"Algorithms; COVID-19; Databases, Factual; Humans; Image Processing, Computer-Assisted; Lung; SARS-CoV-2; Tomography, X-Ray Computed",2021-02-23,2021,2021-02-23,2021-04,202,,106004,All OA, Bronze,Article,"Wang, Yixin; Zhang, Yao; Liu, Yang; Tian, Jiang; Zhong, Cheng; Shi, Zhongchao; Zhang, Yang; He, Zhiqiang","Wang, Yixin (Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; AI Lab, Lenovo Research, Beijing, China.); Zhang, Yao (Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; AI Lab, Lenovo Research, Beijing, China.); Liu, Yang (Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; AI Lab, Lenovo Research, Beijing, China.); Tian, Jiang (AI Lab, Lenovo Research, Beijing, China.); Zhong, Cheng (AI Lab, Lenovo Research, Beijing, China.); Shi, Zhongchao (AI Lab, Lenovo Research, Beijing, China.); Zhang, Yang (Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; Lenovo Corporate Research & Development, Lenovo Ltd., Beijing, China. Electronic address: zhangyang20@lenovo.com.); He, Zhiqiang (Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; Lenovo Corporate Research & Development, Lenovo Ltd., Beijing, China. Electronic address: hezq@lenovo.com.)","Zhang, Yang (University of Chinese Academy of Sciences; Lenovo (China)); He, Zhiqiang (University of Chinese Academy of Sciences; Lenovo (China))","Wang, Yixin (University of Chinese Academy of Sciences); Zhang, Yao (University of Chinese Academy of Sciences); Liu, Yang (University of Chinese Academy of Sciences); Tian, Jiang (); Zhong, Cheng (); Shi, Zhongchao (); Zhang, Yang (University of Chinese Academy of Sciences; Lenovo (China)); He, Zhiqiang (University of Chinese Academy of Sciences; Lenovo (China))",30,30,4.5,24.55,https://doi.org/10.1016/j.cmpb.2021.106004,https://app.dimensions.ai/details/publication/pub.1135646427,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2725,pub.1148924271,10.3390/diagnostics12071527,35885433,PMC9319098,A Light Deep Learning Algorithm for CT Diagnosis of COVID-19 Pneumonia,"A large number of reports present artificial intelligence (AI) algorithms, which support pneumonia detection caused by COVID-19 from chest CT (computed tomography) scans. Only a few studies provided access to the source code, which limits the analysis of the out-of-distribution generalization ability. This study presents Cimatec-CovNet-19, a new light 3D convolutional neural network inspired by the VGG16 architecture that supports COVID-19 identification from chest CT scans. We trained the algorithm with a dataset of 3000 CT Scans (1500 COVID-19-positive) with images from different parts of the world, enhanced with 3000 images obtained with data augmentation techniques. We introduced a novel pre-processing approach to perform a slice-wise selection based solely on the lung CT masks and an empirically chosen threshold for the very first slice. It required only 16 slices from a CT examination to identify COVID-19. The model achieved a recall of 0.88, specificity of 0.88, ROC-AUC of 0.95, PR-AUC of 0.95, and F1-score of 0.88 on a test set with 414 samples (207 COVID-19). These results support Cimatec-CovNet-19 as a good and light screening tool for COVID-19 patients. The whole code is freely available for the scientific community.","We gratefully acknowledge the support of SENAI CIMATEC AI Reference Center and the SENAI CIMATEC/NVDIA AI Joint Center for scientific and technical support; the SENAI CIMATEC Supercomputing Center for Industry Innovation for granting access to the necessary hardware and technical support; Repsol Sinopec Brazil, ABDI, SENAI, and EMBRAPII for providing the funding for this research; HP Brazil for providing support; and Hospital Santa Izabel, MedSenior and HM Hospitales for providing data for this research.","This research was funded by ABDI, SENAI, EMBRAPII, REPSOL SINOPEC BRASIL grant âMissÃ£o contra a COVID-19 do Edital de InovaÃ§Ã£o para a IndÃºstriaâ.",Diagnostics,,,2022-06-23,2022,2022-06-23,,12,7,1527,All OA, Gold,Article,"Furtado, Adhvan; da PurificaÃ§Ã£o, Carlos Alberto Campos; BadarÃ³, Roberto; Nascimento, Erick Giovani Sperandio","Furtado, Adhvan (Supercomputing Center SENAI CIMATEC, Av. Orlando Gomes, 1845, PiatÃ£, Salvador 41560-010, Brazil;, adhvan@fieb.org.br, (A.F.);, carlos.purificacao@fieb.org.br, (C.A.C.d.P.)); da PurificaÃ§Ã£o, Carlos Alberto Campos (Supercomputing Center SENAI CIMATEC, Av. Orlando Gomes, 1845, PiatÃ£, Salvador 41560-010, Brazil;, adhvan@fieb.org.br, (A.F.);, carlos.purificacao@fieb.org.br, (C.A.C.d.P.)); BadarÃ³, Roberto (Instituto SENAI de InovaÃ§Ã£o em SaÃºde, Av. Orlando Gomes, 1845, PiatÃ£, Salvador 41560-010, Brazil;, badaro@fieb.org.br); Nascimento, Erick Giovani Sperandio (Supercomputing Center SENAI CIMATEC, Av. Orlando Gomes, 1845, PiatÃ£, Salvador 41560-010, Brazil;, adhvan@fieb.org.br, (A.F.);, carlos.purificacao@fieb.org.br, (C.A.C.d.P.))","Nascimento, Erick Giovani Sperandio ","Furtado, Adhvan (); da PurificaÃ§Ã£o, Carlos Alberto Campos (); BadarÃ³, Roberto (); Nascimento, Erick Giovani Sperandio ()",1,1,,,https://www.mdpi.com/2075-4418/12/7/1527/pdf?version=1655965973,https://app.dimensions.ai/details/publication/pub.1148924271,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,3 Good Health and Well Being,,,,,,,,,
2723,pub.1152410243,10.1038/s41467-022-34257-x,36323677,PMC9630370,Deep learning empowered volume delineation of whole-body organs-at-risk for accelerated radiotherapy,"In radiotherapy for cancer patients, an indispensable process is to delineate organs-at-risk (OARs) and tumors. However, it is the most time-consuming step as manual delineation is always required from radiation oncologists. Herein, we propose a lightweight deep learning framework for radiotherapy treatment planning (RTP), named RTP-Net, to promote an automatic, rapid, and precise initialization of whole-body OARs and tumors. Briefly, the framework implements a cascade coarse-to-fine segmentation, with adaptive module for both small and large organs, and attention mechanisms for organs and boundaries. Our experiments show three merits: 1) Extensively evaluates on 67 delineation tasks on a large-scale dataset of 28,581 cases; 2) Demonstrates comparable or superior accuracy with an average Dice of 0.95; 3) Achieves near real-time delineation in most tasks with <2âs. This framework could be utilized to accelerate the contouring process in the All-in-One radiotherapy scheme, and thus greatly shorten the turnaround time of patients.","The study is supported by the following funding: National Natural Science Foundation of China 62131015 (to Dinggang Shen) and 81830056 (to Feng Shi); Key R&amp;D Program of Guangdong Province, China 2021B0101420006 (to Xiaohuan Cao, Dinggang Shen); Science and Technology Commission of Shanghai Municipality (STCSM) 21010502600 (to Dinggang Shen).",,Nature Communications,,"Humans; Deep Learning; Tomography, X-Ray Computed; Organs at Risk; Neoplasms; Image Processing, Computer-Assisted",2022-11-02,2022,2022-11-02,,13,1,6566,All OA, Gold,Article,"Shi, Feng; Hu, Weigang; Wu, Jiaojiao; Han, Miaofei; Wang, Jiazhou; Zhang, Wei; Zhou, Qing; Zhou, Jingjie; Wei, Ying; Shao, Ying; Chen, Yanbo; Yu, Yue; Cao, Xiaohuan; Zhan, Yiqiang; Zhou, Xiang Sean; Gao, Yaozong; Shen, Dinggang","Shi, Feng (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Hu, Weigang (Department of Radiation Oncology, Fudan University Shanghai Cancer Center, Shanghai, China; Department of Oncology, Shanghai Medical College, Fudan University, Shanghai, China); Wu, Jiaojiao (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Han, Miaofei (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Wang, Jiazhou (Department of Radiation Oncology, Fudan University Shanghai Cancer Center, Shanghai, China; Department of Oncology, Shanghai Medical College, Fudan University, Shanghai, China); Zhang, Wei (Radiotherapy Business Unit, Shanghai United Imaging Healthcare Co., Ltd., Shanghai, China); Zhou, Qing (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Zhou, Jingjie (Radiotherapy Business Unit, Shanghai United Imaging Healthcare Co., Ltd., Shanghai, China); Wei, Ying (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Shao, Ying (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Chen, Yanbo (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Yu, Yue (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Cao, Xiaohuan (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Zhan, Yiqiang (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Zhou, Xiang Sean (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Gao, Yaozong (Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Shen, Dinggang (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China; Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China; Shanghai Clinical Research and Trial Center, Shanghai, China)","Gao, Yaozong ; Shen, Dinggang (ShanghaiTech University; ; Shanghai Clinical Research Center)","Shi, Feng (); Hu, Weigang (Fudan University Shanghai Cancer Center; Shanghai Medical College of Fudan University); Wu, Jiaojiao (); Han, Miaofei (); Wang, Jiazhou (Fudan University Shanghai Cancer Center; Shanghai Medical College of Fudan University); Zhang, Wei (United Imaging Healthcare (China)); Zhou, Qing (); Zhou, Jingjie (United Imaging Healthcare (China)); Wei, Ying (); Shao, Ying (); Chen, Yanbo (); Yu, Yue (); Cao, Xiaohuan (); Zhan, Yiqiang (); Zhou, Xiang Sean (); Gao, Yaozong (); Shen, Dinggang (ShanghaiTech University; Shanghai Clinical Research Center)",0,0,,,https://www.nature.com/articles/s41467-022-34257-x.pdf,https://app.dimensions.ai/details/publication/pub.1152410243,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
2715,pub.1137363216,10.3390/jimaging7040074,34460524,PMC8321330,Deep Learning in Medical Image Analysis,"Over recent years, deep learning (DL) has established itself as a powerful tool across a broad spectrum of domains in imaging-e [...].",,,Journal of Imaging,,,2021-04-20,2021,2021-04-20,,7,4,74,All OA, Gold,Article,"Zhang, Yudong; Gorriz, Juan Manuel; Dong, Zhengchao","Zhang, Yudong (School of Informatics, University of Leicester, Leicester LE1 7RH, UK); Gorriz, Juan Manuel (Department of Signal Theory, Telematics and Communications, University of Granada, 18071 Granada, Spain;, gorriz@ugr.es); Dong, Zhengchao (Molecular Imaging and Neuropathology Division, Columbia University and New York State Psychiatric Institute, New York, NY 10032, USA;, zhengchao.dong@nyspi.columbia.edu)","Zhang, Yudong (University of Leicester)","Zhang, Yudong (University of Leicester); Gorriz, Juan Manuel (University of Granada); Dong, Zhengchao (NewYorkâPresbyterian Hospital)",11,11,1.27,9.0,https://www.mdpi.com/2313-433X/7/4/74/pdf?version=1618909549,https://app.dimensions.ai/details/publication/pub.1137363216,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2714,pub.1153171629,10.1109/jbhi.2022.3225205,36441878,,FCSN: Global Context Aware Segmentation by Learning the Fourier Coefficients of Objects in Medical Images,"The encoder-decoder model is a commonly used Deep Neural Network (DNN) model for medical image segmentation. Conventional encoder-decoder models make pixel-wise predictions focusing heavily on local patterns around the pixel. This makes it challenging to give segmentation that preserves the object's shape and topology, which often requires an understanding of the global context. In this work, we propose a Fourier Coefficient Segmentation NetworkÂ (FCSN)-a novel global context-aware DNN model that segments an object by learning the complex Fourier coefficients of the object's masks. The Fourier coefficients are calculated by integrating over the whole contour. Therefore, for our model to make a precise estimation of the coefficients, the model is motivated to incorporate the global context of the object, leading to a more accurate segmentation of the object's shape. This global context awareness also makes our model robust to unseen local perturbations during inference, such as additive noise or motion blur that are prevalent in medical images. We compare FCSN with other state-of-the-art global context-aware models (UNet++, DeepLabV3+, UNETR) on 5 medical image segmentation tasks, of which 3 are camera imaging datasets (ISIC_2018, RIM_CUP, RIM_DISC) and 2 are medical imaging datasets (PROSTATE, FETAL). When FCSN is compared with UNETR, FCSN attains significantly lower Hausdorff scores with 19.14 (6%), 17.42 (6%), 9.16 (14%), 11.18 (22%), and 5.98 (6%) for ISIC_2018, RIM_CUP, RIM_DISC, PROSTATE, and FETAL tasks respectively. Moreover, FCSN is lightweight by discarding the decoder module, which incurs significant computational overhead. FCSN only requires 29.7Â M parameters which are 75.6Â M and 9.9Â M fewer parameters than UNETR and DeepLabV3+, respectively. FCSN attains inference and training speeds of 1.6Â ms/img and 6.3Â ms/img, which is 8Ã and 3Ã faster than UNet and UNETR. The code for FCSN is made publicly available at https://github.com/nus-mornin-lab/FCSN.",,,IEEE Journal of Biomedical and Health Informatics,,,2022-11-28,2022,2022-11-28,2022-11-28,PP,99,1-11,All OA, Hybrid,Article,"Jeon, Young Seok; Yang, Hongfei; Feng, Mengling","Jeon, Young Seok (Saw Swee Hock School of Public Health and Institute of Data Science, National University of Singapore, Singapore); Yang, Hongfei (Saw Swee Hock School of Public Health and Institute of Data Science, National University of Singapore, Singapore); Feng, Mengling (Saw Swee Hock School of Public Health and Institute of Data Science, National University of Singapore, Singapore)",,"Jeon, Young Seok (National University of Singapore); Yang, Hongfei (National University of Singapore); Feng, Mengling (National University of Singapore)",0,0,,,https://ieeexplore.ieee.org/ielx7/6221020/6363502/09964417.pdf,https://app.dimensions.ai/details/publication/pub.1153171629,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
2714,pub.1146412527,10.1016/j.media.2021.102342,35354108,,A dual meta-learning framework based on idle data for enhancing segmentation of pancreatic cancer,"Automated segmentation of pancreatic cancer is vital for clinical diagnosis and treatment. However, the small size and inconspicuous boundaries limit the segmentation performance, which is further exacerbated for deep learning techniques with the few training samples due to the high threshold of image acquisition and annotation. To alleviate this issue caused by the small-scale dataset, we collect idle multi-parametric MRIs of pancreatic cancer from different studies to construct a relatively large dataset for enhancing the CT pancreatic cancer segmentation. Therefore, we propose a deep learning segmentation model with the dual meta-learning framework for pancreatic cancer. It can integrate the common knowledge of tumors obtained from idle MRIs and salient knowledge from CT images, making high-level features more discriminative. Specifically, the random intermediate modalities between MRIs and CT are first generated to smoothly fill in the gaps in visual appearance and provide rich intermediate representations for ensuing meta-learning scheme. Subsequently, we employ intermediate modalities-based model-agnostic meta-learning to capture and transfer commonalities. At last, a meta-optimizer is utilized to adaptively learn the salient features within CT data, thus alleviating the interference due to internal differences. Comprehensive experimental results demonstrated our method achieved the promising segmentation performance, with a max Dice score of 64.94% on our private dataset, and outperformed state-of-the-art methods on a public pancreatic cancer CT dataset. The proposed method is an effective pancreatic cancer segmentation framework, which can be easily integrated into other segmentation networks and thus promises to be a potential paradigm for alleviating data scarcity challenges using idle data.","This work was supported by the National Natural Science Foundation of China (No. 62171273), Natural Science Foundation of Shanghai (No. 22ZR1432100), Innovation Research Plan from the Shanghai Municipal Education Commission (No. WF220408215, ZXWF082101/072), Med-Engineering Crossing Foundation from Shanghai Jiao Tong University (No. AH0820009).",,Medical Image Analysis,,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Pancreatic Neoplasms",2022-03-19,2022,2022-03-19,2022-05,78,,102342,Closed,Article,"Li, Jun; Qi, Liang; Chen, Qingzhong; Zhang, Yu-Dong; Qian, Xiaohua","Li, Jun (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200030, China.); Qi, Liang (Department of Radiology, the First Affiliated Hospital with Nanjing Medical University, Nanjing, 210009, China.); Chen, Qingzhong (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200030, China.); Zhang, Yu-Dong (Department of Radiology, the First Affiliated Hospital with Nanjing Medical University, Nanjing, 210009, China.); Qian, Xiaohua (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200030, China. Electronic address: xiaohua.qian@sjtu.edu.cn.)","Qian, Xiaohua (Shanghai Jiao Tong University)","Li, Jun (Shanghai Jiao Tong University); Qi, Liang (Jiangsu Province Hospital); Chen, Qingzhong (Shanghai Jiao Tong University); Zhang, Yu-Dong (Jiangsu Province Hospital); Qian, Xiaohua (Shanghai Jiao Tong University)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1146412527,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,,
2578,pub.1144294635,10.1109/tmi.2021.3139637,34968179,PMC9167782,External Attention Assisted Multi-Phase Splenic Vascular Injury Segmentation With Limited Data,"The spleen is one of the most commonly injured solid organs in blunt abdominal trauma. The development of automatic segmentation systems from multi-phase CT for splenic vascular injury can augment severity grading for improving clinical decision support and outcome prediction. However, accurate segmentation of splenic vascular injury is challenging for the following reasons: 1) Splenic vascular injury can be highly variant in shape, texture, size, and overall appearance; and 2) Data acquisition is a complex and expensive procedure that requires intensive efforts from both data scientists and radiologists, which makes large-scale well-annotated datasets hard to acquire in general. In light of these challenges, we hereby design a novel framework for multi-phase splenic vascular injury segmentation, especially with limited data. On the one hand, we propose to leverage external data to mine pseudo splenic masks as the spatial attention, dubbed external attention, for guiding the segmentation of splenic vascular injury. On the other hand, we develop a synthetic phase augmentation module, which builds upon generative adversarial networks, for populating the internal data by fully leveraging the relation between different phases. By jointly enforcing external attention and populating internal data representation during training, our proposed method outperforms other competing methods and substantially improves the popular DeepLab-v3+ baseline by more than 7% in terms of average DSC, which confirms its effectiveness.",,This work was supported by the University of Maryland Accelerated Translational Incubator Pilot (ATIP) Program under Grant NIH K08 EB027141-01A1.,IEEE Transactions on Medical Imaging,,"Abdomen; Attention; Humans; Image Processing, Computer-Assisted; Spleen; Tomography, X-Ray Computed; Vascular System Injuries",2022-06-01,2022,2022-06-01,2022-06,41,6,1346-1357,All OA, Green,Article,"Zhou, Yuyin; Dreizin, David; Wang, Yan; Liu, Fengze; Shen, Wei; Yuille, Alan L.","Zhou, Yuyin (Department of Computer Science and Engineering, University of California at Santa Cruz, Santa Cruz, CA, 95064, USA); Dreizin, David (R Adams Cowley Shock Trauma Center, Department of Radiology, University of Maryland at Baltimore, Baltimore, MD, 21201, USA); Wang, Yan (Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, 200241, China); Liu, Fengze (Department of Computer Science, Johns Hopkins University, Baltimore, MD, 21218, USA); Shen, Wei (MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, 200240, China); Yuille, Alan L. (Department of Computer Science, Johns Hopkins University, Baltimore, MD, 21218, USA)","Zhou, Yuyin (University of California, Santa Cruz)","Zhou, Yuyin (University of California, Santa Cruz); Dreizin, David (University of Maryland, Baltimore; University of Maryland Medical Center); Wang, Yan (East China Normal University); Liu, Fengze (Johns Hopkins University); Shen, Wei (Shanghai Jiao Tong University); Yuille, Alan L. (Johns Hopkins University)",5,5,,,http://arxiv.org/pdf/2201.00942,https://app.dimensions.ai/details/publication/pub.1144294635,46 Information and Computing Sciences,,,,,,,,,,,
2576,pub.1155321550,10.3390/diagnostics13040651,36832138,PMC9955350,Investigating the Impact of Two Major Programming Environments on the Accuracy of Deep Learning-Based Glioma Detection from MRI Images,"Brain tumors have been the subject of research for many years. Brain tumors are typically classified into two main groups: benign and malignant tumors. The most common tumor type among malignant brain tumors is known as glioma. In the diagnosis of glioma, different imaging technologies could be used. Among these techniques, MRI is the most preferred imaging technology due to its high-resolution image data. However, the detection of gliomas from a huge set of MRI data could be challenging for the practitioners. In order to solve this concern, many Deep Learning (DL) models based on Convolutional Neural Networks (CNNs) have been proposed to be used in detecting glioma. However, understanding which CNN architecture would work efficiently under various conditions including development environment or programming aspects as well as performance analysis has not been studied so far. In this research work, therefore, the purpose is to investigate the impact of two major programming environments (namely, MATLAB and Python) on the accuracy of CNN-based glioma detection from Magnetic Resonance Imaging (MRI) images. To this end, experiments on the Brain Tumor Segmentation (BraTS) dataset (2016 and 2017) consisting of multiparametric magnetic MRI images are performed by implementing two popular CNN architectures, the three-dimensional (3D) U-Net and the V-Net in the programming environments. From the results, it is concluded that the use of Python with Google Colaboratory (Colab) might be highly useful in the implementation of CNN-based models for glioma detection. Moreover, the 3D U-Net model is found to perform better, attaining a high accuracy on the dataset. The authors believe that the results achieved from this study would provide useful information to the research community in their appropriate implementation of DL approaches for brain tumor detection.",Authors would like to thank students of Atilim University for their voluntary contribution to this study.,This work was supported by Atilim University Undergraduate Research Projects: [Grant Number ATU-LAP-2021-05].,Diagnostics,,,2023-02-09,2023,2023-02-09,,13,4,651,All OA, Gold,Article,"Yilmaz, Vadi Su; Akdag, Metehan; Dalveren, Yaser; Doruk, Resat Ozgur; Kara, Ali; Soylu, Ahmet","Yilmaz, Vadi Su (Department of Electrical and Electronics Engineering, Atilim University, Kizilcasar Mahallesi, Incek Golbasi, Ankara 06830, Turkey); Akdag, Metehan (Fonet Information Technologies, Kizilirmak Mahallesi, Cukurambar Cankaya, Ankara 06520, Turkey); Dalveren, Yaser (Department of Electrical and Electronics Engineering, Atilim University, Kizilcasar Mahallesi, Incek Golbasi, Ankara 06830, Turkey); Doruk, Resat Ozgur (Department of Electrical and Electronics Engineering, Atilim University, Kizilcasar Mahallesi, Incek Golbasi, Ankara 06830, Turkey); Kara, Ali (Department of Electrical and Electronics Engineering, Gazi University, Eti Mahallesi, Yukselis Sokak, Maltepe, Ankara 06570, Turkey); Soylu, Ahmet (Department of Computer Science, OsloMetâOslo Metropolitan University, Pilestredet 35, Oslo 0167, Norway)","Kara, Ali (Gazi University)","Yilmaz, Vadi Su (Atilim University); Akdag, Metehan (); Dalveren, Yaser (Atilim University); Doruk, Resat Ozgur (Atilim University); Kara, Ali (Gazi University); Soylu, Ahmet (OsloMet â Oslo Metropolitan University)",0,0,,,https://www.mdpi.com/2075-4418/13/4/651/pdf?version=1675939489,https://app.dimensions.ai/details/publication/pub.1155321550,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
2569,pub.1145925497,10.1007/s10462-022-10152-1,35250146,PMC8886195,Modality specific U-Net variants for biomedical image segmentation: a survey,"With the advent of advancements in deep learning approaches, such as deep convolution neural network, residual neural network, adversarial network; U-Net architectures are most widely utilized in biomedical image segmentation to address the automation in identification and detection of the target regions or sub-regions. In recent studies, U-Net based approaches have illustrated state-of-the-art performance in different applications for the development of computer-aided diagnosis systems for early diagnosis and treatment of diseases such as brain tumor, lung cancer, alzheimer, breast cancer, etc., using various modalities. This article contributes in presenting the success of these approaches by describing the U-Net framework, followed by the comprehensive analysis of the U-Net variants by performing (1) inter-modality, and (2) intra-modality categorization to establish better insights into the associated challenges and solutions. Besides, this article also highlights the contribution of U-Net based frameworks in the ongoing pandemic, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19. Finally, the strengths and similarities of these U-Net variants are analysed along with the challenges involved in biomedical image segmentation to uncover promising future research directions in this area.","We thank our institute, Indian Institute of Information Technology Allahabad (IIITA), India and Big Data Analytics (BDA) lab for allocating the necessary resources to perform this research. We extend our thanks to our colleagues for their valuable guidance and suggestions.",,Artificial Intelligence Review,,,2022-03-01,2022,2022-03-01,2022-10,55,7,5845-5889,All OA, Bronze,Article,"Punn, Narinder Singh; Agarwal, Sonali","Punn, Narinder Singh (IIIT Allahabad, 211015, Prayagraj, India); Agarwal, Sonali (IIIT Allahabad, 211015, Prayagraj, India)","Punn, Narinder Singh ","Punn, Narinder Singh (); Agarwal, Sonali ()",25,25,,,https://link.springer.com/content/pdf/10.1007/s10462-022-10152-1.pdf,https://app.dimensions.ai/details/publication/pub.1145925497,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2564,pub.1143965985,10.1002/jmri.27978,34918423,PMC9106804,CrossâCohort Automatic Knee MRI Segmentation With MultiâPlanar UâNets,"BACKGROUND: Segmentation of medical image volumes is a time-consuming manual task. Automatic tools are often tailored toward specific patient cohorts, and it is unclear how they behave in other clinical settings.
PURPOSE: To evaluate the performance of the open-source Multi-Planar U-Net (MPUnet), the validated Knee Imaging Quantification (KIQ) framework, and a state-of-the-art two-dimensional (2D) U-Net architecture on three clinical cohorts without extensive adaptation of the algorithms.
STUDY TYPE: Retrospective cohort study.
SUBJECTS: A total of 253 subjects (146 females, 107 males, ages 57âÂ±â12âyears) from three knee osteoarthritis (OA) studies (Center for Clinical and Basic Research [CCBR], Osteoarthritis Initiative [OAI], and Prevention of OA in Overweight Females [PROOF]) with varying demographics and OA severity (64/37/24/53/2 scans of Kellgren and Lawrence [KL] grades 0-4).
FIELD STRENGTH/SEQUENCE: 0.18âT, 1.0âT/1.5âT, and 3âT sagittal three-dimensional fast-spin echo T1w and dual-echo steady-state sequences.
ASSESSMENT: All models were fit without tuning to knee magnetic resonance imaging (MRI) scans with manual segmentations from three clinical cohorts. All models were evaluated across KL grades.
STATISTICAL TESTS: Segmentation performance differences as measured by Dice coefficients were tested with paired, two-sided Wilcoxon signed-rank statistics with significance threshold Î±Â =Â 0.05.
RESULTS: The MPUnet performed superior or equal to KIQ and 2D U-Net on all compartments across three cohorts. Mean Dice overlap was significantly higher for MPUnet compared to KIQ and U-Net on CCBR ( 0.83Â±0.04 vs. 0.81Â±0.06 and 0.82Â±0.05 ), significantly higher than KIQ and U-Net OAI ( 0.86Â±0.03 vs. 0.84Â±0.04 and 0.85Â±0.03) , and not significantly different from KIQ while significantly higher than 2D U-Net on PROOF ( 0.78Â±0.07 vs. 0.77Â±0.07 , P=0.10 , and 0.73Â±0.07) . The MPUnet performed significantly better on N=22 KL grade 3 CCBR scans with 0.78Â±0.06 vs. 0.75Â±0.08 for KIQ and 0.76Â±0.06 for 2D U-Net.
DATA CONCLUSION: The MPUnet matched or exceeded the performance of state-of-the-art knee MRI segmentation models across cohorts of variable sequences and patient demographics. The MPUnet required no manual tuning making it both accurate and easy-to-use.
LEVEL OF EVIDENCE: 3 TECHNICAL EFFICACY: Stage 2.","Mathias Perslev and Christian Igel gratefully acknowledge support from the Independent Research Fund Denmark through the project âUâSleepâ (project number 9131â00099B). Akshay Pai and Christian Igel gratefully acknowledge support from The Danish Industry Foundation as part of the initiative AI Denmark. The OAI collection was provided by the OAI. The OAI is a publicâprivate partnership comprised of five contracts (N01âARâ2â2258, N01âARâ2â2259, N01âARâ2â2260, N01âARâ2â2261, and N01â ARâ2â2262) funded by the National Institutes of Health, a branch of the Department of Health and Human Services, and conducted by the OAI Study Investigators. Private funding partners include Merck Research Laboratories; Novartis Pharmaceuticals Corporation, GlaxoSmithKline; and Pfizer, Inc. Private sector funding for the OAI is managed by the Foundation for the National Institutes of Health. This manuscript was prepared using an OAI public use dataset and does not necessarily reflect the opinions or views of the OAI investigators, the NIH, or the private funding partners.",,Journal of Magnetic Resonance Imaging,,"Aged; Cohort Studies; Female; Humans; Knee; Knee Joint; Magnetic Resonance Imaging; Male; Middle Aged; Osteoarthritis, Knee; Retrospective Studies",2021-12-17,2021,2021-12-17,2022-06,55,6,1650-1663,All OA, Green,Article,"Perslev, Mathias; Pai, Akshay; Runhaar, Jos; Igel, Christian; Dam, Erik B.","Perslev, Mathias (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark); Pai, Akshay (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Cerebriu A/S, Copenhagen, Denmark); Runhaar, Jos (Erasmus MC, Rotterdam University); Igel, Christian (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark); Dam, Erik B. (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Cerebriu A/S, Copenhagen, Denmark)","Perslev, Mathias (University of Copenhagen)","Perslev, Mathias (University of Copenhagen); Pai, Akshay (University of Copenhagen); Runhaar, Jos (); Igel, Christian (University of Copenhagen); Dam, Erik B. (University of Copenhagen)",5,5,,4.38,https://pure.eur.nl/ws/files/50172878/Cross_Cohort_Automatic_Knee_MRI_Segmentation_With_Multi_Planar_U_Nets.pdf,https://app.dimensions.ai/details/publication/pub.1143965985,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
2564,pub.1134740668,10.1109/tmi.2021.3053008,33471751,,Few-Shot Learning by a Cascaded Framework With Shape-Constrained Pseudo Label Assessment for Whole Heart Segmentation,"Automatic and accurate 3D cardiac image segmentation plays a crucial role in cardiac disease diagnosis and treatment. Even though CNN based techniques have achieved great success in medical image segmentation, the expensive annotation, large memory consumption, and insufficient generalization ability still pose challenges to their application in clinical practice, especially in the case of 3D segmentation from high-resolution and large-dimension volumetric imaging. In this paper, we propose a few-shot learning framework by combining ideas of semi-supervised learning and self-training for whole heart segmentation and achieve promising accuracy with a Dice score of 0.890 and a Hausdorff distance of 18.539 mm with only four labeled data for training. When more labeled data provided, the model can generalize better across institutions. The key to success lies in the selection and evolution of high-quality pseudo labels in cascaded learning. A shape-constrained network is built to assess the quality of pseudo labels, and the self-training stages with alternative global-local perspectives are employed to improve the pseudo labels. We evaluate our method on the CTA dataset of the MM-WHS 2017 Challenge and a larger multi-center dataset. In the experiments, our method outperforms the state-of-the-art methods significantly and has great generalization ability on the unseen data. We also demonstrate, by a study of two 4D (3D+T) CTA data, the potential of our method to be applied in clinical practice.","This work was supported in part by the Beijing Nova Program under Grant Z201100006820064, in part by the National Key Research and Development Project of China under Grant 2020YFC2004800, in part by the STCSMunder Grant 19511121400, and in part by the Beijing Postdoctoral Research Foundation.",,IEEE Transactions on Medical Imaging,,"Heart; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Supervised Machine Learning",2021-09-30,2021,2021-09-30,2021-10,40,10,2629-2641,Closed,Article,"Wang, Wenji; Xia, Qing; Hu, Zhiqiang; Yan, Zhennan; Li, Zhuowei; Wu, Yang; Huang, Ning; Gao, Yue; Metaxas, Dimitris; Zhang, Shaoting","Wang, Wenji (SenseTime Research, Beijing, 100080, China); Xia, Qing (SenseTime Research, Beijing, 100080, China; Department of Software, Tsinghua University, Beijing, 100084, China); Hu, Zhiqiang (SenseTime Research, Beijing, 100080, China); Yan, Zhennan (SenseBrain Technology Limited LLC, Princeton, NJ, 08540, USA); Li, Zhuowei (SenseTime Research, Beijing, 100080, China); Wu, Yang (Chinese PLA General Hospital, Beijing, 100853, China); Huang, Ning (SenseTime Research, Beijing, 100080, China); Gao, Yue (Department of Software, Tsinghua University, Beijing, 100084, China); Metaxas, Dimitris (Department of Computer Science, Rutgers University, Piscataway, NJ, 08854, USA); Zhang, Shaoting (SenseTime Research, Shanghai, 200233, China; Qing Yuan Research Institute, Shanghai Jiao Tong University, Shanghai, 200240, China)","Xia, Qing (; Tsinghua University)","Wang, Wenji (); Xia, Qing (Tsinghua University); Hu, Zhiqiang (); Yan, Zhennan (); Li, Zhuowei (); Wu, Yang (Chinese PLA General Hospital); Huang, Ning (); Gao, Yue (Tsinghua University); Metaxas, Dimitris (Rutgers, The State University of New Jersey); Zhang, Shaoting (Shanghai Jiao Tong University)",16,16,1.87,13.1,,https://app.dimensions.ai/details/publication/pub.1134740668,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
2563,pub.1140386802,10.1109/tmi.2021.3104460,34383646,,Model-Driven Deep Learning Method for Pancreatic Cancer Segmentation Based on Spiral-Transformation,"Pancreatic cancer is a lethal malignant tumor with one of the worst prognoses. Accurate segmentation of pancreatic cancer is vital in clinical diagnosis and treatment. Due to the unclear boundary and small size of cancers, it is challenging to both manually annotate and automatically segment cancers. Considering 3D information utilization and small sample sizes, we propose a model-driven deep learning method for pancreatic cancer segmentation based on spiral transformation. Specifically, a spiral-transformation algorithm with uniform sampling was developed to map 3D images onto 2D planes while preserving the spatial relationship between textures, thus addressing the challenge in effectively applying 3D contextual information in a 2D model. This study is the first to introduce spiral transformation in a segmentation task to provide effective data augmentation, alleviating the issue of small sample size. Moreover, a transformation-weight-corrected module was embedded into the deep learning model to unify the entire framework. It can achieve 2D segmentation and corresponding 3D rebuilding constraint to overcome non-unique 3D rebuilding results due to the uniform and dense sampling. A smooth regularization based on rebuilding prior knowledge was also designed to optimize segmentation results. The extensive experiments showed that the proposed method achieved a promising segmentation performance on multi-parametric MRIs, where T2, T1, ADC, DWI images obtained the DSC of 65.6%, 64.0%, 64.5%, 65.3%, respectively. This method can provide a novel paradigm to efficiently apply 3D information and augment sample sizes in the development of artificial intelligence for cancer segmentation. Our source codes will be released at https://github.com/SJTUBME-QianLab/ Spiral-Segmentation.",This work was supported in part by the Innovation Research Plan from the Shanghai Municipal Education Commission under Grant WF220408215 and Grant ZXWF082101/072 and in part by the Med-Engineering Crossing Foundation from Shanghai Jiao Tong University under Grant AH0820009.,,IEEE Transactions on Medical Imaging,,"Algorithms; Artificial Intelligence; Deep Learning; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Pancreatic Neoplasms",2021-12-30,2021,2021-12-30,2022-01,41,1,75-87,Closed,Article,"Chen, Xiahan; Chen, Zihao; Li, Jun; Zhang, Yu-Dong; Lin, Xiaozhu; Qian, Xiaohua","Chen, Xiahan (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China); Chen, Zihao (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China); Li, Jun (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China); Zhang, Yu-Dong (Department of Radiology, The First Affiliated Hospital of Nanjing Medical University, Nanjing, 210029, China); Lin, Xiaozhu (Department of Nuclear Medicine, Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, 200127, China); Qian, Xiaohua (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China)","Qian, Xiaohua (Shanghai Jiao Tong University)","Chen, Xiahan (Shanghai Jiao Tong University); Chen, Zihao (Shanghai Jiao Tong University); Li, Jun (Shanghai Jiao Tong University); Zhang, Yu-Dong (Jiangsu Province Hospital); Lin, Xiaozhu (Shanghai Jiao Tong University; Ruijin Hospital); Qian, Xiaohua (Shanghai Jiao Tong University)",7,7,,,,https://app.dimensions.ai/details/publication/pub.1140386802,46 Information and Computing Sciences,,,,,,,,,,,,
2563,pub.1138505272,10.1038/s41598-021-90294-4,34075061,PMC8169882,Domain adaptation for segmentation of critical structures for prostate cancer therapy,"Preoperative assessment of the proximity of critical structures to the tumors is crucial in avoiding unnecessary damage during prostate cancer treatment. A patient-specific 3D anatomical model of those structures, namely the neurovascular bundles (NVB) and the external urethral sphincters (EUS), can enable physicians to perform such assessments intuitively. As a crucial step to generate
 a patient-specific anatomical model from preoperative MRI in a clinical routine, we propose a multi-class automatic segmentation based on an anisotropic convolutional network. Our specific challenge is to train the network model on a unique source dataset only available at a single clinical site and deploy it to another target site without sharing the original images or labels. As network models trained on data from a single source suffer from quality loss due to the domain shift, we propose a semi-supervised domain adaptation (DA) method to refine the modelâs performance in the target domain. Our DA method combines transfer learning and uncertainty guided self-learning based on deep ensembles. Experiments on the segmentation of the prostate, NVB, and EUS, show significant performance gain with the combination of those techniques compared to pure TL and the combination of TL with simple self-learning (p<0.005\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}<0.005$$\end{document} for all structures using a Wilcoxonâs signed-rank test). Results on a different task and data (Pancreas CT segmentation) demonstrate our methodâs generic application capabilities. Our method has the advantage that it does not require any further data from the source domain, unlike the majority of recent domain adaptation strategies. This makes our method suitable for clinical applications, where the sharing of patient data is restricted.","The study was funded in part by the U.S. National Institutes of Health (R01EB020667, R01CA235134, P41EB015898, P41EB028741), and the EU and the federal state of Saxony-Anhalt, Germany (ZS/2016/08/80388). The content of the material is solely the responsibility of the authors and does not necessarily represent the official views of these agencies. The Titan Xp used for this research was donated by the NVIDIA Corporation.",Open Access funding enabled and organized by Projekt DEAL.,Scientific Reports,,"Humans; Male; Neural Networks, Computer; Prostate; Prostatic Neoplasms; Tomography, X-Ray Computed",2021-06-01,2021,2021-06-01,,11,1,11480,All OA, Gold,Article,"Meyer, Anneke; Mehrtash, Alireza; Rak, Marko; Bashkanov, Oleksii; Langbein, Bjoern; Ziaei, Alireza; Kibel, Adam S.; Tempany, Clare M.; Hansen, Christian; Tokuda, Junichi","Meyer, Anneke (Department of Simulation and Graphics and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany); Mehrtash, Alireza (Department of Radiology, Brigham and Womenâs Hospital, Harvard Medical School, Boston, MA, USA); Rak, Marko (Department of Simulation and Graphics and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany); Bashkanov, Oleksii (Department of Simulation and Graphics and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany); Langbein, Bjoern (Department of Radiology, Brigham and Womenâs Hospital, Harvard Medical School, Boston, MA, USA); Ziaei, Alireza (Department of Radiology, Brigham and Womenâs Hospital, Harvard Medical School, Boston, MA, USA); Kibel, Adam S. (Division of Urology, Department of Surgery, Brigham and Womenâs Hospital, Harvard Medical School, Boston, MA, USA); Tempany, Clare M. (Department of Radiology, Brigham and Womenâs Hospital, Harvard Medical School, Boston, MA, USA); Hansen, Christian (Department of Simulation and Graphics and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany); Tokuda, Junichi (Department of Radiology, Brigham and Womenâs Hospital, Harvard Medical School, Boston, MA, USA)","Meyer, Anneke (Otto-von-Guericke University Magdeburg)","Meyer, Anneke (Otto-von-Guericke University Magdeburg); Mehrtash, Alireza (Brigham and Women's Hospital; Harvard University); Rak, Marko (Otto-von-Guericke University Magdeburg); Bashkanov, Oleksii (Otto-von-Guericke University Magdeburg); Langbein, Bjoern (Brigham and Women's Hospital; Harvard University); Ziaei, Alireza (Brigham and Women's Hospital; Harvard University); Kibel, Adam S. (Brigham and Women's Hospital; Harvard University); Tempany, Clare M. (Brigham and Women's Hospital; Harvard University); Hansen, Christian (Otto-von-Guericke University Magdeburg); Tokuda, Junichi (Brigham and Women's Hospital; Harvard University)",2,2,0.41,1.51,https://www.nature.com/articles/s41598-021-90294-4.pdf,https://app.dimensions.ai/details/publication/pub.1138505272,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,
2441,pub.1144287438,10.1016/j.compbiomed.2021.105161,34999468,,Deep Residual Separable Convolutional Neural Network for lung tumor segmentation,"Lung cancer is one of the deadliest types of cancers. Computed Tomography (CT) is a widely used technique to detect tumors present inside the lungs. Delineation of such tumors is particularly essential for analysis and treatment purposes. With the advancement in hardware technologies, Machine Learning and Deep Learning methods are outperforming the traditional methods in the field of medical imaging. In order to delineate lung cancer tumors, we have proposed a deep learning-based methodology which includes a maximum intensity projection based pre-processing method, two novel deep learning networks and an ensemble strategy. The two proposed networks named Deep Residual Separable Convolutional Neural Network 1 and 2 (DRS-CNN1 and DRS-CNN2) achieved better performance over the state-of-the-art U-net network and other segmentation networks. For fair comparison, we have evaluated the performances of all networks on Medical Segmentation Decathlon (MSD) and StructSeg 2019 datasets. The DRS-CNN2 achieved a mean Dice Similarity Coefficient (DSC) of 0.649, mean 95 Hausdorff Distance (HD95) of 18.26, mean Sensitivity 0.737 and a mean Precision of 0.765 on independent test sets.","This work was supported by the Rajiv Gandhi Science and Technology Commission, Mumbai, Govt. of Maharashtra, India for research, technological development, and demonstration under Grant Agreement No. RGSTC/File-2015/DPP-153/CR-58.",,Computers in Biology and Medicine,,"Humans; Image Processing, Computer-Assisted; Lung; Lung Neoplasms; Neural Networks, Computer; Tomography, X-Ray Computed",2021-12-30,2021,2021-12-30,2022-02,141,,105161,Closed,Article,"Dutande, Prasad; Baid, Ujjwal; Talbar, Sanjay","Dutande, Prasad (Center of Excellence in Signal and Image Processing, SGGS Institute of Engineering and Technology, Nanded, India. Electronic address: dutandeprasad@sggs.ac.in.); Baid, Ujjwal (Center of Excellence in Signal and Image Processing, SGGS Institute of Engineering and Technology, Nanded, India.); Talbar, Sanjay (Center of Excellence in Signal and Image Processing, SGGS Institute of Engineering and Technology, Nanded, India.)","Dutande, Prasad ","Dutande, Prasad (); Baid, Ujjwal (); Talbar, Sanjay ()",9,9,2.5,7.37,,https://app.dimensions.ai/details/publication/pub.1144287438,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
2441,pub.1139173944,10.1016/j.media.2021.102146,34274692,,Self-paced and self-consistent co-training for semi-supervised image segmentation,"Deep co-training has recently been proposed as an effective approach for image segmentation when annotated data is scarce. In this paper, we improve existing approaches for semi-supervised segmentation with a self-paced and self-consistent co-training method. To help distillate information from unlabeled images, we first design a self-paced learning strategy for co-training that lets jointly-trained neural networks focus on easier-to-segment regions first, and then gradually consider harder ones. This is achieved via an end-to-end differentiable loss in the form of a generalized Jensen Shannon Divergence (JSD). Moreover, to encourage predictions from different networks to be both consistent and confident, we enhance this generalized JSD loss with an uncertainty regularizer based on entropy. The robustness of individual models is further improved using a self-ensembling loss that enforces their prediction to be consistent across different training iterations. We demonstrate the potential of our method on three challenging image segmentation problems with different image modalities, using a small fraction of labeled data. Results show clear advantages in terms of performance compared to the standard co-training baselines and recently proposed state-of-the-art approaches for semi-supervised segmentation.","This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grants Program under grant RGPIN-2018-05715;by the NSFC-Zhejiang Joint Fund of the Integration of Informatization and Industrialization under Grant No.U1909210, the National Natural Science Foundation of China under Grant (No.61772312).",,Medical Image Analysis,,"Entropy; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Supervised Machine Learning; Uncertainty",2021-06-26,2021,2021-06-26,2021-10,73,,102146,All OA, Green,Article,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (Department of Software and IT Engineering, Ecole de technologie supÃ©rieure, Montreal, H3C1K3, Canada. Electronic address: ping.wang.1@ens.etsmtl.ca.); Peng, Jizong (Department of Software and IT Engineering, Ecole de technologie supÃ©rieure, Montreal, H3C1K3, Canada. Electronic address: jizong.peng.1@etsmtl.net.); Pedersoli, Marco (Department of Software and IT Engineering, Ecole de technologie supÃ©rieure, Montreal, H3C1K3, Canada. Electronic address: marco.pedersoli@etsmtl.ca.); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, 250101, China. Electronic address: yfzhou@sdu.edu.cn.); Zhang, Caiming (School of Software, Shandong University, Jinan, 250101, China. Electronic address: czhang@sdu.edu.cn.); Desrosiers, Christian (Department of Software and IT Engineering, Ecole de technologie supÃ©rieure, Montreal, H3C1K3, Canada. Electronic address: christian.desrosiers@etsmtl.ca.)","Wang, Ping (Ãcole de Technologie SupÃ©rieure)","Wang, Ping (Ãcole de Technologie SupÃ©rieure); Peng, Jizong (Ãcole de Technologie SupÃ©rieure); Pedersoli, Marco (Ãcole de Technologie SupÃ©rieure); Zhou, Yuanfeng (Shandong University); Zhang, Caiming (Shandong University); Desrosiers, Christian (Ãcole de Technologie SupÃ©rieure)",19,19,1.94,,http://arxiv.org/pdf/2011.00325,https://app.dimensions.ai/details/publication/pub.1139173944,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
2441,pub.1133057937,10.1016/j.compmedimag.2020.101828,33571780,PMC8040671,Analyzing magnetic resonance imaging data from glioma patients using deep learning,"The quantitative analysis of images acquired in the diagnosis and treatment of patients with brain tumors has seen a significant rise in the clinical use of computational tools. The underlying technology to the vast majority of these tools are machine learning methods and, in particular, deep learning algorithms. This review offers clinical background information of key diagnostic biomarkers in the diagnosis of glioma, the most common primary brain tumor. It offers an overview of publicly available resources and datasets for developing new computational tools and image biomarkers, with emphasis on those related to the Multimodal Brain Tumor Segmentation (BraTS) Challenge. We further offer an overview of the state-of-the-art methods in glioma image segmentation, again with an emphasis on publicly available tools and deep learning algorithms that emerged in the context of the BraTS challenge.","This work was partly supported by the National Institutes of Health (NIH) under award numbers NIH/NINDS: R01NS042645, NIH/NCI: U24CA189523, NIH/NCI: U01CA242871. The content of this publication is solely the responsibility of the authors and does not represent the official views of the NIH. We gladly acknowledge the support of the Swiss Cancer League (grant number KFS-3979-08-2016), and of the Swiss Personalized Health Network (SPHN) Imagine project. Conflicts of interest: All authors declare they have no interests conflicting with this manuscript.",,Computerized Medical Imaging and Graphics,,"Algorithms; Brain Neoplasms; Deep Learning; Glioma; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer",2020-12-02,2020,2020-12-02,2021-03,88,,101828,All OA, Green,Article,"Menze, Bjoern; Isensee, Fabian; Wiest, Roland; Wiestler, Bene; Maier-Hein, Klaus; Reyes, Mauricio; Bakas, Spyridon","Menze, Bjoern (Quantitative Biomedicine, University of Zurich, Zurich, Switzerland. Electronic address: bjoern.menze@uzh.ch.); Isensee, Fabian (DKFZ, Heidelberg, Germany. Electronic address: f.isensee@dkfz-heidelberg.de.); Wiest, Roland (Support Center for Advanced Neuroimaging, Institute of Diagnostic and Interventional Neuroradiology, Inselspital, Bern, Switzerland. Electronic address: roland.wiest@insel.ch.); Wiestler, Bene (Neuroradiology, TUM, Munich, Germany. Electronic address: b.wiestler@tum.de.); Maier-Hein, Klaus (DKFZ, Heidelberg, Germany. Electronic address: k.maier-hein@dkfz-heidelberg.de.); Reyes, Mauricio (Data Science Center, Inselspital, Bern, Switzerland. Electronic address: mauricio.reyes@med.unibe.ch.); Bakas, Spyridon (Center for Biomedical Image Computing and Analytics (CBICA), University of Pennsylvania, Philadelphia, PA, USA. Electronic address: sbakas@upenn.edu.)","Menze, Bjoern (University of Zurich)","Menze, Bjoern (University of Zurich); Isensee, Fabian (German Cancer Research Center); Wiest, Roland (University Hospital of Bern); Wiestler, Bene (); Maier-Hein, Klaus (German Cancer Research Center); Reyes, Mauricio (University Hospital of Bern); Bakas, Spyridon (University of Pennsylvania Health System; University of Pennsylvania)",16,16,1.14,6.98,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8040671,https://app.dimensions.ai/details/publication/pub.1133057937,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,
2437,pub.1139954843,10.3390/healthcare9080938,34442075,PMC8393549,Loss Weightings for Improving Imbalanced Brain Structure Segmentation Using Fully Convolutional Networks,"Brain structure segmentation on magnetic resonance (MR) images is important for various clinical applications. It has been automatically performed by using fully convolutional networks. However, it suffers from the class imbalance problem. To address this problem, we investigated how loss weighting strategies work for brain structure segmentation tasks with different class imbalance situations on MR images. In this study, we adopted segmentation tasks of the cerebrum, cerebellum, brainstem, and blood vessels from MR cisternography and angiography images as the target segmentation tasks. We used a U-net architecture with cross-entropy and Dice loss functions as a baseline and evaluated the effect of the following loss weighting strategies: inverse frequency weighting, median inverse frequency weighting, focal weighting, distance map-based weighting, and distance penalty term-based weighting. In the experiments, the Dice loss function with focal weighting showed the best performance and had a high average Dice score of 92.8% in the binary-class segmentation tasks, while the cross-entropy loss functions with distance map-based weighting achieved the Dice score of up to 93.1% in the multi-class segmentation tasks. The results suggested that the distance map-based and the focal weightings could boost the performance of cross-entropy and Dice loss functions in class imbalanced segmentation tasks, respectively.",,,Healthcare,,,2021-07-26,2021,2021-07-26,,9,8,938,All OA, Gold,Article,"Sugino, Takaaki; Kawase, Toshihiro; Onogi, Shinya; Kin, Taichi; Saito, Nobuhito; Nakajima, Yoshikazu","Sugino, Takaaki (Department of Biomedical Information, Institute of Biomaterials and Bioengineering, Tokyo Medical and Dental University, Tokyo 101-0062, Japan;, kawase.bmi@tmd.ac.jp, (T.K.);, onogi.bmi@tmd.ac.jp, (S.O.)); Kawase, Toshihiro (Department of Biomedical Information, Institute of Biomaterials and Bioengineering, Tokyo Medical and Dental University, Tokyo 101-0062, Japan;, kawase.bmi@tmd.ac.jp, (T.K.);, onogi.bmi@tmd.ac.jp, (S.O.)); Onogi, Shinya (Department of Biomedical Information, Institute of Biomaterials and Bioengineering, Tokyo Medical and Dental University, Tokyo 101-0062, Japan;, kawase.bmi@tmd.ac.jp, (T.K.);, onogi.bmi@tmd.ac.jp, (S.O.)); Kin, Taichi (Department of Neurosurgery, Graduate School of Medicine, The University of Tokyo, Tokyo 113-0033, Japan;, tkin-tky@g.ecc.u-tokyo.ac.jp, (T.K.);, nsaito-nsu@m.u-tokyo.ac.jp, (N.S.)); Saito, Nobuhito (Department of Neurosurgery, Graduate School of Medicine, The University of Tokyo, Tokyo 113-0033, Japan;, tkin-tky@g.ecc.u-tokyo.ac.jp, (T.K.);, nsaito-nsu@m.u-tokyo.ac.jp, (N.S.)); Nakajima, Yoshikazu (Department of Biomedical Information, Institute of Biomaterials and Bioengineering, Tokyo Medical and Dental University, Tokyo 101-0062, Japan;, kawase.bmi@tmd.ac.jp, (T.K.);, onogi.bmi@tmd.ac.jp, (S.O.))","Sugino, Takaaki (Tokyo Medical and Dental University; ); Nakajima, Yoshikazu (Tokyo Medical and Dental University; )","Sugino, Takaaki (Tokyo Medical and Dental University); Kawase, Toshihiro (Tokyo Medical and Dental University); Onogi, Shinya (Tokyo Medical and Dental University); Kin, Taichi (University of Tokyo); Saito, Nobuhito (University of Tokyo); Nakajima, Yoshikazu (Tokyo Medical and Dental University)",14,14,2.87,,https://www.mdpi.com/2227-9032/9/8/938/pdf?version=1627287999,https://app.dimensions.ai/details/publication/pub.1139954843,32 Biomedical and Clinical Sciences, 42 Health Sciences,,,,,,,,,,
2437,pub.1135671446,10.1109/tmi.2021.3060634,33617450,PMC8516596,Transferable Visual Words: Exploiting the Semantics of Anatomical Patterns for Self-Supervised Learning,"This paper introduces a new concept called ""transferable visual words"" (TransVW), aiming to achieve annotation efficiency for deep learning in medical image analysis. Medical imaging-focusing on particular parts of the body for defined clinical purposes-generates images of great similarity in anatomy across patients and yields sophisticated anatomical patterns across images, which are associated with rich semantics about human anatomy and which are natural visual words. We show that these visual words can be automatically harvested according to anatomical consistency via self-discovery, and that the self-discovered visual words can serve as strong yet free supervision signals for deep models to learn semantics-enriched generic image representation via self-supervision (self-classification and self-restoration). Our extensive experiments demonstrate the annotation efficiency of TransVW by offering higher performance and faster convergence with reduced annotation cost in several applications. Our TransVW has several important advantages, including (1) TransVW is a fully autodidactic scheme, which exploits the semantics of visual words for self-supervised learning, requiring no expert annotation; (2) visual word learning is an add-on strategy, which complements existing self-supervised methods, boosting their performance; and (3) the learned image representation is semantics-enriched models, which have proven to be more robust and generalizable, saving annotation efforts for a variety of applications through transfer learning. Our code, pre-trained models, and curated visual words are available at https://github.com/JLiangLab/TransVW.","This work was supported in part by the ASU and Mayo Clinic through a Seed Grant and an Innovation Grant, in part by the NIH under Award R01HL128785, in part by the GPUs through the ASU Research Computing, and in part by the Extreme Science and Engineering Discovery Environment (XSEDE) funded by the National Science Foundation (NSF) under Grant ACI-1548562. The author would like to thank Zuwei Guo for implementing Rubikâs cube [14], Md Mahfuzur Rahman Siddiquee for examining NiftyNet [16], Jiaxuan Pang for evaluating I3D [15], Shivam Bajpai for helping in adopting TransVW to nnU-Net [26], and Shrikar Tatapudi for helping improve the writing of this article. The content of this article is covered by patents pending.",,IEEE Transactions on Medical Imaging,,Diagnostic Imaging, Humans, Radiography, Semantics, Supervised Machine Learning,2021-09-30,2021,2021-09-30,2021-10,40,10,2857-2868,All OA, Green,Article,"Haghighi, Fatemeh; Taher, Mohammad Reza Hosseinzadeh; Zhou, Zongwei; Gotway, Michael B.; Liang, Jianming","Haghighi, Fatemeh (School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, 85281, USA); Taher, Mohammad Reza Hosseinzadeh (School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, 85281, USA); Zhou, Zongwei (Department of Biomedical Informatics, Arizona State University, Scottsdale, AZ, 85259, USA); Gotway, Michael B. (Radiology Department, Mayo Clinic, Scottsdale, AZ, 85259, USA); Liang, Jianming (Department of Biomedical Informatics, Arizona State University, Scottsdale, AZ, 85259, USA)","Liang, Jianming (Arizona State University)","Haghighi, Fatemeh (Arizona State University); Taher, Mohammad Reza Hosseinzadeh (Arizona State University); Zhou, Zongwei (Arizona State University); Gotway, Michael B. (Mayo Clinic); Liang, Jianming (Arizona State University)",27,27,3.21,21.5,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8516596,https://app.dimensions.ai/details/publication/pub.1135671446,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,
2428,pub.1153241598,10.1109/tmi.2022.3225667,36449590,,BowelNet: Joint Semantic-Geometric Ensemble Learning for Bowel Segmentation from Both Partially and Fully Labeled CT Images,"Accurate bowel segmentation is essential for diagnosis and treatment of bowel cancers. Unfortunately, segmenting the entire bowel in CT images is quite challenging due to unclear boundary, large shape, size, and appearance variations, as well as diverse filling status within the bowel. In this paper, we present a novel two-stage framework, named BowelNet, to handle the challenging task of bowel segmentation in CT images, with two stages of 1) jointly localizing all types of the bowel, and 2) finely segmenting each type of the bowel. Specifically, in the first stage, we learn a unified localization network from both partially- and fully-labeled CT images to robustly detect all types of the bowel. To better capture unclear bowel boundary and learn complex bowel shapes, in the second stage, we propose to jointly learn semantic information (i.e., bowel segmentation mask) and geometric representations (i.e., bowel boundary and bowel skeleton) for fine bowel segmentation in a multi-task learning scheme. Moreover, we further propose to learn a meta segmentation network via pseudo labels to improve segmentation accuracy. By evaluating on a large abdominal CT dataset, our proposed BowelNet method can achieve Dice scores of 0.764, 0.848, 0.835, 0.774, and 0.824 in segmenting the duodenum, jejunum-ileum, colon, sigmoid, and rectum, respectively. These results demonstrate the effectiveness of our proposed BowelNet framework in segmenting the entire bowel from CT images.",,,IEEE Transactions on Medical Imaging,,,2022-11-30,2022,2022-11-30,2022-11-30,PP,99,1-1,Closed,Article,"Wang, Chong; Cui, Zhiming; Yang, Junwei; Han, Miaofei; Carneiro, Gustavo; Shen, Dinggang","Wang, Chong (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Cui, Zhiming (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Yang, Junwei (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Han, Miaofei (Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China); Carneiro, Gustavo (Australian Institute for Machine Learning, University of Adelaide, SA, Australia); Shen, Dinggang (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China)",,"Wang, Chong (ShanghaiTech University); Cui, Zhiming (ShanghaiTech University); Yang, Junwei (ShanghaiTech University); Han, Miaofei (); Carneiro, Gustavo (University of Adelaide); Shen, Dinggang (ShanghaiTech University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153241598,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
2428,pub.1150990474,10.1016/j.media.2022.102616,36179380,,Fast and Low-GPU-memory abdomen CT organ segmentation: The FLARE challenge,"Automatic segmentation of abdominal organs in CT scans plays an important role in clinical practice. However, most existing benchmarks and datasets only focus on segmentation accuracy, while the model efficiency and its accuracy on the testing cases from different medical centers have not been evaluated. To comprehensively benchmark abdominal organ segmentation methods, we organized the first Fast and Low GPU memory Abdominal oRgan sEgmentation (FLARE) challenge, where the segmentation methods were encouraged to achieve high accuracy on the testing cases from different medical centers, fast inference speed, and low GPU memory consumption, simultaneously. The winning method surpassed the existing state-of-the-art method, achieving a 19Ã faster inference speed and reducing the GPU memory consumption by 60% with comparable accuracy. We provide a summary of the top methods, make their code and Docker containers publicly available, and give practical suggestions on building accurate and efficient abdominal organ segmentation models. The FLARE challenge remains open for future submissions through a live platform for benchmarking further methodology developments at https://flare.grand-challenge.org/.","This project is supported by Chinaâs Ministry of Science and Technology (No. 2020YFA0713800) and National Natural Science Foundation of China (No. 11971229, No. 12090023). The authors would like to thank NVIDIA for supporting the evaluation platform.",,Medical Image Analysis,,"Humans; Algorithms; Tomography, X-Ray Computed; Abdomen; Benchmarking; Image Processing, Computer-Assisted",2022-09-13,2022,2022-09-13,2022-11,82,,102616,Closed,Article,"Ma, Jun; Zhang, Yao; Gu, Song; An, Xingle; Wang, Zhihe; Ge, Cheng; Wang, Congcong; Zhang, Fan; Wang, Yu; Xu, Yinan; Gou, Shuiping; Thaler, Franz; Payer, Christian; Å tern, Darko; Henderson, Edward G A; McSweeney, DÃ³nal M; Green, Andrew; Jackson, Price; McIntosh, Lachlan; Nguyen, Quoc-Cuong; Qayyum, Abdul; Conze, Pierre-Henri; Huang, Ziyan; Zhou, Ziqi; Fan, Deng-Ping; Xiong, Huan; Dong, Guoqiang; Zhu, Qiongjie; He, Jian; Yang, Xiaoping","Ma, Jun (Department of Mathematics, Nanjing University of Science and Technology, 210094, Nanjing, China.); Zhang, Yao (Institute of Computing Technology, Chinese Academy of Sciences and the University of Chinese Academy of Sciences, 100019, Beijing, China.); Gu, Song (Department of Image Reconstruction, Nanjing Anke Medical Technology Co., Ltd., 211113, Nanjing, China.); An, Xingle (Infervision Technology Co. Ltd., 100020, Beijing, China.); Wang, Zhihe (Shenzhen Haichuang Medical Co., Ltd., 518049, Shenzhen, China.); Ge, Cheng (Institute of Bioinformatics and Medical Engineering, Jiangsu University of Technology, 213001, Changzhou, China.); Wang, Congcong (School of Computer Science and Engineering, Tianjin University of Technology, 300384, Tianjin, China; Engineering Research Center of Learning-Based Intelligent System, Ministry of Education, 300384, Tianjin, China.); Zhang, Fan (Radiological Algorithm, Fosun Aitrox Information Technology Co., Ltd., 200033, Shanghai, China.); Wang, Yu (Radiological Algorithm, Fosun Aitrox Information Technology Co., Ltd., 200033, Shanghai, China.); Xu, Yinan (Key Lab of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, 710071, Shaanxi, China.); Gou, Shuiping (Key Lab of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, 710071, Shaanxi, China.); Thaler, Franz (Gottfried Schatz Research Center: Biophysics, Medical University of Graz, 8010, Graz, Austria; Institute of Computer Graphics and Vision, Graz University of Technology, 8010, Graz, Austria.); Payer, Christian (Institute of Computer Graphics and Vision, Graz University of Technology, 8010, Graz, Austria.); Å tern, Darko (Gottfried Schatz Research Center: Biophysics, Medical University of Graz, 8010, Graz, Austria.); Henderson, Edward G A (Division of Cancer Sciences, The University of Manchester, M139PL, Manchester, UK; Radiotherapy Related Research, The Christie NHS Foundation Trust, M139PL, Manchester, UK.); McSweeney, DÃ³nal M (Division of Cancer Sciences, The University of Manchester, M139PL, Manchester, UK; Radiotherapy Related Research, The Christie NHS Foundation Trust, M139PL, Manchester, UK.); Green, Andrew (Division of Cancer Sciences, The University of Manchester, M139PL, Manchester, UK; Radiotherapy Related Research, The Christie NHS Foundation Trust, M139PL, Manchester, UK.); Jackson, Price (Peter MacCallum Cancer Centre, 3000, Melbourne, Australia.); McIntosh, Lachlan (Peter MacCallum Cancer Centre, 3000, Melbourne, Australia.); Nguyen, Quoc-Cuong (University of Information Technology, VNU-HCM, 700000, Ho Chi Minh City, Viet Nam.); Qayyum, Abdul (Brest National School of Engineering, UMR CNRS 6285 LabSTICC, 29280, Brest, France.); Conze, Pierre-Henri (IMT Atlantique, LaTIM UMR 1101, Inserm, 29238, Brest, France.); Huang, Ziyan (Institute of Medical Robotics, Shanghai Jiao Tong University, 200240, Shanghai, China.); Zhou, Ziqi (Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, 518000, Shenzhen, China.); Fan, Deng-Ping (College of Computer Science, Nankai University, 300071, Tianjin, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates.); Xiong, Huan (Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Harbin Institute of Technology, 150001, Harbin, China.); Dong, Guoqiang (Department of Nuclear Medicine, Nanjing Drum Tower Hospital, the Affiliated Hospital of Nanjing University Medical School, 210008, Nanjing, China; Department of Interventional Radiology, The Second Affiliated Hospital of Bengbu Medical College, 233017, Bengbu, China.); Zhu, Qiongjie (Department of Nuclear Medicine, Nanjing Drum Tower Hospital, the Affiliated Hospital of Nanjing University Medical School, 210008, Nanjing, China; Department of Radiology, Shidong Hospital, 200438, Shanghai, China.); He, Jian (Department of Nuclear Medicine, Nanjing Drum Tower Hospital, the Affiliated Hospital of Nanjing University Medical School, 210008, Nanjing, China.); Yang, Xiaoping (Department of Mathematics, Nanjing University, 210093, Nanjing, China. Electronic address: xpyang@nju.edu.cn.)","Yang, Xiaoping (Nanjing University)","Ma, Jun (Nanjing University of Science and Technology); Zhang, Yao (Institute of Computing Technology); Gu, Song (); An, Xingle (); Wang, Zhihe (); Ge, Cheng (Jiangsu University of Technology); Wang, Congcong (Tianjin University of Technology; Ministry of Education of the People's Republic of China); Zhang, Fan (); Wang, Yu (); Xu, Yinan (Xidian University); Gou, Shuiping (Xidian University); Thaler, Franz (Medical University of Graz; Graz University of Technology); Payer, Christian (Graz University of Technology); Å tern, Darko (Medical University of Graz); Henderson, Edward G A (University of Manchester; Christie Hospital NHS Foundation Trust); McSweeney, DÃ³nal M (University of Manchester; Christie Hospital NHS Foundation Trust); Green, Andrew (University of Manchester; Christie Hospital NHS Foundation Trust); Jackson, Price (Peter MacCallum Cancer Centre); McIntosh, Lachlan (Peter MacCallum Cancer Centre); Nguyen, Quoc-Cuong (Vietnam National University, Ho Chi Minh City); Qayyum, Abdul (); Conze, Pierre-Henri (IMT Atlantique); Huang, Ziyan (Shanghai Jiao Tong University); Zhou, Ziqi (Shenzhen University); Fan, Deng-Ping (Nankai University; Inception Institute of Artificial Intelligence); Xiong, Huan (Mohamed bin Zayed University of Artificial Intelligence; Harbin Institute of Technology); Dong, Guoqiang (The Second Affiliated Hospital of Bengbu Medical College); Zhu, Qiongjie (); He, Jian (); Yang, Xiaoping (Nanjing University)",17,17,,,,https://app.dimensions.ai/details/publication/pub.1150990474,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,,
2320,pub.1144719050,10.1016/j.compbiomed.2022.105237,35074737,,Segmenting pediatric optic pathway gliomas from MRI using deep learning,"Optic pathway gliomas are low-grade neoplastic lesions that account for approximately 3-5% of brain tumors in children. Assessing tumor burden from magnetic resonance imaging (MRI) plays a central role in its efficient management, yet it is a challenging and human-dependent task due to the difficult and error-prone process of manual segmentation of such lesions, as they can easily manifest different location and appearance characteristics. In this paper, we tackle this issue and propose a fully-automatic and reproducible deep learning algorithm built upon the recent advances in the field which is capable of detecting and segmenting optical pathway gliomas from MRI. The proposed training strategies help us elaborate well-generalizing deep models even in the case of limited ground-truth MRIs presenting example optic pathway gliomas. The rigorous experimental study, performed over two clinical datasets of 22 and 51 multi-modal MRIs acquired for 22 and 51 patients with optical pathway gliomas, and a public dataset of 494 pre-surgery low-/high-grade glioma patients (corresponding to 494 multi-modal MRIs), and involving quantitative, qualitative and statistical analysis revealed that the suggested technique can not only effectively delineate optic pathway gliomas, but can also be applied for detecting other brain tumors. The experiments indicate high agreement between automatically calculated and ground-truth volumetric measurements of the tumors and very fast operation of the proposed approach, both of which can increase the clinical utility of the suggested software tool. Finally, our deep architectures have been made open-sourced to ensure full reproducibility of the method over other MRI data.","JN was partially supported by the Silesian University of Technology grant for maintaining and developing research potential. The authors would like to thank Marek Pitura (Future Processing Healthcare) for his valuable help in managing this study. This paper is in memory of Dr. Grzegorz Nalepa, an extraordinary scientist and pediatric hematologist/oncologist at Riley Hospital for Children, Indianapolis, USA, who helped countless patients and their families through some of the most challenging moments of their lives.",,Computers in Biology and Medicine,,"Brain Neoplasms; Child; Deep Learning; Glioma; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Reproducibility of Results",2022-01-17,2022,2022-01-17,2022-03,142,,105237,Closed,Article,"Nalepa, Jakub; Adamski, Szymon; Kotowski, Krzysztof; Chelstowska, Sylwia; Machnikowska-Sokolowska, Magdalena; Bozek, Oskar; Wisz, Agata; Jurkiewicz, Elzbieta","Nalepa, Jakub (Department of Algorithmics and Software, Silesian University of Technology, Gliwice, Poland; Graylight Imaging, Gliwice, Poland. Electronic address: jnalepa@ieee.org.); Adamski, Szymon (Graylight Imaging, Gliwice, Poland.); Kotowski, Krzysztof (Graylight Imaging, Gliwice, Poland.); Chelstowska, Sylwia (Children's Memorial Health Institute, Warsaw, Poland.); Machnikowska-Sokolowska, Magdalena (Division of Diagnostic Imaging, Department of Radiology and Nuclear Medicine, Faculty of Medical Sciences in Katowice, Medical University of Silesia, Katowice, Poland.); Bozek, Oskar (Division of Diagnostic Imaging, Department of Radiology and Nuclear Medicine, Faculty of Medical Sciences in Katowice, Medical University of Silesia, Katowice, Poland.); Wisz, Agata (Division of Diagnostic Imaging, Department of Radiology and Nuclear Medicine, Faculty of Medical Sciences in Katowice, Medical University of Silesia, Katowice, Poland.); Jurkiewicz, Elzbieta (Children's Memorial Health Institute, Warsaw, Poland.)","Nalepa, Jakub (Silesian University of Technology; )","Nalepa, Jakub (Silesian University of Technology); Adamski, Szymon (); Kotowski, Krzysztof (); Chelstowska, Sylwia (Children's Memorial Health Institute); Machnikowska-Sokolowska, Magdalena (Medical University of Silesia); Bozek, Oskar (Medical University of Silesia); Wisz, Agata (Medical University of Silesia); Jurkiewicz, Elzbieta (Children's Memorial Health Institute)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1144719050,31 Biological Sciences, 3102 Bioinformatics and Computational Biology, 42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences, 4601 Applied Computing,,,,,,,
2319,pub.1150426091,10.1016/j.media.2022.102589,36095905,,Auto-DenseUNet: Searchable neural network architecture for mass segmentation in 3D automated breast ultrasound,"Accurate segmentation of breast mass in 3D automated breast ultrasound (ABUS) plays an important role in breast cancer analysis. Deep convolutional networks have become a promising approach in segmenting ABUS images. However, designing an effective network architecture is time-consuming, and highly relies on specialist's experience and prior knowledge. To address this issue, we introduce a searchable segmentation network (denoted as Auto-DenseUNet) based on the neural architecture search (NAS) to search the optimal architecture automatically for the ABUS mass segmentation task. Concretely, a novel search space is designed based on a densely connected structure to enhance the gradient and information flows throughout the network. Then, to encourage multiscale information fusion, a set of searchable multiscale aggregation nodes between the down-sampling and up-sampling parts of the network are further designed. Thus, all the operators within the dense connection structure or between any two aggregation nodes can be searched to find the optimal structure. Finally, a novel decoupled search training strategy during architecture search is also introduced to alleviate the memory limitation caused by continuous relaxation in NAS. The proposed Auto-DenseUNet method has been evaluated on our ABUS dataset with 170 volumes (from 107 patients), including 120 training volumes and 50 testing volumes split at patient level. Experimental results on testing volumes show that our searched architecture performed better than several human-designed segmentation models on the 3D ABUS mass segmentation task, indicating the effectiveness of our proposed method.",,,Medical Image Analysis,,"Humans; Female; Imaging, Three-Dimensional; Ultrasonography, Mammary; Neural Networks, Computer; Breast; Breast Neoplasms; Image Processing, Computer-Assisted",2022-08-23,2022,2022-08-23,2022-11,82,,102589,Closed,Article,"Cao, Xuyang; Chen, Houjin; Li, Yanfeng; Peng, Yahui; Zhou, Yue; Cheng, Lin; Liu, Tianming; Shen, Dinggang","Cao, Xuyang (School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, 100044, China.); Chen, Houjin (School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, 100044, China.); Li, Yanfeng (School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, 100044, China. Electronic address: yf.li@bjtu.edu.cn.); Peng, Yahui (School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, 100044, China.); Zhou, Yue (School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, 100044, China.); Cheng, Lin (Peking University People's Hospital, Beijing, 100044, China.); Liu, Tianming (Department of Computer Science, University of Georgia, Athens, GA 30602, USA.); Shen, Dinggang (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China; Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China.)","Li, Yanfeng (Beijing Jiaotong University)","Cao, Xuyang (Beijing Jiaotong University); Chen, Houjin (Beijing Jiaotong University); Li, Yanfeng (Beijing Jiaotong University); Peng, Yahui (Beijing Jiaotong University); Zhou, Yue (Beijing Jiaotong University); Cheng, Lin (Peking University People's Hospital); Liu, Tianming (University of Georgia); Shen, Dinggang (ShanghaiTech University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150426091,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,,
2307,pub.1152946348,10.3389/fninf.2022.933230,36483313,PMC9724825,Review of Generative Adversarial Networks in mono- and cross-modal biomedical image registration,"Biomedical image registration refers to aligning corresponding anatomical structures among different images, which is critical to many tasks, such as brain atlas building, tumor growth monitoring, and image fusion-based medical diagnosis. However, high-throughput biomedical image registration remains challenging due to inherent variations in the intensity, texture, and anatomy resulting from different imaging modalities, different sample preparation methods, or different developmental stages of the imaged subject. Recently, Generative Adversarial Networks (GAN) have attracted increasing interest in both mono- and cross-modal biomedical image registrations due to their special ability to eliminate the modal variance and their adversarial training strategy. This paper provides a comprehensive survey of the GAN-based mono- and cross-modal biomedical image registration methods. According to the different implementation strategies, we organize the GAN-based mono- and cross-modal biomedical image registration methods into four categories: modality translation, symmetric learning, adversarial strategies, and joint training. The key concepts, the main contributions, and the advantages and disadvantages of the different strategies are summarized and discussed. Finally, we analyze the statistics of all the cited works from different points of view and reveal future trends for GAN-based biomedical image registration studies.",The authors acknowledge the high-performance computing platform of Anhui University for providing computing resources.,"This research was funded by the National Natural Science Foundation of China (61871411, 62271003, and 62201008), the Sci-Tech Innovation 2030 Agenda (2022ZD0205200 and 2022ZD0205204), the University Synergy Innovation Program of Anhui Province (GXXT-2021-001), and the Natural Science Foundation of the Education Department of Anhui Province (KJ2021A0017).",Frontiers in Neuroinformatics,,,2022-11-22,2022,2022-11-22,,16,,933230,All OA, Gold,Article,"Han, Tingting; Wu, Jun; Luo, Wenting; Wang, Huiming; Jin, Zhe; Qu, Lei","Han, Tingting (Ministry of Education Key Laboratory of Intelligent Computing and Signal Processing, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, China); Wu, Jun (Ministry of Education Key Laboratory of Intelligent Computing and Signal Processing, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, China); Luo, Wenting (Ministry of Education Key Laboratory of Intelligent Computing and Signal Processing, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, China); Wang, Huiming (Ministry of Education Key Laboratory of Intelligent Computing and Signal Processing, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, China); Jin, Zhe (School of Artificial Intelligence, Anhui University, Hefei, China); Qu, Lei (Ministry of Education Key Laboratory of Intelligent Computing and Signal Processing, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China; SEU-ALLEN Joint Center, Institute for Brain and Intelligence, Southeast University, Nanjing, China)","Qu, Lei (Anhui University; ; Southeast University)","Han, Tingting (Anhui University); Wu, Jun (Anhui University); Luo, Wenting (Anhui University); Wang, Huiming (Anhui University); Jin, Zhe (Anhui University); Qu, Lei (Anhui University; Southeast University)",0,0,,,https://www.frontiersin.org/articles/10.3389/fninf.2022.933230/pdf,https://app.dimensions.ai/details/publication/pub.1152946348,32 Biomedical and Clinical Sciences, 3209 Neurosciences, 46 Information and Computing Sciences, 4601 Applied Computing, 4611 Machine Learning,,,,,,,
2307,pub.1148154979,10.1016/j.media.2022.102491,35653902,,Prior-aware autoencoders for lung pathology segmentation.,"Segmentation of lung pathology in Computed Tomography (CT) images is of great importance for lung disease screening. However, the presence of different types of lung pathologies with a wide range of heterogeneities in size, shape, location, and texture, on one side, and their visual similarity with respect to surrounding tissues, on the other side, make it challenging to perform reliable automatic lesion segmentation. To leverage segmentation performance, we propose a deep learning framework comprising a Normal Appearance Autoencoder (NAA) model to learn the distribution of healthy lung regions and reconstruct pathology-free images from the corresponding pathological inputs by replacing the pathological regions with the characteristics of healthy tissues. Detected regions that represent prior information regarding the shape and location of pathologies are then integrated into a segmentation network to guide the attention of the model into more meaningful delineations. The proposed pipeline was tested on three types of lung pathologies, including pulmonary nodules, Non-Small Cell Lung Cancer (NSCLC), and Covid-19 lesion on five comprehensive datasets. The results show the superiority of the proposed prior model, which outperformed the baseline segmentation models in all the cases with significant margins. On average, adding the prior model improved the Dice coefficient for the segmentation of lung nodules by 0.038, NSCLCs by 0.101, and Covid-19 lesions by 0.041. We conclude that the proposed NAA model produces reliable prior knowledge regarding the lung pathologies, and integrating such knowledge into a prior segmentation network leads to more accurate delineations.",,,Medical Image Analysis,,"COVID-19; Carcinoma, Non-Small-Cell Lung; Humans; Image Processing, Computer-Assisted; Lung; Lung Neoplasms; Tomography, X-Ray Computed",2022-05-25,2022,2022-05-25,2022-08,80,,102491,All OA, Hybrid,Article,"Astaraki, Mehdi; Smedby, Ãrjan; Wang, Chunliang","Astaraki, Mehdi (Department of Biomedical Engineering and Healthy Systems, KTH Royal Institute of Technology, Huddinge SE-14157, Sweden; Department of Oncology-Pathology, Karolinska Institutet, Karolinska Universitetssjukhuset, Solna, Stockholm SE-17176, Sweden. Electronic address: mehast@kth.se.); Smedby, Ãrjan (Department of Biomedical Engineering and Healthy Systems, KTH Royal Institute of Technology, Huddinge SE-14157, Sweden.); Wang, Chunliang (Department of Biomedical Engineering and Healthy Systems, KTH Royal Institute of Technology, Huddinge SE-14157, Sweden. Electronic address: chunwan@kth.se.)",,"Astaraki, Mehdi (Royal Institute of Technology; Karolinska Institute; Karolinska University Hospital); Smedby, Ãrjan (Royal Institute of Technology); Wang, Chunliang (Royal Institute of Technology)",3,3,,,https://doi.org/10.1016/j.media.2022.102491,https://app.dimensions.ai/details/publication/pub.1148154979,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
2306,pub.1153013167,10.1109/tmi.2022.3224067,36417741,,Causality-inspired Single-source Domain Generalization for Medical Image Segmentation,"Deep learning models usually suffer from the domain shift issue, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data are only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. In this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation scenarios: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-site prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains.",,,IEEE Transactions on Medical Imaging,,,2022-11-23,2022,2022-11-23,2022-11-23,PP,99,1-1,All OA, Green,Article,"Ouyang, Cheng; Chen, Chen; Li, Surui; Li, Zeju; Qin, Chen; Bai, Wenjia; Rueckert, Daniel","Ouyang, Cheng (Department of Computing, Imperial College London, UK); Chen, Chen (Department of Computing, Imperial College London, UK); Li, Surui (Department of Computing, Imperial College London, UK); Li, Zeju (Department of Computing, Imperial College London, UK); Qin, Chen (Department of Electrical and Electronic Engineering, Imperial College London, UK); Bai, Wenjia (Department of Computing, Imperial College London, UK); Rueckert, Daniel (Department of Computing, Imperial College London, UK)",,"Ouyang, Cheng (Imperial College London); Chen, Chen (Imperial College London); Li, Surui (Imperial College London); Li, Zeju (Imperial College London); Qin, Chen (Imperial College London); Bai, Wenjia (Imperial College London); Rueckert, Daniel (Imperial College London)",5,5,,,http://arxiv.org/pdf/2111.12525,https://app.dimensions.ai/details/publication/pub.1153013167,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2210,pub.1153204132,10.1016/j.media.2022.102706,36516557,,Factorizer: A scalable interpretable approach to context modeling for medical image segmentation,"Convolutional Neural Networks (CNNs) with U-shaped architectures have dominated medical image segmentation, which is crucial for various clinical purposes. However, the inherent locality of convolution makes CNNs fail to fully exploit global context, essential for better recognition of some structures, e.g., brain lesions. Transformers have recently proven promising performance on vision tasks, including semantic segmentation, mainly due to their capability of modeling long-range dependencies. Nevertheless, the quadratic complexity of attention makes existing Transformer-based models use self-attention layers only after somehow reducing the image resolution, which limits the ability to capture global contexts present at higher resolutions. Therefore, this work introduces a family of models, dubbed Factorizer, which leverages the power of low-rank matrix factorization for constructing an end-to-end segmentation model. Specifically, we propose a linearly scalable approach to context modeling, formulating Nonnegative Matrix Factorization (NMF) as a differentiable layer integrated into a U-shaped architecture. The shifted window technique is also utilized in combination with NMF to effectively aggregate local information. Factorizers compete favorably with CNNs and Transformers in terms of accuracy, scalability, and interpretability, achieving state-of-the-art results on the BraTS dataset for brain tumor segmentation and ISLES'22 dataset for stroke lesion segmentation. Highly meaningful NMF components give an additional interpretability advantage to Factorizers over CNNs and Transformers. Moreover, our ablation studies reveal a distinctive feature of Factorizers that enables a significant speed-up in inference for a trained Factorizer without any extra steps and without sacrificing much accuracy. The code and models are publicly available at https://github.com/pashtari/factorizer.","The research leading to these results was supported by the European Unionâs Horizon 2020 MSCA-ITN program, funded by the European Commission under the grant agreement No 813120 (INSPiRE-MED). This research also received funding from the Flemish Government (AI Research Program). Sabine Van Huffel, Frederik Maes, and Pooya Ashtari are affiliated to Leuven.AI - KU Leuven institute for AI, B-3000, Leuven, Belgium.",,Medical Image Analysis,,"Humans; Algorithms; Brain Neoplasms; Neural Networks, Computer; Semantics; Stroke; Image Processing, Computer-Assisted",2022-11-29,2022,2022-11-29,2023-02,84,,102706,All OA, Green,Article,"Ashtari, Pooya; Sima, Diana M; De Lathauwer, Lieven; Sappey-Marinier, Dominique; Maes, Frederik; Van Huffel, Sabine","Ashtari, Pooya (Department of Electrical Engineering (ESAT), STADIUS Center, KU Leuven, Leuven, Belgium; CREATIS (CNRS UMR5220 & INSERM U1294), UniversitÃ© Claude Bernard Lyon 1, Lyon, France. Electronic address: pooya.ashtari@esat.kuleuven.be.); Sima, Diana M (icometrix, Research and Development, Leuven, Belgium; AI Supported Modelling in Clinical Sciences (AIMS), Vrije Universiteit Brussel, Brussels, Belgium.); De Lathauwer, Lieven (Group Science, Engineering and Technology, KU Leuven Kulak, Kortrijk, Belgium.); Sappey-Marinier, Dominique (CREATIS (CNRS UMR5220 & INSERM U1294), UniversitÃ© Claude Bernard Lyon 1, Lyon, France.); Maes, Frederik (Department of Electrical Engineering (ESAT), PSI, KU Leuven, Leuven, Belgium.); Van Huffel, Sabine (Department of Electrical Engineering (ESAT), STADIUS Center, KU Leuven, Leuven, Belgium.)","Ashtari, Pooya (KU Leuven; Claude Bernard University Lyon 1)","Ashtari, Pooya (KU Leuven; Claude Bernard University Lyon 1); Sima, Diana M (Icometrix (Belgium); Vrije Universiteit Brussel); De Lathauwer, Lieven (KU Leuven); Sappey-Marinier, Dominique (Claude Bernard University Lyon 1); Maes, Frederik (KU Leuven); Van Huffel, Sabine (KU Leuven)",1,1,,,http://arxiv.org/pdf/2202.12295,https://app.dimensions.ai/details/publication/pub.1153204132,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
2210,pub.1140316142,10.1038/s41598-021-94071-1,34376714,PMC8355223,3D brain tumor segmentation using a two-stage optimal mass transport algorithm,"Optimal mass transport (OMT) theory, the goal of which is to move any irregular 3D object (i.e., the brain) without causing significant distortion, is used to preprocess brain tumor datasets for the first time in this paper. The first stage of a two-stage OMT (TSOMT) procedure transforms the brain into a unit solid ball. The second stage transforms the unit ball into a cube, as it is easier to apply a 3D convolutional neural network to rectangular coordinates. Small variations in the local mass-measure stretch ratio among all the brain tumor datasets confirm the robustness of the transform. Additionally, the distortion is kept at a minimum with a reasonable transport cost. The original 240Ã240Ã155Ã4\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$240 \times 240 \times 155 \times 4$$\end{document} dataset is thus reduced to a cube of 128Ã128Ã128Ã4\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$128 \times 128 \times 128 \times 4$$\end{document}, which is a 76.6% reduction in the total number of voxels, without losing much detail. Three typical U-Nets are trained separately to predict the whole tumor (WT), tumor core (TC), and enhanced tumor (ET) from the cube. An impressive training accuracy of 0.9822 in the WT cube is achieved at 400 epochs. An inverse TSOMT method is applied to the predicted cube to obtain the brain results. The conversion loss from the TSOMT method to the inverse TSOMT method is found to be less than one percent. For training, good Dice scores (0.9781 for the WT, 0.9637 for the TC, and 0.9305 for the ET) can be obtained. Significant improvements in brain tumor detection and the segmentation accuracy are achieved. For testing, postprocessing (rotation) is added to the TSOMT, U-Net prediction, and inverse TSOMT methods for an accuracy improvement of one to two percent. It takes 200 seconds to complete the whole segmentation process on each new brain tumor dataset.","This work was partially supported by the Ministry of Science and Technology (MoST), the National Center for Theoretical Sciences (NCTS), the Big Data Computing Center of Southeast University, the Nanjing Center for Applied Mathematics, the ST Yau Center in Taiwan, and the Shing-Tung Yau Center at Southeast University. W.-W. Lin was partially supported by MoST 109-2123-M-009-002-. T.-M. Huang was partially supported by MoST 108-2115-M-003-012-MY2. T. Li was supported in part by the National Natural Science Foundation of China (NSFC) 11971105. M.-H. Yueh was partially supported by MoST 109-2115-M-003-010-MY2. The implementation was partially performed on TianHe-2 thanks to the support of the National Supercomputing Center in Guangzhou (NSCC-GZ).",,Scientific Reports,,"Algorithms; Brain Mapping; Brain Neoplasms; Datasets as Topic; Humans; Imaging, Three-Dimensional; Neural Networks, Computer",2021-08-10,2021,2021-08-10,,11,1,14686,All OA, Gold,Article,"Lin, Wen-Wei; Juang, Cheng; Yueh, Mei-Heng; Huang, Tsung-Ming; Li, Tiexiang; Wang, Sheng; Yau, Shing-Tung","Lin, Wen-Wei (Department of Applied Mathematics, National Yang Ming Chiao Tung University, 300, Hsinchu, Taiwan); Juang, Cheng (Electronics Department, Ming Hsin University of Science and Technology, 304, Hsinchu, Taiwan); Yueh, Mei-Heng (Department of Mathematics, National Taiwan Normal University, 116, Taipei, Taiwan); Huang, Tsung-Ming (Department of Mathematics, National Taiwan Normal University, 116, Taipei, Taiwan); Li, Tiexiang (School of Mathematics, Southeast University, 211189, Nanjing, Peopleâs Republic of China; Nanjing Center for Applied Mathematics, 211135, Nanjing, Peopleâs Republic of China); Wang, Sheng (Department of Applied Mathematics, National Yang Ming Chiao Tung University, 300, Hsinchu, Taiwan); Yau, Shing-Tung (Department of Mathematics, Harvard University, Cambridge, USA)","Huang, Tsung-Ming (National Taiwan Normal University); Li, Tiexiang (Southeast University; )","Lin, Wen-Wei (National Yang Ming Chiao Tung University); Juang, Cheng (Minghsin University of Science and Technology); Yueh, Mei-Heng (National Taiwan Normal University); Huang, Tsung-Ming (National Taiwan Normal University); Li, Tiexiang (Southeast University); Wang, Sheng (National Yang Ming Chiao Tung University); Yau, Shing-Tung (Harvard University)",4,4,0.45,3.27,https://www.nature.com/articles/s41598-021-94071-1.pdf,https://app.dimensions.ai/details/publication/pub.1140316142,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
2208,pub.1154363801,10.1002/acm2.13898,36626026,PMC9924115,"Reinforcement learning in medical image analysis: Concepts, applications, challenges, and future directions","MOTIVATION: Medical image analysis involves a series of tasks used to assist physicians in qualitative and quantitative analyses of lesions or anatomical structures which can significantly improve the accuracy and reliability of medical diagnoses and prognoses. Traditionally, these tedious tasks were finished by experienced physicians or medical physicists and were marred with two major problems, low efficiency and bias. In the past decade, many machine learning methods have been applied to accelerate and automate the image analysis process. Compared to the enormous deployments of supervised and unsupervised learning models, attempts to use reinforcement learning in medical image analysis are still scarce. We hope that this review article could serve as the stepping stone for related research in the future.
SIGNIFICANCE: We found that although reinforcement learning has gradually gained momentum in recent years, many researchers in the medical analysis field still find it hard to understand and deploy in clinical settings. One possible cause is a lack of well-organized review articles intended for readers without professional computer science backgrounds. Rather than to provide a comprehensive list of all reinforcement learning models applied in medical image analysis, the aim of this review is to help the readers formulate and solve their medical image analysis research through the lens of reinforcement learning.
APPROACH & RESULTS: We selected published articles from Google Scholar and PubMed. Considering the scarcity of related articles, we also included some outstanding newest preprints. The papers were carefully reviewed and categorized according to the type of image analysis task. In this article, we first reviewed the basic concepts and popular models of reinforcement learning. Then, we explored the applications of reinforcement learning models in medical image analysis. Finally, we concluded the article by discussing the reviewed reinforcement learning approaches' limitations and possible future improvements.","This research is supported in part by the National Institutes of Health under Award Number R01CA215718, R56EB033332 and R01EB032680.",,Journal of Applied Clinical Medical Physics,,"Humans; Reproducibility of Results; Machine Learning; Image Processing, Computer-Assisted",2023-01-10,2023,2023-01-10,2023-02,24,2,e13898,All OA, Green,Article,"Hu, Mingzhe; Zhang, Jiahan; Matkovic, Luke; Liu, Tian; Yang, Xiaofeng","Hu, Mingzhe (Department of Radiation Oncology, School of Medicine, Emory University, Atlanta, Georgia, USA; Department of Computer Science and Informatics, Emory University, Atlanta, Georgia, USA); Zhang, Jiahan (Department of Radiation Oncology, School of Medicine, Emory University, Atlanta, Georgia, USA); Matkovic, Luke (Department of Radiation Oncology, School of Medicine, Emory University, Atlanta, Georgia, USA); Liu, Tian (Department of Radiation Oncology, School of Medicine, Emory University, Atlanta, Georgia, USA); Yang, Xiaofeng (Department of Radiation Oncology, School of Medicine, Emory University, Atlanta, Georgia, USA; Department of Computer Science and Informatics, Emory University, Atlanta, Georgia, USA)","Yang, Xiaofeng (Emory University; Emory University)","Hu, Mingzhe (Emory University; Emory University); Zhang, Jiahan (Emory University); Matkovic, Luke (Emory University); Liu, Tian (Emory University); Yang, Xiaofeng (Emory University; Emory University)",0,0,,,http://arxiv.org/pdf/2206.14302,https://app.dimensions.ai/details/publication/pub.1154363801,32 Biomedical and Clinical Sciences, 3208 Medical Physiology, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
2199,pub.1146703031,10.3390/e24040465,35455128,PMC9031516,Techniques and Algorithms for Hepatic Vessel Skeletonization in Medical Images: A Survey,"Hepatic vessel skeletonization serves as an important means of hepatic vascular analysis and vessel segmentation. This paper presents a survey of techniques and algorithms for hepatic vessel skeletonization in medical images. We summarized the latest developments and classical approaches in this field. These methods are classified into five categories according to their methodological characteristics. The overview and brief assessment of each category are provided in the corresponding chapters, respectively. We provide a comprehensive summary among the cited publications, image modalities and datasets from various aspects, which hope to reveal the pros and cons of every method, summarize its achievements and discuss the challenges and future trends.",The authors would like to thank the editors and anonymous reviewers for their valuable comments. This research is supported by the National Natural Science Foundation of China (No. 12090020 and No. 12090025), Zhejiang Provincial Natural Science Foundation of China under Grant No. LSD19H180005, and Zhejiang Provincial Science and Technology Project (No. 2019C03003).,,Entropy,,,2022-03-28,2022,2022-03-28,,24,4,465,All OA, Gold,Article,"Zhang, Jianfeng; Wu, Fa; Chang, Wanru; Kong, Dexing","Zhang, Jianfeng (School of Mathematical Sciences, Zhejiang University, Hangzhou 310027, China;, jfzhang2018@zju.edu.cn, (J.Z.);, 11935029@zju.edu.cn, (W.C.); College of Mathematical Medicine, Zhejiang Normal University, Jinhua 321004, China); Wu, Fa (Zhejiang Demetics Medical Technology Co., Ltd., Hangzhou 310012, China;, wufa85@zju.edu.cn); Chang, Wanru (School of Mathematical Sciences, Zhejiang University, Hangzhou 310027, China;, jfzhang2018@zju.edu.cn, (J.Z.);, 11935029@zju.edu.cn, (W.C.)); Kong, Dexing (School of Mathematical Sciences, Zhejiang University, Hangzhou 310027, China;, jfzhang2018@zju.edu.cn, (J.Z.);, 11935029@zju.edu.cn, (W.C.); College of Mathematical Medicine, Zhejiang Normal University, Jinhua 321004, China)","Kong, Dexing (Zhejiang University; Zhejiang Normal University)","Zhang, Jianfeng (Zhejiang University; Zhejiang Normal University); Wu, Fa (); Chang, Wanru (Zhejiang University); Kong, Dexing (Zhejiang University; Zhejiang Normal University)",1,1,,,https://www.mdpi.com/1099-4300/24/4/465/pdf?version=1648449616,https://app.dimensions.ai/details/publication/pub.1146703031,49 Mathematical Sciences, 51 Physical Sciences,,,,,,,,
2199,pub.1146338404,10.1016/j.cmpb.2022.106752,35338887,,H-ProSeg: Hybrid ultrasound prostate segmentation based on explainability-guided mathematical model,"BACKGROUND AND OBJECTIVE: Accurate and robust prostate segmentation in transrectal ultrasound (TRUS) images is of great interest for image-guided prostate interventions and prostate cancer diagnosis. However, it remains a challenging task for various reasons, including a missing or ambiguous boundary between the prostate and surrounding tissues, the presence of shadow artifacts, intra-prostate intensity heterogeneity, and anatomical variations.
METHODS: Here, we present a hybrid method for prostate segmentation (H-ProSeg) in TRUS images, using a small number of radiologist-defined seed points as the prior points. This method consists of three subnetworks. The first subnetwork uses an improved principal curve-based model to obtain data sequences consisting of seed points and their corresponding projection index. The second subnetwork uses an improved differential evolution-based artificial neural network for training to decrease the model error. The third subnetwork uses the parameters of the artificial neural network to explain the smooth mathematical description of the prostate contour. The performance of the H-ProSeg method was assessed in 55 brachytherapy patients using Dice similarity coefficient (DSC), Jaccard similarity coefficient (Î©), and accuracy (ACC) values.
RESULTS: The H-ProSeg method achieved excellent segmentation accuracy, with DSC, Î©, and ACC values of 95.8%, 94.3%, and 95.4%, respectively. Meanwhile, the DSC, Î©, and ACC values of the proposed method were as high as 93.3%, 91.9%, and 93%, respectively, due to the influence of Gaussian noise (standard deviation of Gaussian function, ÏÂ =Â 50). Although the Ï increased from 10 to 50, the DSC, Î©, and ACC values fluctuated by a maximum of approximately 2.5%, demonstrating the excellent robustness of our method.
CONCLUSIONS: Here, we present a hybrid method for accurate and robust prostate ultrasound image segmentation. The H-ProSeg method achieved superior performance compared with current state-of-the-art techniques. The knowledge of precise boundaries of the prostate is crucial for the conservation of risk structures. The proposed models have the potential to improve prostate cancer diagnosis and therapeutic outcomes.","This work was partly supported by Innovation and Technology Fund Projects, Hong Kong, No. ITS/080/19.",,Computer Methods and Programs in Biomedicine,,"Brachytherapy; Humans; Image Processing, Computer-Assisted; Male; Models, Theoretical; Prostate; Prostatic Neoplasms; Ultrasonography",2022-03-17,2022,2022-03-17,2022-06,219,,106752,Closed,Article,"Peng, Tao; Wu, Yiyun; Qin, Jing; Wu, Qingrong Jackie; Cai, Jing","Peng, Tao (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China.); Wu, Yiyun (Department of Medical Technology, Jiangsu Province Hospital, Nanjing, Jiangsu, China.); Qin, Jing (Department of Nursing, The Hong Kong Polytechnic University, Hong Kong, China.); Wu, Qingrong Jackie (Department of Radiation Oncology, Duke University Medical Center, Durham, NC, USA.); Cai, Jing (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China. Electronic address: jing.cai@polyu.edu.hk.)","Cai, Jing (Hong Kong Polytechnic University)","Peng, Tao (Hong Kong Polytechnic University); Wu, Yiyun (Jiangsu Province Hospital); Qin, Jing (Hong Kong Polytechnic University); Wu, Qingrong Jackie (Duke University Hospital); Cai, Jing (Hong Kong Polytechnic University)",5,5,,,,https://app.dimensions.ai/details/publication/pub.1146338404,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,
2197,pub.1151410179,10.3389/fninf.2022.1006532,36246394,PMC9554654,Accurate segmentation of neonatal brain MRI with deep learning,"An important step toward delivering an accurate connectome of the human brain is robust segmentation of 3D Magnetic Resonance Imaging (MRI) scans, which is particularly challenging when carried out on perinatal data. In this paper, we present an automated, deep learning-based pipeline for accurate segmentation of tissues from neonatal brain MRI and extend it by introducing an age prediction pathway. A major constraint to using deep learning techniques on developing brain data is the need to collect large numbers of ground truth labels. We therefore also investigate two practical approaches that can help alleviate the problem of label scarcity without loss of segmentation performance. First, we examine the efficiency of different strategies of distributing a limited budget of annotated 2D slices over 3D training images. In the second approach, we compare the segmentation performance of pre-trained models with different strategies of fine-tuning on a small subset of preterm infants. Our results indicate that distributing labels over a larger number of brain scans can improve segmentation performance. We also show that even partial fine-tuning can be superior in performance to a model trained from scratch, highlighting the relevance of transfer learning strategies under conditions of label scarcity. We illustrate our findings on large, publicly available T1- and T2-weighted MRI scans (n = 709, range of ages at scan: 26-45 weeks) obtained retrospectively from the Developing Human Connectome Project (dHCP) cohort.","AF would like to thank the UK Research and Innovation Centre for Doctoral Training in Artificial Intelligence for Healthcare for supporting his research in his role as a Senior Teaching Fellow (Grant number EP/S023283/1). The authors would also like to thank Dr. Benjamin Hou for his excellent technical feedback on the work. Data were provided by the developing Human Connectome Project, KCL-Imperial-Oxford Consortium funded by the European Research Council under the European Union Seventh Framework Programme (FP/2007-2013)/ERC Grant Agreement no. (319456). We are grateful to the families who generously supported this trial.",AF is supported by the UK Research and Innovation Centre for Doctoral Training in Artificial Intelligence for Healthcare under Grant EP/S023283/1.,Frontiers in Neuroinformatics,,,2022-09-28,2022,2022-09-28,,16,,1006532,All OA, Gold,Article,"Richter, Leonie; Fetit, Ahmed E.","Richter, Leonie (Department of Computing, Imperial College London, London, United Kingdom); Fetit, Ahmed E. (Department of Computing, Imperial College London, London, United Kingdom; UKRI CDT in Artificial Intelligence for Healthcare, Imperial College London, London, United Kingdom)","Richter, Leonie (Imperial College London)","Richter, Leonie (Imperial College London); Fetit, Ahmed E. (Imperial College London; Imperial College London)",0,0,,,https://www.frontiersin.org/articles/10.3389/fninf.2022.1006532/pdf,https://app.dimensions.ai/details/publication/pub.1151410179,32 Biomedical and Clinical Sciences, 3213 Paediatrics,,,,,,,,,,
2005,pub.1151185049,10.1016/j.media.2022.102628,36283200,,CrossMoDA 2021 challenge: Benchmark of cross-modality domain adaptation techniques for vestibular schwannoma and cochlea segmentation,"Domain Adaptation (DA) has recently been of strong interest in the medical imaging community. While a large variety of DA techniques have been proposed for image segmentation, most of these techniques have been validated either on private datasets or on small publicly available datasets. Moreover, these datasets mostly addressed single-class problems. To tackle these limitations, the Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in conjunction with the 24th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large and multi-class benchmark for unsupervised cross-modality Domain Adaptation. The goal of the challenge is to segment two key brain structures involved in the follow-up and treatment planning of vestibular schwannoma (VS): the VS and the cochleas. Currently, the diagnosis and surveillance in patients with VS are commonly performed using contrast-enhanced T1 (ceT1) MR imaging. However, there is growing interest in using non-contrast imaging sequences such as high-resolution T2 (hrT2) imaging. For this reason, we established an unsupervised cross-modality segmentation benchmark. The training dataset provides annotated ceT1 scans (N=105) and unpaired non-annotated hrT2 scans (N=105). The aim was to automatically perform unilateral VS and bilateral cochlea segmentation on hrT2 scans as provided in the testing set (N=137). This problem is particularly challenging given the large intensity distribution gap across the modalities and the small volume of the structures. A total of 55 teams from 16 countries submitted predictions to the validation leaderboard. Among them, 16 teams from 9 different countries submitted their algorithm for the evaluation phase. The level of performance reached by the top-performing teams is strikingly high (best median Dice score - VS: 88.4%; Cochleas: 85.7%) and close to full supervision (median Dice score - VS: 92.5%; Cochleas: 87.7%). All top-performing methods made use of an image-to-image translation approach to transform the source-domain images into pseudo-target-domain images. A segmentation network was then trained using these generated images and the manual annotations provided for the source image.","We would like to thank all the other team members that helped during the challenge: Sewon Kim, Yohan Jun, Taejoon Eo, Dosik Hwang (Samoyed); Fei Yu, Jie Zhao, Bin Dong (PKU_BIALAB); Can Cui, Dingjie Su, Andrew Mcneil (MIP); Xi Yang, Kaizhu Huang, Jie Sun (PremiLab); Yingyu Yang, Aurelien Maillot, Marta Nunez-Garcia, Maxime Sermesant (Epione-Liryc); Dewei Hu, Qibang Zhu, Kathleen E Larson, Huahong Zhang (MedICL); Mingming Gong (DBMI_pitt); Ran Gu, Shuwei Zhai, Wenhui Lei (Hi-Lib); Richard Osuala, Carlos MartÄ±n-Isla, Victor M. Campello, Carla Sendra-Balcells, Karim Lekadir (smriti161096); Mikhail Belyaev (IRA). This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) [NS/A000049/1, NS/A000050/1], MRC (MC/PC/180520) and Wellcome Trust [203145Z/16/Z, 203148/Z/16/Z, WT106882]. TV is supported by a Medtronic/Royal Academy of Engineering Research Chair [RCSRF1819/7/34]. Z.S and K.Y. are supported by the National Natural Science Foundation of China [No. 61876155], the Jiangsu Science and Technology Programme (Natural Science Foundation of Jiangsu Province) [No. BE2020006-4] and the Key Program Special Fund in Xiâan Jiaotong-Liverpool University (XJTLU) [KSF-E-37]. C.K. and M.H. are supported by the Federal Ministry of Education and Research [No. 031L0202B]. H.S. and H.G.K. are supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT [2019R1A2B5B01070488, 2021R1A4A1031437], Brain Research Program through the NRF funded by the Ministry of Science, ICT Future Planning [2018M3C7A1024734], Y-BASE RE Institute a Brain Korea 21, Yonsei University, and the Artificial Intelligence Graduate School Program, Yonsei University [No. 2020-0-01361]. H.Liu, Y.F. and B.D. are supported by the National Institute of Health (NIH) [R01 DC014462]. L.H. and M.H. are supported by the German Research Foundation (DFG) under grant number 320997906 [HE 7364/2-1]. S.J. and S.E. are supported by the Spanish project PID2019-105093GB-I00 and by ICREA under the ICREA Academia programme B.L. and V.K. are supported by the French Government, through the National Research Agency (ANR) 3IA CÃ´te dâAzur [ANR-19-P3IA-0002], IHU Liryc [ANR- 10-IAHU-04]. The Epione-Liryc team is grateful to the OPAL infrastructure from UniversitÃ© CÃ´te dâAzur for providing resources and support. H.Li and I.O are supported by the National Institute of Health (NIH) [R01-NS094456]. L.Z. and H.D. are supported by the Natural Science Foundation of China (NSFC) under Grants 81801778, 12090022, 11831002. Y.X. and K.B. are supported by NIH Award Number 1R01HL141813-01, NSF 1839332 Tripod+X, SAP SE, and Pennsylvaniaâs Department of Health and are grateful for the computational resources provided by Pittsburgh Super Computing grant number TG-ASC170024. S.B. is supported by the National Cancer Institute (NCI) and the National Institute of Neurological Disorders and Stroke (NINDS) of the National Institutes of Health (NIH) , under award numbers NCI:U01CA242871 and NINDS:R01NS042645. The content of this publication is solely the responsibility of the authors and does not represent the official views of the NIH.",,Medical Image Analysis,,"Humans; Neuroma, Acoustic",2022-09-21,2022,2022-09-21,2023-01,83,,102628,All OA, Green,Article,"Dorent, Reuben; Kujawa, Aaron; Ivory, Marina; Bakas, Spyridon; Rieke, Nicola; Joutard, Samuel; Glocker, Ben; Cardoso, Jorge; Modat, Marc; Batmanghelich, Kayhan; Belkov, Arseniy; Calisto, Maria Baldeon; Choi, Jae Won; Dawant, Benoit M; Dong, Hexin; Escalera, Sergio; Fan, Yubo; Hansen, Lasse; Heinrich, Mattias P; Joshi, Smriti; Kashtanova, Victoriya; Kim, Hyeon Gyu; Kondo, Satoshi; Kruse, Christian N; Lai-Yuen, Susana K; Li, Hao; Liu, Han; Ly, Buntheng; Oguz, Ipek; Shin, Hyungseob; Shirokikh, Boris; Su, Zixian; Wang, Guotai; Wu, Jianghao; Xu, Yanwu; Yao, Kai; Zhang, Li; Ourselin, SÃ©bastien; Shapey, Jonathan; Vercauteren, Tom","Dorent, Reuben (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom. Electronic address: reuben.dorent@kcl.ac.uk.); Kujawa, Aaron (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom.); Ivory, Marina (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom.); Bakas, Spyridon (Center for Biomedical Image Computing and Analytics (CBICA), University of Pennsylvania, Philadelphia, USA; Department of Pathology and Laboratory Medicine, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA; Department of Radiology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA.); Rieke, Nicola (NVIDIA, Germany.); Joutard, Samuel (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom.); Glocker, Ben (Department of Computing, Imperial College London, Department of Computing, London, United Kingdom.); Cardoso, Jorge (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom.); Modat, Marc (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom.); Batmanghelich, Kayhan (Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, USA.); Belkov, Arseniy (Moscow Institute of Physics and Technology, Moscow, Russia.); Calisto, Maria Baldeon (Universidad San Francisco de Quito, Quito, Ecuador.); Choi, Jae Won (Department of Radiology, Armed Forces Yangju Hospital, Yangju, Republic of Korea.); Dawant, Benoit M (Vanderbilt University, Nashville, USA.); Dong, Hexin (Center for Data Science, Peking University, Beijing, China.); Escalera, Sergio (Artificial Intelligence in Medicine Lab (BCN-AIM) and Human Behavior Analysis Lab (HuPBA), Universitat de Barcelona, Barcelona, Spain.); Fan, Yubo (Vanderbilt University, Nashville, USA.); Hansen, Lasse (Institute of Medical Informatics, UniversitÃ¤t zu LÃ¼beck, Germany.); Heinrich, Mattias P (Institute of Medical Informatics, UniversitÃ¤t zu LÃ¼beck, Germany.); Joshi, Smriti (Artificial Intelligence in Medicine Lab (BCN-AIM) and Human Behavior Analysis Lab (HuPBA), Universitat de Barcelona, Barcelona, Spain.); Kashtanova, Victoriya (Inria, UniversitÃ© CÃ´te d'Azur, Sophia Antipolis, France.); Kim, Hyeon Gyu (School of Electrical and Electronic Engineering, Yonsei University, Seoul, Republic of Korea.); Kondo, Satoshi (Muroran Institute of Technology, Muroran, Japan.); Kruse, Christian N (Institute of Medical Informatics, UniversitÃ¤t zu LÃ¼beck, Germany.); Lai-Yuen, Susana K (University of South Florida, Tampa, USA.); Li, Hao (Vanderbilt University, Nashville, USA.); Liu, Han (Vanderbilt University, Nashville, USA.); Ly, Buntheng (Inria, UniversitÃ© CÃ´te d'Azur, Sophia Antipolis, France.); Oguz, Ipek (Vanderbilt University, Nashville, USA.); Shin, Hyungseob (School of Electrical and Electronic Engineering, Yonsei University, Seoul, Republic of Korea.); Shirokikh, Boris (Skolkovo Institute of Science and Technology, Moscow, Russia; Artificial Intelligence Research Institute (AIRI), Moscow, Russia.); Su, Zixian (University of Liverpool, Liverpool, United Kingdom; School of Advanced Technology, Xi'an Jiaotong-Liverpool University, Suzhou, China.); Wang, Guotai (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China.); Wu, Jianghao (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China.); Xu, Yanwu (Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, USA.); Yao, Kai (University of Liverpool, Liverpool, United Kingdom; School of Advanced Technology, Xi'an Jiaotong-Liverpool University, Suzhou, China.); Zhang, Li (Center for Data Science, Peking University, Beijing, China.); Ourselin, SÃ©bastien (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom.); Shapey, Jonathan (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom; Department of Neurosurgery, King's College Hospital, London, United Kingdom.); Vercauteren, Tom (School of Biomedical Engineering & Imaging Sciences, King's College London, London, United Kingdom.)","Dorent, Reuben (King's College London)","Dorent, Reuben (King's College London); Kujawa, Aaron (King's College London); Ivory, Marina (King's College London); Bakas, Spyridon (University of Pennsylvania Health System; University of Pennsylvania); Rieke, Nicola (); Joutard, Samuel (King's College London); Glocker, Ben (Imperial College London); Cardoso, Jorge (King's College London); Modat, Marc (King's College London); Batmanghelich, Kayhan (University of Pittsburgh); Belkov, Arseniy (Moscow Institute of Physics and Technology); Calisto, Maria Baldeon (Universidad San Francisco de Quito); Choi, Jae Won (); Dawant, Benoit M (Vanderbilt University); Dong, Hexin (Peking University); Escalera, Sergio (University of Barcelona); Fan, Yubo (Vanderbilt University); Hansen, Lasse (University of LÃ¼beck); Heinrich, Mattias P (University of LÃ¼beck); Joshi, Smriti (University of Barcelona); Kashtanova, Victoriya (); Kim, Hyeon Gyu (Yonsei University); Kondo, Satoshi (Muroran Institute of Technology); Kruse, Christian N (University of LÃ¼beck); Lai-Yuen, Susana K (University of South Florida); Li, Hao (Vanderbilt University); Liu, Han (Vanderbilt University); Ly, Buntheng (); Oguz, Ipek (Vanderbilt University); Shin, Hyungseob (Yonsei University); Shirokikh, Boris (Skolkovo Institute of Science and Technology); Su, Zixian (University of Liverpool; Xiâan Jiaotong-Liverpool University); Wang, Guotai (University of Electronic Science and Technology of China); Wu, Jianghao (University of Electronic Science and Technology of China); Xu, Yanwu (University of Pittsburgh); Yao, Kai (University of Liverpool; Xiâan Jiaotong-Liverpool University); Zhang, Li (Peking University); Ourselin, SÃ©bastien (King's College London); Shapey, Jonathan (King's College London; King's College Hospital); Vercauteren, Tom (King's College London)",11,11,,,http://arxiv.org/pdf/2201.02831,https://app.dimensions.ai/details/publication/pub.1151185049,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
1945,pub.1132460023,10.1002/mp.14391,33169365,,Evaluation of multislice inputs to convolutional neural networks for medical image segmentation,"PURPOSE: When using convolutional neural networks (CNNs) for segmentation of organs and lesions in medical images, the conventional approach is to work with inputs and outputs either as single slice [two-dimensional (2D)] or whole volumes [three-dimensional (3D)]. One common alternative, in this study denoted as pseudo-3D, is to use a stack of adjacent slices as input and produce a prediction for at least the central slice. This approach gives the network the possibility to capture 3D spatial information, with only a minor additional computational cost.
METHODS: In this study, we systematically evaluate the segmentation performance and computational costs of this pseudo-3D approach as a function of the number of input slices, and compare the results to conventional end-to-end 2D and 3D CNNs, and to triplanar orthogonal 2D CNNs. The standard pseudo-3D method regards the neighboring slices as multiple input image channels. We additionally design and evaluate a novel, simple approach where the input stack is a volumetric input that is repeatably convolved in 3D to obtain a 2D feature map. This 2D map is in turn fed into a standard 2D network. We conducted experiments using two different CNN backbone architectures and on eight diverse data sets covering different anatomical regions, imaging modalities, and segmentation tasks.
RESULTS: We found that while both pseudo-3D methods can process a large number of slices at once and still be computationally much more efficient than fully 3D CNNs, a significant improvement over a regular 2D CNN was only observed with two of the eight data sets. triplanar networks had the poorest performance of all the evaluated models. An analysis of the structural properties of the segmentation masks revealed no relations to the segmentation performance with respect to the number of input slices. A post hoc rank sum test which combined all metrics and data sets yielded that only our newly proposed pseudo-3D method with an input size of 13 slices outperformed almost all methods.
CONCLUSION: In the general case, multislice inputs appear not to improve segmentation results over using 2D or 3D CNNs. For the particular case of 13 input slices, the proposed novel pseudo-3D method does appear to have a slight advantage across all data sets compared to all other methods evaluated in this work.","The computations were performed on resources provided by the Swedish National Infrastructure for Computing (SNIC) at the HPC2N in UmeÃ¥, Sweden. We are grateful for the financial support obtained from the Cancer Research Fund in Northern Sweden, Karin and Krister Olsson, UmeÃ¥ University, The VÃ¤sterbotten regional county, and Vinnova, the Swedish innovation agency.",,Medical Physics,,"Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Neural Networks, Computer; Spine",2020-11-10,2020,2020-11-10,2020-12,47,12,6216-6231,All OA, Hybrid,Article,"Vu, Minh H.; Grimbergen, Guus; Nyholm, Tufve; LÃ¶fstedt, Tommy","Vu, Minh H. (Department of Radiation Sciences, UmeÃ¥ University, UmeÃ¥, Sweden); Grimbergen, Guus (Department of Biomedical Engineering, Eindhoven University of Technology, Eindhoven, 5612 AZ, the Netherlands); Nyholm, Tufve (Department of Radiation Sciences, UmeÃ¥ University, UmeÃ¥, Sweden); LÃ¶fstedt, Tommy (Department of Radiation Sciences, UmeÃ¥ University, UmeÃ¥, Sweden)","LÃ¶fstedt, Tommy (UmeÃ¥ University)","Vu, Minh H. (UmeÃ¥ University); Grimbergen, Guus (Eindhoven University of Technology); Nyholm, Tufve (UmeÃ¥ University); LÃ¶fstedt, Tommy (UmeÃ¥ University)",24,24,1.23,9.84,https://doi.org/10.1002/mp.14391,https://app.dimensions.ai/details/publication/pub.1132460023,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
1934,pub.1151016560,10.3390/bioengineering9090467,36135013,PMC9495456,Decision Support System for Liver Lesion Segmentation Based on Advanced Convolutional Neural Network Architectures,"Given its essential role in body functions, liver cancer is the third most common cause of death from cancer, despite being the sixth most common type of cancer worldwide. Following advancements in medicine and image processing, medical image segmentation methods are receiving a great deal of attention. As a novelty, the paper proposes an intelligent decision system for segmenting liver and hepatic tumors by integrating four efficient neural networks (ResNet152, ResNeXt101, DenseNet201, and InceptionV3). Images from computed tomography for training, validation, and testing were taken from the public LiTS17 database and preprocessed to better highlight liver tissue and tumors. Global segmentation is done by separately training individual classifiers and the global system of merging individual decisions. For the aforementioned application, classification neural networks have been modified for semantic segmentation. After segmentation based on the neural network system, the images were postprocessed to eliminate artifacts. The segmentation results obtained by the system were better, from the point of view of the Dice coefficient, than those obtained by the individual networks, and comparable with those reported in recent works.",,This research received no external funding.,Bioengineering,,,2022-09-13,2022,2022-09-13,,9,9,467,All OA, Gold,Article,"Popescu, Dan; Stanciulescu, Andrei; Pomohaci, Mihai Dan; Ichim, Loretta","Popescu, Dan (Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, 060042 Bucharest, Romania.); Stanciulescu, Andrei (Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, 060042 Bucharest, Romania.); Pomohaci, Mihai Dan (Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, 060042 Bucharest, Romania.); Ichim, Loretta (Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, 060042 Bucharest, Romania.)","Popescu, Dan (Polytechnic University of Bucharest)","Popescu, Dan (Polytechnic University of Bucharest); Stanciulescu, Andrei (Polytechnic University of Bucharest); Pomohaci, Mihai Dan (Polytechnic University of Bucharest); Ichim, Loretta (Polytechnic University of Bucharest)",0,0,,,https://www.mdpi.com/2306-5354/9/9/467/pdf?version=1663128665,https://app.dimensions.ai/details/publication/pub.1151016560,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,
1920,pub.1151306042,10.1016/j.nicl.2022.103205,36201950,PMC9668629,"Cortical lesions, central vein sign, and paramagnetic rim lesions in multiple sclerosis: Emerging machine learning techniques and future avenues","The current diagnostic criteria for multiple sclerosis (MS) lack specificity, and this may lead to misdiagnosis, which remains an issue in present-day clinical practice. In addition, conventional biomarkers only moderately correlate with MS disease progression. Recently, some MS lesional imaging biomarkers such as cortical lesions (CL), the central vein sign (CVS), and paramagnetic rim lesions (PRL), visible in specialized magnetic resonance imaging (MRI) sequences, have shown higher specificity in differential diagnosis. Moreover, studies have shown that CL and PRL are potential prognostic biomarkers, the former correlating with cognitive impairments and the latter with early disability progression. As machine learning-based methods have achieved extraordinary performance in the assessment of conventional imaging biomarkers, such as white matter lesion segmentation, several automated or semi-automated methods have been proposed as well for CL, PRL, and CVS. In the present review, we first introduce these MS biomarkers and their imaging methods. Subsequently, we describe the corresponding machine learning-based methods that were proposed to tackle these clinical questions, putting them into context with respect to the challenges they are facing, including non-standardized MRI protocols, limited datasets, and moderate inter-rater variability. We conclude by presenting the current limitations that prevent their broader deployment and suggesting future research directions.","F.L.R. is supported by the Swiss National Foundation (SNF) Postdoc Mobility Fellowship (P500PB_206833), the European Unionâs Horizon 2020 research and innovation program under the Marie Sklodowska-Curie project TRABIT (agreement No 765148) and the Novartis Foundation for Medical-Biological Research (#21A032). M.W. is supported by a Swiss government excellence scholarship (#2021.0087). O.A. is supported by a National Multiple Sclerosis Society (NMSS) - American Brain Foundation Clinician Scientist Development Award (FAN-1807â32163). E.S.B. is supported by a Career Transition Fellowship from the National Multiple Sclerosis Society. P.S., O.A., E.S.B., and D.S.R. are supported by the Intramural Research Program of the National Institute of Neurological Disorders and Stroke, National Institutes of Health, Bethesda, Maryland, USA. R.T.S. is partially supported by R01NS060910, R01MH112847, R01MH123550, U01NS116776, and R01NS112274 from the National Institutes of Health. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. M.A. is supported by the Conrad N. Hilton Foundation (Marilyn Hilton Bridging Award for Physician-Scientists, grant #17313), the International Progressive MS alliance, the Roche Foundation for Independent Research, the Cariplo Foundation (grant #1677), and the FRRB Early Career Award (grant#1750327). We acknowledge access to the facilities and expertise of the CIBM Center for Biomedical Imaging, a Swiss research center of excellence founded and supported by Lausanne University Hospital (CHUV), University of Lausanne (UNIL), Ecole Polytechnique fedÃ©rale de Lausanne (EPFL), University of Geneva (UNIGE) and Geneva University Hospitals (HUG). We thank Thomas Yu for proofreading the manuscript.",,NeuroImage Clinical,,Humans, Multiple Sclerosis, White Matter, Magnetic Resonance Imaging, Veins, Machine Learning, Brain,2022-09-24,2022,2022-09-24,2022,36,,103205,All OA, Gold,Article,"La Rosa, Francesco; Wynen, Maxence; Al-Louzi, Omar; Beck, Erin S; Huelnhagen, Till; Maggi, Pietro; Thiran, Jean-Philippe; Kober, Tobias; Shinohara, Russell T; Sati, Pascal; Reich, Daniel S; Granziera, Cristina; Absinta, Martina; Cuadra, Meritxell Bach","La Rosa, Francesco (Signal Processing Laboratory (LTS5), Ecole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL), Lausanne, Switzerland; CIBM Center for Biomedical Imaging, Switzerland; Department of Neurology, Icahn School of Medicine at Mount Sinai, New York, NY, USA); Wynen, Maxence (CIBM Center for Biomedical Imaging, Switzerland; ICTeam, UCLouvain, Louvain-la-Neuve, Belgium; Louvain Inflammation Imaging Lab (NIL), Institute of Neuroscience (IoNS), UCLouvain, Brussels, Belgium; Radiology Department, Lausanne University and University Hospital, Switzerland); Al-Louzi, Omar (Translational Neuroradiology Section, National Institute of Neurological Disorders and Stroke, National Institutes of Health, Bethesda, MD, USA; Department of Neurology, Cedars-Sinai Medical Center, Los Angeles, CA, USA); Beck, Erin S (Department of Neurology, Icahn School of Medicine at Mount Sinai, New York, NY, USA; Translational Neuroradiology Section, National Institute of Neurological Disorders and Stroke, National Institutes of Health, Bethesda, MD, USA); Huelnhagen, Till (Signal Processing Laboratory (LTS5), Ecole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL), Lausanne, Switzerland; Radiology Department, Lausanne University and University Hospital, Switzerland; Advanced Clinical Imaging Technology, Siemens Healthcare AG, Lausanne, Switzerland); Maggi, Pietro (Louvain Inflammation Imaging Lab (NIL), Institute of Neuroscience (IoNS), UCLouvain, Brussels, Belgium; Department of Neurology, Cliniques universitaires Saint-Luc, UniversitÃ© catholique de Louvain, Brussels, Belgium; Department of Neurology, CHUV, Lausanne, Switzerland); Thiran, Jean-Philippe (Signal Processing Laboratory (LTS5), Ecole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL), Lausanne, Switzerland; CIBM Center for Biomedical Imaging, Switzerland; Radiology Department, Lausanne University and University Hospital, Switzerland); Kober, Tobias (Signal Processing Laboratory (LTS5), Ecole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL), Lausanne, Switzerland; Radiology Department, Lausanne University and University Hospital, Switzerland; Advanced Clinical Imaging Technology, Siemens Healthcare AG, Lausanne, Switzerland); Shinohara, Russell T (Center for Biomedical Image Computing and Analysis (CBICA), Department of Radiology, University of Pennsylvania, Philadelphia, PA, USA; Penn Statistics in Imaging and Visualization Endeavor (PennSIVE), Center for Clinical Epidemiology and Biostatistics, University of Pennsylvania, Philadelphia, PA, USA; Department of Biostatistics, Epidemiology, and Informatics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA); Sati, Pascal (Translational Neuroradiology Section, National Institute of Neurological Disorders and Stroke, National Institutes of Health, Bethesda, MD, USA; Department of Neurology, Cedars-Sinai Medical Center, Los Angeles, CA, USA); Reich, Daniel S (Translational Neuroradiology Section, National Institute of Neurological Disorders and Stroke, National Institutes of Health, Bethesda, MD, USA); Granziera, Cristina (Translational Imaging in Neurology (ThINk) Basel, Department of Biomedical Engineering, Faculty of Medicine, University Hospital Basel and University of Basel, Switzerland; Neurologic Clinic and Policlinic, MS Center and Research Center for Clinical Neuroimmunology and Neuroscience Basel (RC2NB), University Hospital Basel and University of Basel, Basel, Switzerland); Absinta, Martina (IRCCS San Raffaele Hospital and Vita-Salute San Raffaele University, Milan, Italy; Department of Neurology, Johns Hopkins University School of Medicine, Baltimore, MD, USA); Cuadra, Meritxell Bach (CIBM Center for Biomedical Imaging, Switzerland; Radiology Department, Lausanne University and University Hospital, Switzerland)","La Rosa, Francesco (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne; Centre d'Imagerie BioMedicale; Icahn School of Medicine at Mount Sinai)","La Rosa, Francesco (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne; Centre d'Imagerie BioMedicale; Icahn School of Medicine at Mount Sinai); Wynen, Maxence (Centre d'Imagerie BioMedicale; UniversitÃ© Catholique de Louvain; UniversitÃ© Catholique de Louvain; University of Lausanne); Al-Louzi, Omar (National Institute of Neurological Disorders and Stroke; Cedars-Sinai Medical Center); Beck, Erin S (Icahn School of Medicine at Mount Sinai; National Institute of Neurological Disorders and Stroke); Huelnhagen, Till (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne; University of Lausanne); Maggi, Pietro (UniversitÃ© Catholique de Louvain; Cliniques Universitaires Saint-Luc; HÃ´pital OrthopÃ©dique de la Suisse Romande); Thiran, Jean-Philippe (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne; Centre d'Imagerie BioMedicale; University of Lausanne); Kober, Tobias (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne; University of Lausanne); Shinohara, Russell T (University of Pennsylvania; University of Pennsylvania; University of Pennsylvania); Sati, Pascal (National Institute of Neurological Disorders and Stroke; Cedars-Sinai Medical Center); Reich, Daniel S (National Institute of Neurological Disorders and Stroke); Granziera, Cristina (University of Basel; University of Basel); Absinta, Martina (IRCCS Ospedale San Raffaele; Johns Hopkins University); Cuadra, Meritxell Bach (Centre d'Imagerie BioMedicale; University of Lausanne)",1,1,,,https://doi.org/10.1016/j.nicl.2022.103205,https://app.dimensions.ai/details/publication/pub.1151306042,32 Biomedical and Clinical Sciences, 3209 Neurosciences,,,,
1920,pub.1137544052,10.1088/1361-6560/abfbf4,33906186,,Anatomy-aided deep learning for medical image segmentation: a review,"Deep learning (DL) has become widely used for medical image segmentation in recent years. However, despite these advances, there are still problems for which DL-based segmentation fails. Recently, some DL approaches had a breakthrough by using anatomical information which is the crucial cue for manual segmentation. In this paper, we provide a review of anatomy-aided DL for medical image segmentation which covers systematically summarized anatomical information categories and corresponding representation methods. We address known and potentially solvable challenges in anatomy-aided DL and present a categorized methodology overview on using anatomical information with DL from over 70 papers. Finally, we discuss the strengths and limitations of the current anatomy-aided DL approaches and suggest potential future work.","The authors would like to thank the scientific advice provided by Prof R Vliegenthart from the Department of Radiology, University Medical Center Groningen. This work is supported by ZonMw under project B3CARE (project number:104006003). This project has received funding from the EU Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 777 826. JW and CB acknowledge support by the Dutch 4TU HTSF program Precision Medicine.",,Physics in Medicine and Biology,,"Deep Learning; Image Processing, Computer-Assisted",2021-05-26,2021,2021-05-26,2021-06-07,66,11,11tr01,All OA, Hybrid,Article,"Liu, Lu; Wolterink, Jelmer M; Brune, Christoph; Veldhuis, Raymond N J","Liu, Lu (Applied Analysis, Department of Applied Mathematics, Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, Drienerlolaan 5, 7522 NB, Enschede, The Netherlands; Data Management and Biometrics, Department of Computer Science, Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, Drienerlolaan 5, 7522 NB, Enschede, The Netherlands); Wolterink, Jelmer M (Applied Analysis, Department of Applied Mathematics, Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, Drienerlolaan 5, 7522 NB, Enschede, The Netherlands); Brune, Christoph (Applied Analysis, Department of Applied Mathematics, Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, Drienerlolaan 5, 7522 NB, Enschede, The Netherlands); Veldhuis, Raymond N J (Data Management and Biometrics, Department of Computer Science, Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, Drienerlolaan 5, 7522 NB, Enschede, The Netherlands)","Liu, Lu (University of Twente; University of Twente)","Liu, Lu (University of Twente; University of Twente); Wolterink, Jelmer M (University of Twente); Brune, Christoph (University of Twente); Veldhuis, Raymond N J (University of Twente)",24,24,5.16,30.05,https://doi.org/10.1088/1361-6560/abfbf4,https://app.dimensions.ai/details/publication/pub.1137544052,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
1850,pub.1154965332,10.1016/j.media.2023.102762,36738650,,"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives","Transformer, one of the latest technological advances of deep learning, has gained prevalence in natural language processing or computer vision. Since medical imaging bear some resemblance to computer vision, it is natural to inquire about the status quo of Transformers in medical imaging and ask the question: can the Transformer models transform medical imaging? In this paper, we attempt to make a response to the inquiry. After a brief introduction of the fundamentals of Transformers, especially in comparison with convolutional neural networks (CNNs), and highlighting key defining properties that characterize the Transformers, we offer a comprehensive review of the state-of-the-art Transformer-based approaches for medical imaging and exhibit current research progresses made in the areas of medical image segmentation, recognition, detection, registration, reconstruction, enhancement, etc. In particular, what distinguishes our review lies in its organization based on the Transformer's key defining properties, which are mostly derived from comparing the Transformer and CNN, and its type of architecture, which specifies the manner in which the Transformer and CNN are combined, all helping the readers to best understand the rationale behind the reviewed approaches. We conclude with discussions of future perspectives.","Li and Zhou are supported by National Natural Science Foundation of China (NSFC) under grant No. 62271465. Chen is supported by U01-CA140204 and R01-EB031023 from the National Institutes of Health, USA .",,Medical Image Analysis,,"Humans; Diagnostic Imaging; Neural Networks, Computer",2023-01-31,2023,2023-01-31,2023-04,85,,102762,All OA, Green,Article,"Li, Jun; Chen, Junyu; Tang, Yucheng; Wang, Ce; Landman, Bennett A; Zhou, S Kevin","Li, Jun (Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China.); Chen, Junyu (Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins Medical Institutes, Baltimore, MD, USA.); Tang, Yucheng (Department of Electrical and Computer Engineering, Vanderbilt University, Nashville, TN, USA.); Wang, Ce (Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China.); Landman, Bennett A (Department of Electrical and Computer Engineering, Vanderbilt University, Nashville, TN, USA.); Zhou, S Kevin (Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China; School of Biomedical Engineering & Suzhou Institute for Advanced Research, Center for Medical Imaging, Robotics, and Analytic Computing & Learning (MIRACLE), University of Science and Technology of China, Suzhou 215123, China. Electronic address: skevinzhou@ustc.edu.cn.)","Zhou, S Kevin (Institute of Computing Technology; University of Science and Technology of China)","Li, Jun (Institute of Computing Technology); Chen, Junyu (Johns Hopkins University); Tang, Yucheng (Vanderbilt University); Wang, Ce (Institute of Computing Technology); Landman, Bennett A (Vanderbilt University); Zhou, S Kevin (Institute of Computing Technology; University of Science and Technology of China)",3,3,,,http://arxiv.org/pdf/2206.01136,https://app.dimensions.ai/details/publication/pub.1154965332,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
1774,pub.1152678177,10.1016/j.clinimag.2022.11.003,36462229,,A comprehensive survey of deep learning research on medical image analysis with focus on transfer learning,"This survey aims to identify commonly used methods, datasets, future trends, knowledge gaps, constraints, and limitations in the field to provide an overview of current solutions used in medical image analysis in parallel with the rapid developments in transfer learning (TL). Unlike previous studies, this survey grouped the last five years of current studies for the period between January 2017 and February 2021 according to different anatomical regions and detailed the modality, medical task, TL method, source data, target data, and public or private datasets used in medical imaging. Also, it provides readers with detailed information on technical challenges, opportunities, and future research trends. In this way, an overview of recent developments is provided to help researchers to select the most effective and efficient methods and access widely used and publicly available medical datasets, research gaps, and limitations of the available literature.",,,Clinical Imaging,,Humans, Deep Learning, Evidence Gaps,2022-11-12,2022,2022-11-12,2023-02,94,,18-41,Closed,Article,"Atasever, Sema; Azginoglu, Nuh; Terzi, Duygu Sinanc; Terzi, Ramazan","Atasever, Sema (Computer Engineering Department, Nevsehir HacÄ± Bektas Veli University, Nevsehir, Turkey. Electronic address: sema@nevsehir.edu.tr.); Azginoglu, Nuh (Computer Engineering Department, Kayseri University, Kayseri, Turkey. Electronic address: nuhazginoglu@kayseri.edu.tr.); Terzi, Duygu Sinanc (Computer Engineering Department, Amasya University, Amasya, Turkey. Electronic address: duygu.terzi@amasya.edu.tr.); Terzi, Ramazan (Computer Engineering Department, Amasya University, Amasya, Turkey. Electronic address: ramazan.terzi@amasya.edu.tr.)","Atasever, Sema (NevÅehir HacÄ± BektaÅ Veli University)","Atasever, Sema (NevÅehir HacÄ± BektaÅ Veli University); Azginoglu, Nuh (); Terzi, Duygu Sinanc (Amasya University); Terzi, Ramazan (Amasya University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1152678177,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,
1654,pub.1139964265,10.1016/j.media.2021.102193,34371440,,Deep reinforcement learning in medical imaging: A literature review,"Deep reinforcement learning (DRL) augments the reinforcement learning framework, which learns a sequence of actions that maximizes the expected reward, with the representative power of deep neural networks. Recent works have demonstrated the great potential of DRL in medicine and healthcare. This paper presents a literature review of DRL in medical imaging. We start with a comprehensive tutorial of DRL, including the latest model-free and model-based algorithms. We then cover existing DRL applications for medical imaging, which are roughly divided into three main categories: (i) parametric medical image analysis tasks including landmark detection, object/lesion detection, registration, and view plane localization; (ii) solving optimization tasks including hyperparameter tuning, selecting augmentation strategies, and neural architecture search; and (iii) miscellaneous applications including surgical gesture segmentation, personalized mobile health intervention, and computational model personalization. The paper concludes with discussions of future perspectives.",The work of Thi Hoang Ngan Le is partly supported supported by the National Science Foundation under Award No OIA-1946391. The work of Hien Van Nguyen is partly supported by the National Science Foundation (1910973).,,Medical Image Analysis,,"Algorithms; Diagnostic Imaging; Forecasting; Humans; Learning; Neural Networks, Computer",2021-07-27,2021,2021-07-27,2021-10,73,,102193,All OA, Green,Article,"Zhou, S Kevin; Le, Hoang Ngan; Luu, Khoa; V Nguyen, Hien; Ayache, Nicholas","Zhou, S Kevin (Medical Imaging, Robotics, and Analytic Computing Laboratory and Enigineering (MIRACLE) Center, School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China. Electronic address: skevinzhou@ustc.edu.com.); Le, Hoang Ngan (CSCE Department, University of Arkansas, US. Electronic address: lthngan@gmail.com.); Luu, Khoa (CSCE Department, University of Arkansas, US.); V Nguyen, Hien (ECE Department, University of Houston, US.); Ayache, Nicholas (INRIA, Sophia Antipolis-Mediterranean Centre, France.)","Zhou, S Kevin (University of Science and Technology of China; Institute of Computing Technology)","Zhou, S Kevin (University of Science and Technology of China; Institute of Computing Technology); Le, Hoang Ngan (University of Arkansas at Fayetteville); Luu, Khoa (University of Arkansas at Fayetteville); V Nguyen, Hien (University of Houston); Ayache, Nicholas (French Institute for Research in Computer Science and Automation)",45,45,4.39,,http://arxiv.org/pdf/2103.05115,https://app.dimensions.ai/details/publication/pub.1139964265,32 Biomedical and Clinical Sciences, 40 Engineering,3 Good Health and Well Being,,,,,,,,,
1644,pub.1148221787,10.1016/j.media.2022.102488,35667327,,Robust deep learning-based semantic organ segmentation in hyperspectral images,"Semantic image segmentation is an important prerequisite for context-awareness and autonomous robotics in surgery. The state of the art has focused on conventional RGB video data acquired during minimally invasive surgery, but full-scene semantic segmentation based on spectral imaging data and obtained during open surgery has received almost no attention to date. To address this gap in the literature, we are investigating the following research questions based on hyperspectral imaging (HSI) data of pigs acquired in an open surgery setting: (1) What is an adequate representation of HSI data for neural network-based fully automated organ segmentation, especially with respect to the spatial granularity of the data (pixels vs. superpixels vs. patches vs. full images)? (2) Is there a benefit of using HSI data compared to other modalities, namely RGB data and processed HSI data (e.g. tissue parameters like oxygenation), when performing semantic organ segmentation? According to a comprehensive validation study based on 506 HSI images from 20 pigs, annotated with a total of 19 classes, deep learning-based segmentation performance increases - consistently across modalities - with the spatial context of the input data. Unprocessed HSI data offers an advantage over RGB data or processed data from the camera provider, with the advantage increasing with decreasing size of the input to the neural network. Maximum performance (HSI applied to whole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04), which is in the range of the inter-rater variability (DSC of 0.89 ((standard deviation (SD)) 0.07)). We conclude that HSI could become a powerful image modality for fully-automatic surgical scene understanding with many advantages over traditional imaging, including the ability to recover additional functional tissue information. Our code and pre-trained models are available at https://github.com/IMSY-DKFZ/htc.","This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (NEURAL SPICING, grant agreement No. 101002198) and was supported by the German Cancer Research Center (DKFZ) and the Helmholtz Association under the joint research school HIDSS4Health (Helmholtz Information and Data Science School for Health). Martin Wagner, Lena Maier-Hein and Beat P. MÃ¼ller-Stich worked with the medical device manufacturer KARL STORZ SE Co. KG in the projects âInnOPlanâ and âOP 4.1â, funded by the German Federal Ministry of Economic Affairs and Energy (grant agreement No. BMWI 01MD15002E and BMWI 01MT17001E) and âSurgomicsâ, funded by the German Federal Ministry of Health (grant agreement No. BMG 2520DAT82D).",,Medical Image Analysis,,"Animals; Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer; Semantics; Swine",2022-05-27,2022,2022-05-27,2022-08,80,,102488,All OA, Hybrid,Article,"Seidlitz, Silvia; Sellner, Jan; Odenthal, Jan; Ãzdemir, Berkin; Studier-Fischer, Alexander; KnÃ¶dler, Samuel; Ayala, Leonardo; Adler, Tim J; Kenngott, Hannes G; Tizabi, Minu; Wagner, Martin; Nickel, Felix; MÃ¼ller-Stich, Beat P; Maier-Hein, Lena","Seidlitz, Silvia (Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany; Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany. Electronic address: s.seidlitz@dkfz-heidelberg.de.); Sellner, Jan (Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany; Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany. Electronic address: j.sellner@dkfz-heidelberg.de.); Odenthal, Jan (Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany; Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany.); Ãzdemir, Berkin (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany.); Studier-Fischer, Alexander (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany.); KnÃ¶dler, Samuel (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany.); Ayala, Leonardo (Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany.); Adler, Tim J (Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany.); Kenngott, Hannes G (Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany; Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany.); Tizabi, Minu (Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany.); Wagner, Martin (Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany; Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany.); Nickel, Felix (Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany; Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany.); MÃ¼ller-Stich, Beat P (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany.); Maier-Hein, Lena (Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany; Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany; HIP Helmholtz Imaging Platform, German Cancer Research Center (DKFZ), Heidelberg, Germany; Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany.)","Seidlitz, Silvia (German Cancer Research Center; )","Seidlitz, Silvia (German Cancer Research Center); Sellner, Jan (German Cancer Research Center); Odenthal, Jan (German Cancer Research Center; University Hospital Heidelberg); Ãzdemir, Berkin (University Hospital Heidelberg; Heidelberg University); Studier-Fischer, Alexander (University Hospital Heidelberg; Heidelberg University); KnÃ¶dler, Samuel (University Hospital Heidelberg; Heidelberg University); Ayala, Leonardo (German Cancer Research Center; Heidelberg University); Adler, Tim J (German Cancer Research Center; Heidelberg University); Kenngott, Hannes G (University Hospital Heidelberg); Tizabi, Minu (German Cancer Research Center); Wagner, Martin (University Hospital Heidelberg; Heidelberg University); Nickel, Felix (University Hospital Heidelberg; Heidelberg University); MÃ¼ller-Stich, Beat P (University Hospital Heidelberg; Heidelberg University); Maier-Hein, Lena (German Cancer Research Center; Heidelberg University)",7,7,,,https://doi.org/10.1016/j.media.2022.102488,https://app.dimensions.ai/details/publication/pub.1148221787,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
1547,pub.1149702060,10.3390/jimaging8080205,35893083,PMC9331677,"Brain Tumor Diagnosis Using Machine Learning, Convolutional Neural Networks, Capsule Neural Networks and Vision Transformers, Applied to MRI: A Survey","Management of brain tumors is based on clinical and radiological information with presumed grade dictating treatment. Hence, a non-invasive assessment of tumor grade is of paramount importance to choose the best treatment plan. Convolutional Neural Networks (CNNs) represent one of the effective Deep Learning (DL)-based techniques that have been used for brain tumor diagnosis. However, they are unable to handle input modifications effectively. Capsule neural networks (CapsNets) are a novel type of machine learning (ML) architecture that was recently developed to address the drawbacks of CNNs. CapsNets are resistant to rotations and affine translations, which is beneficial when processing medical imaging datasets. Moreover, Vision Transformers (ViT)-based solutions have been very recently proposed to address the issue of long-range dependency in CNNs. This survey provides a comprehensive overview of brain tumor classification and segmentation techniques, with a focus on ML-based, CNN-based, CapsNet-based, and ViT-based techniques. The survey highlights the fundamental contributions of recent studies and the performance of state-of-the-art techniques. Moreover, we present an in-depth discussion of crucial issues and open challenges. We also identify some key limitations and promising future research directions. We envisage that this survey shall serve as a good springboard for further study.",,"We gratefully acknowledge financial support from FCT FundaÃ§Ã£o para a CiÃªncia e a Tecnologia (Portugal), national funding through research grant Information Management Research CenterâMagIC/NOVA IMS (UIDB/04152/2020).",Journal of Imaging,,,2022-07-22,2022,2022-07-22,,8,8,205,All OA, Gold,Article,"Akinyelu, Andronicus A.; Zaccagna, Fulvio; Grist, James T.; Castelli, Mauro; Rundo, Leonardo","Akinyelu, Andronicus A. (NOVA Information Management School (NOVA IMS), Universidade NOVA de Lisboa, Campus de Campolide, 1070-312 Lisboa, Portugal;, mcastelli@novaims.unl.pt; Department of Computer Science and Informatics, University of the Free State, Phuthaditjhaba 9866, South Africa); Zaccagna, Fulvio (Department of Biomedical and Neuromotor Sciences, Alma Mater Studiorum-University of Bologna, 40138 Bologna, Italy;, fulvio.zaccagna@unibo.it; IRCCS Istituto delle Scienze Neurologiche di Bologna, Functional and Molecular Neuroimaging Unit, 40139 Bologna, Italy); Grist, James T. (Department of Physiology, Anatomy, and Genetics, University of Oxford, Oxford OX1 3PT, UK;, james.grist@dpag.ox.ac.uk; Department of Radiology, Oxford University Hospitals NHS Foundation Trust, Oxford OX3 9DU, UK; Oxford Centre for Clinical Magnetic Research Imaging, University of Oxford, Oxford OX3 9DU, UK; Institute of Cancer and Genomic Sciences, University of Birmingham, Birmingham B15 2SY, UK); Castelli, Mauro (NOVA Information Management School (NOVA IMS), Universidade NOVA de Lisboa, Campus de Campolide, 1070-312 Lisboa, Portugal;, mcastelli@novaims.unl.pt); Rundo, Leonardo (Department of Information and Electrical Engineering and Applied Mathematics, University of Salerno, 84084 Fisciano, Italy)","Akinyelu, Andronicus A. (Universidade Nova de Lisboa; University of the Free State); Rundo, Leonardo (University of Salerno)","Akinyelu, Andronicus A. (Universidade Nova de Lisboa; University of the Free State); Zaccagna, Fulvio (University of Bologna; Istituto delle Scienze Neurologiche di Bologna); Grist, James T. (University of Oxford; Oxford University Hospitals NHS Trust; University of Oxford; University of Birmingham); Castelli, Mauro (Universidade Nova de Lisboa); Rundo, Leonardo (University of Salerno)",2,2,,,https://www.mdpi.com/2313-433X/8/8/205/pdf?version=1658731086,https://app.dimensions.ai/details/publication/pub.1149702060,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1353,pub.1148548118,10.1016/j.artmed.2022.102331,35809970,,Deep learning for image-based liver analysis â A comprehensive review focusing on malignant lesions,"Deep learning-based methods, in particular, convolutional neural networks and fully convolutional networks are now widely used in the medical image analysis domain. The scope of this review focuses on the analysis using deep learning of focal liver lesions, with a special interest in hepatocellular carcinoma and metastatic cancer; and structures like the parenchyma or the vascular system. Here, we address several neural network architectures used for analyzing the anatomical structures and lesions in the liver from various imaging modalities such as computed tomography, magnetic resonance imaging and ultrasound. Image analysis tasks like segmentation, object detection and classification for the liver, liver vessels and liver lesions are discussed. Based on the qualitative search, 91 papers were filtered out for the survey, including journal publications and conference proceedings. The papers reviewed in this work are grouped into eight categories based on the methodologies used. By comparing the evaluation metrics, hybrid models performed better for both the liver and the lesion segmentation tasks, ensemble classifiers performed better for the vessel segmentation tasks and combined approach performed better for both the lesion classification and detection tasks. The performance was measured based on the Dice score for the segmentation, and accuracy for the classification and detection tasks, which are the most commonly used metrics.","This work is supported by H2020-MSCA-ITN Marie SkÅodowska-Curie Actions, Innovative Training Networks (ITN) -H2020 MSCA ITN 2016 GA EU project number 722068 High Performance Soft Tissue Navigation (HiPerNav). Fig. 2 that has been used in the paper is from Orcutt et al. [29] and Figs. 3, 4 and 5 are from radiopaedia.org. The authors would like to acknowledge Orcutt et al. [29] and radiopaedia.org for providing the images as open-source.",,Artificial Intelligence in Medicine,,"Deep Learning; Humans; Image Processing, Computer-Assisted; Liver Neoplasms; Neural Networks, Computer",2022-06-09,2022,2022-06-09,2022-08,130,,102331,All OA, Hybrid,Article,"Survarachakan, Shanmugapriya; Prasad, Pravda Jith Ray; Naseem, Rabia; PÃ©rez de Frutos, Javier; Kumar, Rahul Prasanna; LangÃ¸, Thomas; Alaya Cheikh, Faouzi; Elle, Ole Jakob; Lindseth, Frank","Survarachakan, Shanmugapriya (Department of Computer Science, Norwegian University of Science and Technology, 7491 Trondheim, Norway. Electronic address: shanmugapriya.survarachakan@ntnu.no.); Prasad, Pravda Jith Ray (The Intervention Centre, Oslo University Hospital, 0372 Oslo, Norway; Department of Informatics, University of Oslo, 0315 Oslo, Norway.); Naseem, Rabia (Department of Computer Science, Norwegian University of Science and Technology, 2815 GjÃ¸vik, Norway.); PÃ©rez de Frutos, Javier (Department of Health Research, SINTEF A.S., 7030 Trondheim, Norway.); Kumar, Rahul Prasanna (The Intervention Centre, Oslo University Hospital, 0372 Oslo, Norway.); LangÃ¸, Thomas (Department of Health Research, SINTEF A.S., 7030 Trondheim, Norway.); Alaya Cheikh, Faouzi (Department of Computer Science, Norwegian University of Science and Technology, 2815 GjÃ¸vik, Norway.); Elle, Ole Jakob (The Intervention Centre, Oslo University Hospital, 0372 Oslo, Norway; Department of Informatics, University of Oslo, 0315 Oslo, Norway.); Lindseth, Frank (Department of Computer Science, Norwegian University of Science and Technology, 7491 Trondheim, Norway.)","Survarachakan, Shanmugapriya (Norwegian University of Science and Technology)","Survarachakan, Shanmugapriya (Norwegian University of Science and Technology); Prasad, Pravda Jith Ray (Oslo University Hospital; University of Oslo); Naseem, Rabia (Norwegian University of Science and Technology); PÃ©rez de Frutos, Javier (SINTEF); Kumar, Rahul Prasanna (Oslo University Hospital); LangÃ¸, Thomas (SINTEF); Alaya Cheikh, Faouzi (Norwegian University of Science and Technology); Elle, Ole Jakob (Oslo University Hospital; University of Oslo); Lindseth, Frank (Norwegian University of Science and Technology)",4,4,,,https://doi.org/10.1016/j.artmed.2022.102331,https://app.dimensions.ai/details/publication/pub.1148548118,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1306,pub.1127767771,10.48550/arxiv.2005.09978,,,AutoML Segmentation for 3D Medical Image Data: Contribution to the MSD  Challenge 2018,"Fueled by recent advances in machine learning, there has been tremendous
progress in the field of semantic segmentation for the medical image computing
community. However, developed algorithms are often optimized and validated by
hand based on one task only. In combination with small datasets, interpreting
the generalizability of the results is often difficult. The Medical
Segmentation Decathlon challenge addresses this problem, and aims to facilitate
development of generalizable 3D semantic segmentation algorithms that require
no manual parametrization. Such an algorithm was developed and is presented in
this paper. It consists of a 3D convolutional neural network with
encoder-decoder architecture employing residual-connections, skip-connections
and multi-level generation of predictions. It works on anisotropic
voxel-geometries and has anisotropic depth, i.e., the number of downsampling
steps is a task-specific parameter. These depths are automatically inferred for
each task prior to training. By combining this flexible architecture with
on-the-fly data augmentation and little-to-no pre-- or postprocessing,
promising results could be achieved. The code developed for this challenge will
be available online after the final deadline at:
https://github.com/ORippler/MSD_2018",,,arXiv,,,2020-05-20,2020,,,,,,All OA, Green,Preprint,"Rippel, Oliver; Weninger, Leon; Merhof, Dorit","Rippel, Oliver (); Weninger, Leon (); Merhof, Dorit ()",,"Rippel, Oliver (); Weninger, Leon (); Merhof, Dorit ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1127767771,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1288,pub.1147620436,10.1016/j.compbiomed.2022.105580,35551012,,Recent advancement in cancer diagnosis using machine learning and deep learning techniques: A comprehensive review,"Being a second most cause of mortality worldwide, cancer has been identified as a perilous disease for human beings, where advance stage diagnosis may not help much in safeguarding patients from mortality. Thus, efforts to provide a sustainable architecture with proven cancer prevention estimate and provision for early diagnosis of cancer is the need of hours. Advent of machine learning methods enriched cancer diagnosis area with its overwhelmed efficiency & low error-rate then humans. A significant revolution has been witnessed in the development of machine learning & deep learning assisted system for segmentation & classification of various cancers during past decade. This research paper includes a review of various types of cancer detection via different data modalities using machine learning & deep learning-based methods along with different feature extraction techniques and benchmark datasets utilized in the recent six years studies. The focus of this study is to review, analyse, classify, and address the recent development in cancer detection and diagnosis of six types of cancers i.e., breast, lung, liver, skin, brain and pancreatic cancer, using machine learning & deep learning techniques. Various state-of-the-art technique are clustered into same group and results are examined through key performance indicators like accuracy, area under the curve, precision, sensitivity, dice score on benchmark datasets and concluded with future research work challenges.","Author is thankful to the all researchers, whose articles have been utilized in this study, anonymous reviewers for their forthcoming positive comments and also apologize to those researchers, whom work is overlooked in this research.","This research did not receive any specific grant from funding agencies in the public, commercial, or not âfor-profit sectors.",Computers in Biology and Medicine,,"Breast Neoplasms; Deep Learning; Diagnosis, Computer-Assisted; Early Detection of Cancer; Female; Humans; Image Processing, Computer-Assisted; Machine Learning",2022-05-05,2022,2022-05-05,2022-07,146,,105580,Closed,Article,"Painuli, Deepak; Bhardwaj, Suyash; KÃ¶se, Utku","Painuli, Deepak (Department of Computer Science and Engineering, Gurukula Kangri Vishwavidyalaya, Haridwar, India. Electronic address: deepak.painuli@gmail.com.); Bhardwaj, Suyash (Department of Computer Science and Engineering, Gurukula Kangri Vishwavidyalaya, Haridwar, India.); KÃ¶se, Utku (Department of Computer Engineering, Suleyman Demirel University, Isparta, Turkey.)","Painuli, Deepak (Gurukul Kangri Vishwavidyalaya)","Painuli, Deepak (Gurukul Kangri Vishwavidyalaya); Bhardwaj, Suyash (Gurukul Kangri Vishwavidyalaya); KÃ¶se, Utku (SÃ¼leyman Demirel University)",10,10,,,,https://app.dimensions.ai/details/publication/pub.1147620436,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,,
1220,pub.1139630780,10.48550/arxiv.2107.04263,,,Towards Robust General Medical Image Segmentation,"The reliability of Deep Learning systems depends on their accuracy but also
on their robustness against adversarial perturbations to the input data.
Several attacks and defenses have been proposed to improve the performance of
Deep Neural Networks under the presence of adversarial noise in the natural
image domain. However, robustness in computer-aided diagnosis for volumetric
data has only been explored for specific tasks and with limited attacks. We
propose a new framework to assess the robustness of general medical image
segmentation systems. Our contributions are two-fold: (i) we propose a new
benchmark to evaluate robustness in the context of the Medical Segmentation
Decathlon (MSD) by extending the recent AutoAttack natural image classification
framework to the domain of volumetric data segmentation, and (ii) we present a
novel lattice architecture for RObust Generic medical image segmentation (ROG).
Our results show that ROG is capable of generalizing across different tasks of
the MSD and largely surpasses the state-of-the-art under sophisticated
adversarial attacks.",,,arXiv,,,2021-07-09,2021,,,,,,All OA, Green,Preprint,"Daza, Laura; PÃ©rez, Juan C.; ArbelÃ¡ez, Pablo","Daza, Laura (); PÃ©rez, Juan C. (); ArbelÃ¡ez, Pablo ()",,"Daza, Laura (); PÃ©rez, Juan C. (); ArbelÃ¡ez, Pablo ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139630780,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1220,pub.1152214753,10.48550/arxiv.2210.14677,,,How precise are performance estimates for typical medical image  segmentation tasks?,"An important issue in medical image processing is to be able to estimate not
only the performances of algorithms but also the precision of the estimation of
these performances. Reporting precision typically amounts to reporting
standard-error of the mean (SEM) or equivalently confidence intervals. However,
this is rarely done in medical image segmentation studies. In this paper, we
aim to estimate what is the typical confidence that can be expected in such
studies. To that end, we first perform experiments for Dice metric estimation
using a standard deep learning model (U-net) and a classical task from the
Medical Segmentation Decathlon. We extensively study precision estimation using
both Gaussian assumption and bootstrapping (which does not require any
assumption on the distribution). We then perform simulations for other test set
sizes and performance spreads. Overall, our work shows that small test sets
lead to wide confidence intervals (e.g. $\sim$8 points of Dice for 20 samples
with $\sigma \simeq 10$).",,,arXiv,,,2022-10-26,2022,,,,,,All OA, Green,Preprint,"Jurdi, Rosana El; Colliot, Olivier","Jurdi, Rosana El (); Colliot, Olivier ()",,"Jurdi, Rosana El (); Colliot, Olivier ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152214753,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1219,pub.1143536258,10.1109/ci-ibbi54220.2021.9626056,,,ROG for biomedical semantic segmentation of lung cancer tumors,"Lung cancer causes the higher number of deaths each year of any type of cancer. Generally this cancer is detected when no possible to perform an effective treatment and the usage of computational methods for segmentation can improve the time to initiate this treatments. The medical segmentation decathlon is a challenge to do a generic segmentation method for 11 different tasks. However, the current methods have poor results on the lung nodule segmentation task. This work focuses on implementing changes made by state of the art methods in lung nodule segmentation to the network developed by the Biomedical Computer Vision group from the Universidad de los Andes for the challenge. All of this reducing the computational cost of the network. The final network implemented, a deeper model of the original referred to as ROG+ reduce significantly the computational cost of the method while maintaining similar but lower metrics with a 27.58% dice score.","Our baseline (ROG) was developed by Laura Daza, Ph.D student at Universidad de los Andes on the Biomedical Computer Vision group. She mentored and accompanied all the process of this project as well as Maria Camila Escobar and professor Pablo ArbelÃ¡ez also from Universidad de los Andes. Our implementation is available on this github repository but requires an invitation.",,,2021 IEEE 2nd International Congress of Biomedical Engineering and Bioengineering (CI-IB&BI),,2021-10-15,2021,,2021-10-15,0,,1-5,Closed,Proceeding,"PÃ¡ez, Felipe EscallÃ³n; Abril-Nova, Jose Miguel","PÃ¡ez, Felipe EscallÃ³n (Biomedical Engineering Department, Universidad de los Andes, BogotÃ¡, Colombia); Abril-Nova, Jose Miguel (Biomedical Engineering Department, Universidad de los Andes, BogotÃ¡, Colombia)","PÃ¡ez, Felipe EscallÃ³n (Universidad de Los Andes)","PÃ¡ez, Felipe EscallÃ³n (Universidad de Los Andes); Abril-Nova, Jose Miguel (Universidad de Los Andes)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143536258,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,,
1218,pub.1154910303,10.1007/978-3-031-24985-3_16,,,U-Net vs. TransUNet: Performance Comparison in Medical Image Segmentation,"Image segmentation is a fundamental task in computer-aided diagnosis systems. Correct image segmentation can help healthcare professionals to have better arguments to define a diagnosis and a possible treatment. Among the existing methods and techniques for this task is the TransUNet architecture. This architecture comes from the U-Net architecture, its predecessor, and has the characteristic of incorporating transformers in its encoder. This paper analyzes TransUNet and contrasts its performance with its predecessor. For this purpose, the architectures were implemented, trained, and tested using the Medical Segmentation Decathlon dataset, which provides new challenges and tasks different from those usually addressed. The experimental results reveal the superiority of TransUNet over U-Net in both spleen and left atrium segmentation by reducing the loss in 83% and 72% in the training and testing set, respectively, in the spleen task; and 94% and 92% in the left atrium task. Likewise, in the dice score, TransUNet achieved a superiority of 3.42% and 0.34% for the spleen and left atrium task, respectively, over U-Net.",The authors are grateful for the valuable support given by the SDAS Research Group (www.sdas-group.com - Last accessed: 07 - Jun - 2022).,,Communications in Computer and Information Science,Applied Technologies,,2023-01-28,2023,2023-01-28,2023,1755,,212-226,Closed,Chapter,"Castro, Roberto; Ramos, Leo; RomÃ¡n, Stadyn; Bermeo, Mike; Crespo, Anthony; Cuenca, Erick","Castro, Roberto (Yachay Tech University, UrcuquÃ­, Ecuador); Ramos, Leo (Yachay Tech University, UrcuquÃ­, Ecuador); RomÃ¡n, Stadyn (Yachay Tech University, UrcuquÃ­, Ecuador); Bermeo, Mike (Yachay Tech University, UrcuquÃ­, Ecuador); Crespo, Anthony (Yachay Tech University, UrcuquÃ­, Ecuador); Cuenca, Erick (Yachay Tech University, UrcuquÃ­, Ecuador)","Castro, Roberto (Universidad Yachay Tech)","Castro, Roberto (Universidad Yachay Tech); Ramos, Leo (Universidad Yachay Tech); RomÃ¡n, Stadyn (Universidad Yachay Tech); Bermeo, Mike (Universidad Yachay Tech); Crespo, Anthony (Universidad Yachay Tech); Cuenca, Erick (Universidad Yachay Tech)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154910303,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,,
1202,pub.1141326796,10.1007/978-3-030-87199-4_1,,,Towards Robust General Medical Image Segmentation,"The reliability of Deep Learning systems depends on their accuracy but also on their robustness against adversarial perturbations to the input data. Several attacks and defenses have been proposed to improve the performance of Deep Neural Networks under the presence of adversarial noise in the natural image domain. However, robustness in computer-aided diagnosis for volumetric data has only been explored for specific tasks and with limited attacks. We propose a new framework to assess the robustness of general medical image segmentation systems. Our contributions are two-fold: (i) we propose a new benchmark to evaluate robustness in the context of the Medical Segmentation Decathlon (MSD) by extending the recent AutoAttack natural image classification framework to the domain of volumetric data segmentation, and (ii) we present a novel lattice architecture for RObust Generic medical image segmentation (ROG). Our results show that ROG is capable of generalizing across different tasks of the MSD and largely surpasses the state-of-the-art under sophisticated adversarial attacks.",We thank Amazon Web Services (AWS) for a computational research grant used for the development of this project.,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12903,,3-13,All OA, Green,Chapter,"Daza, Laura; PÃ©rez, Juan C.; ArbelÃ¡ez, Pablo","Daza, Laura (Universidad de los Andes, BogotÃ¡, Colombia); PÃ©rez, Juan C. (Universidad de los Andes, BogotÃ¡, Colombia; King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia); ArbelÃ¡ez, Pablo (Universidad de los Andes, BogotÃ¡, Colombia)","Daza, Laura (Universidad de Los Andes)","Daza, Laura (Universidad de Los Andes); PÃ©rez, Juan C. (Universidad de Los Andes; King Abdullah University of Science and Technology); ArbelÃ¡ez, Pablo (Universidad de Los Andes)",6,6,,4.91,http://arxiv.org/pdf/2107.04263,https://app.dimensions.ai/details/publication/pub.1141326796,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1172,pub.1131195776,10.1007/978-3-030-60548-3_1,,,-UNet++: A Data-Driven Neural Network Architecture for Medical Image Segmentation,"UNet++, an encoder-decoder architecture constructed based on the famous UNet, has achieved state-of-the-art results on many medical image segmentation tasks. Despite improved performance, UNet++ introduces densely connected decoding blocks, some of which, however, are redundant for a specific task. In this paper, we propose \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha $$\end{document}-UNet++ that allows us to automatically identify and discard redundant decoding blocks without the loss of precision. To this end, we design an auxiliary indicator function layer to compress the network architecture via removing a decoding block, in which all individual responses are less than a given threshold \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha $$\end{document}. We evaluated the segmentation architecture obtained respectively for liver segmentation and nuclei segmentation, denoted by UNet++\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^C$$\end{document}, against UNet and UNet++. Comparing to UNet++, our UNet++\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^C$$\end{document} reduces the parameters by 18.89% in liver segmentation and 34.17% in nuclei segmentation, yielding an average improvement of IoU by 0.27% and 0.11% on two tasks. Our results suggest that the UNet++\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^C$$\end{document} produced by the proposed \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha $$\end{document}-UNet++ not only improves the segmentation accuracy slightly but also reduces the model complexity considerably.","This work was supported in part by the Science and Technology Innovation Committee of Shenzhen Municipality, China, under Grants JCYJ20180306171334997, and in part by the National Natural Science Foundation of China under Grants 61771397. We appreciate the efforts devoted by the organizers of the Medical Segmentation Decathlon (MSD) Challenge and 2018 Data Science Bowl Segmentation Challenge to collect and share the data for comparing medical image segmentation algorithms.",,Lecture Notes in Computer Science,"Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning",,2020-09-26,2020,2020-09-26,2020,12444,,3-12,Closed,Chapter,"Chen, Yaxin; Ma, Benteng; Xia, Yong","Chen, Yaxin (Research and Development Institute of Northwestern Polytechnical University in Shenzhen, 518057, Shenzhen, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, 710072, Xiâan, China; Institute of Medical Research, Northwestern Polytechnical University, 710072, Xiâan, China); Ma, Benteng (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, 710072, Xiâan, China); Xia, Yong (Research and Development Institute of Northwestern Polytechnical University in Shenzhen, 518057, Shenzhen, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, 710072, Xiâan, China)","Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University)","Chen, Yaxin (Northwestern Polytechnical University; Northwestern Polytechnical University; Northwestern Polytechnical University); Ma, Benteng (Northwestern Polytechnical University); Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University)",1,1,,0.52,,https://app.dimensions.ai/details/publication/pub.1131195776,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1166,pub.1148408170,10.48550/arxiv.2206.00771,,,Dynamic Linear Transformer for 3D Biomedical Image Segmentation,"Transformer-based neural networks have surpassed promising performance on
many biomedical image segmentation tasks due to a better global information
modeling from the self-attention mechanism. However, most methods are still
designed for 2D medical images while ignoring the essential 3D volume
information. The main challenge for 3D transformer-based segmentation methods
is the quadratic complexity introduced by the self-attention mechanism
\cite{vaswani2017attention}. In this paper, we propose a novel transformer
architecture for 3D medical image segmentation using an encoder-decoder style
architecture with linear complexity. Furthermore, we newly introduce a dynamic
token concept to further reduce the token numbers for self-attention
calculation. Taking advantage of the global information modeling, we provide
uncertainty maps from different hierarchy stages. We evaluate this method on
multiple challenging CT pancreas segmentation datasets. Our promising results
show that our novel 3D Transformer-based segmentor could provide promising
highly feasible segmentation performance and accurate uncertainty
quantification using single annotation. Code is available
https://github.com/freshman97/LinTransUNet.",,,arXiv,,,2022-06-01,2022,,,,,,All OA, Green,Preprint,"Zhang, Zheyuan; Bagci, Ulas","Zhang, Zheyuan (); Bagci, Ulas ()",,"Zhang, Zheyuan (); Bagci, Ulas ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148408170,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1166,pub.1145527833,10.48550/arxiv.2202.06373,,,Scheduling Techniques for Liver Segmentation: ReduceLRonPlateau Vs  OneCycleLR,"Machine learning and computer vision techniques have influenced many fields
including the biomedical one. The aim of this paper is to investigate the
important concept of schedulers in manipulating the learning rate (LR), for the
liver segmentation task, throughout the training process, focusing on the newly
devised OneCycleLR against the ReduceLRonPlateau. A dataset, published in 2018
and produced by the Medical Segmentation Decathlon Challenge organizers, called
Task 8 Hepatic Vessel (MSDC-T8) has been used for testing and validation. The
reported results that have the same number of maximum epochs (75), and are the
average of 5-fold cross-validation, indicate that ReduceLRonPlateau converges
faster while maintaining a similar or even better loss score on the validation
set when compared to OneCycleLR. The epoch at which the peak LR occurs perhaps
should be made early for the OneCycleLR such that the super-convergence feature
can be observed. Moreover, the overall results outperform the state-of-the-art
results from the researchers who published the liver masks for this dataset. To
conclude, both schedulers are suitable for medical segmentation challenges,
especially the MSDC-T8 dataset, and can be used confidently in rapidly
converging the validation loss with a minimal number of epochs.",,,arXiv,,,2022-02-13,2022,,,,,,All OA, Green,Preprint,"Al-Kababji, Ayman; Bensaali, Faycal; Dakua, Sarada Prasad","Al-Kababji, Ayman (); Bensaali, Faycal (); Dakua, Sarada Prasad ()",,"Al-Kababji, Ayman (); Bensaali, Faycal (); Dakua, Sarada Prasad ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1145527833,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1165,pub.1148730228,10.1007/978-3-031-08277-1_17,,,Scheduling Techniques for Liver Segmentation: ReduceLRonPlateau vs OneCycleLR,"Machine learning and computer vision techniques have influenced many fields including the biomedical one. The aim of this paper is to investigate the important concept of schedulers in manipulating the learning rate (LR), for the liver segmentation task, throughout the training process, focusing on the newly devised OneCycleLR against the ReduceLRonPlateau. A dataset, published in 2018 and produced by the Medical Segmentation Decathlon Challenge organizers, called Task 8 Hepatic Vessel (MSDC-T8) has been used for testing and validation. The reported results that have the same number of maximum epochs (75), and are the average of 5-fold cross-validation, indicate that ReduceLRonPlateau converges faster while maintaining a similar or even better loss score on the validation set when compared to OneCycleLR. The epoch at which the peak LR occurs perhaps should be made early for the OneCycleLR such that the super-convergence feature can be observed. Moreover, the overall results outperform the state-of-the-art results from the researchers who published the liver masks for this dataset. To conclude, both schedulers are suitable for medical segmentation challenges, especially the MSDC-T8 dataset, and can be used confidently in rapidly converging the validation loss with a minimal number of epochs.",,,Communications in Computer and Information Science,Intelligent Systems and Pattern Recognition,,2022-06-17,2022,2022-06-17,2022,1589,,204-212,All OA, Green,Chapter,"Al-Kababji, Ayman; Bensaali, Faycal; Dakua, Sarada Prasad","Al-Kababji, Ayman (College of Engineering, Qatar University, Doha, Qatar); Bensaali, Faycal (College of Engineering, Qatar University, Doha, Qatar); Dakua, Sarada Prasad (Department of Surgery, Hamad Medical Corporation, Doha, Qatar)","Al-Kababji, Ayman (Qatar University)","Al-Kababji, Ayman (Qatar University); Bensaali, Faycal (Qatar University); Dakua, Sarada Prasad (Hamad Medical Corporation)",7,7,,,http://arxiv.org/pdf/2202.06373,https://app.dimensions.ai/details/publication/pub.1148730228,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1154,pub.1149238412,10.48550/arxiv.2207.00845,,,Less Is More: A Comparison of Active Learning Strategies for 3D Medical  Image Segmentation,"Since labeling medical image data is a costly and labor-intensive process,
active learning has gained much popularity in the medical image segmentation
domain in recent years. A variety of active learning strategies have been
proposed in the literature, but their effectiveness is highly dependent on the
dataset and training scenario. To facilitate the comparison of existing
strategies and provide a baseline for evaluating novel strategies, we evaluate
the performance of several well-known active learning strategies on three
datasets from the Medical Segmentation Decathlon. Additionally, we consider a
strided sampling strategy specifically tailored to 3D image data. We
demonstrate that both random and strided sampling act as strong baselines and
discuss the advantages and disadvantages of the studied methods. To allow other
researchers to compare their work to our results, we provide an open-source
framework for benchmarking active learning strategies on a variety of medical
segmentation datasets.",,,arXiv,,,2022-07-02,2022,,,,,,All OA, Green,Preprint,"Burmeister, Josafat-Mattias; Rosas, Marcel Fernandez; Hagemann, Johannes; Kordt, Jonas; Blum, Jasper; Shabo, Simon; Bergner, Benjamin; Lippert, Christoph","Burmeister, Josafat-Mattias (Digital Health & Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany); Rosas, Marcel Fernandez (Digital Health & Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany); Hagemann, Johannes (Digital Health & Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany); Kordt, Jonas (Digital Health & Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany); Blum, Jasper (Digital Health & Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany); Shabo, Simon (Digital Health & Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany); Bergner, Benjamin (Digital Health & Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany); Lippert, Christoph (Digital Health & Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany; Hasso Plattner Institute for Digital Health at Mount Sinai, Icahn School of Medicine at Mount Sinai, NYC, USA)",,"Burmeister, Josafat-Mattias (Hasso Plattner Institute); Rosas, Marcel Fernandez (Hasso Plattner Institute); Hagemann, Johannes (Hasso Plattner Institute); Kordt, Jonas (Hasso Plattner Institute); Blum, Jasper (Hasso Plattner Institute); Shabo, Simon (Hasso Plattner Institute); Bergner, Benjamin (Hasso Plattner Institute); Lippert, Christoph (Hasso Plattner Institute; Icahn School of Medicine at Mount Sinai)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149238412,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1148,pub.1149151664,10.48550/arxiv.2206.15217,,,Implicit U-Net for volumetric medical image segmentation,"U-Net has been the go-to architecture for medical image segmentation tasks,
however computational challenges arise when extending the U-Net architecture to
3D images. We propose the Implicit U-Net architecture that adapts the efficient
Implicit Representation paradigm to supervised image segmentation tasks. By
combining a convolutional feature extractor with an implicit localization
network, our implicit U-Net has 40% less parameters than the equivalent U-Net.
Moreover, we propose training and inference procedures to capitalize sparse
predictions. When comparing to an equivalent fully convolutional U-Net,
Implicit U-Net reduces by approximately 30% inference and training time as well
as training memory footprint while achieving comparable results in our
experiments with two different abdominal CT scan datasets.",,,arXiv,,,2022-06-30,2022,,,,,,All OA, Green,Preprint,"Marimont, Sergio Naval; Tarroni, Giacomo","Marimont, Sergio Naval (); Tarroni, Giacomo ()",,"Marimont, Sergio Naval (); Tarroni, Giacomo ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149151664,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1141,pub.1149727540,10.1007/978-3-031-12053-4_29,,,Implicit U-Net for Volumetric Medical Image Segmentation,"U-Net has been the go-to architecture for medical image segmentation tasks, however computational challenges arise when extending the U-Net architecture to 3D images. We propose the Implicit U-Net architecture that adapts the efficient Implicit Representation paradigm to supervised image segmentation tasks. By combining a convolutional feature extractor with an implicit localization network, our implicit U-Net has 40% less parameters than the equivalent U-Net. Moreover, we propose training and inference procedures to capitalize sparse predictions. When comparing to an equivalent fully convolutional U-Net, Implicit U-Net reduces by approximately 30% inference and training time as well as training memory footprint while achieving comparable results in our experiments with two different abdominal CT scan datasets.",,,Lecture Notes in Computer Science,Medical Image Understanding and Analysis,,2022-07-25,2022,2022-07-25,2022,13413,,387-397,All OA, Green,Chapter,"Marimont, Sergio Naval; Tarroni, Giacomo","Marimont, Sergio Naval (CitAI Research Centre, City, University of London, London, UK); Tarroni, Giacomo (CitAI Research Centre, City, University of London, London, UK; BioMedIA, Imperial College, London, UK)","Marimont, Sergio Naval (University of London)","Marimont, Sergio Naval (University of London); Tarroni, Giacomo (University of London; Imperial College London)",0,0,,,http://arxiv.org/pdf/2206.15217,https://app.dimensions.ai/details/publication/pub.1149727540,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1132,pub.1150895567,10.1109/nss/mic44867.2021.9875838,,,Deep Attention-based Seminal Segmentation: A Practical Deep Learning Framework for Accurate Segmentation of the Hippocampus from Magnetic Resonance Images,"Accurate segmentation of the hippocampus head and body from MR images is routinely performed to examine the link between the hippocampus deformation and neurological diseases, such as Alzheimer's and epilepsy. State-of-the-art seminal segmentation methods (including hippocampus segmentation) are based on deep learning algorithms. Most studies focused on developing robust deep learning algorithms to achieve satisfactory performance in hippocampus body and head segmentation. A critical aspect that has been overlooked in these studies is the strategy adopted to train deep learning algorithms when there are two or more target structures. In this work, we examine which deep learning training strategies would be more effective. These strategies include simultaneous (parallel segmentation of the head and body), serial (first head and then body), independent, and attention-based training and segmentation of the target structures. To this end, the hippocampus dataset from the Decathlon challenge and a residual neural network (Resnet) were employed to compare the above-mentioned strategies for hippocampus head and body segmentation. The Dice similarity coefficient and Hausdorff distance were calculated for the outcome of each strategy versus the manually defined hippocampus head and body masks. The quantitative analysis of the outcomes of different training frameworks demonstrated the superior performance of the attention-based training framework with Dice index of 0.89Â±0.03 (body) and 0.88Â±0.04 (head) compared to simultaneous, serial, and independent training frameworks with Dice indices of 0.88Â±0.04 (body) and 0.87Â±0.04 (head), 0.88Â±0.04 (body) and 0.87Â±0.04 (head), and 0.88Â±0.04 (body) and 0.86Â±0.04 (head), respectively. The statistical analysis demonstrated the significantly superior performance of the attention-based training framework (p-value<0.0001). In conclusion, the attention-based training framework is recommended for multi-structure seminal segmentation.",,This work was supported by the Swiss National Science Foundation under grant SNRF 320030_176052.,,2021 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),,2021-10-23,2021,,2021-10-23,0,,1-3,Closed,Proceeding,"Arabian, Hanieh; Karimian, Alireza; Rasti, Reza; Arabi, Hossein; Zaidi, Habib","Arabian, Hanieh (Department of Biomedical Engineering, Faculty of Engineering, University of Isfahan, Isfahan, Iran); Karimian, Alireza (Department of Biomedical Engineering, Faculty of Engineering, University of Isfahan, Isfahan, Iran); Rasti, Reza (Department of Biomedical Engineering, Faculty of Engineering, University of Isfahan, Iran; Department of Biomedical Engineering, Pratt School of Engineering, Duke University, Durham, NC, 27708, USA); Arabi, Hossein (Division of Nuclear Medicine & Molecular Imaging, Geneva University Hospital, CH-1211, Geneva, Switzerland); Zaidi, Habib (Division of Nuclear Medicine & Molecular Imaging, Geneva University Hospital, CH-1211, Geneva, Switzerland)","Zaidi, Habib (University Hospital of Geneva)","Arabian, Hanieh (University of Isfahan); Karimian, Alireza (University of Isfahan); Rasti, Reza (University of Isfahan; Duke University); Arabi, Hossein (University Hospital of Geneva); Zaidi, Habib (University Hospital of Geneva)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1150895567,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1132,pub.1146557292,10.1007/978-3-030-98385-7_19,,,3D U-Net Based Semantic Segmentation of Kidneys and Renal Masses on Contrast-Enhanced CT,"The accurate, automated detection and segmentation of renal tumors is of great interest for the imaging-based diagnosis, histologic subtyping, and management of suspected renal malignancy. The KiTS21 Grand Challenge provides 300 contrast enhanced CT images with kidney, tumors and cysts with corresponding manual annotation, to facilitate the development of robust segmentation algorithms for this task. In this work, we present an adaptation of the historically-successful 3D U-Net architecture, combined with deep supervision, foreground oversampling and large-scale image context, and trained on the majority-prediction segmentation masks. Our model achieved test-set performance of 97.0%, 85.1%, and 81.9% volumetric Dice score, and 93.7%, 72.0%, and 70.0% surface Dice score, on combined foreground, renal masses, and renal tumors, respectively, which tied for sixth place among challenge participants.",,,Lecture Notes in Computer Science,Kidney and Kidney Tumor Segmentation,,2022-03-25,2022,2022-03-25,2022,13168,,143-150,Closed,Chapter,"Zang, Mingyang; Wysoczanski, Artur; Angelini, Elsa; Laine, Andrew F.","Zang, Mingyang (Department of Biomedical Engineering, Columbia University, New York, NY, USA); Wysoczanski, Artur (Department of Biomedical Engineering, Columbia University, New York, NY, USA); Angelini, Elsa (Department of Biomedical Engineering, Columbia University, New York, NY, USA; ITMAT Data Science Group, NIHR Imperial BRC, Imperial College, London, UK); Laine, Andrew F. (Department of Biomedical Engineering, Columbia University, New York, NY, USA)","Laine, Andrew F. (Columbia University)","Zang, Mingyang (Columbia University); Wysoczanski, Artur (Columbia University); Angelini, Elsa (Columbia University; Imperial College London); Laine, Andrew F. (Columbia University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146557292,46 Information and Computing Sciences,,,,,,,,,,,,
1128,pub.1143572755,10.1007/978-3-030-85990-9_54,,,Adam Optimized Deep Learning Model for Segmenting ROI Region in Medical Imaging,"Medical image processing played a crucial role in healthcare sectors because it produces visible images of inner body structure and internal tissues. With medical imaging, the organsâ functions are analyzed, and anomalies are detected effectively. The imaging process utilizes enhancement, analysis, and recognition procedures that maximize disease detection accuracy. However, the gathered medical images have noise while performing the medical research that affects the ROI region segmentation process. The improper identification of the ROI region causes to reduce disease detection accuracy and create complexity. Therefore, this paper uses the Adam optimization technique with deep learning approaches for examining the medical images. The deep learning (DL) approaches utilizing the multiple layers, expert-tuned parameters, and learning function to deriving the affected ROT region. The DL method automatically extracts the medical imaging features that are optimized according to the RMS prop and momentum-based learning parameters. The optimized parameter minimizes the complexity while deriving the disease affected region. This process ensures high region segmentation accuracy with minimum time. The process has detected the affected region with 99.27% of accuracy.",,,Lecture Notes in Networks and Systems,Proceedings of International Conference on Emerging Technologies and Intelligent Systems,,2021-12-03,2021,2021-12-03,2022,322,,669-691,Closed,Chapter,"Jaber, Mustafa Musa; Abd, Sura Khalil; Ali, Saif Mohammed","Jaber, Mustafa Musa (Information Technology Department, Dijlah University College, Baghdad, Iraq; Al-Turath University College, Baghdad, Iraq); Abd, Sura Khalil (Computer Techniques Engineering, Dijlah University College, Baghdad, Iraq); Ali, Saif Mohammed (Computer Science Department, Dijlah University College, Baghdad, Iraq)","Jaber, Mustafa Musa (Dijlah University College; Al Turath University College)","Jaber, Mustafa Musa (Dijlah University College; Al Turath University College); Abd, Sura Khalil (Dijlah University College); Ali, Saif Mohammed (Dijlah University College)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143572755,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,,
1126,pub.1144939658,10.1016/j.displa.2022.102151,,,Class center attention network with spatial adaption for enhancing hepatic segments classification with low-visibility vascular,"Automatic classification of hepatic segments is of great use for liver surgical resection planning. However, conventional computer-aided annotation methods have difficulty annotating cases with weakly visible hepatic vascular structures. To address this issue, we proposed a class center attention convolutional neural network with spatial adaption. In our method, an improved 2.5D input moduleâone with spatial adaptionâwas used to enhance difficult to observe information on the target slices. Furthermore, a class center attention branch was introduced, which refines the unclear boundaries caused by the low-visibility hepatic vascular system by detecting the consistency of the class centers and the voxels near boundaries. Our proposed framework is trained and evaluated on 112 CT scans which were produced by the Task08_HepaticVessel in the Medical segmentation Decathlon study. Experimental results demonstrate the final algorithm for classification of hepatic segments obtained large overlaps comparable to that of manual annotations, even if the visibility of the hepatic vascular system is low.","This work was supported by the Shenzhen Science and Technology Program of China grant JCYJ20200109115420720; National Natural Science Foundation of China (No. 61901463, No. 51877015 and U20A20373); Guangdong province key research and development areas grant 2020B1111140001.",,Displays,,,2022-04,2022,,2022-04,72,,102151,Closed,Article,"Tian, Yinli; Sun, Peiwei; Xue, Fei; Lambo, Ricardo; Yue, Meiyan; An, Chao; Diao, Songhui; Lv, Jianping; Xie, Yaoqin; Gong, Peng; Cao, Hailin; Qin, Wenjian","Tian, Yinli (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing 400044, China); Sun, Peiwei (Department of General Surgery, Shenzhen University General Hospital, Shenzhen 518055, China); Xue, Fei (Osaka University, 1-1 Yamadaoka, Suita, Osaka 5650871, Japan); Lambo, Ricardo (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China); Yue, Meiyan (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China); An, Chao (Department of Minimal Invasive Intervention, Sun Yat-sen University Cancer Center, Guangzhou 510060, China); Diao, Songhui (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China); Lv, Jianping (Department of Neurosurgery, Guangzhou First People's Hospital, School of Medicine, South China University of Technology, Guangzhou, Guangdong 510180, China); Xie, Yaoqin (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China); Gong, Peng (Department of General Surgery, Shenzhen University General Hospital, Shenzhen 518055, China); Cao, Hailin (School of Microelectronics and Communication Engineering, Chongqing University, Chongqing 400044, China); Qin, Wenjian (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China)","Qin, Wenjian (Shenzhen Institutes of Advanced Technology)","Tian, Yinli (Shenzhen Institutes of Advanced Technology; Chongqing University); Sun, Peiwei (Shenzhen University); Xue, Fei (Osaka University); Lambo, Ricardo (Shenzhen Institutes of Advanced Technology); Yue, Meiyan (Shenzhen Institutes of Advanced Technology); An, Chao (Sun Yat-sen University Cancer Center); Diao, Songhui (Shenzhen Institutes of Advanced Technology); Lv, Jianping (); Xie, Yaoqin (Shenzhen Institutes of Advanced Technology); Gong, Peng (Shenzhen University); Cao, Hailin (Chongqing University); Qin, Wenjian (Shenzhen Institutes of Advanced Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144939658,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1123,pub.1146908994,10.48550/arxiv.2204.00631,,,UNetFormer: A Unified Vision Transformer Model and Pre-Training  Framework for 3D Medical Image Segmentation,"Vision Transformers (ViT)s have recently become popular due to their
outstanding modeling capabilities, in particular for capturing long-range
information, and scalability to dataset and model sizes which has led to
state-of-the-art performance in various computer vision and medical image
analysis tasks. In this work, we introduce a unified framework consisting of
two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder
and Convolutional Neural Network (CNN) and transformer-based decoders. In the
proposed model, the encoder is linked to the decoder via skip connections at
five different resolutions with deep supervision. The design of proposed
architecture allows for meeting a wide range of trade-off requirements between
accuracy and computational cost. In addition, we present a methodology for
self-supervised pre-training of the encoder backbone via learning to predict
randomly masked volumetric tokens using contextual information of visible
tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered
from publicly available CT datasets, and present a systematic investigation of
various components such as masking ratio and patch size that affect the
representation learning capability and performance of downstream tasks. We
validate the effectiveness of our pre-training approach by fine-tuning and
testing our model on liver and liver tumor segmentation task using the Medical
Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance
in terms of various segmentation metrics. To demonstrate its generalizability,
we train and test the model on BraTS 21 dataset for brain tumor segmentation
using MRI images and outperform other methods in terms of Dice score. Code:
https://github.com/Project-MONAI/research-contributions",,,arXiv,,,2022-04-01,2022,,,,,,All OA, Green,Preprint,"Hatamizadeh, Ali; Xu, Ziyue; Yang, Dong; Li, Wenqi; Roth, Holger; Xu, Daguang","Hatamizadeh, Ali (); Xu, Ziyue (); Yang, Dong (); Li, Wenqi (); Roth, Holger (); Xu, Daguang ()",,"Hatamizadeh, Ali (); Xu, Ziyue (); Yang, Dong (); Li, Wenqi (); Roth, Holger (); Xu, Daguang ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146908994,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1120,pub.1155807811,,,,Frequency Disentangled Learning for Segmentation of Midbrain Structures  from Quantitative Susceptibility Mapping Data,"One often lacks sufficient annotated samples for training deep segmentation
models. This is in particular the case for less common imaging modalities such
as Quantitative Susceptibility Mapping (QSM). It has been shown that deep
models tend to fit the target function from low to high frequencies. One may
hypothesize that such property can be leveraged for better training of deep
learning models. In this paper, we exploit this property to propose a new
training method based on frequency-domain disentanglement. It consists of two
main steps: i) disentangling the image into high- and low-frequency parts and
feature learning; ii) frequency-domain fusion to complete the task. The
approach can be used with any backbone segmentation network. We apply the
approach to the segmentation of the red and dentate nuclei from QSM data which
is particularly relevant for the study of parkinsonian syndromes. We
demonstrate that the proposed method provides considerable performance
improvements for these tasks. We further applied it to three public datasets
from the Medical Segmentation Decathlon (MSD) challenge. For two MSD tasks, it
provided smaller but still substantial improvements (up to 7 points of Dice),
especially under small training set situations.",,,arXiv,,,2023-02-24,2023,,,,,,All OA, Green,Preprint,"Fu, Guanghui; Jimenez, Gabriel; Loizillon, Sophie; Chougar, Lydia; Dormont, Didier; Valabregue, Romain; Burgos, Ninon; LehÃ©ricy, StÃ©phane; Racoceanu, Daniel; Colliot, Olivier; Group, the ICEBERG Study","Fu, Guanghui (); Jimenez, Gabriel (); Loizillon, Sophie (); Chougar, Lydia (); Dormont, Didier (); Valabregue, Romain (); Burgos, Ninon (); LehÃ©ricy, StÃ©phane (); Racoceanu, Daniel (); Colliot, Olivier (); Group, the ICEBERG Study ()",,"Fu, Guanghui (); Jimenez, Gabriel (); Loizillon, Sophie (); Chougar, Lydia (); Dormont, Didier (); Valabregue, Romain (); Burgos, Ninon (); LehÃ©ricy, StÃ©phane (); Racoceanu, Daniel (); Colliot, Olivier (); Group, the ICEBERG Study ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155807811,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1119,pub.1155376301,,,,Detection and Segmentation of Pancreas using Morphological Snakes and  Deep Convolutional Neural Networks,"Pancreatic cancer is one of the deadliest types of cancer, with 25% of the
diagnosed patients surviving for only one year and 6% of them for five.
Computed tomography (CT) screening trials have played a key role in improving
early detection of pancreatic cancer, which has shown significant improvement
in patient survival rates. However, advanced analysis of such images often
requires manual segmentation of the pancreas, which is a time-consuming task.
Moreover, pancreas presents high variability in shape, while occupying only a
very small area of the entire abdominal CT scans, which increases the
complexity of the problem. The rapid development of deep learning can
contribute to offering robust algorithms that provide inexpensive, accurate,
and user-independent segmentation results that can guide the domain experts.
This dissertation addresses this task by investigating a two-step approach for
pancreas segmentation, by assisting the task with a prior rough localization or
detection of pancreas. This rough localization of the pancreas is provided by
an estimated probability map and the detection task is achieved by using the
YOLOv4 deep learning algorithm. The segmentation task is tackled by a modified
U-Net model applied on cropped data, as well as by using a morphological active
contours algorithm. For comparison, the U-Net model was also applied on the
full CT images, which provide a coarse pancreas segmentation to serve as
reference. Experimental results of the detection network on the National
Institutes of Health (NIH) dataset and the pancreas tumour task dataset within
the Medical Segmentation Decathlon show 50.67% mean Average Precision. The best
segmentation network achieved good segmentation results on the NIH dataset,
reaching 67.67% Dice score.",,,arXiv,,,2023-02-13,2023,,,,,,All OA, Green,Preprint,"Davradou, Agapi","Davradou, Agapi ()",,"Davradou, Agapi ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155376301,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
1096,pub.1151032986,10.1007/978-3-031-16443-9_16,,,A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation,"We propose a Transformer architecture for volumetric segmentation, a challenging task that requires keeping a complex balance in encoding local and global spatial cues, and preserving information along all axes of the volume. Encoder of the proposed design benefits from self-attention mechanism to simultaneously encode local and global cues, while the decoder employs a parallel self and cross attention formulation to capture fine details for boundary refinement. Empirically, we show that the proposed design choices result in a computationally efficient model, with competitive and promising results on the Medical Segmentation Decathlon (MSD) brain tumor segmentation (BraTS) Task. We further show that the representations learned by our model are robust against data corruptions. Our code implementation is publicly available.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13435,,162-172,All OA, Green,Chapter,"Peiris, Himashi; Hayat, Munawar; Chen, Zhaolin; Egan, Gary; Harandi, Mehrtash","Peiris, Himashi (Department of Electrical and Computer Systems Engineering, Faculty of Engineering, Monash University, Melbourne, Australia); Hayat, Munawar (Department of Data Science and AI, Faculty of IT, Monash University, Melbourne, Australia); Chen, Zhaolin (Department of Electrical and Computer Systems Engineering, Faculty of Engineering, Monash University, Melbourne, Australia; Monash Biomedical Imaging (MBI), Monash University, Melbourne, Australia); Egan, Gary (Monash Biomedical Imaging (MBI), Monash University, Melbourne, Australia); Harandi, Mehrtash (Department of Electrical and Computer Systems Engineering, Faculty of Engineering, Monash University, Melbourne, Australia)","Peiris, Himashi (Monash University)","Peiris, Himashi (Monash University); Hayat, Munawar (Monash University); Chen, Zhaolin (Monash University; Monash University); Egan, Gary (Monash University); Harandi, Mehrtash (Monash University)",14,14,,,http://arxiv.org/pdf/2111.13300,https://app.dimensions.ai/details/publication/pub.1151032986,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1085,pub.1138796801,10.48550/arxiv.2106.05735,,,The Medical Segmentation Decathlon,"International challenges have become the de facto standard for comparative
assessment of image analysis algorithms given a specific task. Segmentation is
so far the most widely investigated medical image processing task, but the
various segmentation challenges have typically been organized in isolation,
such that algorithm development was driven by the need to tackle a single
specific clinical problem. We hypothesized that a method capable of performing
well on multiple tasks will generalize well to a previously unseen task and
potentially outperform a custom-designed solution. To investigate the
hypothesis, we organized the Medical Segmentation Decathlon (MSD) - a
biomedical image analysis challenge, in which algorithms compete in a multitude
of both tasks and modalities. The underlying data set was designed to explore
the axis of difficulties typically encountered when dealing with medical
images, such as small data sets, unbalanced labels, multi-site data and small
objects. The MSD challenge confirmed that algorithms with a consistent good
performance on a set of tasks preserved their good average performance on a
different set of previously unseen tasks. Moreover, by monitoring the MSD
winner for two years, we found that this algorithm continued generalizing well
to a wide range of other clinical problems, further confirming our hypothesis.
Three main conclusions can be drawn from this study: (1) state-of-the-art image
segmentation algorithms are mature, accurate, and generalize well when
retrained on unseen tasks; (2) consistent algorithmic performance across
multiple tasks is a strong surrogate of algorithmic generalizability; (3) the
training of accurate AI segmentation models is now commoditized to non AI
experts.",,,arXiv,,,2021-06-10,2021,,,,,,All OA, Green,Preprint,"Antonelli, Michela; Reinke, Annika; Bakas, Spyridon; Farahani, Keyvan; AnnetteKopp-Schneider; Landman, Bennett A.; Litjens, Geert; Menze, Bjoern; Ronneberger, Olaf; Summers, Ronald M.; van Ginneken, Bram; Bilello, Michel; Bilic, Patrick; Christ, Patrick F.; Do, Richard K. G.; Gollub, Marc J.; Heckers, Stephan H.; Huisman, Henkjan; Jarnagin, William R.; McHugo, Maureen K.; Napel, Sandy; Pernicka, Jennifer S. Goli; Rhode, Kawal; Tobon-Gomez, Catalina; Vorontsov, Eugene; Huisman, Henkjan; Meakin, James A.; Ourselin, Sebastien; Wiesenfarth, Manuel; Arbelaez, Pablo; Bae, Byeonguk; Chen, Sihong; Daza, Laura; Feng, Jianjiang; He, Baochun; Isensee, Fabian; Ji, Yuanfeng; Jia, Fucang; Kim, Namkug; Kim, Ildoo; Merhof, Dorit; Pai, Akshay; Park, Beomhee; Perslev, Mathias; Rezaiifar, Ramin; Rippel, Oliver; Sarasua, Ignacio; Shen, Wei; Son, Jaemin; Wachinger, Christian; Wang, Liansheng; Wang, Yan; Xia, Yingda; Xu, Daguang; Xu, Zhanwei; Zheng, Yefeng; Simpson, Amber L.; Maier-Hein, Lena; Cardoso, M. Jorge","Antonelli, Michela (); Reinke, Annika (); Bakas, Spyridon (); Farahani, Keyvan (); AnnetteKopp-Schneider (); Landman, Bennett A. (); Litjens, Geert (); Menze, Bjoern (); Ronneberger, Olaf (); Summers, Ronald M. (); van Ginneken, Bram (); Bilello, Michel (); Bilic, Patrick (); Christ, Patrick F. (); Do, Richard K. G. (); Gollub, Marc J. (); Heckers, Stephan H. (); Huisman, Henkjan (); Jarnagin, William R. (); McHugo, Maureen K. (); Napel, Sandy (); Pernicka, Jennifer S. Goli (); Rhode, Kawal (); Tobon-Gomez, Catalina (); Vorontsov, Eugene (); Meakin, James A. (); Ourselin, Sebastien (); Wiesenfarth, Manuel (); Arbelaez, Pablo (); Bae, Byeonguk (); Chen, Sihong (); Daza, Laura (); Feng, Jianjiang (); He, Baochun (); Isensee, Fabian (); Ji, Yuanfeng (); Jia, Fucang (); Kim, Namkug (); Kim, Ildoo (); Merhof, Dorit (); Pai, Akshay (); Park, Beomhee (); Perslev, Mathias (); Rezaiifar, Ramin (); Rippel, Oliver (); Sarasua, Ignacio (); Shen, Wei (); Son, Jaemin (); Wachinger, Christian (); Wang, Liansheng (); Wang, Yan (); Xia, Yingda (); Xu, Daguang (); Xu, Zhanwei (); Zheng, Yefeng (); Simpson, Amber L. (); Maier-Hein, Lena (); Cardoso, M. Jorge ()",,"Antonelli, Michela (); Reinke, Annika (); Bakas, Spyridon (); Farahani, Keyvan (); AnnetteKopp-Schneider (); Landman, Bennett A. (); Litjens, Geert (); Menze, Bjoern (); Ronneberger, Olaf (); Summers, Ronald M. (); van Ginneken, Bram (); Bilello, Michel (); Bilic, Patrick (); Christ, Patrick F. (); Do, Richard K. G. (); Gollub, Marc J. (); Heckers, Stephan H. (); Huisman, Henkjan (); Jarnagin, William R. (); McHugo, Maureen K. (); Napel, Sandy (); Pernicka, Jennifer S. Goli (); Rhode, Kawal (); Tobon-Gomez, Catalina (); Vorontsov, Eugene (); Meakin, James A. (); Ourselin, Sebastien (); Wiesenfarth, Manuel (); Arbelaez, Pablo (); Bae, Byeonguk (); Chen, Sihong (); Daza, Laura (); Feng, Jianjiang (); He, Baochun (); Isensee, Fabian (); Ji, Yuanfeng (); Jia, Fucang (); Kim, Namkug (); Kim, Ildoo (); Merhof, Dorit (); Pai, Akshay (); Park, Beomhee (); Perslev, Mathias (); Rezaiifar, Ramin (); Rippel, Oliver (); Sarasua, Ignacio (); Shen, Wei (); Son, Jaemin (); Wachinger, Christian (); Wang, Liansheng (); Wang, Yan (); Xia, Yingda (); Xu, Daguang (); Xu, Zhanwei (); Zheng, Yefeng (); Simpson, Amber L. (); Maier-Hein, Lena (); Cardoso, M. Jorge ()",4,4,,3.1,,https://app.dimensions.ai/details/publication/pub.1138796801,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1078,pub.1151381614,10.1109/cvpr52688.2022.02008,,,HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D Medical Image Segmentation using HyperNet,"Semantic segmentation of 3D medical images is a challenging task due to the high variability of the shape and pattern of objects (such as organs or tumors). Given the recent success of deep learning in medical image segmentation, Neural Architecture Search (NAS) has been introduced to find high-performance 3D segmentation network architectures. However, because of the massive computational requirements of 3D data and the discrete optimization nature of architecture search, previous NAS methods require a long search time or necessary continuous relaxation, and commonly lead to sub-optimal network architectures. While one-shot NAS can potentially address these disadvantages, its application in the segmentation domain has not been well studied in the expansive multi-scale multi-path search space. To enable one-shot NAS for medical image segmentation, our method, named HyperSegNAS, introduces a HyperNet to assist super-net training by incorporating architecture topology information. Such a HyperNet can be removed once the super-net is trained and introduces no overhead during architecture search. We show that HyperSegNAS yields better performing and more intuitive architectures compared to the previous state-of-the-art (SOTA) segmentation networks; furthermore, it can quickly and accurately find good architecture candidates under different computing constraints. Our method is evaluated on public datasets from the Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.",,,,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2022-06-24,2022,,2022-06-24,0,,20709-20719,All OA, Green,Proceeding,"Peng, Cheng; Myronenko, Andriy; Hatamizadeh, Ali; Nath, Vishwesh; Siddiquee, Mahfuzur Rahman; He, Yufan; Xu, Daguang; Chellappa, Rama; Yang, Dong","Peng, Cheng (Johns Hopkins University); Myronenko, Andriy (NVIDIA); Hatamizadeh, Ali (NVIDIA); Nath, Vishwesh (NVIDIA); Siddiquee, Mahfuzur Rahman (Arizona State University); He, Yufan (NVIDIA); Xu, Daguang (NVIDIA); Chellappa, Rama (Johns Hopkins University); Yang, Dong (NVIDIA)",,"Peng, Cheng (Johns Hopkins University); Myronenko, Andriy (Nvidia (United States)); Hatamizadeh, Ali (Nvidia (United States)); Nath, Vishwesh (Nvidia (United States)); Siddiquee, Mahfuzur Rahman (Arizona State University); He, Yufan (Nvidia (United States)); Xu, Daguang (Nvidia (United States)); Chellappa, Rama (Johns Hopkins University); Yang, Dong (Nvidia (United States))",2,2,,,http://arxiv.org/pdf/2112.10652,https://app.dimensions.ai/details/publication/pub.1151381614,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1076,pub.1144062417,10.48550/arxiv.2112.10652,,,HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D  Medical Image Segmentation using HyperNet,"Semantic segmentation of 3D medical images is a challenging task due to the
high variability of the shape and pattern of objects (such as organs or
tumors). Given the recent success of deep learning in medical image
segmentation, Neural Architecture Search (NAS) has been introduced to find
high-performance 3D segmentation network architectures. However, because of the
massive computational requirements of 3D data and the discrete optimization
nature of architecture search, previous NAS methods require a long search time
or necessary continuous relaxation, and commonly lead to sub-optimal network
architectures. While one-shot NAS can potentially address these disadvantages,
its application in the segmentation domain has not been well studied in the
expansive multi-scale multi-path search space. To enable one-shot NAS for
medical image segmentation, our method, named HyperSegNAS, introduces a
HyperNet to assist super-net training by incorporating architecture topology
information. Such a HyperNet can be removed once the super-net is trained and
introduces no overhead during architecture search. We show that HyperSegNAS
yields better performing and more intuitive architectures compared to the
previous state-of-the-art (SOTA) segmentation networks; furthermore, it can
quickly and accurately find good architecture candidates under different
computing constraints. Our method is evaluated on public datasets from the
Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.",,,arXiv,,,2021-12-20,2021,,,,,,All OA, Green,Preprint,"Peng, Cheng; Myronenko, Andriy; Hatamizadeh, Ali; Nath, Vish; Siddiquee, Md Mahfuzur Rahman; He, Yufan; Xu, Daguang; Chellappa, Rama; Yang, Dong","Peng, Cheng (); Myronenko, Andriy (); Hatamizadeh, Ali (); Nath, Vish (); Siddiquee, Md Mahfuzur Rahman (); He, Yufan (); Xu, Daguang (); Chellappa, Rama (); Yang, Dong ()",,"Peng, Cheng (); Myronenko, Andriy (); Hatamizadeh, Ali (); Nath, Vish (); Siddiquee, Md Mahfuzur Rahman (); He, Yufan (); Xu, Daguang (); Chellappa, Rama (); Yang, Dong ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144062417,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1075,pub.1145334066,10.48550/arxiv.2202.02078,,,Heed the Noise in Performance Evaluations in Neural Architecture Search,"Neural Architecture Search (NAS) has recently become a topic of great
interest. However, there is a potentially impactful issue within NAS that
remains largely unrecognized: noise. Due to stochastic factors in neural
network initialization, training, and the chosen train/validation dataset
split, the performance evaluation of a neural network architecture, which is
often based on a single learning run, is also stochastic. This may have a
particularly large impact if a dataset is small. We therefore propose to reduce
this noise by evaluating architectures based on average performance over
multiple network training runs using different random seeds and
cross-validation. We perform experiments for a combinatorial optimization
formulation of NAS in which we vary noise reduction levels. We use the same
computational budget for each noise level in terms of network training runs,
i.e., we allow less architecture evaluations when averaging over more training
runs. Multiple search algorithms are considered, including evolutionary
algorithms which generally perform well for NAS. We use two publicly available
datasets from the medical image segmentation domain where datasets are often
limited and variability among samples is often high. Our results show that
reducing noise in architecture evaluations enables finding better architectures
by all considered search algorithms.",,,arXiv,,,2022-02-04,2022,,,,,,All OA, Green,Preprint,"Dushatskiy, Arkadiy; Alderliesten, Tanja; Bosman, Peter A. N.","Dushatskiy, Arkadiy (); Alderliesten, Tanja (); Bosman, Peter A. N. ()",,"Dushatskiy, Arkadiy (); Alderliesten, Tanja (); Bosman, Peter A. N. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1145334066,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4611 Machine Learning,,,,,,,,,
1062,pub.1142696895,10.1109/niles53778.2021.9600556,,,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,,,2021 3rd Novel Intelligent and Leading Emerging Sciences Conference (NILES),,2021-10-25,2021,,2021-10-25,0,,269-272,Closed,Proceeding,"Gamal, Ali; Bedda, Khaled; Ashraf, Nada; Ayman, Salma; AbdAllah, Mohamed; Rushdi, Muhammad A.","Gamal, Ali (Department of Biomedical Engineering and Systems Faculty of Engineering, Cairo University); Bedda, Khaled (Department of Biomedical Engineering and Systems Faculty of Engineering, Cairo University); Ashraf, Nada (Department of Biomedical Engineering and Systems Faculty of Engineering, Cairo University); Ayman, Salma (Department of Biomedical Engineering and Systems Faculty of Engineering, Cairo University); AbdAllah, Mohamed (Department of Biomedical Engineering and Systems Faculty of Engineering, Cairo University); Rushdi, Muhammad A. (Department of Biomedical Engineering and Systems Faculty of Engineering, Cairo University)",,"Gamal, Ali (Cairo University); Bedda, Khaled (Cairo University); Ashraf, Nada (Cairo University); Ayman, Salma (Cairo University); AbdAllah, Mohamed (Cairo University); Rushdi, Muhammad A. (Cairo University)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142696895,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
1054,pub.1143451590,10.48550/arxiv.2111.13300,,,A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation,"We propose a Transformer architecture for volumetric segmentation, a
challenging task that requires keeping a complex balance in encoding local and
global spatial cues, and preserving information along all axes of the volume.
Encoder of the proposed design benefits from self-attention mechanism to
simultaneously encode local and global cues, while the decoder employs a
parallel self and cross attention formulation to capture fine details for
boundary refinement. Empirically, we show that the proposed design choices
result in a computationally efficient model, with competitive and promising
results on the Medical Segmentation Decathlon (MSD) brain tumor segmentation
(BraTS) Task. We further show that the representations learned by our model are
robust against data corruptions.
\href{https://github.com/himashi92/VT-UNet}{Our code implementation is publicly
available}.",,,arXiv,,,2021-11-25,2021,,,,,,All OA, Green,Preprint,"Peiris, Himashi; Hayat, Munawar; Chen, Zhaolin; Egan, Gary; Harandi, Mehrtash","Peiris, Himashi (); Hayat, Munawar (); Chen, Zhaolin (); Egan, Gary (); Harandi, Mehrtash ()",,"Peiris, Himashi (); Hayat, Munawar (); Chen, Zhaolin (); Egan, Gary (); Harandi, Mehrtash ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143451590,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1051,pub.1153416396,10.1117/12.2660662,,,An overview of abdominal segmentation methods using General Adversarial Network (GAN),"Applications ranging from image-guided surgery to computer-assisted diagnosis depend on the segmentation of abdominal anatomy. The uses of the General Adversarial Network (GAN) in the automatic segmentation of abdominal pictures are discussed in this review article. Discussion in this paper starts off by reviewing the underlying theory of GAN and its variations, then covering the most recent abdominal segmentation techniques. In addition, it introduces the variety of applications and datasets included for these imaging studies. The quantitative performances (evaluated using Dice Similarity Coefficient and F-scores) of 13 representative studies are summarized and compared. Overall, a more precise abdominal segmentation can be achieved using GAN-based methods of image analysis.",,,Proceedings of SPIE,International Conference on Biomedical and Intelligent Systems (IC-BIS 2022),,2022-12-06,2022,,,12458,,1245843-1245843-7,Closed,Proceeding,"Liu, Ying","Liu, Ying (The University of Hong Kong (Hong Kong, China))",,"Liu, Ying (University of Hong Kong)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153416396,"51 Physical Sciences; 5102 Atomic, Molecular and Optical Physics",,,,,,,,,,,,
1050,pub.1150422227,10.48550/arxiv.2208.09592,,,Transforming the Interactive Segmentation for Medical Imaging,"The goal of this paper is to interactively refine the automatic segmentation
on challenging structures that fall behind human performance, either due to the
scarcity of available annotations or the difficulty nature of the problem
itself, for example, on segmenting cancer or small organs. Specifically, we
propose a novel Transformer-based architecture for Interactive Segmentation
(TIS), that treats the refinement task as a procedure for grouping pixels with
similar features to those clicks given by the end users. Our proposed
architecture is composed of Transformer Decoder variants, which naturally
fulfills feature comparison with the attention mechanisms. In contrast to
existing approaches, our proposed TIS is not limited to binary segmentations,
and allows the user to edit masks for arbitrary number of categories. To
validate the proposed approach, we conduct extensive experiments on three
challenging datasets and demonstrate superior performance over the existing
state-of-the-art methods. The project page is: https://wtliu7.github.io/tis/.",,,arXiv,,,2022-08-19,2022,,,,,,All OA, Green,Preprint,"Liu, Wentao; Ma, Chaofan; Yang, Yuhuan; Xie, Weidi; Zhang, Ya","Liu, Wentao (); Ma, Chaofan (); Yang, Yuhuan (); Xie, Weidi (); Zhang, Ya ()",,"Liu, Wentao (); Ma, Chaofan (); Yang, Yuhuan (); Xie, Weidi (); Zhang, Ya ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150422227,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
1047,pub.1151032971,10.1007/978-3-031-16440-8_67,,,Transforming the Interactive Segmentation for Medical Imaging,"The goal of this paper is to interactively refine the automatic segmentation on challenging structures that fall behind human performance, either due to the scarcity of available annotations or the difficulty nature of the problem itself, for example, on segmenting cancer or small organs. Specifically, we propose a novel Transformer-based architecture for Interactive SegmentationÂ (TIS), that treats the refinement task as a procedure for grouping pixels with similar features to those clicks given by the end users. Our proposed architecture is composed of Transformer Decoder variants, which naturally fulfills feature comparison with the attention mechanisms. In contrast to existing approaches, our proposed TIS is not limited to binary segmentations, and allows the user to edit masks for arbitrary number of categories. To validate the proposed approach, we conduct extensive experiments on three challenging datasets and demonstrate superior performance over the existing state-of-the-art methods. The project page is: https://wtliu7.github.io/tis/.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13434,,704-713,All OA, Green,Chapter,"Liu, Wentao; Ma, Chaofan; Yang, Yuhuan; Xie, Weidi; Zhang, Ya","Liu, Wentao (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China); Ma, Chaofan (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China); Yang, Yuhuan (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China); Xie, Weidi (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China); Zhang, Ya (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China)","Zhang, Ya (Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory)","Liu, Wentao (Shanghai Jiao Tong University); Ma, Chaofan (Shanghai Jiao Tong University); Yang, Yuhuan (Shanghai Jiao Tong University); Xie, Weidi (Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory); Zhang, Ya (Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory)",0,0,,,http://arxiv.org/pdf/2208.09592,https://app.dimensions.ai/details/publication/pub.1151032971,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
1043,pub.1134980301,10.1016/j.sigpro.2021.108013,,,Learned snakes for 3D image segmentation,"Snakes or active contour models are classical methods for boundary detection and segmentation, which deform an initial contour (for 2D image) or a surface (for 3D image) towards the boundary of the desired object. Such snakes models are ideal choices for handling medical image segmentation problems since they are very efficient and require fewer memories by solely tracking the explicit curves or surfaces. However, traditional snakes models solved by the level set method suffer from numerical instabilities and are usually difficult to deal with topological changes. In this paper, we propose a learned snakes model for 3D medical image segmentation, where both the initial and final surfaces are estimated using deep neural networks in end-to-end regimes. The merit of our learned snakes model is that we can realize 3D segmentation by finding a 2D surface based on 2D convolutional neural networks rather than using 3D networks or cutting the volume into 2D slices. Experiments on the Medical Segmentation Decathlon spleen dataset against both 2D- and 3D-based networks demonstrate our model achieving the state-of-the-art accuracy and efficiency, which not only enjoys a 1% higher DSC but also saves more than 90% computational time compared to the well-established elastic boundary projection model Ni etÂ al. [1].","The work was supported by the National Natural Science Foundation of China (NSFC 12071345, 11701418), Major Science and Technology Project of Tianjin 18ZXRHSY00160, and Recruitment Program of Global Young Expert. The work of X.C. Tai was supported by Hong Kong Baptist University startup grants RG(R)-RC/17-18/02-MATH and FRG2/17-18/033.",,Signal Processing,,,2021-06,2021,,2021-06,183,,108013,Closed,Article,"Guo, Lihong; Liu, Yueyun; Wang, Yu; Duan, Yuping; Tai, Xue-Cheng","Guo, Lihong (Center for Applied Mathematics, Tianjin University, Tianjin 300072, China); Liu, Yueyun (Center for Applied Mathematics, Tianjin University, Tianjin 300072, China); Wang, Yu (Alibaba Group, Hangzhou, China); Duan, Yuping (Center for Applied Mathematics, Tianjin University, Tianjin 300072, China); Tai, Xue-Cheng (Department of Mathematics, Hong Kong Baptist University, Kowloon Tong, Hong Kong)","Duan, Yuping (Tianjin University)","Guo, Lihong (Tianjin University); Liu, Yueyun (Tianjin University); Wang, Yu (Alibaba Group (China)); Duan, Yuping (Tianjin University); Tai, Xue-Cheng (Hong Kong Baptist University)",6,6,,4.91,,https://app.dimensions.ai/details/publication/pub.1134980301,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1023,pub.1153912904,10.1109/i-smac55078.2022.9987387,,,Fully Automated Spleen Segmentation in Patients using Convolutional Neural Network on CT Images,"A deep learning network was created for the purpose of segmenting the spleen on thorax-abdomen CT images for the purpose of this retrospective investigation. The purpose of this research was to create a model that is based on deep learning and is capable of fully automated spleen fragmentation utilizing computerized tomography images. Additionally, the effectiveness of this algorithm was going to be evaluated either directly or indirectly affect the spleen. In order to do this, UNet segmentation is used to train the dataset that had illnesses with and without splenic involvement, as well as a dataset obtained from the Medical Segmentation Decathlon that did not contain splenic abnormalities. The Dice Similarity Coefficient was one of the four developed metrics that were used in the evaluation of the two modelsâ respective segmentation abilities. A modified deep learning neural network is used in this work by blending convolutional neural network and XGBoost classifier (XGNN). The training database of a Deep learning model should contain factors that impact the spleen in order to achieve best precision, completely automated spleen fragmentation in clinical practice.",,,,"2022 Sixth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",,2022-11-12,2022,,2022-11-12,0,,743-748,Closed,Proceeding,"Ramkumar, G.; Prabu, R. Thandaiah; Anitha, G; Mohanavel, V.; Tamilselvi, M.","Ramkumar, G. (Department of Electronics and Communication Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Thandalam, Chennai, 602105); Prabu, R. Thandaiah (Department of Electronics and Communication Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Thandalam, Chennai, 602105); Anitha, G (Department of Electronics and Communication Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Thandalam, Chennai, 602105); Mohanavel, V. (Centre for Materials Engineering and Regenerative Medicine, Bharath Institute of Hig her Education and Research, Chennai, 600073, Tamil Nadu, India; Department of Mechanical Engineering, Chandigarh University, Mohali, 140413, Punjab, India); Tamilselvi, M. (Department of Electronics and Communication, SRM Institute of Science and Technology, Ramapuram Campus, Chennai, Tamil Nadu, India)","Ramkumar, G. (Saveetha Institute of Medical And Technical Sciences)","Ramkumar, G. (Saveetha Institute of Medical And Technical Sciences); Prabu, R. Thandaiah (Saveetha Institute of Medical And Technical Sciences); Anitha, G (Saveetha Institute of Medical And Technical Sciences); Mohanavel, V. (Chandigarh University); Tamilselvi, M. (SRM Institute of Science and Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153912904,46 Information and Computing Sciences, 4611 Machine Learning, 51 Physical Sciences, 5105 Medical and Biological Physics,3 Good Health and Well Being,,,,,,,,
1023,pub.1151694525,10.1007/978-3-031-18523-6_17,,,Verifiable and Energy Efficient Medical Image Analysis with Quantised Self-attentive Deep Neural Networks,"Convolutional Neural Networks have played a significant role in various medical imaging tasks like classification and segmentation. They provide state-of-the-art performance compared to classical image processing algorithms. However, the major downside of these methods is the high computational complexity, reliance on high-performance hardware like GPUs and the inherent black-box nature of the model. In this paper, we propose quantised stand-alone self-attention based models as an alternative to traditional CNNs. In the proposed class of networks, convolutional layers are replaced with stand-alone self-attention layers, and the network parameters are quantised after training. We experimentally validate the performance of our method on classification and segmentation tasks. We observe 50â80% reduction in model size, 60â80% lesser number of parameters, 40â85% fewer FLOPs and 65â80% more energy efficiency during inference on CPUs. The code will be available at https://github.com/Rakshith2597/Quantised-Self-Attentive-Deep-Neural-Network.",,,Lecture Notes in Computer Science,"Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health",,2022-10-07,2022,2022-10-07,2022,13573,,178-189,All OA, Green,Chapter,"Sathish, Rakshith; Khare, Swanand; Sheet, Debdoot","Sathish, Rakshith (Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India); Khare, Swanand (Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India); Sheet, Debdoot (Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India)","Sathish, Rakshith (Indian Institute of Technology Kharagpur)","Sathish, Rakshith (Indian Institute of Technology Kharagpur); Khare, Swanand (Indian Institute of Technology Kharagpur); Sheet, Debdoot (Indian Institute of Technology Kharagpur)",0,0,,,http://arxiv.org/pdf/2209.15287,https://app.dimensions.ai/details/publication/pub.1151694525,46 Information and Computing Sciences, 4611 Machine Learning,7 Affordable and Clean Energy,,,,,,,,,
1023,pub.1150639911,10.1109/cbms55023.2022.00045,,,"Integrating Residual, Dense, and Inception Blocks into the nnUNet","The nnUNet is a fully automated and generalisable framework which automatically configures the full training pipeline for the segmentation task it is applied on, while taking into account dataset properties and hardware constraints. It utilises a basic UNet type architecture which is self-configuring in terms of topology. In this work, we propose to extend the nnUNet by integrating mechanisms from more advanced UNet variations such as the residual, dense, and inception blocks, resulting in three new nnUNet variations, namely the Residual-nnUNet, Dense-nnUNet, and Inception-nnUNet. We have evaluated the segmentation performance on eight datasets consisting of 20 target anatomical structures. Our results demonstrate that altering network architecture may lead to performance gains, but the extent of gains and the optimally chosen nnUNet variation is dataset dependent.",,,,2022 IEEE 35th International Symposium on Computer-Based Medical Systems (CBMS),,2022-07-23,2022,,2022-07-23,0,,217-222,All OA, Green,Proceeding,"McConnell, NiccolÃ²; Miron, Alina; Wang, Zidong; Li, Yongmin","McConnell, NiccolÃ² (Dept. of Computer Science, Brunel University London, London, UK); Miron, Alina (Dept. of Computer Science, Brunel University London, London, UK); Wang, Zidong (Dept. of Computer Science, Brunel University London, London, UK); Li, Yongmin (Dept. of Computer Science, Brunel University London, London, UK)",,"McConnell, NiccolÃ² (Brunel University London); Miron, Alina (Brunel University London); Wang, Zidong (Brunel University London); Li, Yongmin (Brunel University London)",0,0,,,http://bura.brunel.ac.uk/bitstream/2438/25100/1/FullText.pdf,https://app.dimensions.ai/details/publication/pub.1150639911,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1021,pub.1141326833,10.1007/978-3-030-87199-4_43,,,The Power of Proxy Data and Proxy Networks for Hyper-parameter Optimization in Medical Image Segmentation,"Deep learning models for medical image segmentation are primarily data-driven. Models trained with more data lead to improved performance and generalizability. However, training is a computationally expensive process because multiple hyper-parameters need to be tested to find the optimal setting for best performance. In this work, we focus on accelerating the estimation of hyper-parameters by proposing two novel methodologies: proxy data and proxy networks. Both can be useful for estimating hyper-parameters more efficiently. We test the proposed techniques on CT and MR imaging modalities using well-known public datasets. In both cases using one dataset for building proxy data and another data source for external evaluation. For CT, the approach is tested on spleen segmentation with two datasets. The first dataset is from the medical segmentation decathlon (MSD), where the proxy data is constructed, the secondary dataset is utilized as an external validation dataset. Similarly, for MR, the approach is evaluated on prostate segmentation where the first dataset is from MSD and the second dataset is PROSTATEx. First, we show higher correlation to using full data for training when testing on the external validation set using smaller proxy data than a random selection of the proxy data. Second, we show that a high correlation exists for proxy networks when compared with the full network on validation Dice score. Third, we show that the proposed approach of utilizing a proxy network can speed up an AutoML framework for hyper-parameter search by 3.3Ã\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}, and by 4.4Ã\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document} if proxy data and proxy network are utilized together.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12903,,456-465,All OA, Green,Chapter,"Nath, Vishwesh; Yang, Dong; Hatamizadeh, Ali; Abidin, Anas A.; Myronenko, Andriy; Roth, Holger R.; Xu, Daguang","Nath, Vishwesh (NVIDIA, Santa Clara, USA); Yang, Dong (NVIDIA, Santa Clara, USA); Hatamizadeh, Ali (NVIDIA, Santa Clara, USA); Abidin, Anas A. (NVIDIA, Santa Clara, USA); Myronenko, Andriy (NVIDIA, Santa Clara, USA); Roth, Holger R. (NVIDIA, Santa Clara, USA); Xu, Daguang (NVIDIA, Santa Clara, USA)","Nath, Vishwesh (Nvidia (United States))","Nath, Vishwesh (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Hatamizadeh, Ali (Nvidia (United States)); Abidin, Anas A. (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Roth, Holger R. (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",4,4,,,http://arxiv.org/pdf/2107.05471,https://app.dimensions.ai/details/publication/pub.1141326833,46 Information and Computing Sciences,,,,,,,,,,,
1018,pub.1153498861,10.1109/icrito56286.2022.9964991,,,Pancreatic Carcinoma Detection with Publicly available Radiological Images: A Systematic Analysis,"Pancreatic carcinoma is the fifth deadliest melanoma existing worldwide. It shares the maximum percentage of total mortalities caused by cancer every year. The main cause of the high mortality and minimal survival rate is the delayed detection of abnormal cell growth in pancreatic regions of patients diagnosed with this ailment. In recent years, researchers have been putting effort into the early detection of pancreatic carcinoma in radiological imaging scans of the whole abdomen. In this paper, the authors have systematically reviewed the data reported and various works done on publicly available imaging datasets of pancreatic cancer. The analyzed datasets are Pancreas- Computed Tomography, Clinical Proteomic Tumor Analysis Consortium Pancreatic Ductal Adenocarcinoma from The Cancer Imaging Archive, and Pancreas Tumor from the Medical Segmentation Decathlon online repository. The review is supported by reporting incidences depending on age group, clinical history, physical conditions, pathological findings, tumor nature, region, stage, and tumor size of examined patients. The outcomes of categorized subjects will aid academicians, research scholars, and industrialists to understand the propagation of pancreatic cancer for early detection in computer-aided systems.",,,,"2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",,2022-10-14,2022,,2022-10-14,0,,1-6,Closed,Proceeding,"Chhikara, Jasmine; Goel, Nidhi; Rathee, Neeru","Chhikara, Jasmine (Department of ECE, Indira Gandhi Delhi Technical University for Women Maharaja Surajmal Institute of Technology, Delhi, India); Goel, Nidhi (Department of ECE, Indira Gandhi Delhi Technical University for Women, Delhi, India); Rathee, Neeru (Department of ECE, Indira Gandhi Delhi Technical University for Women, Delhi, India)",,"Chhikara, Jasmine (Guru Gobind Singh Indraprastha University); Goel, Nidhi (Indira Gandhi Delhi Technical University for Women); Rathee, Neeru (Indira Gandhi Delhi Technical University for Women)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153498861,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,3 Good Health and Well Being,,,,,,,,,,
1018,pub.1151539572,10.48550/arxiv.2209.15287,,,Verifiable and Energy Efficient Medical Image Analysis with Quantised  Self-attentive Deep Neural Networks,"Convolutional Neural Networks have played a significant role in various
medical imaging tasks like classification and segmentation. They provide
state-of-the-art performance compared to classical image processing algorithms.
However, the major downside of these methods is the high computational
complexity, reliance on high-performance hardware like GPUs and the inherent
black-box nature of the model. In this paper, we propose quantised stand-alone
self-attention based models as an alternative to traditional CNNs. In the
proposed class of networks, convolutional layers are replaced with stand-alone
self-attention layers, and the network parameters are quantised after training.
We experimentally validate the performance of our method on classification and
segmentation tasks. We observe a $50-80\%$ reduction in model size, $60-80\%$
lesser number of parameters, $40-85\%$ fewer FLOPs and $65-80\%$ more energy
efficiency during inference on CPUs. The code will be available at \href
{https://github.com/Rakshith2597/Quantised-Self-Attentive-Deep-Neural-Network}{https://github.com/Rakshith2597/Quantised-Self-Attentive-Deep-Neural-Network}.",,,arXiv,,,2022-09-30,2022,,,,,,All OA, Green,Preprint,"Sathish, Rakshith; Khare, Swanand; Sheet, Debdoot","Sathish, Rakshith (); Khare, Swanand (); Sheet, Debdoot ()",,"Sathish, Rakshith (); Khare, Swanand (); Sheet, Debdoot ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151539572,46 Information and Computing Sciences, 4611 Machine Learning,7 Affordable and Clean Energy,,,,,,,,,
1018,pub.1141326812,10.1007/978-3-030-87199-4_24,,,Fully Test-Time Adaptation for Image Segmentation,"When adopting a model from the source domain to the target domain, its performance usually degrades due to the domain shift problem. In clinical practice, the source data usually cannot be accessed during adaptation for privacy policy and the label for the target domain is in shortage because of the high cost of professional labeling. Therefore, it is worth considering how to efficiently adopt a pretrained model with only unlabeled data from the target domain. In this paper, we propose a novel fully test-time unsupervised adaptation method for image segmentation based on Regional Nuclear-norm (RN) and Contour Regularization (CR). The RN loss is specially designed for segmentation tasks to efficiently improve discriminability and diversity of prediction. The CR loss constrains the continuity and connectivity to enhance the relevance between pixels and their neighbors. Instead of retraining all parameters, we modify only the parameters in batch normalization layers with only a few epochs. We demonstrate the effectiveness and efficiency of the proposed method in the pancreas and liver segmentation dataset from the Medical Segmentation Decathlon and CHAOS challenge.","This work is supported partially by SHEITC (No. 2018-RGZN-02046), 111 plan (No. BP0719010), and STCSM (No. 18DZ2270700).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12903,,251-260,Closed,Chapter,"Hu, Minhao; Song, Tao; Gu, Yujun; Luo, Xiangde; Chen, Jieneng; Chen, Yinan; Zhang, Ya; Zhang, Shaoting","Hu, Minhao (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; SenseTime Research, Shanghai, China); Song, Tao (SenseTime Research, Shanghai, China); Gu, Yujun (SenseTime Research, Shanghai, China); Luo, Xiangde (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China); Chen, Jieneng (College of Electronics and Information Technology, Tongji University, Shanghai, China); Chen, Yinan (SenseTime Research, Shanghai, China); Zhang, Ya (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China); Zhang, Shaoting (SenseTime Research, Shanghai, China)","Zhang, Ya (Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory)","Hu, Minhao (Shanghai Jiao Tong University); Song, Tao (); Gu, Yujun (); Luo, Xiangde (University of Electronic Science and Technology of China); Chen, Jieneng (Tongji University); Chen, Yinan (); Zhang, Ya (Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory); Zhang, Shaoting ()",5,5,,3.87,,https://app.dimensions.ai/details/publication/pub.1141326812,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
1018,pub.1139662333,10.48550/arxiv.2107.05471,,,The Power of Proxy Data and Proxy Networks for Hyper-Parameter  Optimization in Medical Image Segmentation,"Deep learning models for medical image segmentation are primarily
data-driven. Models trained with more data lead to improved performance and
generalizability. However, training is a computationally expensive process
because multiple hyper-parameters need to be tested to find the optimal setting
for best performance. In this work, we focus on accelerating the estimation of
hyper-parameters by proposing two novel methodologies: proxy data and proxy
networks. Both can be useful for estimating hyper-parameters more efficiently.
We test the proposed techniques on CT and MR imaging modalities using
well-known public datasets. In both cases using one dataset for building proxy
data and another data source for external evaluation. For CT, the approach is
tested on spleen segmentation with two datasets. The first dataset is from the
medical segmentation decathlon (MSD), where the proxy data is constructed, the
secondary dataset is utilized as an external validation dataset. Similarly, for
MR, the approach is evaluated on prostate segmentation where the first dataset
is from MSD and the second dataset is PROSTATEx. First, we show higher
correlation to using full data for training when testing on the external
validation set using smaller proxy data than a random selection of the proxy
data. Second, we show that a high correlation exists for proxy networks when
compared with the full network on validation Dice score. Third, we show that
the proposed approach of utilizing a proxy network can speed up an AutoML
framework for hyper-parameter search by 3.3x, and by 4.4x if proxy data and
proxy network are utilized together.",,,arXiv,,,2021-07-12,2021,,,,,,All OA, Green,Preprint,"Nath, Vishwesh; Yang, Dong; Hatamizadeh, Ali; Abidin, Anas A.; Myronenko, Andriy; Roth, Holger; Xu, Daguang","Nath, Vishwesh (); Yang, Dong (); Hatamizadeh, Ali (); Abidin, Anas A. (); Myronenko, Andriy (); Roth, Holger (); Xu, Daguang ()",,"Nath, Vishwesh (); Yang, Dong (); Hatamizadeh, Ali (); Abidin, Anas A. (); Myronenko, Andriy (); Roth, Holger (); Xu, Daguang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139662333,46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
1012,pub.1141446362,10.1016/s0167-8140(21)08147-0,,,PO-1696 Clinical commissioning a synchrotron-based proton therapy system,,,,Radiotherapy and Oncology,,,2021-08,2021,,2021-08,161,,s1422-s1423,Closed,Article,"Azcona, J.D.; Aguilar, B.; Irazola, L.; ViÃ±als, A.; Cabello, P.; Zucca, D.; Perales, A.; Polo, R.; Delgado, J.M.; Burguete, J.","Azcona, J.D. (); Aguilar, B. (); Irazola, L. (); ViÃ±als, A. (); Cabello, P. (); Zucca, D. (); Perales, A. (); Polo, R. (); Delgado, J.M. (); Burguete, J. ()",,"Azcona, J.D. (); Aguilar, B. (); Irazola, L. (); ViÃ±als, A. (); Cabello, P. (); Zucca, D. (); Perales, A. (); Polo, R. (); Delgado, J.M. (); Burguete, J. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141446362,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
1002,pub.1151381146,10.1109/cvpr52688.2022.01540,,,ImplicitAtlas: Learning Deformable Shape Templates in Medical Imaging,"Deep implicit shape models have become popular in the computer vision community at large but less so for biomed-ical applications. This is in part because large training databases do not exist and in part because biomedical an-notations are often noisy. In this paper, we show that by introducing templates within the deep learning pipeline we can overcome these problems. The proposed framework, named ImplicitAtlas, represents a shape as a deformation field from a learned template field, where multiple templates could be integrated to improve the shape representation ca-pacity at negligible computational cost. Extensive experi-ments on three medical shape datasets prove the superiority over current implicit representation methods.","This work was supported by National Science Foundation of China (U20B2072, 61976137). This work was also supported in part by a Swiss National Science Foundation grant. We would like to thank Jan Bednarik, Benoit Guillard, and Oner Doruk for their generous help in proofreading, and the anonymous (meta- )reviewers for their valuable comments.","This work was supported by National Science Foundation of China (U20B2072, 61976137). This work was also supported in part by a Swiss National Science Foundation grant. We would like to thank Jan Bednarik, Benoit Guillard, and Oner Doruk for their generous help in proofreading, and the anonymous (meta- )reviewers for their valuable comments.",,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2022-06-24,2022,,2022-06-24,0,,15840-15850,Closed,Proceeding,"Yang, Jiancheng; Wickramasinghe, Udaranga; Ni, Bingbing; Fua, Pascal","Yang, Jiancheng (Shanghai Jiao Tong University, Shanghai, China; EPFL, Lausanne, Switzerland); Wickramasinghe, Udaranga (EPFL, Lausanne, Switzerland); Ni, Bingbing (Shanghai Jiao Tong University, Shanghai, China); Fua, Pascal (EPFL, Lausanne, Switzerland)","Ni, Bingbing (Shanghai Jiao Tong University)","Yang, Jiancheng (Shanghai Jiao Tong University; Ãcole Polytechnique FÃ©dÃ©rale de Lausanne); Wickramasinghe, Udaranga (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne); Ni, Bingbing (Shanghai Jiao Tong University); Fua, Pascal (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne)",4,4,,,,https://app.dimensions.ai/details/publication/pub.1151381146,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1002,pub.1127622974,10.1109/wacv45572.2020.9093608,,,3D Semi-Supervised Learning with Uncertainty-Aware Multi-View Co-Training,"While making a tremendous impact in various fields, deep neural networks usually require large amounts of la-beled data for training which are expensive to collect in many applications, especially in the medical domain. Un-labeled data, on the other hand, is much more abundant. Semi-supervised learning techniques, such as co-training, could provide a powerful tool to leverage unlabeled data. In this paper, we propose a novel framework, uncertainty-aware multi-view co-training (UMCT), to address semi-supervised learning on 3D data, such as volumetric data from medical imaging. In our work, co-training is achieved by exploiting multi-viewpoint consistency of 3D data. We generate different views by rotating or permuting the 3D data and utilize asymmetrical 3D kernels to encourage diversified features in different sub-networks. In addition, we propose an uncertainty-weighted label fusion mechanism to estimate the reliability of each viewâs prediction with Bayesian deep learning. As one view requires the supervision from other views in co-training, our self-adaptive approach computes a confidence score for the prediction of each unlabeled sample in order to assign a reliable pseudo label. Thus, our approach can take advantage of unlabeled data during training. We show the effectiveness of our proposed semi-supervised method on several public datasets from medical image segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile, a fully-supervised method based on our approach achieved state-of-the-art performances on both the LiTS liver tumor segmentation and the Medical Segmentation Decathlon (MSD) challenge, demonstrating the robustness and value of our framework, even when fully supervised training is feasible.","We thank Dr. Lingxi Xie, Siyuan Qiao and Yuyin Zhou for instructive discussions.",,,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,2020-03-05,2020,,2020-03-05,0,,3635-3644,All OA, Green,Proceeding,"Xia, Yingda; Liu, Fengze; Yang, Dong; Cai, Jinzheng; Yu, Lequan; Zhu, Zhuotun; Xu, Daguang; Yuille, Alan; Roth, Holger","Xia, Yingda (Johns Hopkins University); Liu, Fengze (Johns Hopkins University); Yang, Dong (NVIDIA); Cai, Jinzheng (University of Florida); Yu, Lequan (The Chinese University of Hong Kong); Zhu, Zhuotun (Johns Hopkins University); Xu, Daguang (NVIDIA); Yuille, Alan (Johns Hopkins University); Roth, Holger (NVIDIA)","Xia, Yingda (Johns Hopkins University)","Xia, Yingda (Johns Hopkins University); Liu, Fengze (Johns Hopkins University); Yang, Dong (Nvidia (United States)); Cai, Jinzheng (University of Florida); Yu, Lequan (Chinese University of Hong Kong); Zhu, Zhuotun (Johns Hopkins University); Xu, Daguang (Nvidia (United States)); Yuille, Alan (Johns Hopkins University); Roth, Holger (Nvidia (United States))",74,60,,34.59,http://arxiv.org/pdf/1811.12506,https://app.dimensions.ai/details/publication/pub.1127622974,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
1001,pub.1151381613,10.1109/cvpr52688.2022.02007,,,Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis,"Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pretraining; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art on the public test leaderboards of both MSD11https://decathlon-10.grand-challenge.org/evaluation/challenge/leaderboard/ and BTCV 22https://www.synapse.org/#!Synapse:syn3193805/wiki/217785/ datasets. Code: https://monai.io/research/swin-unetr. https://decathlon-10.grand-challenge.org/evaluation/challenge/leaderboard/ https://www.synapse.org/#!Synapse:syn3193805/wiki/217785/",,,,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2022-06-24,2022,,2022-06-24,0,,20698-20708,All OA, Green,Proceeding,"Tang, Yucheng; Yang, Dong; Li, Wenqi; Roth, Holger R.; Landman, Bennett; Xu, Daguang; Nath, Vishwesh; Hatamizadeh, Ali","Tang, Yucheng (Vanderbilt University); Yang, Dong (NVIDIA); Li, Wenqi (NVIDIA); Roth, Holger R. (NVIDIA); Landman, Bennett (Vanderbilt University); Xu, Daguang (NVIDIA); Nath, Vishwesh (Vanderbilt University); Hatamizadeh, Ali (Vanderbilt University)","Hatamizadeh, Ali (Vanderbilt University)","Tang, Yucheng (Vanderbilt University); Yang, Dong (Nvidia (United States)); Li, Wenqi (Nvidia (United States)); Roth, Holger R. (Nvidia (United States)); Landman, Bennett (Vanderbilt University); Xu, Daguang (Nvidia (United States)); Nath, Vishwesh (Vanderbilt University); Hatamizadeh, Ali (Vanderbilt University)",44,44,,,http://arxiv.org/pdf/2111.14791,https://app.dimensions.ai/details/publication/pub.1151381613,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1001,pub.1142382343,10.1109/cvpr46437.2021.00578,,,DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation,"Recently, neural architecture search (NAS) has been applied to automatically search high-performance networks for medical image segmentation. The NAS search space usually contains a network topology level (controlling connections among cells with different spatial scales) and a cell level (operations within each cell). Existing methods either require long searching time for large-scale 3D image datasets, or are limited to pre-defined topologies (such as U-shaped or single-path) . In this work, we focus on three important aspects of NAS in 3D medical image segmentation: flexible multi-path network topology, high search efficiency, and budgeted GPU memory usage. A novel differentiable search framework is proposed to support fast gradient-based search within a highly flexible network topology search space. The discretization of the searched optimal continuous model in differentiable scheme may produce a sub-optimal final discrete model (discretization gap). Therefore, we propose a topology loss to alleviate this problem. In addition, the GPU memory usage for the searched 3D model is limited with budget constraints during search. Our Differentiable Network Topology Search scheme (DiNTS) is evaluated on the Medical Segmentation Decathlon (MSD) challenge, which contains ten challenging segmentation tasks. Our method achieves the state-of-the-art performance and the top ranking on the MSD challenge leaderboard.",,,,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2021-06-25,2021,,2021-06-25,0,,5837-5846,All OA, Green,Proceeding,"He, Yufan; Yang, Dong; Roth, Holger; Zhao, Can; Xu, Daguang","He, Yufan (Johns Hopkins University); Yang, Dong (NVIDIA); Roth, Holger (NVIDIA); Zhao, Can (NVIDIA); Xu, Daguang (NVIDIA)","He, Yufan (Johns Hopkins University)","He, Yufan (Johns Hopkins University); Yang, Dong (Nvidia (United States)); Roth, Holger (Nvidia (United States)); Zhao, Can (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",26,26,,21.28,http://arxiv.org/pdf/2103.15954,https://app.dimensions.ai/details/publication/pub.1142382343,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1001,pub.1136816579,10.48550/arxiv.2103.15954,,,DiNTS: Differentiable Neural Network Topology Search for 3D Medical  Image Segmentation,"Recently, neural architecture search (NAS) has been applied to automatically
search high-performance networks for medical image segmentation. The NAS search
space usually contains a network topology level (controlling connections among
cells with different spatial scales) and a cell level (operations within each
cell). Existing methods either require long searching time for large-scale 3D
image datasets, or are limited to pre-defined topologies (such as U-shaped or
single-path). In this work, we focus on three important aspects of NAS in 3D
medical image segmentation: flexible multi-path network topology, high search
efficiency, and budgeted GPU memory usage. A novel differentiable search
framework is proposed to support fast gradient-based search within a highly
flexible network topology search space. The discretization of the searched
optimal continuous model in differentiable scheme may produce a sub-optimal
final discrete model (discretization gap). Therefore, we propose a topology
loss to alleviate this problem. In addition, the GPU memory usage for the
searched 3D model is limited with budget constraints during search. Our
Differentiable Network Topology Search scheme (DiNTS) is evaluated on the
Medical Segmentation Decathlon (MSD) challenge, which contains ten challenging
segmentation tasks. Our method achieves the state-of-the-art performance and
the top ranking on the MSD challenge leaderboard.",,,arXiv,,,2021-03-29,2021,,,,,,All OA, Green,Preprint,"He, Yufan; Yang, Dong; Roth, Holger; Zhao, Can; Xu, Daguang","He, Yufan (); Yang, Dong (); Roth, Holger (); Zhao, Can (); Xu, Daguang ()",,"He, Yufan (); Yang, Dong (); Roth, Holger (); Zhao, Can (); Xu, Daguang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136816579,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
995,pub.1152464198,10.1109/icip46576.2022.9897847,,,Deep Learning Meets Radiomics For End-To-End Brain Tumor MRI Analysis,"Magnetic resonance imaging is the modality of choice in brain tumors to diagnose and monitor the patients with such lesions. However, its manual analysis is user-dependent and cumbersome, and is affected by significant intra- and inter-rater variability which hampers the objective assessment of the progression of the disease. In this work, we tackle this issue and introduce an end-to-end pipeline to not only segment the brain tumors in a fully-automated and reproducible way and calculate their volumetric characteristics, but also to benefit from radiomic features extracted from the tumorous tissue for classifying the scan as low- or high-grade gliomas. Our extensive experiments revealed that the automatically calculated volumetric measurements are in almost perfect agreement with human readers, and that the ensemble classifiers utilizing radiomic features extracted from whole-tumor areas deliver high-quality classification for a range of segmentation approaches. To make the experiments fully reproducible and to encourage other research groups to exploit our end-to-end pipeline, we made the implementation open-sourced.",,JN was supported by the Silesian University of Technology funds for developing and maintaining research potential.,,2022 IEEE International Conference on Image Processing (ICIP),,2022-10-19,2022,,2022-10-19,0,,1301-1305,Closed,Proceeding,"Ponikiewski, Wojciech; Nalepa, Jakub","Ponikiewski, Wojciech (Silesian University of Technology, Gliwice, Poland); Nalepa, Jakub (Silesian University of Technology, Gliwice, Poland)","Ponikiewski, Wojciech (Silesian University of Technology)","Ponikiewski, Wojciech (Silesian University of Technology); Nalepa, Jakub (Silesian University of Technology)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1152464198,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,,
995,pub.1148523314,10.1109/tcsii.2022.3181132,,,Implementation of a Modified U-Net for Medical Image Segmentation on Edge Devices,"Deep learning techniques, particularly convolutional neural networks, have shown great potential in computer vision and medical imaging applications. However, deep learning models are computationally demanding as they require enormous computational power and specialized processing hardware for model training. To make these models portable and compatible for prototyping, their implementation on low-power devices is imperative. In this brief, we present the implementation of Modified U-Net on Intel/Movidius Neural Compute Stick 2 (NCS-2) for the segmentation of medical images. We selected U-Net because, in medical image segmentation, U-Net is a prominent model that provides improved performance for medical image segmentation even if the dataset size is small. The modified U-Net model is evaluated for performance in terms of dice score. Experiments are reported for segmentation task on three medical imaging datasets: BraTs dataset of brain MRI, heart MRI dataset, and ZiehlâNeelsen sputum smear microscopy image (ZNSDB) dataset. For the proposed model, we reduced the number of parameters from 30 million in the U-Net model to 0.49 million in the proposed architecture. Experimental results show that the modified U-Net provides comparable performance while requiring significantly lower resources and provides inference on the NCS-2. The maximum dice scores recorded are 0.96 for the BraTs dataset, 0.94 for the heart MRI dataset, and 0.74 for the ZNSDB dataset.",,,IEEE Transactions on Circuits & Systems II Express Briefs,,,2022-06-08,2022,2022-06-08,2022-11-01,69,11,4593-4597,All OA, Green,Article,"Ali, Owais; Ali, Hazrat; Shah, Syed Ayaz Ali; Shahzad, Aamir","Ali, Owais (Department of Electrical and Computer Engineering, COMSATS University Islamabad, Abbottabad Campus, Abbottabad, 22060, Pakistan); Ali, Hazrat (Department of Electrical and Computer Engineering, COMSATS University Islamabad, Abbottabad Campus, Abbottabad, 22060, Pakistan); Shah, Syed Ayaz Ali (Department of Electrical and Computer Engineering, COMSATS University Islamabad, Abbottabad Campus, Abbottabad, 22060, Pakistan); Shahzad, Aamir (Department of Electrical and Computer Engineering, COMSATS University Islamabad, Abbottabad Campus, Abbottabad, 22060, Pakistan)","Ali, Hazrat (COMSATS University Islamabad)","Ali, Owais (COMSATS University Islamabad); Ali, Hazrat (COMSATS University Islamabad); Shah, Syed Ayaz Ali (COMSATS University Islamabad); Shahzad, Aamir (COMSATS University Islamabad)",2,2,,,http://arxiv.org/pdf/2206.02358,https://app.dimensions.ai/details/publication/pub.1148523314,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,
995,pub.1150453005,10.48550/arxiv.2208.10553,,,Split-U-Net: Preventing Data Leakage in Split Learning for Collaborative  Multi-Modal Brain Tumor Segmentation,"Split learning (SL) has been proposed to train deep learning models in a
decentralized manner. For decentralized healthcare applications with vertical
data partitioning, SL can be beneficial as it allows institutes with
complementary features or images for a shared set of patients to jointly
develop more robust and generalizable models. In this work, we propose
""Split-U-Net"" and successfully apply SL for collaborative biomedical image
segmentation. Nonetheless, SL requires the exchanging of intermediate
activation maps and gradients to allow training models across different feature
spaces, which might leak data and raise privacy concerns. Therefore, we also
quantify the amount of data leakage in common SL scenarios for biomedical image
segmentation and provide ways to counteract such leakage by applying
appropriate defense strategies.",,,arXiv,,,2022-08-22,2022,,,,,,All OA, Green,Preprint,"Roth, Holger R.; Hatamizadeh, Ali; Xu, Ziyue; Zhao, Can; Li, Wenqi; Myronenko, Andriy; Xu, Daguang","Roth, Holger R. (); Hatamizadeh, Ali (); Xu, Ziyue (); Zhao, Can (); Li, Wenqi (); Myronenko, Andriy (); Xu, Daguang ()",,"Roth, Holger R. (); Hatamizadeh, Ali (); Xu, Ziyue (); Zhao, Can (); Li, Wenqi (); Myronenko, Andriy (); Xu, Daguang ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150453005,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
993,pub.1148489483,10.48550/arxiv.2206.02358,,,Implementation of a Modified U-Net for Medical Image Segmentation on  Edge Devices,"Deep learning techniques, particularly convolutional neural networks, have
shown great potential in computer vision and medical imaging applications.
However, deep learning models are computationally demanding as they require
enormous computational power and specialized processing hardware for model
training. To make these models portable and compatible for prototyping, their
implementation on low-power devices is imperative. In this work, we present the
implementation of Modified U-Net on Intel Movidius Neural Compute Stick 2
(NCS-2) for the segmentation of medical images. We selected U-Net because, in
medical image segmentation, U-Net is a prominent model that provides improved
performance for medical image segmentation even if the dataset size is small.
The modified U-Net model is evaluated for performance in terms of dice score.
Experiments are reported for segmentation task on three medical imaging
datasets: BraTs dataset of brain MRI, heart MRI dataset, and Ziehl-Neelsen
sputum smear microscopy image (ZNSDB) dataset. For the proposed model, we
reduced the number of parameters from 30 million in the U-Net model to 0.49
million in the proposed architecture. Experimental results show that the
modified U-Net provides comparable performance while requiring significantly
lower resources and provides inference on the NCS-2. The maximum dice scores
recorded are 0.96 for the BraTs dataset, 0.94 for the heart MRI dataset, and
0.74 for the ZNSDB dataset.",,,arXiv,,,2022-06-06,2022,,,,,,All OA, Green,Preprint,"Ali, Owais; Ali, Hazrat; Shah, Syed Ayaz Ali; Shahzad, Aamir","Ali, Owais (); Ali, Hazrat (); Shah, Syed Ayaz Ali (); Shahzad, Aamir ()",,"Ali, Owais (); Ali, Hazrat (); Shah, Syed Ayaz Ali (); Shahzad, Aamir ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148489483,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,
992,pub.1151694530,10.1007/978-3-031-18523-6_5,,,Split-U-Net: Preventing Data Leakage in Split Learning for Collaborative Multi-modal Brain Tumor Segmentation,"Split learning (SL) has been proposed to train deep learning models in a decentralized manner. For decentralized healthcare applications with vertical data partitioning, SL can be beneficial as it allows institutes with complementary features or images for a shared set of patients to jointly develop more robust and generalizable models. In this work, we propose âSplit-U-Net"" and successfully apply SL for collaborative biomedical image segmentation. Nonetheless, SL requires the exchanging of intermediate activation maps and gradients to allow training models across different feature spaces, which might leak data and raise privacy concerns. Therefore, we also quantify the amount of data leakage in common SL scenarios for biomedical image segmentation and provide ways to counteract such leakage by applying appropriate defense strategies.",,,Lecture Notes in Computer Science,"Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health",,2022-10-07,2022,2022-10-07,2022,13573,,47-57,Closed,Chapter,"Roth, Holger R.; Hatamizadeh, Ali; Xu, Ziyue; Zhao, Can; Li, Wenqi; Myronenko, Andriy; Xu, Daguang","Roth, Holger R. (NVIDIA, Bethesda, USA); Hatamizadeh, Ali (NVIDIA, Bethesda, USA); Xu, Ziyue (NVIDIA, Bethesda, USA); Zhao, Can (NVIDIA, Bethesda, USA); Li, Wenqi (NVIDIA, Bethesda, USA); Myronenko, Andriy (NVIDIA, Bethesda, USA); Xu, Daguang (NVIDIA, Bethesda, USA)","Roth, Holger R. (Nvidia (United States))","Roth, Holger R. (Nvidia (United States)); Hatamizadeh, Ali (Nvidia (United States)); Xu, Ziyue (Nvidia (United States)); Zhao, Can (Nvidia (United States)); Li, Wenqi (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151694530,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
992,pub.1151189616,10.1007/978-3-031-17027-0_2,,,DeepEdit: Deep Editable Learning for Interactive Segmentation of 3D Medical Images,"Abstract
Automatic segmentation of medical images is a key step for diagnostic and interventional tasks. However, achieving this requires large amounts of annotated volumes, which can be tedious and time-consuming task for expert annotators. In this paper, we introduce DeepEdit, a deep learning-based method for volumetric medical image annotation, that allows automatic and semi-automatic segmentation, and click-based refinement. DeepEdit combines the power of two methods: a non-interactive (i.e. automatic segmentation using nnU-Net, UNET or UNETR) and an interactive segmentation method (i.e. DeepGrow), into a single deep learning model. It allows easy integration of uncertainty-based ranking strategies (i.e. aleatoric and epistemic uncertainty computation) and active learning. We propose and implement a method for training DeepEdit by using standard training combined with user interaction simulation. Once trained, DeepEdit allows clinicians to quickly segment their datasets by using the algorithm in auto segmentation mode or by providing clicks via a user interface (i.e. 3D Slicer, OHIF). We show the value of DeepEdit through evaluation on the PROSTATEx dataset for prostate/prostatic lesions and the Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) dataset for abdominal CT segmentation, using state-of-the-art network architectures as baseline for comparison. DeepEdit could reduce the time and effort annotating 3D medical images compared to DeepGrow alone. Source code is available at https://github.com/Project-MONAI/MONAILabel.",,,Lecture Notes in Computer Science,"Data Augmentation, Labelling, and Imperfections",,2022-09-16,2022,2022-09-16,2022,13567,,11-21,Closed,Chapter,"Diaz-Pinto, Andres; Mehta, Pritesh; Alle, Sachidanand; Asad, Muhammad; Brown, Richard; Nath, Vishwesh; Ihsani, Alvin; Antonelli, Michela; Palkovics, Daniel; Pinter, Csaba; Alkalay, Ron; Pieper, Steve; Roth, Holger R.; Xu, Daguang; Dogra, Prerna; Vercauteren, Tom; Feng, Andrew; Quraini, Abood; Ourselin, Sebastien; Cardoso, M. Jorge","Diaz-Pinto, Andres (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, UK; NVIDIA Santa Clara, Santa Clara, CA, USA); Mehta, Pritesh (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, UK); Alle, Sachidanand (NVIDIA Santa Clara, Santa Clara, CA, USA); Asad, Muhammad (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, UK); Brown, Richard (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, UK); Nath, Vishwesh (NVIDIA Santa Clara, Santa Clara, CA, USA); Ihsani, Alvin (NVIDIA Santa Clara, Santa Clara, CA, USA); Antonelli, Michela (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, UK); Palkovics, Daniel (Department of Periodontology, Semmelweis University, Budapest, Hungary); Pinter, Csaba (EBATINCA, S.L. Canary Islands, Spain); Alkalay, Ron (Beth Israel Deaconess Medical Center, Boston, MA, USA); Pieper, Steve (Isomics, Inc., Cambridge, MA, USA); Roth, Holger R. (NVIDIA Santa Clara, Santa Clara, CA, USA); Xu, Daguang (NVIDIA Santa Clara, Santa Clara, CA, USA); Dogra, Prerna (NVIDIA Santa Clara, Santa Clara, CA, USA); Vercauteren, Tom (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, UK); Feng, Andrew (NVIDIA Santa Clara, Santa Clara, CA, USA); Quraini, Abood (NVIDIA Santa Clara, Santa Clara, CA, USA); Ourselin, Sebastien (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, UK); Cardoso, M. Jorge (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, UK)","Diaz-Pinto, Andres (King's College London; )","Diaz-Pinto, Andres (King's College London); Mehta, Pritesh (King's College London); Alle, Sachidanand (); Asad, Muhammad (King's College London); Brown, Richard (King's College London); Nath, Vishwesh (); Ihsani, Alvin (); Antonelli, Michela (King's College London); Palkovics, Daniel (Semmelweis University); Pinter, Csaba (); Alkalay, Ron (Beth Israel Deaconess Medical Center); Pieper, Steve (); Roth, Holger R. (); Xu, Daguang (); Dogra, Prerna (); Vercauteren, Tom (King's College London); Feng, Andrew (); Quraini, Abood (); Ourselin, Sebastien (King's College London); Cardoso, M. Jorge (King's College London)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1151189616,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,,
990,pub.1150069389,10.48550/arxiv.2208.03217,,,Distance-based detection of out-of-distribution silent failures for  Covid-19 lung lesion segmentation,"Automatic segmentation of ground glass opacities and consolidations in chest
computer tomography (CT) scans can potentially ease the burden of radiologists
during times of high resource utilisation. However, deep learning models are
not trusted in the clinical routine due to failing silently on
out-of-distribution (OOD) data. We propose a lightweight OOD detection method
that leverages the Mahalanobis distance in the feature space and seamlessly
integrates into state-of-the-art segmentation pipelines. The simple approach
can even augment pre-trained models with clinically relevant uncertainty
quantification. We validate our method across four chest CT distribution shifts
and two magnetic resonance imaging applications, namely segmentation of the
hippocampus and the prostate. Our results show that the proposed method
effectively detects far- and near-OOD samples across all explored scenarios.",,,arXiv,,,2022-08-05,2022,,,,,,All OA, Green,Preprint,"Gonzalez, Camila; Gotkowski, Karol; Fuchs, Moritz; Bucher, Andreas; Dadras, Armin; Fischbach, Ricarda; Kaltenborn, Isabel; Mukhopadhyay, Anirban","Gonzalez, Camila (); Gotkowski, Karol (); Fuchs, Moritz (); Bucher, Andreas (); Dadras, Armin (); Fischbach, Ricarda (); Kaltenborn, Isabel (); Mukhopadhyay, Anirban ()",,"Gonzalez, Camila (); Gotkowski, Karol (); Fuchs, Moritz (); Bucher, Andreas (); Dadras, Armin (); Fischbach, Ricarda (); Kaltenborn, Isabel (); Mukhopadhyay, Anirban ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150069389,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
989,pub.1141448633,10.1016/s0167-8140(21)08148-2,,,"PO-1697 Gradient-based neuroevolution of augmenting topologies for compact, low compute deep ANN search",,,,Radiotherapy and Oncology,,,2021-08,2021,,2021-08,161,,s1423-s1425,Closed,Article,"Seetha, S. Thulasi; Driessens, K.; Woodruff, H.; Rancati, T.; Bertocchi, E.; Pastorino, U.; Lambin, P.","Seetha, S. Thulasi (); Driessens, K. (); Woodruff, H. (); Rancati, T. (); Bertocchi, E. (); Pastorino, U. (); Lambin, P. ()",,"Seetha, S. Thulasi (); Driessens, K. (); Woodruff, H. (); Rancati, T. (); Bertocchi, E. (); Pastorino, U. (); Lambin, P. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141448633,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
981,pub.1149409776,10.48550/arxiv.2207.04812,,,A clinically motivated self-supervised approach for content-based image  retrieval of CT liver images,"Deep learning-based approaches for content-based image retrieval (CBIR) of CT
liver images is an active field of research, but suffers from some critical
limitations. First, they are heavily reliant on labeled data, which can be
challenging and costly to acquire. Second, they lack transparency and
explainability, which limits the trustworthiness of deep CBIR systems. We
address these limitations by (1) proposing a self-supervised learning framework
that incorporates domain-knowledge into the training procedure and (2)
providing the first representation learning explainability analysis in the
context of CBIR of CT liver images. Results demonstrate improved performance
compared to the standard self-supervised approach across several metrics, as
well as improved generalisation across datasets. Further, we conduct the first
representation learning explainability analysis in the context of CBIR, which
reveals new insights into the feature extraction process. Lastly, we perform a
case study with cross-examination CBIR that demonstrates the usability of our
proposed framework. We believe that our proposed framework could play a vital
role in creating trustworthy deep CBIR systems that can successfully take
advantage of unlabeled data.",,,arXiv,,,2022-07-11,2022,,,,,,All OA, Green,Preprint,"WickstrÃ¸m, Kristoffer Knutsen; Ãstmo, Eirik Agnalt; Radiya, Keyur; Mikalsen, Karl Ãyvind; Kampffmeyer, Michael Christian; Jenssen, Robert","WickstrÃ¸m, Kristoffer Knutsen (); Ãstmo, Eirik Agnalt (); Radiya, Keyur (); Mikalsen, Karl Ãyvind (); Kampffmeyer, Michael Christian (); Jenssen, Robert ()",,"WickstrÃ¸m, Kristoffer Knutsen (); Ãstmo, Eirik Agnalt (); Radiya, Keyur (); Mikalsen, Karl Ãyvind (); Kampffmeyer, Michael Christian (); Jenssen, Robert ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149409776,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
969,pub.1131662241,10.1007/978-3-030-60799-9_35,,,A Novel Approach Based on Region Growing Algorithm for Liver and Spleen Segmentation from CT Scans,"In this paper, we propose a novel approach to adapt 2D region growing algorithms to volumetric segmentation of liver and spleen from Computed Tomography (CT) scans. Abdominal organ segmentation is an essential and time-consuming task in clinical radiology. The possibility to implement a semi-automatic segmentation system could speed up the time required to label the images and to improve the delineation results, minimizing both intra- and inter-operator variability.The proposed region growing algorithm exploits an initial seed point to perform the first slice-wise segmentation. Then, starting from this area, all other seeds are automatically discovered taking advantage of two data structures that we called Moving Average Seed Heatmap (MASH) and Area Union Map (AUM). The implemented mechanism avoids the choice of unsuitable seeds and the exclusion of irrelevant organs and tissues from the CT scan.We assessed the validity of the proposed liver and spleen segmentation method on two publicly available datasets: SLIVER07 and Medical Segmentation Decathlon Task 09 (MSD 09), respectively.The proposed method allowed us to obtain promising results for both liver and spleen segmentation, with a Dice Coefficient higher than 93% for the liver segmentation task and a Dice Coefficient greater than 92% for the spleen segmentation task on the designated validation sets.",,,Lecture Notes in Computer Science,Intelligent Computing Theories and Application,,2020-10-05,2020,2020-10-05,2020,12463,,398-410,Closed,Chapter,"Prencipe, Berardino; Altini, Nicola; Cascarano, Giacomo Donato; Guerriero, Andrea; Brunetti, Antonio","Prencipe, Berardino (Polytechnic University of Bari, 70126, Bari, Italy); Altini, Nicola (Polytechnic University of Bari, 70126, Bari, Italy); Cascarano, Giacomo Donato (Polytechnic University of Bari, 70126, Bari, Italy; Apulian Bioengineering srl, Via delle Violette 14, 70026, Modugno, BA, Italy); Guerriero, Andrea (Polytechnic University of Bari, 70126, Bari, Italy); Brunetti, Antonio (Polytechnic University of Bari, 70126, Bari, Italy; Apulian Bioengineering srl, Via delle Violette 14, 70026, Modugno, BA, Italy)","Brunetti, Antonio (Polytechnic University of Bari; )","Prencipe, Berardino (Polytechnic University of Bari); Altini, Nicola (Polytechnic University of Bari); Cascarano, Giacomo Donato (Polytechnic University of Bari); Guerriero, Andrea (Polytechnic University of Bari); Brunetti, Antonio (Polytechnic University of Bari)",5,5,,,,https://app.dimensions.ai/details/publication/pub.1131662241,46 Information and Computing Sciences,,,,,,,,,,,,
969,pub.1137706308,10.1145/3442555.3442570,,,A Content-Driven Architecture for Medical Image Segmentation,"As the U-Net architecture continues to push the state of the art in medical image segmentation, and keeping track of the multitude of derived models becomes increasingly difficult, opposing trends have emerged that prefer a coherent framework of ancillary processing tasks over the use of highly optimized and sophisticated models. This trend has culminated in the framework nnU-Net, which adapts preprocessing, training and inference to the respective dataset and thus managed to lead the Medical Segmentation Decathlon challenge despite relying on comparably simple models. In this paper, we focus on one ancillary technique that is commonly used but poorly addressed in literature: patchwise training. Since computational costs tend to increase drastically when using naive strategies, we discuss the benefits of content-sensitive sampling for patchwise training of deep learning segmentation models. We adapt this strategy in a content-driven architecture for abdominal aorta and stent-graft segmentation, where we evaluate and compare it with traditional sampling strategies based on a real-world clinical dataset.",,,,Proceedings of the 6th International Conference on Communication and Information Processing,,2020-11-27,2020,2021-05-02,2020-11-27,,,89-96,Closed,Proceeding,"Sabrowsky-Hirsch, Bertram; Thumfart, Stefan; Hofer, Richard; Fenz, Wolfgang","Sabrowsky-Hirsch, Bertram (RISC Software GmbH, Austria); Thumfart, Stefan (RISC Software GmbH, Austria); Hofer, Richard (RISC Software GmbH, Austria); Fenz, Wolfgang (RISC Software GmbH, Austria)",,"Sabrowsky-Hirsch, Bertram (RISC Software (Austria)); Thumfart, Stefan (RISC Software (Austria)); Hofer, Richard (RISC Software (Austria)); Fenz, Wolfgang (RISC Software (Austria))",1,1,,0.52,,https://app.dimensions.ai/details/publication/pub.1137706308,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
950,pub.1152570366,10.48550/arxiv.2211.02701,,,MONAI: An open-source framework for deep learning in healthcare,"Artificial Intelligence (AI) is having a tremendous impact across most areas
of science. Applications of AI in healthcare have the potential to improve our
ability to detect, diagnose, prognose, and intervene on human disease. For AI
models to be used clinically, they need to be made safe, reproducible and
robust, and the underlying software framework must be aware of the
particularities (e.g. geometry, physiology, physics) of medical data being
processed. This work introduces MONAI, a freely available, community-supported,
and consortium-led PyTorch-based framework for deep learning in healthcare.
MONAI extends PyTorch to support medical data, with a particular focus on
imaging, and provide purpose-specific AI model architectures, transformations
and utilities that streamline the development and deployment of medical AI
models. MONAI follows best practices for software-development, providing an
easy-to-use, robust, well-documented, and well-tested software framework. MONAI
preserves the simple, additive, and compositional approach of its underlying
PyTorch libraries. MONAI is being used by and receiving contributions from
research, clinical and industrial teams from around the world, who are pursuing
applications spanning nearly every aspect of healthcare.",,,arXiv,,,2022-11-04,2022,,,,,,All OA, Green,Preprint,"Cardoso, M. Jorge; Li, Wenqi; Brown, Richard; Ma, Nic; Kerfoot, Eric; Wang, Yiheng; Murrey, Benjamin; Myronenko, Andriy; Zhao, Can; Yang, Dong; Nath, Vishwesh; He, Yufan; Xu, Ziyue; Hatamizadeh, Ali; Myronenko, Andriy; Zhu, Wentao; Liu, Yun; Zheng, Mingxin; Tang, Yucheng; Yang, Isaac; Zephyr, Michael; Hashemian, Behrooz; Alle, Sachidanand; Darestani, Mohammad Zalbagi; Budd, Charlie; Modat, Marc; Vercauteren, Tom; Wang, Guotai; Li, Yiwen; Hu, Yipeng; Fu, Yunguan; Gorman, Benjamin; Johnson, Hans; Genereaux, Brad; Erdal, Barbaros S.; Gupta, Vikash; Diaz-Pinto, Andres; Dourson, Andre; Maier-Hein, Lena; Jaeger, Paul F.; Baumgartner, Michael; Kalpathy-Cramer, Jayashree; Flores, Mona; Kirby, Justin; Cooper, Lee A. D.; Roth, Holger R.; Xu, Daguang; Bericat, David; Floca, Ralf; Zhou, S. Kevin; Shuaib, Haris; Farahani, Keyvan; Maier-Hein, Klaus H.; Aylward, Stephen; Dogra, Prerna; Ourselin, Sebastien; Feng, Andrew","Cardoso, M. Jorge (); Li, Wenqi (); Brown, Richard (); Ma, Nic (); Kerfoot, Eric (); Wang, Yiheng (); Murrey, Benjamin (); Myronenko, Andriy (); Zhao, Can (); Yang, Dong (); Nath, Vishwesh (); He, Yufan (); Xu, Ziyue (); Hatamizadeh, Ali (); Zhu, Wentao (); Liu, Yun (); Zheng, Mingxin (); Tang, Yucheng (); Yang, Isaac (); Zephyr, Michael (); Hashemian, Behrooz (); Alle, Sachidanand (); Darestani, Mohammad Zalbagi (); Budd, Charlie (); Modat, Marc (); Vercauteren, Tom (); Wang, Guotai (); Li, Yiwen (); Hu, Yipeng (); Fu, Yunguan (); Gorman, Benjamin (); Johnson, Hans (); Genereaux, Brad (); Erdal, Barbaros S. (); Gupta, Vikash (); Diaz-Pinto, Andres (); Dourson, Andre (); Maier-Hein, Lena (); Jaeger, Paul F. (); Baumgartner, Michael (); Kalpathy-Cramer, Jayashree (); Flores, Mona (); Kirby, Justin (); Cooper, Lee A. D. (); Roth, Holger R. (); Xu, Daguang (); Bericat, David (); Floca, Ralf (); Zhou, S. Kevin (); Shuaib, Haris (); Farahani, Keyvan (); Maier-Hein, Klaus H. (); Aylward, Stephen (); Dogra, Prerna (); Ourselin, Sebastien (); Feng, Andrew ()",,"Cardoso, M. Jorge (); Li, Wenqi (); Brown, Richard (); Ma, Nic (); Kerfoot, Eric (); Wang, Yiheng (); Murrey, Benjamin (); Myronenko, Andriy (); Zhao, Can (); Yang, Dong (); Nath, Vishwesh (); He, Yufan (); Xu, Ziyue (); Hatamizadeh, Ali (); Zhu, Wentao (); Liu, Yun (); Zheng, Mingxin (); Tang, Yucheng (); Yang, Isaac (); Zephyr, Michael (); Hashemian, Behrooz (); Alle, Sachidanand (); Darestani, Mohammad Zalbagi (); Budd, Charlie (); Modat, Marc (); Vercauteren, Tom (); Wang, Guotai (); Li, Yiwen (); Hu, Yipeng (); Fu, Yunguan (); Gorman, Benjamin (); Johnson, Hans (); Genereaux, Brad (); Erdal, Barbaros S. (); Gupta, Vikash (); Diaz-Pinto, Andres (); Dourson, Andre (); Maier-Hein, Lena (); Jaeger, Paul F. (); Baumgartner, Michael (); Kalpathy-Cramer, Jayashree (); Flores, Mona (); Kirby, Justin (); Cooper, Lee A. D. (); Roth, Holger R. (); Xu, Daguang (); Bericat, David (); Floca, Ralf (); Zhou, S. Kevin (); Shuaib, Haris (); Farahani, Keyvan (); Maier-Hein, Klaus H. (); Aylward, Stephen (); Dogra, Prerna (); Ourselin, Sebastien (); Feng, Andrew ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1152570366,46 Information and Computing Sciences, 4612 Software Engineering,,,,,,,,,,
950,pub.1137337991,10.48550/arxiv.2104.08717,,,The hidden label-marginal biases of segmentation losses,"Most segmentation losses are arguably variants of the Cross-Entropy (CE) or
Dice losses. In the abundant segmentation literature, there is no clear
consensus as to which of these losses is a better choice, with varying
performances for each across different benchmarks and applications. In this
work, we develop a theoretical analysis that links these two types of losses,
exposing their advantages and weaknesses. First, we provide a
constrained-optimization perspective showing that CE and Dice share a much
deeper connection than previously thought: They both decompose into
label-marginal penalties and closely related ground-truth matching penalties.
Then, we provide bound relationships and an information-theoretic analysis,
which uncover hidden label-marginal biases: Dice has an intrinsic bias towards
specific extremely imbalanced solutions, whereas CE implicitly encourages the
ground-truth region proportions. Our theoretical results explain the wide
experimental evidence in the medical-imaging literature, whereby Dice losses
bring improvements for imbalanced segmentation. It also explains why CE
dominates natural-image problems with diverse class proportions, in which case
Dice might have difficulty adapting to different label-marginal distributions.
Based on our theoretical analysis, we propose a principled and simple solution,
which enables to control explicitly the label-marginal bias. Our loss
integrates CE with explicit ${\cal L}_1$ regularization, which encourages label
marginals to match target class proportions, thereby mitigating class imbalance
but without losing generality. Comprehensive experiments and ablation studies
over different losses and applications validate our theoretical analysis, as
well as the effectiveness of our explicit label-marginal regularizers.",,,arXiv,,,2021-04-18,2021,,,,,,All OA, Green,Preprint,"Liu, Bingyuan; Dolz, Jose; Galdran, Adrian; Kobbi, Riadh; Ayed, Ismail Ben","Liu, Bingyuan (); Dolz, Jose (); Galdran, Adrian (); Kobbi, Riadh (); Ayed, Ismail Ben ()",,"Liu, Bingyuan (); Dolz, Jose (); Galdran, Adrian (); Kobbi, Riadh (); Ayed, Ismail Ben ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1137337991,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
922,pub.1131294258,10.1109/ijcnn48605.2020.9206944,,,PGD-UNet: A Position-Guided Deformable Network for Simultaneous Segmentation of Organs and Tumors,"Precise segmentation of organs and tumors plays a crucial role in clinical applications. It is a challenging task due to the irregular shapes and various sizes of organs and tumors as well as the significant class imbalance between the anatomy of interest (AOI) and the background region. In addition, in most situation tumors and normal organs often overlap in medical images, but current approaches fail to delineate both tumors and organs accurately. To tackle such challenges, we propose a position-guided deformable UNet, namely PGD-UNet, which exploits the spatial deformation capabilities of deformable convolution to deal with the geometric transformation of both organs and tumors. Position information is explicitly encoded into the network to enhance the capabilities of deformation. Meanwhile, we introduce a new pooling module to preserve position information lost in conventional max-pooling operation. Besides, due to unclear boundaries between different structures as well as the subjectivity of annotations, labels are not necessarily accurate for medical image segmentation tasks. It may cause the overfitting of the trained network due to label noise. To address this issue, we formulate a novel loss function to suppress the influence of potential label noise on the training process. Our method was evaluated on two challenging segmentation tasks and achieved very promising segmentation accuracy in both tasks.","This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant No. 61671151 and 61573097, the Natural Science Foundation of JiangSu Province under Grant No. BK20181265, the Australian Research Council (ARC) under Grant No. LP170100416, LP180100114 and DP200102611, and the Research Grants Council of the Hong Kong SAR under Project CityU11202418. This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant No. 61671151 and 61573097, the Natural Science Foundation of JiangSu Province under Grant No. BK20181265, the Australian Research Council (ARC) under Grant No. LP170100416, LP180100114 and DP200102611, and the Research Grants Council of the Hong Kong SAR under Project CityU11202418.",,,2020 International Joint Conference on Neural Networks (IJCNN),,2020-07-24,2020,,2020-07-24,0,,1-8,All OA, Green,Proceeding,"Li, Ziqiang; Pan, Hong; Zhu, Yaping; Qin, A. K.","Li, Ziqiang (School of Automation, Southeast University, No.2 Sipailou, Nanjing, China, 210096); Pan, Hong (Department of Computer Science and Software Engineering, Swinburne University of Technology, John St, Hawthorn, VIC, 3122, Australia); Zhu, Yaping (School of Information and Communication Engineering, Communication University of China, No.1 Dingfuzhuang East Street, Beijing, China, 100024); Qin, A. K. (Department of Computer Science and Software Engineering, Swinburne University of Technology, John St, Hawthorn, VIC, 3122, Australia)","Li, Ziqiang (Southeast University)","Li, Ziqiang (Southeast University); Pan, Hong (Swinburne University of Technology); Zhu, Yaping (Communication University of China); Qin, A. K. (Swinburne University of Technology)",4,4,,2.43,http://arxiv.org/pdf/2007.01001,https://app.dimensions.ai/details/publication/pub.1131294258,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
921,pub.1128948243,10.48550/arxiv.2007.01001,,,PGD-UNet: A Position-Guided Deformable Network for Simultaneous  Segmentation of Organs and Tumors,"Precise segmentation of organs and tumors plays a crucial role in clinical
applications. It is a challenging task due to the irregular shapes and various
sizes of organs and tumors as well as the significant class imbalance between
the anatomy of interest (AOI) and the background region. In addition, in most
situation tumors and normal organs often overlap in medical images, but current
approaches fail to delineate both tumors and organs accurately. To tackle such
challenges, we propose a position-guided deformable UNet, namely PGD-UNet,
which exploits the spatial deformation capabilities of deformable convolution
to deal with the geometric transformation of both organs and tumors. Position
information is explicitly encoded into the network to enhance the capabilities
of deformation. Meanwhile, we introduce a new pooling module to preserve
position information lost in conventional max-pooling operation. Besides, due
to unclear boundaries between different structures as well as the subjectivity
of annotations, labels are not necessarily accurate for medical image
segmentation tasks. It may cause the overfitting of the trained network due to
label noise. To address this issue, we formulate a novel loss function to
suppress the influence of potential label noise on the training process. Our
method was evaluated on two challenging segmentation tasks and achieved very
promising segmentation accuracy in both tasks.",,,arXiv,,,2020-07-02,2020,,,,,,All OA, Green,Preprint,"Li, Ziqiang; Pan, Hong; Zhu, Yaping; Qin, A. K.","Li, Ziqiang (); Pan, Hong (); Zhu, Yaping (); Qin, A. K. ()",,"Li, Ziqiang (); Pan, Hong (); Zhu, Yaping (); Qin, A. K. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1128948243,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
920,pub.1155339932,10.21037/qims-22-544,,,UMRFormer-net: a three-dimensional U-shaped pancreas segmentation method based on a double-layer bridged transformer network,UMRFormer-net: a three-dimensional U-shaped pancreas segmentation method based on a double-layer bridged transformer network,,,Quantitative Imaging in Medicine and Surgery,,,2023-03,2023,2023-03,2023-03,0,0,0-0,All OA, Gold,Article,"Fang, Kun; He, Baochun; Liu, Libo; Hu, Haoyu; Fang, Chihua; Huang, Xuguang; Jia, Fucang","Fang, Kun (); He, Baochun (); Liu, Libo (); Hu, Haoyu (); Fang, Chihua (); Huang, Xuguang (); Jia, Fucang ()",,"Fang, Kun (); He, Baochun (); Liu, Libo (); Hu, Haoyu (); Fang, Chihua (); Huang, Xuguang (); Jia, Fucang ()",0,0,,,https://qims.amegroups.com/article/viewFile/109618/pdf,https://app.dimensions.ai/details/publication/pub.1155339932,40 Engineering, 4006 Communications Engineering,,,,,,,,,,
919,pub.1128884240,10.48550/arxiv.2006.16806,,,Uncertainty-aware multi-view co-training for semi-supervised medical  image segmentation and domain adaptation,"Although having achieved great success in medical image segmentation, deep
learning-based approaches usually require large amounts of well-annotated data,
which can be extremely expensive in the field of medical image analysis.
Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised
learning and unsupervised domain adaptation both take the advantage of
unlabeled data, and they are closely related to each other. In this paper, we
propose uncertainty-aware multi-view co-training (UMCT), a unified framework
that addresses these two tasks for volumetric medical image segmentation. Our
framework is capable of efficiently utilizing unlabeled data for better
performance. We firstly rotate and permute the 3D volumes into multiple views
and train a 3D deep network on each view. We then apply co-training by
enforcing multi-view consistency on unlabeled data, where an uncertainty
estimation of each view is utilized to achieve accurate labeling. Experiments
on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset
show state-of-the-art performance of the proposed framework on semi-supervised
medical image segmentation. Under unsupervised domain adaptation settings, we
validate the effectiveness of this work by adapting our multi-organ
segmentation model to two pathological organs from the Medical Segmentation
Decathlon Datasets. Additionally, we show that our UMCT-DA model can even
effectively handle the challenging situation where labeled source data is
inaccessible, demonstrating strong potentials for real-world applications.",,,arXiv,,,2020-06-28,2020,,,,,,All OA, Green,Preprint,"Xia, Yingda; Yang, Dong; Yu, Zhiding; Liu, Fengze; Cai, Jinzheng; Yu, Lequan; Zhu, Zhuotun; Xu, Daguang; Yuille, Alan; Roth, Holger","Xia, Yingda (); Yang, Dong (); Yu, Zhiding (); Liu, Fengze (); Cai, Jinzheng (); Yu, Lequan (); Zhu, Zhuotun (); Xu, Daguang (); Yuille, Alan (); Roth, Holger ()",,"Xia, Yingda (); Yang, Dong (); Yu, Zhiding (); Liu, Fengze (); Cai, Jinzheng (); Yu, Lequan (); Zhu, Zhuotun (); Xu, Daguang (); Yuille, Alan (); Roth, Holger ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1128884240,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
918,pub.1143469147,10.48550/arxiv.2111.14791,,,Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image  Analysis,"Vision Transformers (ViT)s have shown great performance in self-supervised
learning of global and local representations that can be transferred to
downstream applications. Inspired by these results, we introduce a novel
self-supervised learning framework with tailored proxy tasks for medical image
analysis. Specifically, we propose: (i) a new 3D transformer-based model,
dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for
self-supervised pre-training; (ii) tailored proxy tasks for learning the
underlying pattern of human anatomy. We demonstrate successful pre-training of
the proposed model on 5,050 publicly available computed tomography (CT) images
from various body organs. The effectiveness of our approach is validated by
fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV)
Segmentation Challenge with 13 abdominal organs and segmentation tasks from the
Medical Segmentation Decathlon (MSD) dataset. Our model is currently the
state-of-the-art (i.e. ranked 1st) on the public test leaderboards of both MSD
and BTCV datasets. Code: https://monai.io/research/swin-unetr",,,arXiv,,,2021-11-29,2021,,,,,,All OA, Green,Preprint,"Tang, Yucheng; Yang, Dong; Li, Wenqi; Roth, Holger; Landman, Bennett; Xu, Daguang; Nath, Vishwesh; Hatamizadeh, Ali","Tang, Yucheng (); Yang, Dong (); Li, Wenqi (); Roth, Holger (); Landman, Bennett (); Xu, Daguang (); Nath, Vishwesh (); Hatamizadeh, Ali ()",,"Tang, Yucheng (); Yang, Dong (); Li, Wenqi (); Roth, Holger (); Landman, Bennett (); Xu, Daguang (); Nath, Vishwesh (); Hatamizadeh, Ali ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143469147,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
918,pub.1148386351,10.1109/access.2022.3179844,,,Reducing CNN Textural Bias With k-Space Artifacts Improves Robustness,"Convolutional neural networks (CNNs) have become the de facto algorithms of choice for semantic segmentation tasks in biomedical image processing. Yet, models based on CNNs remain susceptible to the domain shift problem, where a mismatch between source and target distributions could lead to a drop in performance. CNNs were recently shown to exhibit a textural bias when processing natural images, and recent studies suggest that this bias also extends to the context of biomedical imaging. In this paper, we focus on Magnetic Resonance Images (MRI) and investigate textural bias in the context of ${k}$ -space artifacts (Gibbs, spike, and wraparound artifacts), which naturally manifest in clinical MRI scans. We show that carefully introducing such artifacts at training time can help reduce textural bias, and consequently lead to CNN models that are more robust to acquisition noise and out-of-distribution inference, including scans from hospitals not seen during training. We also present Gibbs ResUnet; a novel, end-to-end framework that automatically finds an optimal combination of Gibbs ${k}$ -space stylizations and segmentation model weights. We illustrate our findings on multimodal and multi-institutional clinical MRI datasets obtained retrospectively from the Medical Segmentation Decathlon $(n=750)$ and The Cancer Imaging Archive $(n=243)$ .",Ahmed E. Fetit would like to thank the UK Research and Innovation Centre for Doctoral Training in Artificial Intelligence for Healthcare for supporting his research in his role as a Senior Teaching Fellow. The results published here are in whole or part based upon data generated by the TCGA Research Network: http://cancergenome.nih.gov/.,The work of Ahmed E. Fetit was supported by the UK Research and Innovation Centre for Doctoral Training in Artificial Intelligence for Healthcare under Grant EP/S023283/1.,IEEE Access,,,2022-01-01,2022,2022-06-02,2022-01-01,10,,58431-58446,All OA, Gold,Article,"Cabrera, Yaniel; Fetit, Ahmed E.","Cabrera, Yaniel (Department of Computing, Imperial College London, London, SW7 2AZ, U.K.); Fetit, Ahmed E. (Department of Computing, Imperial College London, London, SW7 2AZ, U.K.; UKRI CDT in Artificial Intelligence for Healthcare, Imperial College London, London, SW7 2AZ, U.K.)","Fetit, Ahmed E. (Imperial College London; Imperial College London)","Cabrera, Yaniel (Imperial College London); Fetit, Ahmed E. (Imperial College London; Imperial College London)",0,0,,,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09786829.pdf,https://app.dimensions.ai/details/publication/pub.1148386351,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
914,pub.1140866840,10.48550/arxiv.2109.00903,,,Effect of the output activation function on the probabilities and errors  in medical image segmentation,"The sigmoid activation is the standard output activation function in binary
classification and segmentation with neural networks. Still, there exist a
variety of other potential output activation functions, which may lead to
improved results in medical image segmentation. In this work, we consider how
the asymptotic behavior of different output activation and loss functions
affects the prediction probabilities and the corresponding segmentation errors.
For cross entropy, we show that a faster rate of change of the activation
function correlates with better predictions, while a slower rate of change can
improve the calibration of probabilities. For dice loss, we found that the
arctangent activation function is superior to the sigmoid function.
Furthermore, we provide a test space for arbitrary output activation functions
in the area of medical image segmentation. We tested seven activation functions
in combination with three loss functions on four different medical image
segmentation tasks to provide a classification of which function is best suited
in this application scenario.",,,arXiv,,,2021-09-02,2021,,,,,,All OA, Green,Preprint,"Nieradzik, Lars; Scheuermann, Gerik; Saur, Dorothee; Gillmann, Christina","Nieradzik, Lars (); Scheuermann, Gerik (); Saur, Dorothee (); Gillmann, Christina ()",,"Nieradzik, Lars (); Scheuermann, Gerik (); Saur, Dorothee (); Gillmann, Christina ()",1,1,,0.82,,https://app.dimensions.ai/details/publication/pub.1140866840,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
899,pub.1143461502,10.1515/jisys-2021-0172,,,Radiography image analysis using cat swarm optimized deep belief networks,"Abstract Radiography images are widely utilized in the health sector to recognize the patient health condition. The noise and irrelevant region information minimize the entire disease detection accuracy and computation complexity. Therefore, in this study, statistical KolmogorovâSmirnov test has been integrated with wavelet transform to overcome the de-noising issues. Then the cat swarm-optimized deep belief network is applied to extract the features from the affected region. The optimized deep learning model reduces the feature training cost and time and improves the overall disease detection accuracy. The network learning process is enhanced according to the AdaDelta learning process, which replaces the learning parameter with a delta value. This process minimizes the error rate while recognizing the disease. The efficiency of the system evaluated using image retrieval in medical application dataset. This process helps to determine the various diseases such as breast, lung, and pediatric studies.",We would like to thank Dijlah University College for funding this research.,Funding information: This research was supported by the Dijlah University College l [grant number G2021-1].,Journal of Intelligent Systems,,,2021-11-30,2021,2021-11-30,2022-01-01,31,1,40-54,All OA, Gold,Article,"Elameer, Amer S.; Jaber, Mustafa Musa; Abd, Sura Khalil","Elameer, Amer S. (Biomedical Informatics College, University of Information Technology and Communications (UOITC), Baghdad, Iraq); Jaber, Mustafa Musa (Department of Computer Science, Dijlah University Collage, Baghdad, 00964, Iraq; Department of Computer Science, Al-Turath University College, Baghdad, Iraq); Abd, Sura Khalil (Department of Computer Science, Dijlah University Collage, Baghdad, 00964, Iraq)","Jaber, Mustafa Musa (; Al Turath University College)","Elameer, Amer S. (University of Information Technology and Communications); Jaber, Mustafa Musa (Al Turath University College); Abd, Sura Khalil ()",3,3,,2.46,https://www.degruyter.com/document/doi/10.1515/jisys-2021-0172/pdf,https://app.dimensions.ai/details/publication/pub.1143461502,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
898,pub.1146522595,10.48550/arxiv.2203.12362,,,MONAI Label: A framework for AI-assisted Interactive Labeling of 3D  Medical Images,"The lack of annotated datasets is a major challenge in training new
task-specific supervised AI algorithms as manual annotation is expensive and
time-consuming. To address this problem, we present MONAI Label, a free and
open-source platform that facilitates the development of AI-based applications
that aim at reducing the time required to annotate 3D medical image datasets.
Through MONAI Label researchers can develop annotation applications focusing on
their domain of expertise. It allows researchers to readily deploy their apps
as services, which can be made available to clinicians via their preferred
user-interface. Currently, MONAI Label readily supports locally installed
(3DSlicer) and web-based (OHIF) frontends, and offers two Active learning
strategies to facilitate and speed up the training of segmentation algorithms.
MONAI Label allows researchers to make incremental improvements to their
labeling apps by making them available to other researchers and clinicians
alike. Lastly, MONAI Label provides sample labeling apps, namely DeepEdit and
DeepGrow, demonstrating dramatically reduced annotation times.",,,arXiv,,,2022-03-23,2022,,,,,,All OA, Green,Preprint,"Diaz-Pinto, Andres; Alle, Sachidanand; Ihsani, Alvin; Asad, Muhammad; Nath, Vishwesh; PÃ©rez-GarcÃ­a, Fernando; Mehta, Pritesh; Li, Wenqi; Roth, Holger R.; Vercauteren, Tom; Xu, Daguang; Dogra, Prerna; Ourselin, Sebastien; Feng, Andrew; Cardoso, M. Jorge","Diaz-Pinto, Andres (); Alle, Sachidanand (); Ihsani, Alvin (); Asad, Muhammad (); Nath, Vishwesh (); PÃ©rez-GarcÃ­a, Fernando (); Mehta, Pritesh (); Li, Wenqi (); Roth, Holger R. (); Vercauteren, Tom (); Xu, Daguang (); Dogra, Prerna (); Ourselin, Sebastien (); Feng, Andrew (); Cardoso, M. Jorge ()",,"Diaz-Pinto, Andres (); Alle, Sachidanand (); Ihsani, Alvin (); Asad, Muhammad (); Nath, Vishwesh (); PÃ©rez-GarcÃ­a, Fernando (); Mehta, Pritesh (); Li, Wenqi (); Roth, Holger R. (); Vercauteren, Tom (); Xu, Daguang (); Dogra, Prerna (); Ourselin, Sebastien (); Feng, Andrew (); Cardoso, M. Jorge ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146522595,46 Information and Computing Sciences, 4608 Human-Centred Computing,,,,,,,,,,
890,pub.1135535334,10.3390/electronics10040431,,,Numerical Evaluation on Parametric Choices Influencing Segmentation Results in Radiology ImagesâA Multi-Dataset Study,"Medical image segmentation has gained greater attention over the past decade, especially in the field of image-guided surgery. Here, robust, accurate and fast segmentation tools are important for planning and navigation. In this work, we explore the Convolutional Neural Network (CNN) based approaches for multi-dataset segmentation from CT examinations. We hypothesize that selection of certain parameters in the network architecture design critically influence the segmentation results. We have employed two different CNN architectures, 3D-UNet and VGG-16, given that both networks are well accepted in the medical domain for segmentation tasks. In order to understand the efficiency of different parameter choices, we have adopted two different approaches. The first one combines different weight initialization schemes with different activation functions, whereas the second approach combines different weight initialization methods with a set of loss functions and optimizers. For evaluation, the 3D-UNet was trained with the Medical Segmentation Decathlon dataset and VGG-16 using LiTS data. The quality assessment done using eight quantitative metrics enhances the probability of using our proposed strategies for enhancing the segmentation results. Following a systematic approach in the evaluation of the results, we propose a few strategies that can be adopted for obtaining good segmentation results. Both of the architectures used in this work were selected on the basis of general acceptance in segmentation tasks for medical images based on their promising results compared to other state-of-the art networks. The highest Dice score obtained in 3D-UNet for the liver, pancreas and cardiac data was 0.897, 0.691 and 0.892. In the case of VGG-16, it was solely developed to work with liver data and delivered a Dice score of 0.921. From all the experiments conducted, we observed that two of the combinations with Xavier weight initialization (also known as Glorot), Adam optimiser, Cross Entropy loss (GloCEAdam) and LeCun weight initialization, cross entropy loss and Adam optimiser LecCEAdam worked best for most of the metrics in a 3D-UNet setting, while Xavier together with cross entropy loss and Tanh activation function (GloCEtanh) worked best for the VGG-16 network. Here, the parameter combinations are proposed on the basis of their contributions in obtaining optimal outcomes in segmentation evaluations. Moreover, we discuss that the preliminary evaluation results show that these parameters could later on be used for gaining more insights into model convergence and optimal solutions.The results from the quality assessment metrics and the statistical analysis validate our conclusions and we propose that the presented work can be used as a guide in choosing parameters for the best possible segmentation results for future works.",All the datasets that have been used for the training and validation are obtained from the public database. The authors would like to acknowledge the coordinators of LiTS Challenge dataset and Medical Segmentation Decathlon (MSD) challenge datasets for providing the same in public environment.,"This work is supported by H2020-MSCA-ITN Marie SkÅodowska-Curie Actions, Innovative Training Networks (ITN) -H2020 MSCA ITN 2016 GA EU project number 722068 High Performance Soft Tissue Navigation (HiPerNav).",Electronics,,,2021-02-10,2021,2021-02-10,,10,4,431,All OA, Gold,Article,"Prasad, Pravda Jith Ray; Survarachakan, Shanmugapriya; Khan, Zohaib Amjad; Lindseth, Frank; Elle, Ole Jakob; Albregtsen, Fritz; Kumar, Rahul Prasanna","Prasad, Pravda Jith Ray (The Intervention Centre, Oslo University Hospital, 0372 Oslo, Norway;, pjprasad@student.matnat.uio.no, (P.J.R.P.);, oleje@ifi.uio.no, (O.J.E.); Department of Informatics, University of Oslo, 0315 Oslo, Norway;, fritz@ifi.uio.no); Survarachakan, Shanmugapriya (Department of Computer Science, Norwegian University of Science and Technology, 7491 Trondheim, Norway;, shanmugapriya.survarachakan@ntnu.no, (S.S.);, frankl@ntnu.no, (F.L.)); Khan, Zohaib Amjad (L2TI, Institut GalilÃ©e, UniversitÃ© Sorbonne Paris Nord, UR 3043, 93430 Villetaneuse, France;, zohaibamjad.khan@univ-paris13.fr); Lindseth, Frank (Department of Computer Science, Norwegian University of Science and Technology, 7491 Trondheim, Norway;, shanmugapriya.survarachakan@ntnu.no, (S.S.);, frankl@ntnu.no, (F.L.)); Elle, Ole Jakob (The Intervention Centre, Oslo University Hospital, 0372 Oslo, Norway;, pjprasad@student.matnat.uio.no, (P.J.R.P.);, oleje@ifi.uio.no, (O.J.E.); Department of Informatics, University of Oslo, 0315 Oslo, Norway;, fritz@ifi.uio.no); Albregtsen, Fritz (Department of Informatics, University of Oslo, 0315 Oslo, Norway;, fritz@ifi.uio.no; Institute for Cancer Genetics and Informatics, Oslo University Hospital, 0379 Oslo, Norway); Kumar, Rahul Prasanna (The Intervention Centre, Oslo University Hospital, 0372 Oslo, Norway;, pjprasad@student.matnat.uio.no, (P.J.R.P.);, oleje@ifi.uio.no, (O.J.E.))","Kumar, Rahul Prasanna (Oslo University Hospital)","Prasad, Pravda Jith Ray (Oslo University Hospital; University of Oslo); Survarachakan, Shanmugapriya (Norwegian University of Science and Technology); Khan, Zohaib Amjad (Paris 13 University); Lindseth, Frank (Norwegian University of Science and Technology); Elle, Ole Jakob (Oslo University Hospital; University of Oslo); Albregtsen, Fritz (University of Oslo; Oslo University Hospital); Kumar, Rahul Prasanna (Oslo University Hospital)",4,4,,2.68,https://www.mdpi.com/2079-9292/10/4/431/pdf?version=1612933758,https://app.dimensions.ai/details/publication/pub.1135535334,"40 Engineering; 4009 Electronics, Sensors and Digital Hardware",,,,,,,,,,,
889,pub.1133320644,10.48550/arxiv.2012.03352,,,An Uncertainty-Driven GCN Refinement Strategy for Organ Segmentation,"Organ segmentation in CT volumes is an important pre-processing step in many
computer assisted intervention and diagnosis methods. In recent years,
convolutional neural networks have dominated the state of the art in this task.
However, since this problem presents a challenging environment due to high
variability in the organ's shape and similarity between tissues, the generation
of false negative and false positive regions in the output segmentation is a
common issue. Recent works have shown that the uncertainty analysis of the
model can provide us with useful information about potential errors in the
segmentation. In this context, we proposed a segmentation refinement method
based on uncertainty analysis and graph convolutional networks. We employ the
uncertainty levels of the convolutional network in a particular input volume to
formulate a semi-supervised graph learning problem that is solved by training a
graph convolutional network. To test our method we refine the initial output of
a 2D U-Net. We validate our framework with the NIH pancreas dataset and the
spleen dataset of the medical segmentation decathlon. We show that our method
outperforms the state-of-the-art CRF refinement method by improving the dice
score by 1% for the pancreas and 2% for spleen, with respect to the original
U-Net's prediction. Finally, we perform a sensitivity analysis on the
parameters of our proposal and discuss the applicability to other CNN
architectures, the results, and current limitations of the model for future
work in this research direction. For reproducibility purposes, we make our code
publicly available at https://github.com/rodsom22/gcn_refinement.",,,arXiv,,,2020-12-06,2020,,,,,,All OA, Green,Preprint,"Soberanis-Mukul, Roger D.; Navab, Nassir; Albarqouni, Shadi","Soberanis-Mukul, Roger D. (); Navab, Nassir (); Albarqouni, Shadi ()",,"Soberanis-Mukul, Roger D. (); Navab, Nassir (); Albarqouni, Shadi ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1133320644,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
879,pub.1150895527,10.1109/nss/mic44867.2021.9875787,,,Segmentation of the Hippocampus Head and Body: Comparison of Single Annotator and Multi-annotator,"The goal of this study was to see how well the multi-annotator segmentation framework performed for hippocampal head and body segmentation. In order to execute organ segmentation separately on the input MR image, the multi-annotator segmentation framework employs a number of distinct deep learning-based models with diverse loss functions, algorithms, and architectures. Each deep learning model generates a probability map, which is then fused by decision fusion algorithms or a deep learning-based combiner to produce a single binary map of the target organ/structure. The purpose of the multi-annotator segmentation method is to make use of the complimentary information generated/provided by the several independent annotators (independent deep learning models). This approach would profit from the synergistic impact of integrating the different annotators' decisions to achieve a better performance than each annotator working alone. For the delineation of the hippocampus head and body from MR images, the performance of the suggested multi-annotator approach was examined. Other segmentation approaches were used for evaluation, including a multi-view deep learning-based technique, Atlas-based methods, shape-based averaging (SBA), STAPLE, and majority voting. A variety of deep learning methods (including several architectures and loss functions) were implemented, with a residual architecture (ResNet-CE) with dilated convolutional kernels and a cross-entropy loss function demon-strating higher performance as a standalone model. For the body and head of the hippocampus, the ResNet-CE model attained Dice indices of 88 Â±61.7 and 88 Â±71.5, respectively. Overall, the suggested multi-annotator segmentation strategy beat other segmentation approaches with Dice indices of 91 Â±01.3 for the body and 91 Â±11.3 for the head, containing six separate deep learning models, including the ResNet model. Dice indices of 88 Â±41.5 for the body and 88 Â±51.5 for the head were obtained using the best atlas-based approach, and Dice indices of 88 Â±91.5 (body) and 89 Â±01.4 (head) were obtained using the multi-view segmentation method (head). This research showed that the proposed multi-annotator technique for seminal segmentation outperforms each of the annotators individually (independent deep learning models which are included in the multi-annotator approach). The proposed method could be used to improve the overall accuracy and performance of machine learning and/or deep learning-based approaches in seminal segmentation challenges.",,This work was supported by the Swiss National Science Foundation under grant SNRF 320030_176052.,,2021 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),,2021-10-23,2021,,2021-10-23,0,,1-3,Closed,Proceeding,"Arabi, Hossein; Zaidi, Habib","Arabi, Hossein (Division of Nuclear Medicine & Molecular Imaging, Geneva University Hospital, CH-1211, Geneva, Switzerland); Zaidi, Habib (Division of Nuclear Medicine & Molecular Imaging, Geneva University Hospital, CH-1211, Geneva, Switzerland)","Zaidi, Habib (University Hospital of Geneva)","Arabi, Hossein (University Hospital of Geneva); Zaidi, Habib (University Hospital of Geneva)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1150895527,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
878,pub.1148868369,10.48550/arxiv.2206.10241,,,Deep Active Latent Surfaces for Medical Geometries,"Shape priors have long been known to be effective when reconstructing 3D
shapes from noisy or incomplete data. When using a deep-learning based shape
representation, this often involves learning a latent representation, which can
be either in the form of a single global vector or of multiple local ones. The
latter allows more flexibility but is prone to overfitting. In this paper, we
advocate a hybrid approach representing shapes in terms of 3D meshes with a
separate latent vector at each vertex. During training the latent vectors are
constrained to have the same value, which avoids overfitting. For inference,
the latent vectors are updated independently while imposing spatial
regularization constraints. We show that this gives us both flexibility and
generalization capabilities, which we demonstrate on several medical image
processing tasks.",,,arXiv,,,2022-06-21,2022,,,,,,All OA, Green,Preprint,"Jensen, Patrick M.; Wickramasinghe, Udaranga; Dahl, Anders B.; Fua, Pascal; Dahl, Vedrana A.","Jensen, Patrick M. (); Wickramasinghe, Udaranga (); Dahl, Anders B. (); Fua, Pascal (); Dahl, Vedrana A. ()",,"Jensen, Patrick M. (); Wickramasinghe, Udaranga (); Dahl, Anders B. (); Fua, Pascal (); Dahl, Vedrana A. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148868369,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4611 Machine Learning",,,,,,,,,,,
878,pub.1148688544,10.48550/arxiv.2206.06657,,,The Open Kidney Ultrasound Data Set,"Ultrasound, because of its low cost, non-ionizing, and non-invasive
characteristics, has established itself as a cornerstone radiological
examination. Research on ultrasound applications has also expanded, especially
with image analysis with machine learning. However, ultrasound data are
frequently restricted to closed data sets, with only a few openly available.
Despite being a frequently examined organ, the kidney lacks a publicly
available ultrasonography data set. The proposed Open Kidney Ultrasound Data
Set is the first publicly available set of kidney brightness mode (B-mode)
ultrasound data that includes annotations for multi-class semantic
segmentation. It is based on data retrospectively collected in a 5-year period
from over 500 patients with a mean age of 53.2 +/- 14.7 years, body mass index
of 27.0 +/- 5.4 kg/m2, and most common primary diseases being diabetes
mellitus, immunoglobulin A (IgA) nephropathy, and hypertension. There are
labels for the view and fine-grained manual annotations from two expert
sonographers. Notably, this data includes native and transplanted kidneys.
Initial bench-marking measurements are performed, demonstrating a
state-of-the-art algorithm achieving a Dice Sorenson Coefficient of 0.85 for
the kidney capsule. This data set is a high-quality data set, including two
sets of expert annotations, with a larger breadth of images than previously
available. In increasing access to kidney ultrasound data, future researchers
may be able to create novel image analysis techniques for tissue
characterization, disease detection, and prognostication.",,,arXiv,,,2022-06-14,2022,,,,,,All OA, Green,Preprint,"Singla, Rohit; Ringstrom, Cailin; Hu, Grace; Lessoway, Victoria; Reid, Janice; Nguan, Christopher; Rohling, Robert","Singla, Rohit (); Ringstrom, Cailin (); Hu, Grace (); Lessoway, Victoria (); Reid, Janice (); Nguan, Christopher (); Rohling, Robert ()",,"Singla, Rohit (); Ringstrom, Cailin (); Hu, Grace (); Lessoway, Victoria (); Reid, Janice (); Nguan, Christopher (); Rohling, Robert ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148688544,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
877,pub.1145639403,10.1109/wacv51458.2022.00181,,,UNETR: Transformers for 3D Medical Image Segmentation,"Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful ""U-shaped"" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",,,,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),,2022-01-08,2022,,2022-01-08,0,,1748-1758,All OA, Green,Proceeding,"Hatamizadeh, Ali; Tang, Yucheng; Nath, Vishwesh; Yang, Dong; Myronenko, Andriy; Landman, Bennett; Roth, Holger R.; Xu, Daguang","Hatamizadeh, Ali (NVIDIA); Tang, Yucheng (Vanderbilt University); Nath, Vishwesh (NVIDIA); Yang, Dong (NVIDIA); Myronenko, Andriy (NVIDIA); Landman, Bennett (Vanderbilt University); Roth, Holger R. (NVIDIA); Xu, Daguang (NVIDIA)","Hatamizadeh, Ali (Nvidia (United States))","Hatamizadeh, Ali (Nvidia (United States)); Tang, Yucheng (Vanderbilt University); Nath, Vishwesh (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Landman, Bennett (Vanderbilt University); Roth, Holger R. (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",218,218,,,http://arxiv.org/pdf/2103.10504,https://app.dimensions.ai/details/publication/pub.1145639403,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
876,pub.1142920748,10.48550/arxiv.2111.11629,,,Uncertainty-Aware Deep Co-training for Semi-supervised Medical Image  Segmentation,"Semi-supervised learning has made significant strides in the medical domain
since it alleviates the heavy burden of collecting abundant pixel-wise
annotated data for semantic segmentation tasks. Existing semi-supervised
approaches enhance the ability to extract features from unlabeled data with
prior knowledge obtained from limited labeled data. However, due to the
scarcity of labeled data, the features extracted by the models are limited in
supervised learning, and the quality of predictions for unlabeled data also
cannot be guaranteed. Both will impede consistency training. To this end, we
proposed a novel uncertainty-aware scheme to make models learn regions
purposefully. Specifically, we employ Monte Carlo Sampling as an estimation
method to attain an uncertainty map, which can serve as a weight for losses to
force the models to focus on the valuable region according to the
characteristics of supervised learning and unsupervised learning.
Simultaneously, in the backward process, we joint unsupervised and supervised
losses to accelerate the convergence of the network via enhancing the gradient
flow between different tasks. Quantitatively, we conduct extensive experiments
on three challenging medical datasets. Experimental results show desirable
improvements to state-of-the-art counterparts.",,,arXiv,,,2021-11-22,2021,,,,,,All OA, Green,Preprint,"Zheng, Xu; Fu, Chong; Xie, Haoyu; Chen, Jialei; Wang, Xingwei; Sham, Chiu-Wing","Zheng, Xu (); Fu, Chong (); Xie, Haoyu (); Chen, Jialei (); Wang, Xingwei (); Sham, Chiu-Wing ()",,"Zheng, Xu (); Fu, Chong (); Xie, Haoyu (); Chen, Jialei (); Wang, Xingwei (); Sham, Chiu-Wing ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142920748,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
876,pub.1136569475,10.48550/arxiv.2103.10504,,,UNETR: Transformers for 3D Medical Image Segmentation,"Fully Convolutional Neural Networks (FCNNs) with contracting and expanding
paths have shown prominence for the majority of medical image segmentation
applications since the past decade. In FCNNs, the encoder plays an integral
role by learning both global and local features and contextual representations
which can be utilized for semantic output prediction by the decoder. Despite
their success, the locality of convolutional layers in FCNNs, limits the
capability of learning long-range spatial dependencies. Inspired by the recent
success of transformers for Natural Language Processing (NLP) in long-range
sequence learning, we reformulate the task of volumetric (3D) medical image
segmentation as a sequence-to-sequence prediction problem. We introduce a novel
architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer
as the encoder to learn sequence representations of the input volume and
effectively capture the global multi-scale information, while also following
the successful ""U-shaped"" network design for the encoder and decoder. The
transformer encoder is directly connected to a decoder via skip connections at
different resolutions to compute the final semantic segmentation output. We
have validated the performance of our method on the Multi Atlas Labeling Beyond
The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical
Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation
tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV
leaderboard. Code: https://monai.io/research/unetr",,,arXiv,,,2021-03-18,2021,,,,,,All OA, Green,Preprint,"Hatamizadeh, Ali; Tang, Yucheng; Nath, Vishwesh; Yang, Dong; Myronenko, Andriy; Landman, Bennett; Roth, Holger; Xu, Daguang","Hatamizadeh, Ali (); Tang, Yucheng (); Nath, Vishwesh (); Yang, Dong (); Myronenko, Andriy (); Landman, Bennett (); Roth, Holger (); Xu, Daguang ()",,"Hatamizadeh, Ali (); Tang, Yucheng (); Nath, Vishwesh (); Yang, Dong (); Myronenko, Andriy (); Landman, Bennett (); Roth, Holger (); Xu, Daguang ()",2,2,,1.58,,https://app.dimensions.ai/details/publication/pub.1136569475,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
863,pub.1146252434,10.48550/arxiv.2203.05709,,,Towards Bi-directional Skip Connections in Encoder-Decoder Architectures  and Beyond,"U-Net, as an encoder-decoder architecture with forward skip connections, has
achieved promising results in various medical image analysis tasks. Many recent
approaches have also extended U-Net with more complex building blocks, which
typically increase the number of network parameters considerably. Such
complexity makes the inference stage highly inefficient for clinical
applications. Towards an effective yet economic segmentation network design, in
this work, we propose backward skip connections that bring decoded features
back to the encoder. Our design can be jointly adopted with forward skip
connections in any encoder-decoder architecture forming a recurrence structure
without introducing extra parameters. With the backward skip connections, we
propose a U-Net based network family, namely Bi-directional O-shape networks,
which set new benchmarks on multiple public medical imaging segmentation
datasets. On the other hand, with the most plain architecture (BiO-Net),
network computations inevitably increase along with the pre-set recurrence
time. We have thus studied the deficiency bottleneck of such recurrent design
and propose a novel two-phase Neural Architecture Search (NAS) algorithm,
namely BiX-NAS, to search for the best multi-scale bi-directional skip
connections. The ineffective skip connections are then discarded to reduce
computational costs and speed up network inference. The finally searched
BiX-Net yields the least network complexity and outperforms other
state-of-the-art counterparts by large margins. We evaluate our methods on both
2D and 3D segmentation tasks in a total of six datasets. Extensive ablation
studies have also been conducted to provide a comprehensive analysis for our
proposed methods.",,,arXiv,,,2022-03-10,2022,,,,,,All OA, Green,Preprint,"Xiang, Tiange; Zhang, Chaoyi; Wang, Xinyi; Song, Yang; Liu, Dongnan; Huang, Heng; Cai, Weidong","Xiang, Tiange (); Zhang, Chaoyi (); Wang, Xinyi (); Song, Yang (); Liu, Dongnan (); Huang, Heng (); Cai, Weidong ()",,"Xiang, Tiange (); Zhang, Chaoyi (); Wang, Xinyi (); Song, Yang (); Liu, Dongnan (); Huang, Heng (); Cai, Weidong ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146252434,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
857,pub.1152494489,10.21203/rs.3.rs-2205040/v1,,,Semantic Attention guided multi-dimension information complementary network for medical image classification,"Biomedical image analysis, such as tissue and disease classification by using small-scale data, is a challenging and meaningful problem for clinical diagnosis. In previous works, most classical methods are using 2D convolution to extract feature from a single direction without making information of z-axis into consideration and havenât considered that information representations are different in every view direction. In this letter, an attention mechanism-based model is proposed to extract and utilize feature more efficiently under the consideration of various information representation from direction to direction. To reach this goal, the work mainly consists of three parts of characteristics. The first characteristic is a plug-and-play structure design with parallel feature extractor in the different axial directions of which is dedicated to solving the problem of losing spatial structure feature by traditional 2D convolution. The second characteristic is intensive skip connection blocks employed to optimize the phenomenon of insufficient transit of information flow which cause the deep layer to fail in learning biomedical attribute. The last step is a designed spatial attention module, whichâs applied to capture the long-range dependencies , suppress irrelated information and enhance significant spatial feature globally. The evaluation on various dataset shows that proposed model can achieve a competitive performance with comparison to other existed methods and reach the best 85.18 accuracy on average.",,,Research Square,,,2022-11-04,2022,2022-11-04,,,,,All OA, Green,Preprint,"Huang, Yixiang; Zhang, Lifu; Song, Ruoxi","Huang, Yixiang (Aerospace Information Research Institute, Chinese Academy of Sciences); Zhang, Lifu (Aerospace Information Research Institute, Chinese Academy of Sciences); Song, Ruoxi (Aerospace Information Research Institute, Chinese Academy of Sciences)",,"Huang, Yixiang (); Zhang, Lifu (); Song, Ruoxi ()",0,0,,,https://www.researchsquare.com/article/rs-2205040/latest.pdf,https://app.dimensions.ai/details/publication/pub.1152494489,40 Engineering, 46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,
843,pub.1152281081,10.48550/arxiv.2210.14974,,,SINCO: A Novel structural regularizer for image compression using  implicit neural representations,"Implicit neural representations (INR) have been recently proposed as deep
learning (DL) based solutions for image compression. An image can be compressed
by training an INR model with fewer weights than the number of image pixels to
map the coordinates of the image to corresponding pixel values. While
traditional training approaches for INRs are based on enforcing pixel-wise
image consistency, we propose to further improve image quality by using a new
structural regularizer. We present structural regularization for INR
compression (SINCO) as a novel INR method for image compression. SINCO imposes
structural consistency of the compressed images to the groundtruth by using a
segmentation network to penalize the discrepancy of segmentation masks
predicted from compressed images. We validate SINCO on brain MRI images by
showing that it can achieve better performance than some recent INR methods.",,,arXiv,,,2022-10-26,2022,,,,,,All OA, Green,Preprint,"Gao, Harry; Gan, Weijie; Sun, Zhixin; Kamilov, Ulugbek S.","Gao, Harry (); Gan, Weijie (); Sun, Zhixin (); Kamilov, Ulugbek S. ()",,"Gao, Harry (); Gan, Weijie (); Sun, Zhixin (); Kamilov, Ulugbek S. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152281081,40 Engineering, 4006 Communications Engineering, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,
843,pub.1151119182,10.48550/arxiv.2209.07851,,,Whole-Body Lesion Segmentation in 18F-FDG PET/CT,"There has been growing research interest in using deep learning based method
to achieve fully automated segmentation of lesion in Positron emission
tomography computed tomography(PET CT) scans for the prognosis of various
cancers. Recent advances in the medical image segmentation shows the nnUNET is
feasible for diverse tasks. However, lesion segmentation in the PET images is
not straightforward, because lesion and physiological uptake has similar
distribution patterns. The Distinction of them requires extra structural
information in the CT images. The present paper introduces a nnUNet based
method for the lesion segmentation task. The proposed model is designed on the
basis of the joint 2D and 3D nnUNET architecture to predict lesions across the
whole body. It allows for automated segmentation of potential lesions. We
evaluate the proposed method in the context of AutoPet Challenge, which
measures the lesion segmentation performance in the metrics of dice score,
false-positive volume and false-negative volume.",,,arXiv,,,2022-09-16,2022,,,,,,All OA, Green,Preprint,"Zhang, Jia; Huang, Yukun; Zhang, Zheng; Shi, Yuhang","Zhang, Jia (); Huang, Yukun (); Zhang, Zheng (); Shi, Yuhang ()",,"Zhang, Jia (); Huang, Yukun (); Zhang, Zheng (); Shi, Yuhang ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151119182,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
843,pub.1126394130,10.48550/arxiv.2004.03466,,,U-Net Using Stacked Dilated Convolutions for Medical Image Segmentation,"This paper proposes a novel U-Net variant using stacked dilated convolutions
for medical image segmentation (SDU-Net). SDU-Net adopts the architecture of
vanilla U-Net with modifications in the encoder and decoder operations (an
operation indicates all the processing for feature maps of the same
resolution). Unlike vanilla U-Net which incorporates two standard convolutions
in each encoder/decoder operation, SDU-Net uses one standard convolution
followed by multiple dilated convolutions and concatenates all dilated
convolution outputs as input to the next operation. Experiments showed that
SDU-Net outperformed vanilla U-Net, attention U-Net (AttU-Net), and recurrent
residual U-Net (R2U-Net) in all four tested segmentation tasks while using
parameters around 40% of vanilla U-Net's, 17% of AttU-Net's, and 15% of
R2U-Net's.",,,arXiv,,,2020-04-07,2020,,,,,,All OA, Green,Preprint,"Wang, Shuhang; Hu, Szu-Yeu; Cheah, Eugene; Wang, Xiaohong; Wang, Jingchao; Chen, Lei; Baikpour, Masoud; Ozturk, Arinc; Li, Qian; Chou, Shinn-Huey; Lehman, Constance D.; Kumar, Viksit; Samir, Anthony","Wang, Shuhang (); Hu, Szu-Yeu (); Cheah, Eugene (); Wang, Xiaohong (); Wang, Jingchao (); Chen, Lei (); Baikpour, Masoud (); Ozturk, Arinc (); Li, Qian (); Chou, Shinn-Huey (); Lehman, Constance D. (); Kumar, Viksit (); Samir, Anthony ()",,"Wang, Shuhang (); Hu, Szu-Yeu (); Cheah, Eugene (); Wang, Xiaohong (); Wang, Jingchao (); Chen, Lei (); Baikpour, Masoud (); Ozturk, Arinc (); Li, Qian (); Chou, Shinn-Huey (); Lehman, Constance D. (); Kumar, Viksit (); Samir, Anthony ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1126394130,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
843,pub.1144683139,10.1007/978-3-030-93722-5_37,,,Automated Segmentation of the Right Ventricle from Magnetic Resonance Imaging Using Deep Convolutional Neural Networks,"Although the left ventricle (LV) is commonly assessed in current clinical practice, the assessment of the right ventricle (RV) also plays an important role in the diagnosis of cardiovascular disease. RV failure has numerous causes, including pulmonary hypertension, myocardial infarction, and congenital heart disease. However, assessment of the RV is more challenging than the LV due to its complex shape and thin walls. This study proposes an automated approach to delineate the RV from magnetic resonance imaging (MRI) scans using a deep convolutional neural network approach. The proposed method uses nnU-Net, a self-adapting framework based on the U-Net neural network approach for the segmentation of the RV from short and long axis MRI images at the end-systolic and end-diastolic phases of the heart. The proposed neural network models were trained using the datasets provided by Multi-Disease, Multi-View & Multi-Center Right Ventricular Segmentation in Cardiac MRI Challenge hosted by MICCAI 2021 conference. The quantitative evaluations were performed by the challenge organizers on a test set consisting of MRI scans acquired from 160 patients where the images and ground truth were blinded to the challenge participants. The proposed method yielded an overall Dice metric of 92.47%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$92.47\%$$\end{document} with 92.73%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$92.73\%$$\end{document} and 91.71%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$91.71\%$$\end{document} for short and long axis images, respectively. The corresponding Hausdorff distance values were 9.08Â mm, 10.05Â mm, and 6.16Â mm, respectively.",The authors wish to thank the challenge organizers for providing train and test datasets as well as performing the algorithm evaluation. The authors of this paper declare that the segmentation method they implemented for participation in the M&amp,Ms challenge has not used any pre-trained models nor additional MRI datasets other than those provided by the organizers. A. Carscadden was supported by an Undergraduate Student Research Award by the Natural Sciences and Engineering Research Council of Canada (NSERC). This research was enabled in part by computing support provided by Compute Canada (www.computecanada.ca) and WestGrid.,,Lecture Notes in Computer Science,"Statistical Atlases and Computational Models of the Heart. Multi-Disease, Multi-View, and Multi-Center Right Ventricular Segmentation in Cardiac MRI Challenge",,2022-01-14,2022,2022-01-14,2022,13131,,344-351,Closed,Chapter,"Punithakumar, Kumaradevan; Carscadden, Adam; Noga, Michelle","Punithakumar, Kumaradevan (Department of Radiology and Diagnostic Imaging, University of Alberta, Edmonton, Canada; Servier Virtual Cardiac Centre, Mazankowski Alberta Heart Institute, Edmonton, Canada); Carscadden, Adam (Department of Radiology and Diagnostic Imaging, University of Alberta, Edmonton, Canada; Servier Virtual Cardiac Centre, Mazankowski Alberta Heart Institute, Edmonton, Canada); Noga, Michelle (Department of Radiology and Diagnostic Imaging, University of Alberta, Edmonton, Canada; Servier Virtual Cardiac Centre, Mazankowski Alberta Heart Institute, Edmonton, Canada)","Punithakumar, Kumaradevan (University of Alberta; Alberta Health Services)","Punithakumar, Kumaradevan (University of Alberta; Alberta Health Services); Carscadden, Adam (University of Alberta; Alberta Health Services); Noga, Michelle (University of Alberta; Alberta Health Services)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144683139,46 Information and Computing Sciences,,,,,,,,,,,
837,pub.1152337565,10.48550/arxiv.2210.15949,,,IB-U-Nets: Improving medical image segmentation tasks with 3D Inductive  Biased kernels,"Despite the success of convolutional neural networks for 3D medical-image
segmentation, the architectures currently used are still not robust enough to
the protocols of different scanners, and the variety of image properties they
produce. Moreover, access to large-scale datasets with annotated regions of
interest is scarce, and obtaining good results is thus difficult. To overcome
these challenges, we introduce IB-U-Nets, a novel architecture with inductive
bias, inspired by the visual processing in vertebrates. With the 3D U-Net as
the base, we add two 3D residual components to the second encoder blocks. They
provide an inductive bias, helping U-Nets to segment anatomical structures from
3D images with increased robustness and accuracy. We compared IB-U-Nets with
state-of-the-art 3D U-Nets on multiple modalities and organs, such as the
prostate and spleen, using the same training and testing pipeline, including
data processing, augmentation and cross-validation. Our results demonstrate the
superior robustness and accuracy of IB-U-Nets, especially on small datasets, as
is typically the case in medical-image analysis. IB-U-Nets source code and
models are publicly available.",,,arXiv,,,2022-10-28,2022,,,,,,All OA, Green,Preprint,"Bhandary, Shrajan; Babaiee, Zahra; Kostyszyn, Dejan; Fechter, Tobias; Zamboglou, Constantinos; Grosu, Anca-Ligia; Grosu, Radu","Bhandary, Shrajan (); Babaiee, Zahra (); Kostyszyn, Dejan (); Fechter, Tobias (); Zamboglou, Constantinos (); Grosu, Anca-Ligia (); Grosu, Radu ()",,"Bhandary, Shrajan (); Babaiee, Zahra (); Kostyszyn, Dejan (); Fechter, Tobias (); Zamboglou, Constantinos (); Grosu, Anca-Ligia (); Grosu, Radu ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152337565,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
837,pub.1145019975,10.1049/ipr2.12423,,,Segmentation of lung airways based on deep learning methods,"Precise segmentation of the lung airways is essential for a quantitative assessment of airway diseases. However, because of the complexity of the airway structure and the different thicknesses of the trachea at different positions, it is extremely difficult to segment the fine bronchial structure using chest computed tomography (CT). Traditional lung airway segmentation methods are generally based on the grayscale, geometric shape of the image, or the use of prior knowledge of anatomy. In recent years, deep learning techniques such as fully convolutional neural networks (FCNs) have achieved a great success in the field of image segmentation. Specifically, the symmetric encoderâdecoder network represented by UâNet has achieved high accuracy in many medical image segmentation tasks. In the airway segmentation challenge task of the 4th International Symposium on Image Computing and Digital Medicine (ISICDM 2020), 9 of the 12 teams participating in the final round used the UâNet network or one of its other forms, obtaining good results for lung airway segmentation. Methods used to improve the segmentation accuracy include attention mechanisms and multiscale feature information fusion. This article provides a detailed description of the methods used by these 12 teams and analyses their results.","In the process of experiment and writing, the contributors get a lot of writing suggestions and work guidance from editors and readers, which helps to make the content more rigorous and easier to understand. Here, we would like to express our most sincere thanks to you.",,IET Image Processing,,,2022-01-26,2022,2022-01-26,2022-04,16,5,1444-1456,All OA, Gold,Article,"Tan, Wenjun; Liu, Pan; Li, Xiaoshuo; Xu, Shaoxun; Chen, Yufei; Yang, Jinzhu","Tan, Wenjun (Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China); Liu, Pan (Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China); Li, Xiaoshuo (Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China); Xu, Shaoxun (College of Electronics and Information Engineering, Tongji University, Shanghai, China); Chen, Yufei (College of Electronics and Information Engineering, Tongji University, Shanghai, China); Yang, Jinzhu (Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China)","Tan, Wenjun (; Northeastern University); Chen, Yufei (Tongji University); Yang, Jinzhu (; Northeastern University)","Tan, Wenjun (Northeastern University); Liu, Pan (Northeastern University); Li, Xiaoshuo (Northeastern University); Xu, Shaoxun (Tongji University); Chen, Yufei (Tongji University); Yang, Jinzhu (Northeastern University)",7,7,,,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/ipr2.12423,https://app.dimensions.ai/details/publication/pub.1145019975,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
836,pub.1138220662,10.48550/arxiv.2105.09624,,,Semantic segmentation of multispectral photoacoustic images using deep  learning,"Photoacoustic (PA) imaging has the potential to revolutionize functional
medical imaging in healthcare due to the valuable information on tissue
physiology contained in multispectral photoacoustic measurements. Clinical
translation of the technology requires conversion of the high-dimensional
acquired data into clinically relevant and interpretable information. In this
work, we present a deep learning-based approach to semantic segmentation of
multispectral photoacoustic images to facilitate image interpretability.
Manually annotated photoacoustic {and ultrasound} imaging data are used as
reference and enable the training of a deep learning-based segmentation
algorithm in a supervised manner. Based on a validation study with
experimentally acquired data from 16 healthy human volunteers, we show that
automatic tissue segmentation can be used to create powerful analyses and
visualizations of multispectral photoacoustic images. Due to the intuitive
representation of high-dimensional information, such a preprocessing algorithm
could be a valuable means to facilitate the clinical translation of
photoacoustic imaging.",,,arXiv,,,2021-05-20,2021,,,,,,All OA, Green,Preprint,"Schellenberg, Melanie; Dreher, Kris; Holzwarth, Niklas; Isensee, Fabian; Reinke, Annika; Schreck, Nicholas; Seitel, Alexander; Tizabi, Minu D.; Maier-Hein, Lena; GrÃ¶hl, Janek","Schellenberg, Melanie (); Dreher, Kris (); Holzwarth, Niklas (); Isensee, Fabian (); Reinke, Annika (); Schreck, Nicholas (); Seitel, Alexander (); Tizabi, Minu D. (); Maier-Hein, Lena (); GrÃ¶hl, Janek ()",,"Schellenberg, Melanie (); Dreher, Kris (); Holzwarth, Niklas (); Isensee, Fabian (); Reinke, Annika (); Schreck, Nicholas (); Seitel, Alexander (); Tizabi, Minu D. (); Maier-Hein, Lena (); GrÃ¶hl, Janek ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1138220662,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 51 Physical Sciences,,,,,,,,,
824,pub.1152874926,10.1101/2022.11.15.22282357,,,Organ Finder â a new AI-based organ segmentation tool for CT,"Abstract  Background Automated organ segmentation in computed tomography (CT) is a vital component in many artificial intelligence-based tools in medical imaging. This study presents a new organ segmentation tool called Organ Finder 2.0. In contrast to most existing methods, Organ Finder was trained and evaluated on a rich multi-origin dataset with both contrast and non-contrast studies from different vendors and patient populations.   Approach A total of 1,171 CT studies from seven different publicly available CT databases were retrospectively included. Twenty CT studies were used as test set and the remaining 1,151 were used to train a convolutional neural network. Twenty-two different organs were studied. Professional annotators segmented a total of 5,826 organs and segmentation quality was assured manually for each of these organs.   Results Organ Finder showed high agreement with manual segmentations in the test set. The average Dice index over all organs was 0.93 and the same high performance was found for four different subgroups of the test set based on the presence or absence of intravenous and oral contrast.   Conclusions An AI-based tool can be used to accurately segment organs in both contrast and non-contrast CT studies. The results indicate that a large training set and high-quality manual segmentations should be used to handle common variations in the appearance of clinical CT studies.",,This study did not receive any funding,medRxiv,,,2022-11-18,2022,2022-11-18,,,,2022.11.15.22282357,All OA, Green,Preprint,"Edenbrandt, Lars; Enqvist, Olof; Larsson, MÃ¥ns; UlÃ©n, Johannes","Edenbrandt, Lars (Department of Molecular and Clinical Medicine. Institute of Medicine, Sahlgrenska Academy, University of Gothenburg, Gothenburg., Sweden; SliceVault AB, . MalmÃ¶, Sweden); Enqvist, Olof (Eigenvision AB, . MalmÃ¶, Sweden); Larsson, MÃ¥ns (Eigenvision AB, . MalmÃ¶, Sweden); UlÃ©n, Johannes (Eigenvision AB, . MalmÃ¶, Sweden)","Edenbrandt, Lars (University of Gothenburg; )","Edenbrandt, Lars (University of Gothenburg); Enqvist, Olof (); Larsson, MÃ¥ns (); UlÃ©n, Johannes ()",1,1,,,https://www.medrxiv.org/content/medrxiv/early/2022/11/18/2022.11.15.22282357.full.pdf,https://app.dimensions.ai/details/publication/pub.1152874926,32 Biomedical and Clinical Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
824,pub.1149275820,10.1101/2022.07.04.22277205,,,AI-based image quality assessment in CT,"ABSTRACT Medical imaging, especially computed tomography (CT), is becoming increasingly important in research studies and clinical trials and adequate image quality is essential for reliable results. The aim of this study was to develop an artificial intelligence (AI)-based method for quality assessment of CT studies, both regarding the parts of the body included (i.e. head, chest, abdomen, pelvis), and other image features (i.e. presence of hip prosthesis, intravenous contrast and oral contrast).  Approach  1, 000 CT studies from eight different publicly available CT databases were retrospectively included. The full dataset was randomly divided into a training ( n = 500), a validation/tuning ( n = 250), and a testing set ( n = 250). All studies were manually classified by an imaging specialist. A deep neural network network was then trained to directly classify the 7 different properties of the image.    Results The classification results on the 250 test CT studies showed accuracy for the anatomical regions and presence of hip prosthesis in the interval 98.4% to 100.0%. The accuracy for intravenous contrast was 89.6% and for oral contrast 82.4%.   Conclusions We have shown that it is feasible to develop an AI-based method to automatically perform a quality assessment regarding if correct body parts are included in CT scans, with a very high accuracy.",We would like to thank MÃ¥ns Larsson and Olof Enqvist for fruitful discussions regarding this study and closely related topics.,This study did not receive any funding,medRxiv,,,2022-07-06,2022,2022-07-06,,,,2022.07.04.22277205,All OA, Green,Preprint,"Edenbrandt, Lars; TrÃ¤gÃ¥rdh, Elin; UlÃ©n, Johannes","Edenbrandt, Lars (Region VÃ¤stra GÃ¶taland, Sahlgrenska University Hospital, Department of Clinical Physiology, Gothenburg, Sweden; Department of Molecular and Clinical Medicine, Institute of Medicine, Sahlgrenska Academy, University of Gothenburg, Gothenburg, Sweden; SliceVault AB, MalmÃ¶, Sweden); TrÃ¤gÃ¥rdh, Elin (Department of Clinical Physiology and Nuclear Medicine, SkÃ¥ne University Hospital and Lund University, MalmÃ¶, Sweden; Wallenberg Center for Molecular Medicine, Lund University, MalmÃ¶, Sweden); UlÃ©n, Johannes (Eigenvision AB, MalmÃ¶, Sweden)","Edenbrandt, Lars (Sahlgrenska University Hospital; University of Gothenburg; )","Edenbrandt, Lars (Sahlgrenska University Hospital; University of Gothenburg); TrÃ¤gÃ¥rdh, Elin (Lund University; Lund University); UlÃ©n, Johannes ()",0,0,,,https://www.medrxiv.org/content/medrxiv/early/2022/07/06/2022.07.04.22277205.full.pdf,https://app.dimensions.ai/details/publication/pub.1149275820,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
824,pub.1140316546,10.48550/arxiv.2108.03300,,,Medical image segmentation with imperfect 3D bounding boxes,"The development of high quality medical image segmentation algorithms depends
on the availability of large datasets with pixel-level labels. The challenges
of collecting such datasets, especially in case of 3D volumes, motivate to
develop approaches that can learn from other types of labels that are cheap to
obtain, e.g. bounding boxes. We focus on 3D medical images with their
corresponding 3D bounding boxes which are considered as series of per-slice
non-tight 2D bounding boxes. While current weakly-supervised approaches that
use 2D bounding boxes as weak labels can be applied to medical image
segmentation, we show that their success is limited in cases when the
assumption about the tightness of the bounding boxes breaks. We propose a new
bounding box correction framework which is trained on a small set of
pixel-level annotations to improve the tightness of a larger set of non-tight
bounding box annotations. The effectiveness of our solution is demonstrated by
evaluating a known weakly-supervised segmentation approach with and without the
proposed bounding box correction algorithm. When the tightness is improved by
our solution, the results of the weakly-supervised segmentation become much
closer to those of the fully-supervised one.",,,arXiv,,,2021-08-06,2021,,,,,,All OA, Green,Preprint,"Redekop, Ekaterina; Chernyavskiy, Alexey","Redekop, Ekaterina (); Chernyavskiy, Alexey ()",,"Redekop, Ekaterina (); Chernyavskiy, Alexey ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140316546,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
824,pub.1142084157,10.48550/arxiv.2110.11312,,,Towards modelling hazard factors in unstructured data spaces using  gradient-based latent interpolation,"The application of deep learning in survival analysis (SA) allows utilizing
unstructured and high-dimensional data types uncommon in traditional survival
methods. This allows to advance methods in fields such as digital health,
predictive maintenance, and churn analysis, but often yields less interpretable
and intuitively understandable models due to the black-box character of deep
learning-based approaches. We close this gap by proposing 1) a multi-task
variational autoencoder (VAE) with survival objective, yielding
survival-oriented embeddings, and 2) a novel method HazardWalk that allows to
model hazard factors in the original data space. HazardWalk transforms the
latent distribution of our autoencoder into areas of maximized/minimized hazard
and then uses the decoder to project changes to the original domain. Our
procedure is evaluated on a simulated dataset as well as on a dataset of CT
imaging data of patients with liver metastases.",,,arXiv,,,2021-10-21,2021,,,,,,All OA, Green,Preprint,"Weber, Tobias; Ingrisch, Michael; Bischl, Bernd; RÃ¼gamer, David","Weber, Tobias (); Ingrisch, Michael (); Bischl, Bernd (); RÃ¼gamer, David ()",,"Weber, Tobias (); Ingrisch, Michael (); Bischl, Bernd (); RÃ¼gamer, David ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142084157,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
820,pub.1147864841,10.1117/12.2637414,,,Colorectal segmentation in medical images,"Colorectal cancer is one of the digestive tract malignant tumors in life. In recent years, the incidence rate of rectal cancer has increased significantly in China. The segmentation of CT images of colorectal tumors is a tedious, time-consuming and expensive job.When diagnosing rectal cancer, it would be helpful to accurately segment the rectal tumor region from CT images so that doctors could make more accurate diagnoses and thus have better results for the disease. Based on the literature review, this paper presents medical image segmentation algorithms, including traditional algorithms based on energy minimization, and evaluates them according to the metrics used. The paper also describes the public datasets available for use in the study. In this paper, various segmentation methods are analyzed in detail. Their theoretical and practical characteristics are described for each group of methods, and the most advanced and promising methods are proposed. This paper can help understand the available colorectal segmentation methods, making developing new and more effective and improving existing methods easier. The method is simple to interact with, robust and accurate, and can effectively assist physicians in achieving diagnosis and surgical planning.",,,Proceedings of SPIE,Second International Conference on Sensors and Information Technology (ICSI 2022),,2022-05-13,2022,,,12248,,122480u-122480u-6,Closed,Proceeding,"Xie, Yichao","Xie, Yichao (Fuzhou University (China))",,"Xie, Yichao (Fuzhou University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1147864841,"40 Engineering; 4006 Communications Engineering; 4009 Electronics, Sensors and Digital Hardware; 51 Physical Sciences; 5102 Atomic, Molecular and Optical Physics",,,,,,,,,,,,
810,pub.1155159970,10.1109/tmi.2023.3243069,,,CAT: Constrained Adversarial Training for Anatomically-plausible Semi-supervised Segmentation,"Deep learning models for semi-supervised medical image segmentation have achieved unprecedented performance for a wide range of tasks. Despite their high accuracy, these models may however yield predictions that are considered anatomically impossible by clinicians. Moreover, incorporating complex anatomical constraints into standard deep learning frameworks remains challenging due to their non-differentiable nature. To address these limitations, we propose a Constrained Adversarial Training (CAT) method that learns how to produce anatomically plausible segmentations. Unlike approaches focusing solely on accuracy measures like Dice, our method considers complex anatomical constraints like connectivity, convexity, and symmetry which cannot be easily modeled in a loss function. The problem of non-differentiable constraints is solved using a Reinforce algorithm which enables to obtain a gradient for violated constraints. To generate constraint-violating examples on the fly, and thereby obtain useful gradients, our method adopts an adversarial training strategy which modifies training images to maximize the constraint loss, and then updates the network to be robust to these adversarial examples. The proposed method offers a generic and efficient way to add complex segmentation constraints on top of any segmentation network. Experiments on synthetic data and four clinically-relevant datasets demonstrate the effectiveness of our method in terms of segmentation accuracy and anatomical plausibility.",,,IEEE Transactions on Medical Imaging,,,2023-02-07,2023,2023-02-07,,PP,99,1-1,Closed,Article,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (Software and IT department, &#x00C9;cole de technologie sup&#x00E9;rieure(ETS), Montreal, Canada); Peng, Jizong (Software and IT department, ETS, Montreal, Canada); Pedersoli, Marco (Software and IT department, ETS, Montreal, Canada); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, China); Zhang, Caiming (School of Software, Shandong University, Jinan, China); Desrosiers, Christian (Software and IT department, ETS, Montreal, Canada)",,"Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (Shandong University); Zhang, Caiming (Shandong University); Desrosiers, Christian ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155159970,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
807,pub.1141489940,10.1007/978-3-030-88210-5_18,,,Medical Image Segmentation with Imperfect 3D Bounding Boxes,"The development of high quality medical image segmentation algorithms depends on the availability of large datasets with pixel-level labels. The challenges of collecting such datasets, especially in case of 3D volumes, motivate to develop approaches that can learn from other types of labels that are cheap to obtain, e.g. bounding boxes. We focus on 3D medical images with their corresponding 3D bounding boxes which are considered as series of per-slice non-tight 2D bounding boxes. While current weakly-supervised approaches that use 2D bounding boxes as weak labels can be applied to medical image segmentation, we show that their success is limited in cases when the assumption about the tightness of the bounding boxes breaks. We propose a new bounding box correction framework which is trained on a small set of pixel-level annotations to improve the tightness of a larger set of non-tight bounding box annotations. The effectiveness of our solution is demonstrated by evaluating a known weakly-supervised segmentation approach with and without the proposed bounding box correction algorithm. When the tightness is improved by our solution, the results of the weakly-supervised segmentation become much closer to those of the fully-supervised one.",,,Lecture Notes in Computer Science,"Deep Generative Models, and Data Augmentation, Labelling, and Imperfections",,2021-09-25,2021,2021-09-25,2021,13003,,193-200,All OA, Green,Chapter,"Redekop, Ekaterina; Chernyavskiy, Alexey","Redekop, Ekaterina (Philips AI Research, Moscow, Russia); Chernyavskiy, Alexey (Philips AI Research, Moscow, Russia)","Chernyavskiy, Alexey ","Redekop, Ekaterina (); Chernyavskiy, Alexey ()",1,1,,0.92,http://arxiv.org/pdf/2108.03300,https://app.dimensions.ai/details/publication/pub.1141489940,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
807,pub.1135528479,10.1117/12.2581111,,,COVID-19 opacity segmentation in chest CT via HydraNet: a joint learning multi-decoder network,"The outbreak of the coronavirus and its rapid spread was recently acknowledged as a worldwide pandemic. Chest CT scans show high potential for detecting pathological manifestations. Hence, the demand for computer-aided tools to support radiologists has grown exponentially. In this work, we developed a deep learning based algorithm, with an emphasis on novel transfer learning methods, to segment COVID-19 opacity in chest CT scans. Our method focuses on creating a deep encoder for feature extraction by using a Fully Convolutional Network (FCN) architecture with one shared encoder and N task-related decoders, named HydraNet. The HydraNet architecture allowed the leverage of a large variety of medical datasets from different domains, in order to achieve better performances on a limited dataset. We achieved a dice score, sensitivity, and precision of 0.724, 0.75, and 0.807 respectively, on the test set, which is competitive with known state-of-the-art results.",,,Progress in Biomedical Optics and Imaging,Medical Imaging 2021: Computer-Aided Diagnosis,,2021-02-15,2021,,,11597,,115971u-115971u-7,Closed,Proceeding,"Sagie, Nimrod; Almog, Shiri; Talby, Ayelet; Greenspan, Hayit","Sagie, Nimrod (Tel Aviv Univ. (Israel)); Almog, Shiri (Tel Aviv Univ. (Israel)); Talby, Ayelet (Tel Aviv Univ. (Israel)); Greenspan, Hayit (Tel Aviv Univ. (Israel))",,"Sagie, Nimrod (Tel Aviv University); Almog, Shiri (Tel Aviv University); Talby, Ayelet (Tel Aviv University); Greenspan, Hayit (Tel Aviv University)",3,3,,2.46,,https://app.dimensions.ai/details/publication/pub.1135528479,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
806,pub.1154628678,10.48550/arxiv.2301.07074,,,SegViz: A Federated Learning Framework for Medical Image Segmentation  from Distributed Datasets with Different and Incomplete Annotations,"Segmentation is one of the primary tasks in the application of deep learning
in medical imaging, owing to its multiple downstream clinical applications. As
a result, many large-scale segmentation datasets have been curated and released
for the segmentation of different anatomical structures. However, these
datasets focus on the segmentation of a subset of anatomical structures in the
body, therefore, training a model for each dataset would potentially result in
hundreds of models and thus limit their clinical translational utility.
Furthermore, many of these datasets share the same field of view but have
different subsets of annotations, thus making individual dataset annotations
incomplete. To that end, we developed SegViz, a federated learning framework
for aggregating knowledge from distributed medical image segmentation datasets
with different and incomplete annotations into a `global` meta-model. The
SegViz framework was trained to build a single model capable of segmenting both
liver and spleen aggregating knowledge from both these nodes by aggregating the
weights after every 10 epochs. The global SegViz model was tested on an
external dataset, Beyond the Cranial Vault (BTCV), comprising both liver and
spleen annotations using the dice similarity (DS) metric. The baseline
individual segmentation models for spleen and liver trained on their respective
datasets produced a DS score of 0.834 and 0.878 on the BTCV test set. In
comparison, the SegViz model produced comparable mean DS scores of 0.829 and
0.899 for the segmentation of the spleen and liver respectively. Our results
demonstrate SegViz as an essential first step towards training clinically
translatable multi-task segmentation models from distributed datasets with
disjoint incomplete annotations with excellent performance.",,,arXiv,,,2023-01-17,2023,,,,,,All OA, Green,Preprint,"Kanhere, Adway U.; Kulkarni, Pranav; Yi, Paul H.; Parekh, Vishwa S.","Kanhere, Adway U. (); Kulkarni, Pranav (); Yi, Paul H. (); Parekh, Vishwa S. ()",,"Kanhere, Adway U. (); Kulkarni, Pranav (); Yi, Paul H. (); Parekh, Vishwa S. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154628678,46 Information and Computing Sciences, 4605 Data Management and Data Science, 51 Physical Sciences,,,,,,,,,
806,pub.1131390394,10.1007/978-3-030-59719-1_30,,,Voxel2Mesh: 3D Mesh Model Generation from Volumetric Data,"CNN-based volumetric methods that label individual voxels now dominate the field of biomedical segmentation. However, 3D surface representations are often required for proper analysis. They can be obtained by post-processing the labeled volumes which typically introduces artifacts and prevents end-to-end training. In this paper, we therefore introduce a novel architecture that goes directly from 3D image volumes to 3D surfaces without post-processing and with better accuracy than current methods. We evaluate it on Electron Microscopy and MRI brain images as well as CT liver scans. We will show that it outperforms state-of-the-art segmentation methods.",This work was supported in part by a Swiss National Science Foundation grant.,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12264,,299-308,All OA, Green,Chapter,"Wickramasinghe, Udaranga; Remelli, Edoardo; Knott, Graham; Fua, Pascal","Wickramasinghe, Udaranga (Computer Vision Laboratory, Ãcole Polytechnique FÃ©dÃ©rale de Lausanne, Lausanne, Switzerland); Remelli, Edoardo (Computer Vision Laboratory, Ãcole Polytechnique FÃ©dÃ©rale de Lausanne, Lausanne, Switzerland); Knott, Graham (BioEM Laboratory, Ãcole Polytechnique FÃ©dÃ©rale de Lausanne, Lausanne, Switzerland); Fua, Pascal (Computer Vision Laboratory, Ãcole Polytechnique FÃ©dÃ©rale de Lausanne, Lausanne, Switzerland)","Wickramasinghe, Udaranga (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne)","Wickramasinghe, Udaranga (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne); Remelli, Edoardo (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne); Knott, Graham (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne); Fua, Pascal (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne)",41,39,,24.9,http://arxiv.org/pdf/1912.03681,https://app.dimensions.ai/details/publication/pub.1131390394,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
806,pub.1147391303,10.1109/isbi52829.2022.9761415,,,Effective 3d Boundary Learning via a Nonlocal Deformable Network,"Due to the unbalance between the boundary pixels and regional pixels, the accuracy of boundary prediction is a challenging issue for learning-based medical segmentation approaches. In this paper, we propose a two-stage segmentation method to identify and refine the object boundary accordingly. By modeling the boundary by the signed distance function, we develop a nonlocal deformable convolutional network to accurately predict the local geometry of boundaries. We also introduce an efficient loss function to enhance the learning ability in the boundary area. Experiments on two public spleen datasets can evidence the superior performance of the proposed model compared to the existing 2D, 3D, and boundary-based learning methods.",,"The work was supported by National Natural Science Foundation of China (NSFC 12071345, 11701418), Major Science and Technology Project of Tianjin 18ZXRHSY00160 and Recruitment Program of Global Young Expert.",,2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),,2022-03-31,2022,,2022-03-31,0,,1-5,Closed,Proceeding,"Liu, Yueyun; Wang, Yu; Duan, Yuping","Liu, Yueyun (Center for Applied Mathematics, Tianjin University, Tianjin, 300072, China); Wang, Yu (DAMO Academy, Alibaba Group, China); Duan, Yuping (Center for Applied Mathematics, Tianjin University, Tianjin, 300072, China)","Duan, Yuping (Tianjin University)","Liu, Yueyun (Tianjin University); Wang, Yu (Alibaba Group (China)); Duan, Yuping (Tianjin University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1147391303,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,,
806,pub.1147391293,10.1109/isbi52829.2022.9761555,,,Improving the Automatic Segmentation of Elongated Organs Using Geometrical Priors,"Deep neural networks are widely used for automated organ segmentation as they achieve promising results for clinical applications. Some organs are more challenging to delineate than others, for instance due to low contrast at their boundaries. In this paper, we propose to improve the segmentation of elongated organs thanks to Geometrical Priors that can be introduced during training, using a local Tversky loss function, or at post-processing, using local thresholds. Both strategies do not introduce additional training parameters and can be easily applied to any existing network. The proposed method is evaluated on the challenging problem of pancreas segmentation. Results show that Geometrical Priors allow us to correct the systematic under-segmentation pattern of a state-of-the-art method, while preserving the overall segmentation quality.","This work was partly funded by a CIFRE grant from ANRT # 2020/1448. Conflict of interest. R.V., A.B. and M.R. are employed by Guerbet.",This work was partly funded by a CIFRE grant from ANRT # 2020/1448.,,2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),,2022-03-31,2022,,2022-03-31,0,,1-4,All OA, Green,Proceeding,"VÃ©til, Rebeca; BÃ´ne, Alexandre; Vullierme, Marie-Pierre; RohÃ©, Marc-Michel; Gori, Pietro; Bloch, Isabelle","VÃ©til, Rebeca (Guerbet Research, Villepinte, France; LTCI, TÃ©lÃ©com Paris, Institut Polytechnique de Paris, France); BÃ´ne, Alexandre (Guerbet Research, Villepinte, France); Vullierme, Marie-Pierre (Department of Radiology, Hospital of Annecy-Genevois, UniversitÃ© de Paris, France); RohÃ©, Marc-Michel (Guerbet Research, Villepinte, France); Gori, Pietro (LTCI, TÃ©lÃ©com Paris, Institut Polytechnique de Paris, France); Bloch, Isabelle (LTCI, TÃ©lÃ©com Paris, Institut Polytechnique de Paris, France; Sorbonne UniversitÃ©, CNRS, LIP6, Paris, France)","VÃ©til, Rebeca (; Laboratoire Traitement et Communication de lâInformation)","VÃ©til, Rebeca (Laboratoire Traitement et Communication de lâInformation); BÃ´ne, Alexandre (); Vullierme, Marie-Pierre (); RohÃ©, Marc-Michel (); Gori, Pietro (Laboratoire Traitement et Communication de lâInformation); Bloch, Isabelle (Laboratoire Traitement et Communication de lâInformation; Laboratoire de Recherche en Informatique de Paris 6)",0,0,,,https://hal.telecom-paris.fr/hal-03628860/file/vetil_isbi_2022_final_version.pdf,https://app.dimensions.ai/details/publication/pub.1147391293,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
802,pub.1152602535,10.48550/arxiv.2211.04180,,,Exploiting segmentation labels and representation learning to forecast  therapy response of PDAC patients,"The prediction of pancreatic ductal adenocarcinoma therapy response is a
clinically challenging and important task in this high-mortality tumour entity.
The training of neural networks able to tackle this challenge is impeded by a
lack of large datasets and the difficult anatomical localisation of the
pancreas. Here, we propose a hybrid deep neural network pipeline to predict
tumour response to initial chemotherapy which is based on the Response
Evaluation Criteria in Solid Tumors (RECIST) score, a standardised method for
cancer response evaluation by clinicians as well as tumour markers, and
clinical evaluation of the patients. We leverage a combination of
representation transfer from segmentation to classification, as well as
localisation and representation learning. Our approach yields a remarkably
data-efficient method able to predict treatment response with a ROC-AUC of
63.7% using only 477 datasets in total.",,,arXiv,,,2022-11-08,2022,,,,,,All OA, Green,Preprint,"Ziller, Alexander; Erdur, Ayhan Can; Jungmann, Friederike; Rueckert, Daniel; Braren, Rickmer; Kaissis, Georgios","Ziller, Alexander (); Erdur, Ayhan Can (); Jungmann, Friederike (); Rueckert, Daniel (); Braren, Rickmer (); Kaissis, Georgios ()",,"Ziller, Alexander (); Erdur, Ayhan Can (); Jungmann, Friederike (); Rueckert, Daniel (); Braren, Rickmer (); Kaissis, Georgios ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152602535,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,
800,pub.1149024566,10.48550/arxiv.2206.13385,,,3D unsupervised anomaly detection and localization through virtual  multi-view projection and reconstruction: Clinical validation on low-dose  chest computed tomography,"Computer-aided diagnosis for low-dose computed tomography (CT) based on deep
learning has recently attracted attention as a first-line automatic testing
tool because of its high accuracy and low radiation exposure. However, existing
methods rely on supervised learning, imposing an additional burden to doctors
for collecting disease data or annotating spatial labels for network training,
consequently hindering their implementation. We propose a method based on a
deep neural network for computer-aided diagnosis called virtual multi-view
projection and reconstruction for unsupervised anomaly detection. Presumably,
this is the first method that only requires data from healthy patients for
training to identify three-dimensional (3D) regions containing any anomalies.
The method has three key components. Unlike existing computer-aided diagnosis
tools that use conventional CT slices as the network input, our method 1)
improves the recognition of 3D lung structures by virtually projecting an
extracted 3D lung region to obtain two-dimensional (2D) images from diverse
views to serve as network inputs, 2) accommodates the input diversity gain for
accurate anomaly detection, and 3) achieves 3D anomaly/disease localization
through a novel 3D map restoration method using multiple 2D anomaly maps. The
proposed method based on unsupervised learning improves the patient-level
anomaly detection by 10% (area under the curve, 0.959) compared with a gold
standard based on supervised learning (area under the curve, 0.848), and it
localizes the anomaly region with 93% accuracy, demonstrating its high
performance.",,,arXiv,,,2022-06-18,2022,,,,,,All OA, Green,Preprint,"Kim, Kyung-Su; Oh, Seong Je; Lee, Ju Hwan; Chung, Myung Jin","Kim, Kyung-Su (); Oh, Seong Je (); Lee, Ju Hwan (); Chung, Myung Jin ()",,"Kim, Kyung-Su (); Oh, Seong Je (); Lee, Ju Hwan (); Chung, Myung Jin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149024566,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
790,pub.1144781974,10.48550/arxiv.2201.05650,,,Disentanglement enables cross-domain Hippocampus Segmentation,"Limited amount of labelled training data are a common problem in medical
imaging. This makes it difficult to train a well-generalised model and
therefore often leads to failure in unknown domains. Hippocampus segmentation
from magnetic resonance imaging (MRI) scans is critical for the diagnosis and
treatment of neuropsychatric disorders. Domain differences in contrast or shape
can significantly affect segmentation. We address this issue by disentangling a
T1-weighted MRI image into its content and domain. This separation enables us
to perform a domain transfer and thus convert data from new sources into the
training domain. This step thus simplifies the segmentation problem, resulting
in higher quality segmentations. We achieve the disentanglement with the
proposed novel methodology 'Content Domain Disentanglement GAN', and we propose
to retrain the UNet on the transformed outputs to deal with GAN-specific
artefacts. With these changes, we are able to improve performance on unseen
domains by 6-13% and outperform state-of-the-art domain transfer methods.",,,arXiv,,,2022-01-14,2022,,,,,,All OA, Green,Preprint,"Kalkhof, John; GonzÃ¡lez, Camila; Mukhopadhyay, Anirban","Kalkhof, John (); GonzÃ¡lez, Camila (); Mukhopadhyay, Anirban ()",,"Kalkhof, John (); GonzÃ¡lez, Camila (); Mukhopadhyay, Anirban ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144781974,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
789,pub.1151033028,10.1007/978-3-031-16443-9_54,,,Efficient Population Based Hyperparameter Scheduling for Medical Image Segmentation,"The training hyperparameters (learning rate, augmentation policies, e.t.c) are key factors affecting the performance of deep networks for medical image segmentation. Manual or automatic hyperparameter optimizationÂ (HPO) is used to improve the performance. However, manual tuning is infeasible for a large number of parameters, and existing automatic HPO methods like Bayesian optimization are extremely time consuming. Moreover, they can only find a fixed set of hyperparameters. Population based training (PBT) has shown its ability to find dynamic hyperparameters and has fast search speed by using parallel training processes. However, it is still expensive for large 3D medical image datasets with limited GPUs, and the performance lower bound is unknown. In this paper, we focus on improving the network performance using hyperparameter scheduling via PBT with limited computation cost. The core idea is to train the network with a default setting from prior knowledge, and finetune using PBT based hyperparameter scheduling. Our method can achieve 1%â3% performance improvements over default setting while only taking 3%â10% computation cost of training from scratch using PBT.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13435,,560-569,Closed,Chapter,"He, Yufan; Yang, Dong; Myronenko, Andriy; Xu, Daguang","He, Yufan (NVidia, Santa Clara, USA); Yang, Dong (NVidia, Santa Clara, USA); Myronenko, Andriy (NVidia, Santa Clara, USA); Xu, Daguang (NVidia, Santa Clara, USA)","He, Yufan (Nvidia (United States))","He, Yufan (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151033028,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
789,pub.1151668467,10.1007/978-3-031-17899-3_5,,,Accurate Hippocampus Segmentation Based on Self-supervised Learning with Fewer Labeled Data,"Brain MRI-based hippocampus segmentation is considered as an important biomedical method for prevention, early detection, and accurate diagnosis of neurodegenerative disorders like Alzheimerâs disease. The recent need for developing accurate as well as robust systems has led to breakthroughs making advantage of deep learning, but requiring significant amounts of labeled data, which, in turn, is costly and hardly obtainable. In this work, we try to address this issue by introducing self-supervised learning for hippocampus segmentation. We devise a new framework, based on the widely known method of Jigsaw puzzle reassembly, in which we first pre-train using one of the unlabeled MRI datasets, and then perform a downstream segmentation training with other labeled datasets. As a result, we found our method to capture local-level features for better learning of anatomical information pertaining to brain MRI images. Experiments with downstream segmentation training show considerable performance gains with self-supervised pre-training over supervised training when compared over multiple label fractions.","This work was supported by the Engineering Research Center of Excellence (ERC) Program supported by National Research Foundation (NRF), Korean Ministry of Science &amp; ICT (MSIT) (Grant No. NRF-2017R1A5A1014708).",,Lecture Notes in Computer Science,Machine Learning in Clinical Neuroimaging,,2022-10-06,2022,2022-10-06,2022,13596,,42-51,Closed,Chapter,"Kunanbayev, Kassymzhomart; Jang, Donggon; Jeong, Woojin; Kim, Nahyun; Kim, Dae-Shik","Kunanbayev, Kassymzhomart (KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon, South Korea); Jang, Donggon (KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon, South Korea); Jeong, Woojin (KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon, South Korea); Kim, Nahyun (KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon, South Korea); Kim, Dae-Shik (KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon, South Korea)","Kunanbayev, Kassymzhomart (Korea Advanced Institute of Science and Technology)","Kunanbayev, Kassymzhomart (Korea Advanced Institute of Science and Technology); Jang, Donggon (Korea Advanced Institute of Science and Technology); Jeong, Woojin (Korea Advanced Institute of Science and Technology); Kim, Nahyun (Korea Advanced Institute of Science and Technology); Kim, Dae-Shik (Korea Advanced Institute of Science and Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151668467,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
789,pub.1130547253,10.48550/arxiv.2009.00872,,,"Efficient, high-performance pancreatic segmentation using multi-scale  feature extraction","For artificial intelligence-based image analysis methods to reach clinical
applicability, the development of high-performance algorithms is crucial. For
example, existent segmentation algorithms based on natural images are neither
efficient in their parameter use nor optimized for medical imaging. Here we
present MoNet, a highly optimized neural-network-based pancreatic segmentation
algorithm focused on achieving high performance by efficient multi-scale image
feature utilization.",,,arXiv,,,2020-09-02,2020,,,,,,All OA, Green,Preprint,"Knolle, Moritz; Kaissis, Georgios; Jungmann, Friederike; Ziegelmayer, Sebastian; Sasse, Daniel; Makowski, Marcus; Rueckert, Daniel; Braren, Rickmer","Knolle, Moritz (Department of diagnostic and interventional Radiology, Technical University of Munich, Munich, Germany; Institute for Artificial Intelligence and Data Science in Medicine and Healthcare, Technical University of Munich, Munich, Germany); Kaissis, Georgios (Department of diagnostic and interventional Radiology, Technical University of Munich, Munich, Germany; Institute for Artificial Intelligence and Data Science in Medicine and Healthcare, Technical University of Munich, Munich, Germany; OpenMined Research; Department of Computing, Imperial College London, London, United Kingdom); Jungmann, Friederike (Department of diagnostic and interventional Radiology, Technical University of Munich, Munich, Germany); Ziegelmayer, Sebastian (Department of diagnostic and interventional Radiology, Technical University of Munich, Munich, Germany); Sasse, Daniel (Department of diagnostic and interventional Radiology, Technical University of Munich, Munich, Germany); Makowski, Marcus (Department of diagnostic and interventional Radiology, Technical University of Munich, Munich, Germany); Rueckert, Daniel (Institute for Artificial Intelligence and Data Science in Medicine and Healthcare, Technical University of Munich, Munich, Germany; Department of Computing, Imperial College London, London, United Kingdom); Braren, Rickmer (Department of diagnostic and interventional Radiology, Technical University of Munich, Munich, Germany)",,"Knolle, Moritz (Technical University of Munich; Technical University of Munich); Kaissis, Georgios (Technical University of Munich; Technical University of Munich; Imperial College London); Jungmann, Friederike (Technical University of Munich); Ziegelmayer, Sebastian (Technical University of Munich); Sasse, Daniel (Technical University of Munich); Makowski, Marcus (Technical University of Munich); Rueckert, Daniel (Technical University of Munich; Imperial College London); Braren, Rickmer (Technical University of Munich)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1130547253,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
788,pub.1145816370,10.48550/arxiv.2202.11401,,,Mixed-Block Neural Architecture Search for Medical Image Segmentation,"Deep Neural Networks (DNNs) have the potential for making various clinical
procedures more time-efficient by automating medical image segmentation. Due to
their strong, in some cases human-level, performance, they have become the
standard approach in this field. The design of the best possible medical image
segmentation DNNs, however, is task-specific. Neural Architecture Search (NAS),
i.e., the automation of neural network design, has been shown to have the
capability to outperform manually designed networks for various tasks. However,
the existing NAS methods for medical image segmentation have explored a quite
limited range of types of DNN architectures that can be discovered. In this
work, we propose a novel NAS search space for medical image segmentation
networks. This search space combines the strength of a generalised
encoder-decoder structure, well known from U-Net, with network blocks that have
proven to have a strong performance in image classification tasks. The search
is performed by looking for the best topology of multiple cells simultaneously
with the configuration of each cell within, allowing for interactions between
topology and cell-level attributes. From experiments on two publicly available
datasets, we find that the networks discovered by our proposed NAS method have
better performance than well-known handcrafted segmentation networks, and
outperform networks found with other NAS approaches that perform only topology
search, and topology-level search followed by cell-level search.",,,arXiv,,,2022-02-23,2022,,,,,,All OA, Green,Preprint,"Bosma, Martijn M. A.; Dushatskiy, Arkadiy; Grewal, Monika; Alderliesten, Tanja; Bosman, Peter A. N.","Bosma, Martijn M. A. (); Dushatskiy, Arkadiy (); Grewal, Monika (); Alderliesten, Tanja (); Bosman, Peter A. N. ()",,"Bosma, Martijn M. A. (); Dushatskiy, Arkadiy (); Grewal, Monika (); Alderliesten, Tanja (); Bosman, Peter A. N. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1145816370,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
787,pub.1147394859,10.1109/isbi52829.2022.9761560,,,Disentanglement Enables Cross-Domain Hippocampus Segmentation,"Limited amount of labelled training data are a common problem in medical imaging. This makes it difficult to train a well-generalised model and therefore often leads to failure in unknown domains. Hippocampus segmentation from magnetic resonance imaging (MRI) scans is critical for the diagnosis and treatment of neuropsychatric disorders. Domain differences in contrast or shape can significantly affect segmentation. We address this issue by disentangling a T1-weighted MRI image into its content and domain. This separation enables us to perform a domain transfer and thus convert data from new sources into the training domain. This step thus simplifies the segmentation problem, resulting in higher quality segmentations. We achieve the disentanglement with the proposed novel methodology âContent Domain Disentanglement GANâ, and we propose to retrain the UNet on the transformed outputs to deal with GAN-specific arte-facts. With these changes, we are able to improve performance on unseen domains by 6-13% and outperform state-of-the-art domain transfer methods.",,,,2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),,2022-03-31,2022,,2022-03-31,0,,1-5,All OA, Green,Proceeding,"Kalkhof, John; GonzÃ¡lez, Camila; Mukhopadhyay, Anirban","Kalkhof, John (Darmstadt University of Technology, Karolinenplatz 5, 64289, Darmstadt, Germany); GonzÃ¡lez, Camila (Darmstadt University of Technology, Karolinenplatz 5, 64289, Darmstadt, Germany); Mukhopadhyay, Anirban (Darmstadt University of Technology, Karolinenplatz 5, 64289, Darmstadt, Germany)","Kalkhof, John (TU Darmstadt)","Kalkhof, John (TU Darmstadt); GonzÃ¡lez, Camila (TU Darmstadt); Mukhopadhyay, Anirban (TU Darmstadt)",2,2,,,http://arxiv.org/pdf/2201.05650,https://app.dimensions.ai/details/publication/pub.1147394859,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
787,pub.1131971114,10.48550/arxiv.2010.11695,,,Automatic Data Augmentation for 3D Medical Image Segmentation,"Data augmentation is an effective and universal technique for improving
generalization performance of deep neural networks. It could enrich diversity
of training samples that is essential in medical image segmentation tasks
because 1) the scale of medical image dataset is typically smaller, which may
increase the risk of overfitting; 2) the shape and modality of different
objects such as organs or tumors are unique, thus requiring customized data
augmentation policy. However, most data augmentation implementations are
hand-crafted and suboptimal in medical image processing. To fully exploit the
potential of data augmentation, we propose an efficient algorithm to
automatically search for the optimal augmentation strategies. We formulate the
coupled optimization w.r.t. network weights and augmentation parameters into a
differentiable form by means of stochastic relaxation. This formulation allows
us to apply alternative gradient-based methods to solve it, i.e. stochastic
natural gradient method with adaptive step-size. To the best of our knowledge,
it is the first time that differentiable automatic data augmentation is
employed in medical image segmentation tasks. Our numerical experiments
demonstrate that the proposed approach significantly outperforms existing
build-in data augmentation of state-of-the-art models.",,,arXiv,,,2020-10-07,2020,,,,,,All OA, Green,Preprint,"Xu, Ju; Li, Mengzhang; Zhu, Zhanxing","Xu, Ju (); Li, Mengzhang (); Zhu, Zhanxing ()",,"Xu, Ju (); Li, Mengzhang (); Zhu, Zhanxing ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1131971114,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
787,pub.1146775571,10.1117/12.2611428,,,Mixed-block neural architecture search for medical image segmentation,"Deep Neural Networks (DNNs) have the potential for making various clinical procedures more time-efficient by automating medical image segmentation. Due to their strong, in some cases human-level, performance, they have become the standard approach in this field. The design of the best possible medical image segmentation DNNs, however, is task-specific. Neural Architecture Search (NAS), i.e., the automation of neural network design, has been shown to have the capability to outperform manually designed networks for various tasks. However, the existing NAS methods for medical image segmentation have explored a quite limited range of types of DNN architectures that can be discovered. In this work, we propose a novel NAS search space for medical image segmentation networks. This search space combines the strength of a generalised encoder-decoder structure, well known from U-Net, with network blocks that have proven to have a strong performance in image classification tasks. The search is performed by looking for the best topology of multiple cells simultaneously with the configuration of each cell within, allowing for interactions between topology and cell-level attributes. From experiments on two publicly available datasets, we find that the networks discovered by our proposed NAS method have better performance than well-known handcrafted segmentation networks, and outperform networks found with other NAS approaches that perform only topology search, and topology-level search followed by cell-level search.",,,Progress in Biomedical Optics and Imaging,Medical Imaging 2022: Image Processing,,2022-04-04,2022,,,12032,,120320s-120320s-7,All OA, Green,Proceeding,"Bosma, Martijn M. A.; Dushatskiy, Arkadiy; Grewal, Monika; Alderliesten, Tanja; Bosman, Peter","Bosma, Martijn M. A. (Ctr. Wiskunde & Informatica (Netherlands)); Dushatskiy, Arkadiy (Ctr. Wiskunde & Informatica (Netherlands)); Grewal, Monika (Ctr. Wiskunde & Informatica (Netherlands)); Alderliesten, Tanja (Leiden Univ. Medical Ctr. (Netherlands)); Bosman, Peter (Ctr. Wiskunde & Informatica (Netherlands))",,"Bosma, Martijn M. A. (); Dushatskiy, Arkadiy (); Grewal, Monika (); Alderliesten, Tanja (Leiden University Medical Center); Bosman, Peter ()",2,2,,,https://repository.tudelft.nl/islandora/object/uuid%3A5298f7da-8d87-46de-8b81-8ca9acd2e43f/datastream/OBJ/download,https://app.dimensions.ai/details/publication/pub.1146775571,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
787,pub.1140192849,10.1109/conit51480.2021.9498384,,,Deep Learning Based Brain Tumor Detection and Classification,"One of the most crucial tasks of neurologists and radiologists is early brain tumor detection. However, manually detecting and segmenting brain tumors from Magnetic Resonance Imaging (MRI) scans is challenging, and prone to errors. That is why an automated brain tumor detection system is required for early diagnosis of the disease. This paper proposes two deep learning based approaches for brain tumor detection and classification using the cutting-edge object detection framework YOLO (You Only Look Once) and the deep learning library FastAi, respectively. This study was done on a subset of the BRATS 2018 dataset that contained 1,992 Brain MRI scans. The YOLOv5 model achieved an accuracy of 85.95% and the FastAi classification model achieved an accuracy of 95.78%. These two models can be applied in real-time brain tumor detection for early diagnosis of brain cancer.",,,,2021 International Conference on Intelligent Technologies (CONIT),,2021-06-27,2021,,2021-06-27,0,,1-6,Closed,Proceeding,"Dipu, Nadim Mahmud; Shohan, Sifatul Alam; Salam, K. M. A.","Dipu, Nadim Mahmud (Dept. of Electrical and Computer Engineering, North South University); Shohan, Sifatul Alam (Dept. of Electrical and Computer Engineering, North South University); Salam, K. M. A. (Dept. of Electrical and Computer Engineering, North South University)","Dipu, Nadim Mahmud ","Dipu, Nadim Mahmud (); Shohan, Sifatul Alam (); Salam, K. M. A. ()",19,19,,11.52,,https://app.dimensions.ai/details/publication/pub.1140192849,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,,
787,pub.1131367025,10.1007/978-3-030-59710-8_37,,,Automatic Data Augmentation for 3D Medical Image Segmentation,"Data augmentation is an effective and universal technique for improving generalization performance of deep neural networks. It could enrich diversity of training samples that is essential in medical image segmentation tasks because 1) the scale of medical image dataset is typically smaller, which may increase the risk of overfitting; 2) the shape and modality of different objects such as organs or tumors are unique, thus requiring customized data augmentation policy. However, most data augmentation implementations are hand-crafted and suboptimal in medical image processing. To fully exploit the potential of data augmentation, we propose an efficient algorithm to automatically search for the optimal augmentation strategies. We formulate the coupled optimization w.r.t. network weights and augmentation parameters into a differentiable form by means of stochastic relaxation. This formulation allows us to apply alternative gradient-based methods to solve it, i.e. stochastic natural gradient method with adaptive step-size. To the best of our knowledge, it is the first time that differentiable automatic data augmentation is employed in medical image segmentation tasks. Our numerical experiments demonstrate that the proposed approach significantly outperforms existing build-in data augmentation of state-of-the-art models.","This project is supported by National Natural Science Foundation of China (No.61806009 and 61932001), PKU-Baidu Funding 2019BD005 and Beijing Academy of Artificial Intelligence (BAAI).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12261,,378-387,All OA, Green,Chapter,"Xu, Ju; Li, Mengzhang; Zhu, Zhanxing","Xu, Ju (Center for Data Science, Peking University, Beijing, China); Li, Mengzhang (Center for Data Science, Peking University, Beijing, China; Canon Medical Systems, Beijing, China); Zhu, Zhanxing (Center for Data Science, Peking University, Beijing, China; School of Mathematical Sciences, Peking University, Beijing, China)","Zhu, Zhanxing (Peking University; Peking University)","Xu, Ju (Peking University); Li, Mengzhang (Peking University); Zhu, Zhanxing (Peking University; Peking University)",33,33,,17.01,http://arxiv.org/pdf/2010.11695,https://app.dimensions.ai/details/publication/pub.1131367025,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
773,pub.1153603596,10.48550/arxiv.2212.05808,,,Z-SSMNet: A Zonal-aware Self-Supervised Mesh Network for Prostate Cancer  Detection and Diagnosis in bpMRI,"Prostate cancer (PCa) is one of the most prevalent cancers in men and many
people around the world die from clinically significant PCa (csPCa). Early
diagnosis of csPCa in bi-parametric MRI (bpMRI), which is non-invasive,
cost-effective, and more efficient compared to multiparametric MRI (mpMRI), can
contribute to precision care for PCa. The rapid rise in artificial intelligence
(AI) algorithms are enabling unprecedented improvements in providing decision
support systems that can aid in csPCa diagnosis and understanding. However,
existing state of the art AI algorithms which are based on deep learning
technology are often limited to 2D images that fails to capture inter-slice
correlations in 3D volumetric images. The use of 3D convolutional neural
networks (CNNs) partly overcomes this limitation, but it does not adapt to the
anisotropy of images, resulting in sub-optimal semantic representation and poor
generalization. Furthermore, due to the limitation of the amount of labelled
data of bpMRI and the difficulty of labelling, existing CNNs are built on
relatively small datasets, leading to a poor performance. To address the
limitations identified above, we propose a new Zonal-aware Self-supervised Mesh
Network (Z-SSMNet) that adaptatively fuses multiple 2D, 2.5D and 3D CNNs to
effectively balance representation for sparse inter-slice information and dense
intra-slice information in bpMRI. A self-supervised learning (SSL) technique is
further introduced to pre-train our network using unlabelled data to learn the
generalizable image features. Furthermore, we constrained our network to
understand the zonal specific domain knowledge to improve the diagnosis
precision of csPCa. Experiments on the PI-CAI Challenge dataset demonstrate our
proposed method achieves better performance for csPCa detection and diagnosis
in bpMRI.",,,arXiv,,,2022-12-12,2022,,,,,,All OA, Green,Preprint,"Yuan, Yuan; Ahn, Euijoon; Feng, Dagan; Khadra, Mohamad; Kim, Jinman","Yuan, Yuan (); Ahn, Euijoon (); Feng, Dagan (); Khadra, Mohamad (); Kim, Jinman ()",,"Yuan, Yuan (); Ahn, Euijoon (); Feng, Dagan (); Khadra, Mohamad (); Kim, Jinman ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153603596,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
773,pub.1131417860,10.1007/978-3-030-60946-7_12,,,LUCAS: LUng CAncer Screening with Multimodal Biomarkers,"We present the LUng CAncer Screening (LUCAS) Dataset for evaluating lung cancer diagnosis with both imaging and clinical biomarkers in a realistic screening setting. We extract key information from anonymized clinical records and radiology reports, and we use it as a natural complement to low-dose chest CT scans of patients. We formulate the task as a detection problem and we develop a deep learning baseline to serve as a future reference of algorithmic performance. Our results provide solid empirical evidence for the difficulty of the task in the LUCAS Dataset and for the interest of including multimodal biomarkers in the analysis. All the resources of the LUCAS Dataset are publicly available.",This project was partially funded by the Google Latin America Research Awards (LARA) 2019.,,Lecture Notes in Computer Science,Multimodal Learning for Clinical Decision Support and Clinical Image-Based Procedures,,2020-10-01,2020,2020-10-01,2020,12445,,115-124,Closed,Chapter,"Daza, Laura; Castillo, Angela; Escobar, MarÃ­a; Valencia, Sergio; PinzÃ³n, Bibiana; ArbelÃ¡ez, Pablo","Daza, Laura (Center for Research and Formation in Artificial Intelligence, Universidad de los Andes, BogotÃ¡, Colombia); Castillo, Angela (Center for Research and Formation in Artificial Intelligence, Universidad de los Andes, BogotÃ¡, Colombia); Escobar, MarÃ­a (Center for Research and Formation in Artificial Intelligence, Universidad de los Andes, BogotÃ¡, Colombia); Valencia, Sergio (FundaciÃ³n Santa Fe de BogotÃ¡, BogotÃ¡, Colombia); PinzÃ³n, Bibiana (FundaciÃ³n Santa Fe de BogotÃ¡, BogotÃ¡, Colombia); ArbelÃ¡ez, Pablo (Center for Research and Formation in Artificial Intelligence, Universidad de los Andes, BogotÃ¡, Colombia)","Daza, Laura (Universidad de Los Andes)","Daza, Laura (Universidad de Los Andes); Castillo, Angela (Universidad de Los Andes); Escobar, MarÃ­a (Universidad de Los Andes); Valencia, Sergio (FundaciÃ³n Santa Fe de BogotÃ¡); PinzÃ³n, Bibiana (FundaciÃ³n Santa Fe de BogotÃ¡); ArbelÃ¡ez, Pablo (Universidad de Los Andes)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1131417860,46 Information and Computing Sciences,,,,,,,,,,,,
773,pub.1140549364,10.48550/arxiv.2108.08537,,,Multi-task Federated Learning for Heterogeneous Pancreas Segmentation,"Federated learning (FL) for medical image segmentation becomes more
challenging in multi-task settings where clients might have different
categories of labels represented in their data. For example, one client might
have patient data with ""healthy'' pancreases only while datasets from other
clients may contain cases with pancreatic tumors. The vanilla federated
averaging algorithm makes it possible to obtain more generalizable deep
learning-based segmentation models representing the training data from multiple
institutions without centralizing datasets. However, it might be sub-optimal
for the aforementioned multi-task scenarios. In this paper, we investigate
heterogeneous optimization methods that show improvements for the automated
segmentation of pancreas and pancreatic tumors in abdominal CT images with FL
settings.",,,arXiv,,,2021-08-19,2021,,,,,,All OA, Green,Preprint,"Shen, Chen; Wang, Pochuan; Roth, Holger R.; Yang, Dong; Xu, Daguang; Oda, Masahiro; Wang, Weichung; Fuh, Chiou-Shann; Chen, Po-Ting; Liu, Kao-Lang; Liao, Wei-Chih; Mori, Kensaku","Shen, Chen (); Wang, Pochuan (); Roth, Holger R. (); Yang, Dong (); Xu, Daguang (); Oda, Masahiro (); Wang, Weichung (); Fuh, Chiou-Shann (); Chen, Po-Ting (); Liu, Kao-Lang (); Liao, Wei-Chih (); Mori, Kensaku ()",,"Shen, Chen (); Wang, Pochuan (); Roth, Holger R. (); Yang, Dong (); Xu, Daguang (); Oda, Masahiro (); Wang, Weichung (); Fuh, Chiou-Shann (); Chen, Po-Ting (); Liu, Kao-Lang (); Liao, Wei-Chih (); Mori, Kensaku ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140549364,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
773,pub.1138337382,10.1109/isbi48211.2021.9433886,,,Deep Elastica For Image Segmentation,"Image segmentation is a fundamental topic in image processing and has been studied for many decades. Deep learning-based supervised segmentation models have achieved state-of-the-art performance, but most of them are limited by using pixel-wise loss functions for training without geometrical constraints. Inspired by the Eulerâs Elastica model and recent active contour models introduced into deep learning, we propose a novel active contour with an elastic (ACE) loss function. The ACE loss function incorporates Elastica knowledge as geometrically-natural constraints for the image segmentation tasks. In the ACE loss function, we introduce the mean curvature, i.e. the average of all principal curvatures, as a more compelling image prior to represent the curvature. Furthermore, based on the mean curvature definition, we propose a fast solution (Fast-ACE) to approximate our ACE loss with Laplace operators for three-dimensional (3D) image segmentation. We evaluate our ACE loss and Fast-ACE loss functions on one two-dimensional (2D) dataset and one 3D biomedical image dataset. Our results show that the proposed loss function outperforms other mainstream loss functions when different segmentation networks are used. Our source code is available at https://github.com/HiLab-git/ACELoss.","Xu Chen is funded by a studentship funded by the Vascular Surgery Research Fund in Liverpool, and partially funded by The Great Britain-China Educational Trust (no.269944) administered by the Great Britain-China Centre. Xu Chen is funded by a studentship funded by the Vascular Surgery Research Fund in Liverpool, and partially funded by The Great Britain-China Educational Trust (no.269944) administered by the Great Britain-China Centre.",,,2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI),,2021-04-16,2021,,2021-04-16,0,,706-710,Closed,Proceeding,"Chen, Xu; Luo, Xiangde; Wangy, Guotai; Zhengy, Yalin","Chen, Xu (Department of Eye and Vision Science, University of Liverpool); Luo, Xiangde (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China); Wangy, Guotai (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China); Zhengy, Yalin (Department of Eye and Vision Science, University of Liverpool)","Wangy, Guotai (University of Electronic Science and Technology of China); Zhengy, Yalin (University of Liverpool)","Chen, Xu (University of Liverpool); Luo, Xiangde (University of Electronic Science and Technology of China); Wangy, Guotai (University of Electronic Science and Technology of China); Zhengy, Yalin (University of Liverpool)",2,2,,1.64,,https://app.dimensions.ai/details/publication/pub.1138337382,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
773,pub.1136694193,10.1007/978-3-030-72084-1_10,,,Label-Efficient Multi-task Segmentation Using Contrastive Learning,"Obtaining annotations for 3D medical images is expensive and time-consuming, despite its importance for automating segmentation tasks. Although multi-task learning is considered an effective method for training segmentation models using small amounts of annotated data, a systematic understanding of various subtasks is still lacking. In this study, we propose a multi-task segmentation model with a contrastive learning based subtask and compare its performance with other multi-task models, varying the number of labeled data for training. We further extend our model so that it can utilize unlabeled data through the regularization branch in a semi-supervised manner. We experimentally show that our proposed method outperforms other multi-task methods including the state-of-the-art fully supervised model when the amount of annotated data is limited.",J.I. was supported by the Grant-in-Aid for JSPS Fellows JP18J21942.,,Lecture Notes in Computer Science,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",,2021-03-27,2021,2021-03-27,2021,12658,,101-110,All OA, Green,Chapter,"Iwasawa, Junichiro; Hirano, Yuichiro; Sugawara, Yohei","Iwasawa, Junichiro (The University of Tokyo, Tokyo, Japan); Hirano, Yuichiro (Preferred Networks, Tokyo, Japan); Sugawara, Yohei (Preferred Networks, Tokyo, Japan)","Iwasawa, Junichiro (University of Tokyo)","Iwasawa, Junichiro (University of Tokyo); Hirano, Yuichiro (Preferred Networks (Japan)); Sugawara, Yohei (Preferred Networks (Japan))",4,4,,3.27,http://arxiv.org/pdf/2009.11160,https://app.dimensions.ai/details/publication/pub.1136694193,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
772,pub.1153648738,10.1007/978-3-031-21014-3_7,,,A More Design-Flexible Medical Transformer for Volumetric Image Segmentation,"UNet-based encoder-decoder networks dominate volumetric medical image segmentation in the past several years. Many improvements focus on the design of encoders, decoders and skip connections. Due to the intrinsic property of convolutional kernels, convolution-based encoders suffer from limited receptive fields. To deal with that, recently proposed Transformer-based networks leveraging the self-attention mechanism build long-range dependency. However, they are highly reliable on pretrained weights from natural images. In our work, we find out ViT-based (Vision Transformer) modelsâ performance will not decrease significantly without pretrained weights even if there is a limited data source. So we flexibly design a 3D medical Transformer for image segmentation and train it from scratch. Specifically, we introduce Multi-Scale Dynamic Positional Embeddings to ViT to dynamically acquire positional information of each 3D patch. Positional bias can also enrich attention diversities. Moreover, we give detailed reasons why we choose the convolution-based decoder instead of recently proposed Swin Transformer blocks after preliminary experiments on the decoder design. Finally, we propose the Context Enhancement Module to refine skipped features by merging low and high-frequency information via a combination of convolutional kernels and self-attention modules. Experiments show that our model is comparable to nnUNet on segmentation performance of Medical Segmentation Decathlon (Liver) and VerSeâ20 datasets when trained from scratch.",,,Lecture Notes in Computer Science,Machine Learning in Medical Imaging,,2022-12-16,2022,2022-12-16,2022,13583,,62-71,Closed,Chapter,"You, Xin; Gu, Yun; He, Junjun; Sun, Hui; Yang, Jie","You, Xin (Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China); Gu, Yun (Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China); He, Junjun (SenseTime Research, Beijing, China); Sun, Hui (SenseTime Research, Beijing, China); Yang, Jie (Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China)","Gu, Yun (Shanghai Jiao Tong University; Shanghai Jiao Tong University); Yang, Jie (Shanghai Jiao Tong University; Shanghai Jiao Tong University)","You, Xin (Shanghai Jiao Tong University; Shanghai Jiao Tong University); Gu, Yun (Shanghai Jiao Tong University; Shanghai Jiao Tong University); He, Junjun (); Sun, Hui (); Yang, Jie (Shanghai Jiao Tong University; Shanghai Jiao Tong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153648738,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
772,pub.1151856857,10.1007/978-3-031-18814-5_11,,,Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation,"Detection of tumors in metastatic colorectal cancer (mCRC) plays an essential role in the early diagnosis and treatment of liver cancer. Deep learning models backboned by fully convolutional neural networks (FCNNs) have become the dominant model for segmenting 3D computerized tomography (CT) scans. However, since their convolution layers suffer from limited kernel size, they are not able to capture long-range dependencies and global context. To tackle this restriction, vision transformers have been introduced to solve FCNNâs locality of receptive fields. Although transformers can capture long-range features, their segmentation performance decreases with various tumor sizes due to the model sensitivity to the input patch size. While finding an optimal patch size improves the performance of vision transformer-based models on segmentation tasks, it is a time-consuming and challenging procedure. This paper proposes a technique to select the vision transformerâs optimal input multi-resolution image patch size based on the average volume size of metastasis lesions. We further validated our suggested framework using a transfer-learning technique, demonstrating that the highest Dice similarity coefficient (DSC) performance was obtained by pre-training on training data with a larger tumour volume using the suggested ideal patch size and then training with a smaller one. We experimentally evaluate this idea through pre-training our model on a multi-resolution public dataset. Our model showed consistent and improved results when applied to our private multi-resolution mCRC dataset with a smaller average tumor volume. This study lays the groundwork for optimizing semantic segmentation of small objects using vision transformers. The implementation source code is available at: https://github.com/Ramtin-Mojtahedi/OVTPS.",This work was funded in part by National Institutes of Health R01CA233888.,,Lecture Notes in Computer Science,Multiscale Multimodal Medical Imaging,,2022-10-12,2022,2022-10-12,2022,13594,,110-120,Closed,Chapter,"Mojtahedi, Ramtin; Hamghalam, Mohammad; Do, Richard K. G.; Simpson, Amber L.","Mojtahedi, Ramtin (School of Computing, Queenâs University, Kingston, ON, Canada); Hamghalam, Mohammad (School of Computing, Queenâs University, Kingston, ON, Canada; Department of Electrical Engineering, Qazvin Branch, Islamic Azad University, Qazvin, Iran); Do, Richard K. G. (Department of Radiology, Memorial Sloan Kettering Cancer Center, New York, NY, USA); Simpson, Amber L. (School of Computing, Queenâs University, Kingston, ON, Canada; Department of Biomedical and Molecular Sciences, Queenâs University, Kingston, ON, Canada)","Simpson, Amber L. (Queen's University; Queen's University)","Mojtahedi, Ramtin (Queen's University); Hamghalam, Mohammad (Queen's University; Qazvin Islamic Azad University); Do, Richard K. G. (Memorial Sloan Kettering Cancer Center); Simpson, Amber L. (Queen's University; Queen's University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151856857,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
772,pub.1146975998,10.48550/arxiv.2204.02450,,,Federated Cross Learning for Medical Image Segmentation,"Federated learning (FL) can collaboratively train deep learning models using
isolated patient data owned by different hospitals for various clinical
applications, including medical image segmentation. However, a major problem of
FL is its performance degradation when dealing with the data that are not
independently and identically distributed (non-iid), which is often the case in
medical images. In this paper, we first conduct a theoretical analysis on the
FL algorithm to reveal the problem of model aggregation during training on
non-iid data. With the insights gained through the analysis, we propose a
simple and yet effective method, federated cross learning (FedCross), to tackle
this challenging problem. Unlike the conventional FL methods that combine
multiple individually trained local models on a server node, our FedCross
sequentially trains the global model across different clients in a round-robin
manner, and thus the entire training procedure does not involve any model
aggregation steps. To further improve its performance to be comparable with the
centralized learning method, we combine the FedCross with an ensemble learning
mechanism to compose a federated cross ensemble learning (FedCrossEns) method.
Finally, we conduct extensive experiments using a set of public datasets. The
experimental results show that the proposed FedCross training strategy
outperforms the mainstream FL methods on non-iid data. In addition to improving
the segmentation performance, our FedCrossEns can further provide a
quantitative estimation of the model uncertainty, demonstrating the
effectiveness and clinical significance of our designs. Source code will be
made publicly available after paper publication.",,,arXiv,,,2022-04-05,2022,,,,,,All OA, Green,Preprint,"Xu, Xuanang; Chen, Tianyi; Deng, Han; Kuang, Tianshu; Barber, Joshua C.; Kim, Daeseung; Gateno, Jaime; Yan, Pingkun; Xia, James J.","Xu, Xuanang (); Chen, Tianyi (); Deng, Han (); Kuang, Tianshu (); Barber, Joshua C. (); Kim, Daeseung (); Gateno, Jaime (); Yan, Pingkun (); Xia, James J. ()",,"Xu, Xuanang (); Chen, Tianyi (); Deng, Han (); Kuang, Tianshu (); Barber, Joshua C. (); Kim, Daeseung (); Gateno, Jaime (); Yan, Pingkun (); Xia, James J. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146975998,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
772,pub.1142567293,10.1007/978-3-030-90874-4_10,,,Multi-task Federated Learning for Heterogeneous Pancreas Segmentation,"Federated learning (FL) for medical image segmentation becomes more challenging in multi-task settings where clients might have different categories of labels represented in their data. For example, one client might have patient data with âhealthyâ pancreases only while datasets from other clients may contain cases with pancreatic tumors. The vanilla federated averaging algorithm makes it possible to obtain more generalizable deep learning-based segmentation models representing the training data from multiple institutions without centralizing datasets. However, it might be sub-optimal for the aforementioned multi-task scenarios. In this paper, we investigate heterogeneous optimization methods that show improvements for the automated segmentation of pancreas and pancreatic tumors in abdominal CT images with FL settings.","Parts of this research was supported by the MEXT/JSPS KAKENHI (894030, 17H00867).",,Lecture Notes in Computer Science,"Clinical Image-Based Procedures, Distributed and Collaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning",,2021-11-14,2021,2021-11-14,2021,12969,,101-110,All OA, Green,Chapter,"Shen, Chen; Wang, Pochuan; Roth, Holger R.; Yang, Dong; Xu, Daguang; Oda, Masahiro; Wang, Weichung; Fuh, Chiou-Shann; Chen, Po-Ting; Liu, Kao-Lang; Liao, Wei-Chih; Mori, Kensaku","Shen, Chen (Nagoya University, Nagoya, Japan); Wang, Pochuan (National Taiwan University, Taipei, Taiwan); Roth, Holger R. (NVIDIA Corporation, Santa Clara, USA); Yang, Dong (NVIDIA Corporation, Santa Clara, USA); Xu, Daguang (NVIDIA Corporation, Santa Clara, USA); Oda, Masahiro (Nagoya University, Nagoya, Japan); Wang, Weichung (National Taiwan University, Taipei, Taiwan); Fuh, Chiou-Shann (National Taiwan University, Taipei, Taiwan); Chen, Po-Ting (National Taiwan University Hospital, Taipei, Taiwan); Liu, Kao-Lang (National Taiwan University Hospital, Taipei, Taiwan); Liao, Wei-Chih (National Taiwan University Hospital, Taipei, Taiwan); Mori, Kensaku (Nagoya University, Nagoya, Japan)","Wang, Weichung (National Taiwan University)","Shen, Chen (Nagoya University); Wang, Pochuan (National Taiwan University); Roth, Holger R. (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Xu, Daguang (Nvidia (United States)); Oda, Masahiro (Nagoya University); Wang, Weichung (National Taiwan University); Fuh, Chiou-Shann (National Taiwan University); Chen, Po-Ting (National Taiwan University Hospital); Liu, Kao-Lang (National Taiwan University Hospital); Liao, Wei-Chih (National Taiwan University Hospital); Mori, Kensaku (Nagoya University)",5,5,,4.09,http://arxiv.org/pdf/2108.08537,https://app.dimensions.ai/details/publication/pub.1142567293,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
772,pub.1136659913,10.1007/978-3-030-72087-2_30,,,Cerberus: A Multi-headed Network for Brain Tumor Segmentation,"The automated analysis of medical images requires robust and accurate algorithms that address the inherent challenges of identifying heterogeneous anatomical and pathological structures, such as brain tumors, in large volumetric images. In this paper, we present Cerberus, a single lightweight convolutional neural network model for the segmentation of fine-grained brain tumor regions in multichannel MRIs. Cerberus has an encoder-decoder architecture that takes advantage of a shared encoding phase to learn common representations for these regions and, then, uses specialized decoders to produce detailed segmentations. Cerberus learns to combine the weights learned for each category to produce a final multi-label segmentation. We evaluate our approach on the official test set of the Brain Tumor Segmentation Challenge 2020, and we obtain dice scores of 0.807 for enhancing tumor, 0.867 for whole tumor and 0.826 for tumor core.",,,Lecture Notes in Computer Science,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",,2021-03-26,2021,2021-03-26,2021,12659,,342-351,Closed,Chapter,"Daza, Laura; GÃ³mez, Catalina; ArbelÃ¡ez, Pablo","Daza, Laura (Universidad de los Andes, BogotÃ¡, Colombia); GÃ³mez, Catalina (Universidad de los Andes, BogotÃ¡, Colombia); ArbelÃ¡ez, Pablo (Universidad de los Andes, BogotÃ¡, Colombia)","Daza, Laura (Universidad de Los Andes)","Daza, Laura (Universidad de Los Andes); GÃ³mez, Catalina (Universidad de Los Andes); ArbelÃ¡ez, Pablo (Universidad de Los Andes)",3,3,,2.46,,https://app.dimensions.ai/details/publication/pub.1136659913,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
771,pub.1149652600,10.1007/978-3-031-08999-2_31,,,Brain Tumor Segmentation Using Neural Network Topology Search,"We apply a method from Automated Machine Learning (AutoML), namely Neural Architecture Search (NAS), to the task of brain tumor segmentation in MRIs for the BraTS 2021 challenge. NAS methods are known to be compute-intensive, so we use a continuous and differentiable search space in order to apply a DiNTS search for optimal fully convolutional architectures. Our method obtained Dice scores of 0.9161, 0.8707 and 0.8537 for whole tumor, tumor core and enhancing tumor regions respectively on the test dataset, while requiring no manual design of the network architecture, which was found automatically from the provided training data.",,,Lecture Notes in Computer Science,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",,2022-07-22,2022,2022-07-22,2022,12962,,366-376,Closed,Chapter,"Milesi, Alexandre; Futrega, Michal; Marcinkiewicz, Michal; Ribalta, Pablo","Milesi, Alexandre (NVIDIA, 95051, Santa Clara, CA, USA); Futrega, Michal (NVIDIA, 95051, Santa Clara, CA, USA); Marcinkiewicz, Michal (NVIDIA, 95051, Santa Clara, CA, USA); Ribalta, Pablo (NVIDIA, 95051, Santa Clara, CA, USA)","Milesi, Alexandre (Nvidia (United States))","Milesi, Alexandre (Nvidia (United States)); Futrega, Michal (Nvidia (United States)); Marcinkiewicz, Michal (Nvidia (United States)); Ribalta, Pablo (Nvidia (United States))",1,1,,,,https://app.dimensions.ai/details/publication/pub.1149652600,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
771,pub.1141327049,10.1007/978-3-030-87240-3_55,,,Asymmetric 3D Context Fusion for Universal Lesion Detection,"Modeling 3D context is essential for high-performance 3D medical image analysis. Although 2D networks benefit from large-scale 2D supervised pretraining, it is weak in capturing 3D context. 3D networks are strong in 3D context yet lack supervised pretraining. As an emerging technique, 3D context fusion operator, which enables conversion from 2D pretrained networks, leverages the advantages of both and has achieved great success. Existing 3D context fusion operators are designed to be spatially symmetric, i.e., performing identical operations on each 2D slice like convolutions. However, these operators are not truly equivariant to translation, especially when only a few 3D slices are used as inputs. In this paper, we propose a novel asymmetric 3D context fusion operator (A3D), which uses different weights to fuse 3D context from different 2D slices. Notably, A3D is NOT translation-equivariant while it significantly outperforms existing symmetric context fusion operators without introducing large computational overhead. We validate the effectiveness of the proposed method by extensive experiments on DeepLesion benchmark, a large-scale public dataset for universal lesion detection from computed tomography (CT). The proposed A3D consistently outperforms symmetric context fusion operators by considerable margins, and establishes a new state of the art on DeepLesion. To facilitate open research, our code and model in PyTorch is available at https://github.com/M3DV/AlignShift.","This work was supported by National Science Foundation of China (U20B2072, 61976137).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12905,,571-580,All OA, Green,Chapter,"Yang, Jiancheng; He, Yi; Kuang, Kaiming; Lin, Zudi; Pfister, Hanspeter; Ni, Bingbing","Yang, Jiancheng (Shanghai Jiao Tong University, Shanghai, China; Dianei Technology, Shanghai, China); He, Yi (Dianei Technology, Shanghai, China); Kuang, Kaiming (Dianei Technology, Shanghai, China); Lin, Zudi (Harvard University, Cambridge, MA, USA); Pfister, Hanspeter (Harvard University, Cambridge, MA, USA); Ni, Bingbing (Shanghai Jiao Tong University, Shanghai, China)","Ni, Bingbing (Shanghai Jiao Tong University)","Yang, Jiancheng (Shanghai Jiao Tong University); He, Yi (); Kuang, Kaiming (); Lin, Zudi (Harvard University); Pfister, Hanspeter (Harvard University); Ni, Bingbing (Shanghai Jiao Tong University)",8,8,,6.13,http://arxiv.org/pdf/2109.08684,https://app.dimensions.ai/details/publication/pub.1141327049,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
771,pub.1141238863,10.1007/978-3-030-87444-5_5,,,Visual Explanation by Unifying Adversarial Generation and Feature Importance Attributions,"Explaining the decisions of deep learning models is critical for their adoption in medical practice. In this work, we propose to unify existing adversarial explanation methods and path-based feature importance attribution approaches. We consider a path between the input image and a generated adversary and associate a weight depending on the model output variations along this path. We validate our attribution methods on two medical classification tasks. We demonstrate significant improvement compared to state-of-the-art methods in both feature importance attribution and localization performance.",,,Lecture Notes in Computer Science,"Interpretability of Machine Intelligence in Medical Image Computing, and Topological Data Analysis and Its Applications for Medical Data",,2021-09-21,2021,2021-09-21,2021,12929,,44-55,Closed,Chapter,"Charachon, Martin; CournÃ¨de, Paul-Henry; Hudelot, CÃ©line; Ardon, Roberto","Charachon, Martin (Incepto Medical, Paris, France; MICS, UniversitÃ© Paris-Saclay, CentraleSupÃ©lec, France); CournÃ¨de, Paul-Henry (MICS, UniversitÃ© Paris-Saclay, CentraleSupÃ©lec, France); Hudelot, CÃ©line (MICS, UniversitÃ© Paris-Saclay, CentraleSupÃ©lec, France); Ardon, Roberto (Incepto Medical, Paris, France)","Charachon, Martin (; CentraleSupÃ©lec)","Charachon, Martin (CentraleSupÃ©lec); CournÃ¨de, Paul-Henry (CentraleSupÃ©lec); Hudelot, CÃ©line (CentraleSupÃ©lec); Ardon, Roberto ()",1,1,,0.82,,https://app.dimensions.ai/details/publication/pub.1141238863,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
771,pub.1153217995,10.21203/rs.3.rs-2239765/v1,,,Semantic Segmentation Model Performance on Vehicle-induced Dust Cloud Identification on Unsealed Roads,"A substantial proportion of roads, especially in developing countries, remain unsealed. Monitoring of traffic-induced dust is an essential part of unsealed road maintenance. The measurement of the degree of severity of dust emissions is a challenging task due to dust's spatio-temporal characteristics, the requirement for expensive equipment, and rapidly changing environmental conditions. The next evolution of mobility solutions for unsealed roads calls for more robust, time-efficient and smart methods, such as machine learning techniques to solve dust-related road problems. This paper investigates the performance of current semantic segmentation machine learning models on the identification of dust clouds using the previously published URDE dataset. Based on static image performance, the selected models are then used to identify and segment dust clouds in video recordings of road dust emissions. These are used to visually analyse the segmentation quality of the best-performing machine learning models and determine whether the development of novel machine learning models for segmentation of dust is necessary. In this study, we provide code and validation data including segmented videos which will assist in real-world applications.",,,Research Square,,,2022-11-29,2022,2022-11-29,,,,,All OA, Green,Preprint,"De Silva, Asanka; Ranasinghe, Rajitha; Sounthararajah, Arooran; Haghighi, Hamed; Kodikara, Jayantha","De Silva, Asanka (Monash University); Ranasinghe, Rajitha (Monash University); Sounthararajah, Arooran (Monash University); Haghighi, Hamed (Downer EDI Works Pty Ltd); Kodikara, Jayantha (Monash University)",,"De Silva, Asanka (Monash University); Ranasinghe, Rajitha (Monash University); Sounthararajah, Arooran (Monash University); Haghighi, Hamed (); Kodikara, Jayantha (Monash University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153217995,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
771,pub.1151032955,10.1007/978-3-031-16440-8_52,,,DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation,"Self-supervised learning (SSL), enabling advanced performance with few annotations, has demonstrated a proven successful in medical image segmentation. Usually, SSL relies on measuring the similarity of features obtained at the deepest layer to attract the features of positive pairs or repulse the features of negative pairs, and then may suffer from the weak supervision at shallow layers. To address this issue, we reformulate SSL in a Deep Self-Distillation (DeSD) manner to improve the representation quality of both shallow and deep layers. Specifically, the DeSD model is composed of an online student network and a momentum teacher network, both being stacked by multiple sub-encoders. The features produced by each sub-encoder in the student network are trained to match the features produced by the teacher network. Such a deep self-distillation supervision is able to improve the representation quality of all sub-encoders, including both shallow ones and deep ones. We pre-train the DeSD model on a large-scale unlabeled dataset and evaluate it on seven downstream segmentation tasks. Our results indicate that the proposed DeSD model achieves superior pre-training performance over existing SSL methods, setting the new state of the art. The code is available at https://github.com/yeerwen/DeSD.","This work was supported in part by the National Natural Science Foundation of China under Grants 62171377, in part by the Key Research and Development Program of Shaanxi Province under Grant 2022GY-084, and in part by the Natural Science Foundation of Ningbo City, China, under Grant 2021J052.",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13434,,545-555,Closed,Chapter,"Ye, Yiwen; Zhang, Jianpeng; Chen, Ziyang; Xia, Yong","Ye, Yiwen (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, 710072, Xiâan, China); Zhang, Jianpeng (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, 710072, Xiâan, China); Chen, Ziyang (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, 710072, Xiâan, China); Xia, Yong (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, 710072, Xiâan, China; Ningbo Institute of Northwestern Polytechnical University, 315048, Ningbo, China; Research & Development Institute of Northwestern Polytechnical University in Shenzhen, 518057, Shenzhen, China)","Xia, Yong (Northwestern Polytechnical University; ; Northwestern Polytechnical University)","Ye, Yiwen (Northwestern Polytechnical University); Zhang, Jianpeng (Northwestern Polytechnical University); Chen, Ziyang (Northwestern Polytechnical University); Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151032955,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
771,pub.1146094513,10.48550/arxiv.2203.02114,,,MixCL: Pixel label matters to contrastive learning,"Contrastive learning and self-supervised techniques have gained prevalence in
computer vision for the past few years. It is essential for medical image
analysis, which is often notorious for its lack of annotations. Most existing
self-supervised methods applied in natural imaging tasks focus on designing
proxy tasks for unlabeled data. For example, contrastive learning is often
based on the fact that an image and its transformed version share the same
identity. However, pixel annotations contain much valuable information for
medical image segmentation, which is largely ignored in contrastive learning.
In this work, we propose a novel pre-training framework called Mixed
Contrastive Learning (MixCL) that leverages both image identities and pixel
labels for better modeling by maintaining identity consistency, label
consistency, and reconstruction consistency together. Consequently, thus
pre-trained model has more robust representations that characterize medical
images. Extensive experiments demonstrate the effectiveness of the proposed
method, improving the baseline by 5.28% and 14.12% in Dice coefficient when 5%
labeled data of Spleen and 15% of BTVC are used in fine-tuning, respectively.",,,arXiv,,,2022-03-03,2022,,,,,,All OA, Green,Preprint,"Li, Jun; Quan, Quan; Zhou, S. Kevin","Li, Jun (); Quan, Quan (); Zhou, S. Kevin ()",,"Li, Jun (); Quan, Quan (); Zhou, S. Kevin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146094513,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
771,pub.1142315868,10.48550/arxiv.2110.15664,,,3D-OOCS: Learning Prostate Segmentation with Inductive Bias,"Despite the great success of convolutional neural networks (CNN) in 3D
medical image segmentation tasks, the methods currently in use are still not
robust enough to the different protocols utilized by different scanners, and to
the variety of image properties or artefacts they produce. To this end, we
introduce OOCS-enhanced networks, a novel architecture inspired by the innate
nature of visual processing in the vertebrates. With different 3D U-Net
variants as the base, we add two 3D residual components to the second encoder
blocks: on and off center-surround (OOCS). They generalise the ganglion
pathways in the retina to a 3D setting. The use of 2D-OOCS in any standard CNN
network complements the feedforward framework with sharp edge-detection
inductive biases. The use of 3D-OOCS also helps 3D U-Nets to scrutinise and
delineate anatomical structures present in 3D images with increased accuracy.We
compared the state-of-the-art 3D U-Nets with their 3D-OOCS extensions and
showed the superior accuracy and robustness of the latter in automatic prostate
segmentation from 3D Magnetic Resonance Images (MRIs). For a fair comparison,
we trained and tested all the investigated 3D U-Nets with the same pipeline,
including automatic hyperparameter optimisation and data augmentation.",,,arXiv,,,2021-10-29,2021,,,,,,All OA, Green,Preprint,"Bhandary, Shrajan; Babaiee, Zahra; Kostyszyn, Dejan; Fechter, Tobias; Zamboglou, Constantinos; Grosu, Anca-Ligia; Grosu, Radu","Bhandary, Shrajan (); Babaiee, Zahra (); Kostyszyn, Dejan (); Fechter, Tobias (); Zamboglou, Constantinos (); Grosu, Anca-Ligia (); Grosu, Radu ()",,"Bhandary, Shrajan (); Babaiee, Zahra (); Kostyszyn, Dejan (); Fechter, Tobias (); Zamboglou, Constantinos (); Grosu, Anca-Ligia (); Grosu, Radu ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142315868,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,
769,pub.1141270586,10.48550/arxiv.2109.08684,,,Asymmetric 3D Context Fusion for Universal Lesion Detection,"Modeling 3D context is essential for high-performance 3D medical image
analysis. Although 2D networks benefit from large-scale 2D supervised
pretraining, it is weak in capturing 3D context. 3D networks are strong in 3D
context yet lack supervised pretraining. As an emerging technique, \emph{3D
context fusion operator}, which enables conversion from 2D pretrained networks,
leverages the advantages of both and has achieved great success. Existing 3D
context fusion operators are designed to be spatially symmetric, i.e.,
performing identical operations on each 2D slice like convolutions. However,
these operators are not truly equivariant to translation, especially when only
a few 3D slices are used as inputs. In this paper, we propose a novel
asymmetric 3D context fusion operator (A3D), which uses different weights to
fuse 3D context from different 2D slices. Notably, A3D is NOT
translation-equivariant while it significantly outperforms existing symmetric
context fusion operators without introducing large computational overhead. We
validate the effectiveness of the proposed method by extensive experiments on
DeepLesion benchmark, a large-scale public dataset for universal lesion
detection from computed tomography (CT). The proposed A3D consistently
outperforms symmetric context fusion operators by considerable margins, and
establishes a new \emph{state of the art} on DeepLesion. To facilitate open
research, our code and model in PyTorch are available at
https://github.com/M3DV/AlignShift.",,,arXiv,,,2021-09-17,2021,,,,,,All OA, Green,Preprint,"Yang, Jiancheng; He, Yi; Kuang, Kaiming; Lin, Zudi; Pfister, Hanspeter; Ni, Bingbing","Yang, Jiancheng (); He, Yi (); Kuang, Kaiming (); Lin, Zudi (); Pfister, Hanspeter (); Ni, Bingbing ()",,"Yang, Jiancheng (); He, Yi (); Kuang, Kaiming (); Lin, Zudi (); Pfister, Hanspeter (); Ni, Bingbing ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141270586,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
768,pub.1149818879,10.48550/arxiv.2207.13249,,,AADG: Automatic Augmentation for Domain Generalization on Retinal Image  Segmentation,"Convolutional neural networks have been widely applied to medical image
segmentation and have achieved considerable performance. However, the
performance may be significantly affected by the domain gap between training
data (source domain) and testing data (target domain). To address this issue,
we propose a data manipulation based domain generalization method, called
Automated Augmentation for Domain Generalization (AADG). Our AADG framework can
effectively sample data augmentation policies that generate novel domains and
diversify the training set from an appropriate search space. Specifically, we
introduce a novel proxy task maximizing the diversity among multiple augmented
novel domains as measured by the Sinkhorn distance in a unit sphere space,
making automated augmentation tractable. Adversarial training and deep
reinforcement learning are employed to efficiently search the objectives.
Quantitative and qualitative experiments on 11 publicly-accessible fundus image
datasets (four for retinal vessel segmentation, four for optic disc and cup
(OD/OC) segmentation and three for retinal lesion segmentation) are
comprehensively performed. Two OCTA datasets for retinal vasculature
segmentation are further involved to validate cross-modality generalization.
Our proposed AADG exhibits state-of-the-art generalization performance and
outperforms existing approaches by considerable margins on retinal vessel,
OD/OC and lesion segmentation tasks. The learned policies are empirically
validated to be model-agnostic and can transfer well to other models. The
source code is available at https://github.com/CRazorback/AADG.",,,arXiv,,,2022-07-26,2022,,,,,,All OA, Green,Preprint,"Lyu, Junyan; Zhang, Yiqi; Huang, Yijin; Lin, Li; Cheng, Pujin; Tang, Xiaoying","Lyu, Junyan (); Zhang, Yiqi (); Huang, Yijin (); Lin, Li (); Cheng, Pujin (); Tang, Xiaoying ()",,"Lyu, Junyan (); Zhang, Yiqi (); Huang, Yijin (); Lin, Li (); Cheng, Pujin (); Tang, Xiaoying ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149818879,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4611 Machine Learning,,,,,,,,,
766,pub.1155159969,10.1109/tmi.2023.3242838,,,Context Label Learning: Improving Background Class Representations in Semantic Segmentation,"Background samples provide key contextual information for segmenting regions of interest (ROIs). However, they always cover a diverse set of structures, causing difficulties for the segmentation model to learn good decision boundaries with high sensitivity and precision. The issue concerns the highly heterogeneous nature of the background class, resulting in multi-modal distributions. Empirically, we find that neural networks trained with heterogeneous background struggle to map the corresponding contextual samples to compact clusters in feature space. As a result, the distribution over background logit activations may shift across the decision boundary, leading to systematic over-segmentation across different datasets and tasks. In this study, we propose context label learning (CoLab) to improve the context representations by decomposing the background class into several subclasses. Specifically, we train an auxiliary network as a task generator, along with the primary segmentation model, to automatically generate context labels that positively affect the ROI segmentation accuracy. Extensive experiments are conducted on several challenging segmentation tasks and datasets. The results demonstrate that CoLab can guide the segmentation model to map the logits of background samples away from the decision boundary, resulting in significantly improved segmentation accuracy. Code is available.",,,IEEE Transactions on Medical Imaging,,,2023-02-07,2023,2023-02-07,,PP,99,1-1,All OA, Hybrid,Article,"Li, Zeju; Kamnitsas, Konstantinos; Ouyang, Cheng; Chen, Chen; Glocker, Ben","Li, Zeju (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom); Kamnitsas, Konstantinos (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom); Ouyang, Cheng (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom); Chen, Chen (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom); Glocker, Ben (Department of Computing, BioMedIA Group, Imperial College London, United Kingdom)",,"Li, Zeju (Imperial College London); Kamnitsas, Konstantinos (Imperial College London); Ouyang, Cheng (Imperial College London); Chen, Chen (Imperial College London); Glocker, Ben (Imperial College London)",0,0,,,https://ieeexplore.ieee.org/ielx7/42/4359023/10038608.pdf,https://app.dimensions.ai/details/publication/pub.1155159969,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
766,pub.1124871807,10.1016/j.patcog.2020.107269,,,Deep co-training for semi-supervised image segmentation,"In this paper, we aim to improve the performance of semantic image segmentation in a semi-supervised setting where training is performed with a reduced set of annotated images and additional non-annotated images. We present a method based on an ensemble of deep segmentation models. Models are trained on subsets of the annotated data and use non-annotated images to exchange information with each other, similar to co-training. Diversity across models is enforced with the use of adversarial samples. We demonstrate the potential of our method on three challenging image segmentation problems, and illustrate its ability to share information between simultaneously trained models, while preserving their diversity. Results indicate clear advantages in terms of performance compared to recently proposed semi-supervised methods for segmentation.",We thank NVIDIA corporation for supporting this work through their GPU grant program. This project is partially supported by FRQNT scholarship.,,Pattern Recognition,,,2020-11,2020,,2020-11,107,,107269,All OA, Green,Article,"Peng, Jizong; Estrada, Guillermo; Pedersoli, Marco; Desrosiers, Christian","Peng, Jizong (ETS Montreal, 1100 Notre-Dame W., Montreal, Canada); Estrada, Guillermo (PUC-Rio, 225 MarquÃªs de SÃ£o Vicente Street, Rio de Janeiro, Brazil); Pedersoli, Marco (ETS Montreal, 1100 Notre-Dame W., Montreal, Canada); Desrosiers, Christian (ETS Montreal, 1100 Notre-Dame W., Montreal, Canada)","Peng, Jizong ","Peng, Jizong (); Estrada, Guillermo (Pontifical Catholic University of Rio de Janeiro); Pedersoli, Marco (); Desrosiers, Christian ()",87,81,,41.42,http://arxiv.org/pdf/1903.11233,https://app.dimensions.ai/details/publication/pub.1124871807,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
743,pub.1154234378,10.1109/iconda56696.2022.10000292,,,Leveraging Unlabeled Data Using Semi-Supervised Generative Adversarial Network for Medical Image Segmentation,"Medical image analysis has experienced different stages of development, especially with the emergence of deep learning. However, it is difficult to acquire large-scale, highquality labeled data to train the model when using deep learning. This paper proposes a semi-supervised learning method to achieve medical image segmentation using limited labeled data and large-scale unlabeled data. Inspired by the classic Generative Adversarial Network (GAN), we proposed semi-supervised learning based on GAN (semi-GAN) to implement medical image segmentation. In the proposed semiGAN, adversarial training between the generator and discriminator has achieved higher segmentation accuracy. The dataset used was hippocampus data in Medical Segmentation Decathlon (MSD), and there are four training data settings: 25 labeled slices/3,374 unlabeled slices; 50 labeled slices/3,349 unlabeled slices; 100 labeled slices/3,299 unlabeled slices; 200 labeled slices/3,199 unlabeled slices. For each data setting, there are two experiments conducted: fully-supervised learning based on a generator network using only labeled data (F-Generator), and semi-GAN. The experiments showed that semi-GAN can improve segmentation accuracy by an average of 0.4% using unlabeled data compared to F-Generator using labeled data. Further study will be conducted to improve the semi-GAN architecture.",We would like to thank Universiti Teknologi MARA Shah Alam for sponsoring this research under 600-RMC/LESTARI SDG-T 5/3 (139/2019) grant. The research is also done under Scientific and Technological Innovation Programs of Higher Education Institutions in Shanxi (2022L538). Corresponding author can be contacted through raseeda@uitm.edu.my.,We would like to thank Universiti Teknologi MARA Shah Alam for sponsoring this research under 600-RMC/LESTARI SDG-T 5/3 (139/2019) grant. The research is also done under Scientific and Technological Innovation Programs of Higher Education Institutions in Shanxi (2022L538),,2022 International Conference on Computer and Drone Applications (IConDA),,2022-11-29,2022,,2022-11-29,0,,67-72,Closed,Proceeding,"Li, Guoqin; Jamil, Nursuriati; Hamzah, Raseeda","Li, Guoqin (Taiyuan Institute of Technology, China Taiyuan, China); Jamil, Nursuriati (School of Computing Sciences College of Computing, Informatics and Media Universiti Teknologi MARA, Malaysia, Malaysia); Hamzah, Raseeda (School of Computing Sciences College of Computing, Informatics and Media Universiti Teknologi MARA, Malaysia, Malaysia)",,"Li, Guoqin (Taiyuan Institute of Technology); Jamil, Nursuriati (Universiti Teknologi MARA); Hamzah, Raseeda (Universiti Teknologi MARA)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154234378,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
743,pub.1146782323,10.2139/ssrn.4065778,,,Robustness of Different Loss Functions and Their Impact on Network's Learning,"Recent developments in AI have made it ubiquitous, every industry is trying to adopt some form of intelligent processing of their data. Despite so many advances in the field, AIâs full capability is yet to be exploited by the industry. Industries that involve some risk factors still remain cautious about the usage of AI due to the lack of trust in such autonomous systems. Present-day AI might be very good in a lot of things but it is very bad in reasoning and this behavior of AI can lead to catastrophic results. Autonomous cars crashing into a person or a drone getting stuck in a tree are a few examples where AI decisions lead to catastrophic results. To develop insight and generate an explanation about the learning capability of AI, we will try to analyze the working of loss functions. For our case, we will use two sets of loss functions, generalized loss functions like Binary cross-entropy or BCE and specialized loss functions like Dice loss or focal loss. Through a series of experiments, we will establish whether combining different loss functions is better than using a single loss function and if yes, then what is the reason behind it. In order to establish the difference between generalized loss and specialized losses, we will train several models using the above-mentioned losses and then compare their robustness on adversarial examples. In particular, we will look at how fast the accuracy of different models decreases when we change the pixels corresponding to the most salient gradients.",,,SSRN Electronic Journal,,,2022,2022,,,,,,All OA, Green,Preprint,"Rajput, Vishal","Rajput, Vishal (affiliation not provided to SSRN)",,"Rajput, Vishal ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1146782323,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
742,pub.1153158793,10.48550/arxiv.2211.14051,,,Open-Source Skull Reconstruction with MONAI,"We present a deep learning-based approach for skull reconstruction for MONAI,
which has been pre-trained on the MUG500+ skull dataset. The implementation
follows the MONAI contribution guidelines, hence, it can be easily tried out
and used, and extended by MONAI users. The primary goal of this paper lies in
the investigation of open-sourcing codes and pre-trained deep learning models
under the MONAI framework. Nowadays, open-sourcing software, especially
(pre-trained) deep learning models, has become increasingly important. Over the
years, medical image analysis experienced a tremendous transformation. Over a
decade ago, algorithms had to be implemented and optimized with low-level
programming languages, like C or C++, to run in a reasonable time on a desktop
PC, which was not as powerful as today's computers. Nowadays, users have
high-level scripting languages like Python, and frameworks like PyTorch and
TensorFlow, along with a sea of public code repositories at hand. As a result,
implementations that had thousands of lines of C or C++ code in the past, can
now be scripted with a few lines and in addition executed in a fraction of the
time. To put this even on a higher level, the Medical Open Network for
Artificial Intelligence (MONAI) framework tailors medical imaging research to
an even more convenient process, which can boost and push the whole field. The
MONAI framework is a freely available, community-supported, open-source and
PyTorch-based framework, that also enables to provide research contributions
with pre-trained models to others. Codes and pre-trained weights for skull
reconstruction are publicly available at:
https://github.com/Project-MONAI/research-contributions/tree/master/SkullRec",,,arXiv,,,2022-11-25,2022,,,,,,All OA, Green,Preprint,"Li, Jianning; Ferreira, AndrÃ©; Puladi, Behrus; Alves, Victor; Kamp, Michael; Kim, Moon-Sung; Nensa, Felix; Kleesiek, Jens; Ahmadi, Seyed-Ahmad; Egger, Jan","Li, Jianning (); Ferreira, AndrÃ© (); Puladi, Behrus (); Alves, Victor (); Kamp, Michael (); Kim, Moon-Sung (); Nensa, Felix (); Kleesiek, Jens (); Ahmadi, Seyed-Ahmad (); Egger, Jan ()",,"Li, Jianning (); Ferreira, AndrÃ© (); Puladi, Behrus (); Alves, Victor (); Kamp, Michael (); Kim, Moon-Sung (); Nensa, Felix (); Kleesiek, Jens (); Ahmadi, Seyed-Ahmad (); Egger, Jan ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153158793,46 Information and Computing Sciences, 4601 Applied Computing,,,,,,,,,,
742,pub.1151032881,10.1007/978-3-031-16437-8_54,,,DeepCRC: Colorectum and Colorectal Cancer Segmentation in CT Scans via Deep Colorectal Coordinate Transform,"We propose DeepCRC, a topology-aware deep learning-based approach for automated colorectum and colorectal cancer (CRC) segmentation in routine abdominal CT scans. Compared with MRI and CT Colonography, regular CT has a broader application but is more challenging. Standard segmentation algorithms often induce discontinued colon prediction, leading to inaccurate or completely failed CRC segmentation. To tackle this issue, we establish a new 1D colorectal coordinate system that encodes the position information along the colorectal elongated topology. In addition to the regular segmentation task, we propose an auxiliary regression task that directly predicts the colorectal coordinate for each voxel. This task integrates the global topological information into the network embedding and thus improves the continuity of the colorectum and the accuracy of the tumor segmentation. To enhance the modelâs architectural ability of modeling global context, we add self-attention layers to the model backbone, and found it complementary to the proposed algorithm. We validate our approach on a cross-validation of 107 cases and outperform nnUNet by an absolute margin of 1.3% in colorectum segmentation and 8.3% in CRC segmentation. Notably, we achieve comparable tumor segmentation performance with the human inter-observer (DSC: 0.646 vs. 0.639), indicating that our method has similar reproducibility as a human observer.","This study was supported by the National Key Research and Development Program of China [grant number 2021YFF1201003], the National Science Fund for Distinguished Young Scholars [grant number 81925023], the National Natural Scientific Foundation of China [grant number 82072090].",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13433,,564-573,Closed,Chapter,"Yao, Lisha; Xia, Yingda; Zhang, Haochen; Yao, Jiawen; Jin, Dakai; Qiu, Bingjiang; Zhang, Yuan; Li, Suyun; Liang, Yanting; Hua, Xian-Sheng; Lu, Le; Chen, Xin; Liu, Zaiyi; Zhang, Ling","Yao, Lisha (Guangdong Provincial Peopleâs Hospital, Guangzhou, China; South China University of Technology, Guangzhou, China); Xia, Yingda (Alibaba Group, Hangzhou, China); Zhang, Haochen (Guangdong Provincial Peopleâs Hospital, Guangzhou, China; South China University of Technology, Guangzhou, China); Yao, Jiawen (Alibaba Group, Hangzhou, China); Jin, Dakai (Alibaba Group, Hangzhou, China); Qiu, Bingjiang (Guangdong Provincial Peopleâs Hospital, Guangzhou, China); Zhang, Yuan (Guangdong Provincial Peopleâs Hospital, Guangzhou, China); Li, Suyun (Guangdong Provincial Peopleâs Hospital, Guangzhou, China; South China University of Technology, Guangzhou, China); Liang, Yanting (Guangdong Provincial Peopleâs Hospital, Guangzhou, China; South China University of Technology, Guangzhou, China); Hua, Xian-Sheng (Alibaba Group, Hangzhou, China); Lu, Le (Alibaba Group, Hangzhou, China); Chen, Xin (South China University of Technology, Guangzhou, China; Guangzhou First Peopleâs Hospital, Guangzhou, China); Liu, Zaiyi (Guangdong Provincial Peopleâs Hospital, Guangzhou, China; South China University of Technology, Guangzhou, China); Zhang, Ling (Alibaba Group, Hangzhou, China)","Xia, Yingda (Alibaba Group (China))","Yao, Lisha (Guangdong Provincial People's Hospital; South China University of Technology); Xia, Yingda (Alibaba Group (China)); Zhang, Haochen (Guangdong Provincial People's Hospital; South China University of Technology); Yao, Jiawen (Alibaba Group (China)); Jin, Dakai (Alibaba Group (China)); Qiu, Bingjiang (Guangdong Provincial People's Hospital); Zhang, Yuan (Guangdong Provincial People's Hospital); Li, Suyun (Guangdong Provincial People's Hospital; South China University of Technology); Liang, Yanting (Guangdong Provincial People's Hospital; South China University of Technology); Hua, Xian-Sheng (Alibaba Group (China)); Lu, Le (Alibaba Group (China)); Chen, Xin (South China University of Technology; Guangzhou First People's Hospital); Liu, Zaiyi (Guangdong Provincial People's Hospital; South China University of Technology); Zhang, Ling (Alibaba Group (China))",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151032881,46 Information and Computing Sciences,,,,,,,,,,,,
742,pub.1135529077,10.1117/12.2582241,,,Progressive medical image annotation with convolutional neural network-based interactive segmentation method,"Deep learning-based segmentation algorithms for medical image require massive training datasets with accurate annotations, which is costly since it takes much human effort to manually labeling from scratch. Therefore, interactive image segmentation is important and may greatly improve the efficiency and accuracy of medical image labeling. Some interactive segmentation methods (e.g. Deep Extreme Cut and Deepgrow) may improve the labeling through minimal interactive input. However these methods only utilize the initial manually input information, while existing segmentation results (such as annotations produced by nonprofessionals or conventional segmentation algorithms) cannot be utilized. In this paper, an interactive segmentation method is proposed to make use of both existing segmentation results and human interactive information to optimize the segmentation results progressively. In this framework, the user only needs to click on the foreground or background of the target individual on the medical image, the algorithm could adaptively learn the correlation between them, and automatically completes the segmentation of the target. The main contributions of this paper are: (1) We adjusted and applied a convolutional neural network which takes medical image data and user's clicks information as input to achieve more accurate segmentation of medical images. (2) We designed an iterative training strategy to realize the applicability of the model to deal with different number of clicks data input. (3) We designed an algorithm based on false positive and false negative regions to simulate the user's clicks, so as to provide enough training data. By applying the proposed method, users can easily extract the region of interest or modify the segmentation results by multiple clicks. The experimental results of 6 medical image segmentation tasks show that the proposed method achieves more accurate segmentation results by at most five clicks.",,,Progress in Biomedical Optics and Imaging,Medical Imaging 2021: Image Processing,,2021-02-15,2021,,,11596,,115962q-115962q-11,Closed,Proceeding,"Bai, Yunkun; Sun, Guangmin; Li, Yu; Shen, Le; Zhang, Li","Bai, Yunkun (Beijing Univ. of Technology (China)); Sun, Guangmin (Beijing Univ. of Technology (China)); Li, Yu (Beijing Univ. of Technology (China)); Shen, Le (Tsinghua Univ. (China)); Zhang, Li (Tsinghua Univ. (China))",,"Bai, Yunkun (Beijing University of Technology); Sun, Guangmin (Beijing University of Technology); Li, Yu (Beijing University of Technology); Shen, Le (Tsinghua University); Zhang, Li (Tsinghua University)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1135529077,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,,,,,,,,,,
742,pub.1130511850,10.48550/arxiv.2009.00378,,,Deep Structure Learning using Feature Extraction in Trained Projection  Space,"Over the last decade of machine learning, convolutional neural networks have
been the most striking successes for feature extraction of rich sensory and
high-dimensional data. While learning data representations via convolutions is
already well studied and efficiently implemented in various deep learning
libraries, one often faces limited memory capacity and insufficient number of
training data, especially for high-dimensional and large-scale tasks. To
overcome these limitations, we introduce a network architecture using a
self-adjusting and data dependent version of the Radon-transform (linear data
projection), also known as x-ray projection, to enable feature extraction via
convolutions in lower-dimensional space. The resulting framework, named PiNet,
can be trained end-to-end and shows promising performance on volumetric
segmentation tasks. We test proposed model on public datasets to show that our
approach achieves comparable results only using fractional amount of
parameters. Investigation of memory usage and processing time confirms PiNet's
superior efficiency compared to other segmentation models.",,,arXiv,,,2020-09-01,2020,,,,,,All OA, Green,Preprint,"Angermann, Christoph; Haltmeier, Markus","Angermann, Christoph (); Haltmeier, Markus ()",,"Angermann, Christoph (); Haltmeier, Markus ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1130511850,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
742,pub.1154886846,10.1109/bigdata55660.2022.10020468,,,ACL-Net: Adaptive and Collaborative Learning Network for Multi-Site Prostate MRI Segmentation,"High-performance deep learning models require large amounts of data with high quality annotations for model training, while the labeling work usually takes a lot of time for the experts. Meanwhile, the inter-observer variability al-ways exist between annotations from different experts and the distribution shift between the data acquired from different medical institutions. To address these challenges, we propose an end-to-end domain adaptive collaborative learning network for multi-institutional prostate MRI segmentation. Specifically, we introduce an unpaired image translation module to match the image domains between different institutions, which can alleviate the heterogeneity between 1.5T and 3T prostate MR images during model training. Moreover, we design a self-taught strategy to transfer domain-aware knowledge to jointly learn generic and unique representations. Furthermore, we evaluate our approach in scenarios with limited or without annotations, experimental results show that our approach has better adaptation performance than traditional supervised learning approaches, and has the potential to extend to unsupervised domain adaptation scenario. We also evaluate our approach with prostate MRI segmentation benchmark datasets, experimental results show that our approach outperforms several state-of-the-art methods.",,,,2022 IEEE International Conference on Big Data (Big Data),,2022-12-20,2022,,2022-12-20,0,,3112-3117,Closed,Proceeding,"Ma, Zibo; Zhang, Bo; Zhang, Zheng; Wang, Wendong; Mi, Yue; Huang, Haiwen; Wu, Jingyun","Ma, Zibo (Beijing University of Posts and Telecommunications, Beijing, 100876, China; State Key Laboratory of Networking and Switching Technology, School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications); Zhang, Bo (Beijing University of Posts and Telecommunications, Beijing, 100876, China; School of Modern Post, Beijing University of Posts and Telecommunications); Zhang, Zheng (Beijing University of Posts and Telecommunications, Beijing, 100876, China; School of Modern Post, Beijing University of Posts and Telecommunications); Wang, Wendong (State Key Laboratory of Networking and Switching Technology, School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications, Beijing, 100876, China); Mi, Yue (Peking University First Hospital, Beijing, 100034, China; Department of Urology, Peking University First Hospital); Huang, Haiwen (Department of Urology, Peking University First Hospital; Peking University First Hospital, Beijing, 100034, China); Wu, Jingyun (Peking University First Hospital, Beijing, 100034, China; Department of Radiology, Peking University First Hospital)","Ma, Zibo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Zhang, Bo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications)","Ma, Zibo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Zhang, Bo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Zhang, Zheng (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Wang, Wendong (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Mi, Yue (Peking University First Hospital); Huang, Haiwen (Peking University First Hospital); Wu, Jingyun (Peking University First Hospital)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154886846,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
742,pub.1151412643,10.1007/978-3-031-16364-7_10,,,Assessing Layer Normalization with BraTS MRI Data in a Convolution Neural Net,"Deep learning-based Convolutional Neural Network (CNN) architectures are commonly used in medical imaging. Medical imaging data is highly imbalanced. A deep learning architecture on its own is prone to overfit. As a result, we need a generalized model to mitigate total risk. This paper assesses a layer normalization (LN) technique in a CNN-based 3D U-Net for faster training and better generalization in medical imaging. Layer Normalization (LN) is mostly used in Natural Language Processing (NLP) tasks such as question-answering, handwriting sequence generation, etc. along with Recurrent Neural Network (RNN). The usage of LN is yet to be studied in case of medical imaging. In this context, we use brain MRI segmentation and train our model with LN and without normalization. We compare both models and our LN-based model gives 32%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$32\%$$\end{document} less validation loss over without normalization-based model. We achieve validation dice scores of unseen input data passes to LN based model of 0.90 (7.5%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$7.5\%$$\end{document} higher than without normalization) for edema, 0.74 (12.5%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$12.5\%$$\end{document} higher than without normalization) for non-enhancing tumor and 0.95 (1.5%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.5\% $$\end{document}higher than without normalization) for enhancing tumor.",,,IFIP Advances in Information and Communication Technology,Computational Intelligence in Data Science,,2022-09-29,2022,2022-09-29,2022,654,,124-135,Closed,Chapter,"Rawat, Akhilesh; Kumar, Rajeev","Rawat, Akhilesh (Data to Knowledge (D2K) Lab, School of Computer and Systems Sciences, Jawaharlal Nehru University, 110 067, New Delhi, India); Kumar, Rajeev (Data to Knowledge (D2K) Lab, School of Computer and Systems Sciences, Jawaharlal Nehru University, 110 067, New Delhi, India)","Rawat, Akhilesh (Jawaharlal Nehru University)","Rawat, Akhilesh (Jawaharlal Nehru University); Kumar, Rajeev (Jawaharlal Nehru University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151412643,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
742,pub.1151033014,10.1007/978-3-031-16443-9_41,,,Weakly Supervised Volumetric Image Segmentation with Deformed Templates,"There are many approaches to weakly-supervised training of networks to segment 2D images. By contrast, existing approaches to segmenting volumetric images rely on full-supervision of a subset of 2D slices of the 3D volume. We propose an approach to volume segmentation that is truly weakly-supervised in the sense that we only need to provide a sparse set of 3D points on the surface of target objects instead of detailed 2D masks. We use the 3D points to deform a 3D template so that it roughly matches the target object outlines and we introduce an architecture that exploits the supervision it provides to train a network to find accurate boundaries. We evaluate our approach on Computed Tomography (CT), Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets and show that it substantially reduces the required amount of effort.",This work was supported in part by a Swiss National Science Foundation grant.,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13435,,422-432,All OA, Green,Chapter,"Wickramasinghe, Udaranga; Jensen, Patrick; Shah, Mian; Yang, Jiancheng; Fua, Pascal","Wickramasinghe, Udaranga (EPFL, Lausanne, Switzerland); Jensen, Patrick (EPFL, Lausanne, Switzerland; DTU, Lyngby, Denmark); Shah, Mian (EPFL, Lausanne, Switzerland); Yang, Jiancheng (EPFL, Lausanne, Switzerland; Shanghai Jiao Tong University, Shanghai, China); Fua, Pascal (EPFL, Lausanne, Switzerland)","Wickramasinghe, Udaranga (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne)","Wickramasinghe, Udaranga (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne); Jensen, Patrick (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne; Technical University of Denmark); Shah, Mian (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne); Yang, Jiancheng (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne; Shanghai Jiao Tong University); Fua, Pascal (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne)",1,1,,,http://arxiv.org/pdf/2106.03987,https://app.dimensions.ai/details/publication/pub.1151033014,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
742,pub.1149652603,10.1007/978-3-031-08999-2_34,,,A Two-Phase Optimal Mass Transportation Technique for 3D Brain Tumor Detection and Segmentation,"The goal of optimal mass transportation (OMT) is to transform any irregular 3D object (i.e., a brain image) into a cube without creating significant distortion, which is utilized to preprocess irregular brain samples to facilitate the tensor form of the input format of the U-net algorithm. The BraTS 2021 database newly provides a challenging platform for the detection and segmentation of brain tumors, namely, the whole tumor (WT), the tumor core (TC) and the enhanced tumor (ET), by AI techniques. We propose a two-phase OMT algorithm with density estimates for 3D brain tumor segmentation. In the first phase, we construct a volume-mass-preserving OMT via the density determined by the FLAIR grayscale of the scanned modality for the U-net and predict the possible tumor regions. Then, in the second phase, we increase the density on the region of interest and construct a new OMT to enlarge the target region of tumors for the U-net so that the U-net has a better chance to learn how to mark the correct segmentation labels. The application of this preprocessing OMT technique is a new and trending method for CNN training and validation.","This work was partially supported by the Ministry of Science and Technology (MoST), the National Center for Theoretical Sciences, the Big Data Computing Center of Southeast University, the Nanjing Center for Applied Mathematics, the ST Yau Center in Taiwan, and the Shing-Tung Yau Center at Southeast University. W.-W. Lin, T.-M. Huang, and M.-H. Yueh were partially supported by MoST 110-2115-M-A49-004-, 110-2115-M-003-012-MY3, and 109-2115-M-003-010-MY2 and 110-2115-M-003-014-, respectively. T. Li was supported in part by the National Natural Science Foundation of China (NSFC) 11971105.",,Lecture Notes in Computer Science,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",,2022-07-22,2022,2022-07-22,2022,12962,,400-409,Closed,Chapter,"Lin, Wen-Wei; Li, Tiexiang; Huang, Tsung-Ming; Lin, Jia-Wei; Yueh, Mei-Heng; Yau, Shing-Tung","Lin, Wen-Wei (Department of Applied Mathematics, National Yang Ming Chiao Tung University, 300, Hsinchu, Taiwan); Li, Tiexiang (Nanjing Center for Applied Mathematics, 211135, Nanjing, Peopleâs Republic of China; School of Mathematics, Southeast University, 211189, Nanjing, Peopleâs Republic of China); Huang, Tsung-Ming (Department of Mathematics, National Taiwan Normal University, 116, Taipei, Taiwan); Lin, Jia-Wei (Department of Applied Mathematics, National Yang Ming Chiao Tung University, 300, Hsinchu, Taiwan); Yueh, Mei-Heng (Department of Mathematics, National Taiwan Normal University, 116, Taipei, Taiwan); Yau, Shing-Tung (Department of Mathematics, Harvard University, Cambridge, USA)","Li, Tiexiang (; Southeast University); Huang, Tsung-Ming (National Taiwan Normal University)","Lin, Wen-Wei (National Yang Ming Chiao Tung University); Li, Tiexiang (Southeast University); Huang, Tsung-Ming (National Taiwan Normal University); Lin, Jia-Wei (National Yang Ming Chiao Tung University); Yueh, Mei-Heng (National Taiwan Normal University); Yau, Shing-Tung (Harvard University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149652603,46 Information and Computing Sciences,,,,,,,,,,,,
742,pub.1141271395,10.48550/arxiv.2109.09521,,,RibSeg Dataset and Strong Point Cloud Baselines for Rib Segmentation  from CT Scans,"Manual rib inspections in computed tomography (CT) scans are clinically
critical but labor-intensive, as 24 ribs are typically elongated and oblique in
3D volumes. Automatic rib segmentation methods can speed up the process through
rib measurement and visualization. However, prior arts mostly use in-house
labeled datasets that are publicly unavailable and work on dense 3D volumes
that are computationally inefficient. To address these issues, we develop a
labeled rib segmentation benchmark, named \emph{RibSeg}, including 490 CT scans
(11,719 individual ribs) from a public dataset. For ground truth generation, we
used existing morphology-based algorithms and manually refined its results.
Then, considering the sparsity of ribs in 3D volumes, we thresholded and
sampled sparse voxels from the input and designed a point cloud-based baseline
method for rib segmentation. The proposed method achieves state-of-the-art
segmentation performance (Dice~$\approx95\%$) with significant efficiency
($10\sim40\times$ faster than prior arts). The RibSeg dataset, code, and model
in PyTorch are available at https://github.com/M3DV/RibSeg.",,,arXiv,,,2021-09-17,2021,,,,,,All OA, Green,Preprint,"Yang, Jiancheng; Gu, Shixuan; Wei, Donglai; Pfister, Hanspeter; Ni, Bingbing","Yang, Jiancheng (); Gu, Shixuan (); Wei, Donglai (); Pfister, Hanspeter (); Ni, Bingbing ()",,"Yang, Jiancheng (); Gu, Shixuan (); Wei, Donglai (); Pfister, Hanspeter (); Ni, Bingbing ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141271395,40 Engineering, 4013 Geomatic Engineering, 46 Information and Computing Sciences,,,,,,,,,
742,pub.1135419617,10.48550/arxiv.2102.08005,,,TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation,"Medical image segmentation - the prerequisite of numerous clinical needs -
has been significantly prospered by recent advances in convolutional neural
networks (CNNs). However, it exhibits general limitations on modeling explicit
long-range relation, and existing cures, resorting to building deep encoders
along with aggressive downsampling operations, leads to redundant deepened
networks and loss of localized details. Hence, the segmentation task awaits a
better solution to improve the efficiency of modeling global contexts while
maintaining a strong grasp of low-level details. In this paper, we propose a
novel parallel-in-branch architecture, TransFuse, to address this challenge.
TransFuse combines Transformers and CNNs in a parallel style, where both global
dependency and low-level spatial details can be efficiently captured in a much
shallower manner. Besides, a novel fusion technique - BiFusion module is
created to efficiently fuse the multi-level features from both branches.
Extensive experiments demonstrate that TransFuse achieves the newest
state-of-the-art results on both 2D and 3D medical image sets including polyp,
skin lesion, hip, and prostate segmentation, with significant parameter
decrease and inference speed improvement.",,,arXiv,,,2021-02-16,2021,,,,,,All OA, Green,Preprint,"Zhang, Yundong; Liu, Huiye; Hu, Qiang","Zhang, Yundong (); Liu, Huiye (); Hu, Qiang ()",,"Zhang, Yundong (); Liu, Huiye (); Hu, Qiang ()",1,1,,0.87,,https://app.dimensions.ai/details/publication/pub.1135419617,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4611 Machine Learning",,,,,,,,,,,
742,pub.1128719035,10.48550/arxiv.2006.12575,,,LAMP: Large Deep Nets with Automated Model Parallelism for Image  Segmentation,"Deep Learning (DL) models are becoming larger, because the increase in model
size might offer significant accuracy gain. To enable the training of large
deep networks, data parallelism and model parallelism are two well-known
approaches for parallel training. However, data parallelism does not help
reduce memory footprint per device. In this work, we introduce Large deep 3D
ConvNets with Automated Model Parallelism (LAMP) and investigate the impact of
both input's and deep 3D ConvNets' size on segmentation accuracy. Through
automated model parallelism, it is feasible to train large deep 3D ConvNets
with a large input patch, even the whole image. Extensive experiments
demonstrate that, facilitated by the automated model parallelism, the
segmentation accuracy can be improved through increasing model size and input
context size, and large input yields significant inference speedup compared
with sliding window of small patches in the inference. Code is
available\footnote{https://monai.io/research/lamp-automated-model-parallelism}.",,,arXiv,,,2020-06-22,2020,,,,,,All OA, Green,Preprint,"Zhu, Wentao; Zhao, Can; Li, Wenqi; Roth, Holger; Xu, Ziyue; Xu, Daguang","Zhu, Wentao (); Zhao, Can (); Li, Wenqi (); Roth, Holger (); Xu, Ziyue (); Xu, Daguang ()",,"Zhu, Wentao (); Zhao, Can (); Li, Wenqi (); Roth, Holger (); Xu, Ziyue (); Xu, Daguang ()",1,1,,0.52,,https://app.dimensions.ai/details/publication/pub.1128719035,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
741,pub.1141301995,10.1007/978-3-030-87193-2_58,,,RibSeg Dataset and Strong Point Cloud Baselines for Rib Segmentation from CT Scans,"Manual rib inspections in computed tomography (CT) scans are clinically critical but labor-intensive, as 24 ribs are typically elongated and oblique in 3D volumes. Automatic rib segmentation methods can speed up the process through rib measurement and visualization. However, prior arts mostly use in-house labeled datasets that are publicly unavailable and work on dense 3D volumes that are computationally inefficient. To address these issues, we develop a labeled rib segmentation benchmark, named RibSeg, including 490 CT scans (11,719 individual ribs) from a public dataset. For ground truth generation, we used existing morphology-based algorithms and manually refined its results. Then, considering the sparsity of ribs in 3D volumes, we thresholded and sampled sparse voxels from the input and designed a point cloud-based baseline method for rib segmentation. The proposed method achieves state-of-the-art segmentation performance (DiceÂ â95%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\approx 95\%$$\end{document}) with significant efficiency (10â40Ã\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document} faster than prior arts). The RibSeg dataset, code, and model in PyTorch are available at https://github.com/M3DV/RibSeg.","This work was supported by the National Science Foundation of China (U20B2072, 61976137).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12901,,611-621,All OA, Green,Chapter,"Yang, Jiancheng; Gu, Shixuan; Wei, Donglai; Pfister, Hanspeter; Ni, Bingbing","Yang, Jiancheng (Shanghai Jiao Tong University, Shanghai, China; Dianei Technology, Shanghai, China); Gu, Shixuan (Shanghai Jiao Tong University, Shanghai, China); Wei, Donglai (Harvard University, Cambridge, MA, USA); Pfister, Hanspeter (Harvard University, Cambridge, MA, USA); Ni, Bingbing (Shanghai Jiao Tong University, Shanghai, China)","Ni, Bingbing (Shanghai Jiao Tong University)","Yang, Jiancheng (Shanghai Jiao Tong University); Gu, Shixuan (Shanghai Jiao Tong University); Wei, Donglai (Harvard University); Pfister, Hanspeter (Harvard University); Ni, Bingbing (Shanghai Jiao Tong University)",6,6,,,http://arxiv.org/pdf/2109.09521,https://app.dimensions.ai/details/publication/pub.1141301995,46 Information and Computing Sciences,,,,,,,,,,,
741,pub.1136690355,10.1016/j.compeleceng.2021.107097,,,Deep structure learning using feature extraction in trained projection space,"Over the last decade of machine learning, convolutional neural networks have been the most striking successes for feature extraction of rich sensory and high-dimensional data. While learning data representations via convolutions is already well studied and efficiently implemented in various deep learning libraries, one often faces limited memory capacity and insufficient number of training data, especially for high-dimensional and large-scale tasks. To overcome these limitations, we introduce a network architecture using a self-adjusting and data dependent version of the Radon-transform (linear data projection), also known as X-ray projection, to enable feature extraction via convolutions in lower-dimensional space. The resulting framework, named PiNet, can be trained end-to-end and shows promising performance on volumetric segmentation tasks. We test proposed model on public datasets to show that our approach achieves comparable results only using fractional amount of parameters. Investigation of memory usage and processing time confirms PiNetâs superior efficiency compared to other segmentation models.",All authors approved the version of the manuscript to be published.,,Computers & Electrical Engineering,,,2021-06,2021,,2021-06,92,,107097,All OA, Hybrid,Article,"Angermann, Christoph; Haltmeier, Markus","Angermann, Christoph (Department of Mathematics, University of Innsbruck, TechnikerstraÃe 13, A-6020 Innsbruck, Austria); Haltmeier, Markus (Department of Mathematics, University of Innsbruck, TechnikerstraÃe 13, A-6020 Innsbruck, Austria)","Angermann, Christoph (UniversitÃ¤t Innsbruck)","Angermann, Christoph (UniversitÃ¤t Innsbruck); Haltmeier, Markus (UniversitÃ¤t Innsbruck)",3,3,,2.46,https://doi.org/10.1016/j.compeleceng.2021.107097,https://app.dimensions.ai/details/publication/pub.1136690355,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
741,pub.1127711093,10.48550/arxiv.2005.08869,,,Predicting Scores of Medical Imaging Segmentation Methods with  Meta-Learning,"Deep learning has led to state-of-the-art results for many medical imaging
tasks, such as segmentation of different anatomical structures. With the
increased numbers of deep learning publications and openly available code, the
approach to choosing a model for a new task becomes more complicated, while
time and (computational) resources are limited. A possible solution to choosing
a model efficiently is meta-learning, a learning method in which prior
performance of a model is used to predict the performance for new tasks. We
investigate meta-learning for segmentation across ten datasets of different
organs and modalities. We propose four ways to represent each dataset by
meta-features: one based on statistical features of the images and three are
based on deep learning features. We use support vector regression and deep
neural networks to learn the relationship between the meta-features and prior
model performance. On three external test datasets these methods give Dice
scores within 0.10 of the true performance. These results demonstrate the
potential of meta-learning in medical imaging.",,,arXiv,,,2020-05-08,2020,,,,,,All OA, Green,Preprint,"van Sonsbeek, Tom; Cheplygina, Veronika","van Sonsbeek, Tom (); Cheplygina, Veronika ()",,"van Sonsbeek, Tom (); Cheplygina, Veronika ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1127711093,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
740,pub.1131396704,10.1007/978-3-030-59719-1_37,,,LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation,"Deep Learning (DL) models are becoming larger, because the increase in model size might offer significant accuracy gain. To enable the training of large deep networks, data parallelism and model parallelism are two well-known approaches for parallel training. However, data parallelism does not help reduce memory footprint per device. In this work, we introduce Large deep 3D ConvNets with Automated Model Parallelism (LAMP) and investigate the impact of both inputâs and deep 3D ConvNetsâ size on segmentation accuracy. Through automated model parallelism, it is feasible to train large deep 3D ConvNets with a large input patch, even the whole image. Extensive experiments demonstrate that, facilitated by the automated model parallelism, the segmentation accuracy can be improved through increasing model size and input context size, and large input yields significant inference speedup compared with sliding window of small patches in the inference. Code is available (https://monai.io/research/lamp-automated-model-parallelism).",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12264,,374-384,All OA, Green,Chapter,"Zhu, Wentao; Zhao, Can; Li, Wenqi; Roth, Holger; Xu, Ziyue; Xu, Daguang","Zhu, Wentao (NVIDIA, Bethesda, USA); Zhao, Can (NVIDIA, Bethesda, USA); Li, Wenqi (NVIDIA, Bethesda, USA); Roth, Holger (NVIDIA, Bethesda, USA); Xu, Ziyue (NVIDIA, Bethesda, USA); Xu, Daguang (NVIDIA, Bethesda, USA)","Zhu, Wentao (Nvidia (United States))","Zhu, Wentao (Nvidia (United States)); Zhao, Can (Nvidia (United States)); Li, Wenqi (Nvidia (United States)); Roth, Holger (Nvidia (United States)); Xu, Ziyue (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",7,6,,3.61,http://arxiv.org/pdf/2006.12575,https://app.dimensions.ai/details/publication/pub.1131396704,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
740,pub.1131391753,10.1007/978-3-030-59713-9_37,,,Cross-domain Medical Image Translation by Shared Latent Gaussian Mixture Model,"Current deep learning based segmentation models generalize poorly to different domains due to the lack of sufficient labelled image data. An important example in radiology is generalizing from contrast enhanced CT to non-contrast CT. In real-world clinical applications, cross-domain image analysis tools are in high demand since medical images from different domains are generally used to achieve precise diagnoses. For example, contrast enhanced CT at different phases are used to enhance certain pathologies or internal organs. Many existing cross-domain image-to-image translation models show impressive results on large organ segmentation by successfully preserving large structures across domains. However, such models lack the ability to preserve fine structures during the translation process, which is significant for many clinical applications, such as segmenting small calcified plaques in the aorta and pelvic arteries. In order to preserve fine structures during medical image translation, we propose a patch-based model using shared latent variables from a Gaussian mixture. We compare our image translation framework to several state-of-the-art methods on cross-domain image translation and show our model does a better job preserving fine structures. The superior performance of our model is verified by performing two tasks with the translated images - detection and segmentation of aortic plaques and pancreas segmentation. We expect the utility of our framework will extend to other problems beyond segmentation due to the improved quality of the generated images and enhanced ability to preserve small structures.",This research was supported in part by the Intramural Research Program of the National Institutes of Health Clinical Center. We thank NVIDIA for GPU card donations.,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12262,,379-389,All OA, Green,Chapter,"Zhu, Yingying; Tang, Youbao; Tang, Yuxing; Elton, Daniel C.; Lee, Sungwon; Pickhardt, Perry J.; Summers, Ronald M.","Zhu, Yingying (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, National Institutes of Health, Clinical Center, 20892, Bethesda, MD, USA); Tang, Youbao (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, National Institutes of Health, Clinical Center, 20892, Bethesda, MD, USA); Tang, Yuxing (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, National Institutes of Health, Clinical Center, 20892, Bethesda, MD, USA); Elton, Daniel C. (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, National Institutes of Health, Clinical Center, 20892, Bethesda, MD, USA); Lee, Sungwon (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, National Institutes of Health, Clinical Center, 20892, Bethesda, MD, USA); Pickhardt, Perry J. (School of Medicine and Public Health, University of Wisconsin, 53706, Madison, WI, USA); Summers, Ronald M. (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, National Institutes of Health, Clinical Center, 20892, Bethesda, MD, USA)","Zhu, Yingying (National Institutes of Health Clinical Center)","Zhu, Yingying (National Institutes of Health Clinical Center); Tang, Youbao (National Institutes of Health Clinical Center); Tang, Yuxing (National Institutes of Health Clinical Center); Elton, Daniel C. (National Institutes of Health Clinical Center); Lee, Sungwon (National Institutes of Health Clinical Center); Pickhardt, Perry J. (University of WisconsinâMadison); Summers, Ronald M. (National Institutes of Health Clinical Center)",15,14,,7.14,http://arxiv.org/pdf/2007.07230,https://app.dimensions.ai/details/publication/pub.1131391753,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
740,pub.1129359287,10.48550/arxiv.2007.07230,,,Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture  Model,"Current deep learning based segmentation models often generalize poorly
between domains due to insufficient training data. In real-world clinical
applications, cross-domain image analysis tools are in high demand since
medical images from different domains are often needed to achieve a precise
diagnosis. An important example in radiology is generalizing from non-contrast
CT to contrast enhanced CTs. Contrast enhanced CT scans at different phases are
used to enhance certain pathologies or organs. Many existing cross-domain
image-to-image translation models have been shown to improve cross-domain
segmentation of large organs. However, such models lack the ability to preserve
fine structures during the translation process, which is significant for many
clinical applications, such as segmenting small calcified plaques in the aorta
and pelvic arteries. In order to preserve fine structures during medical image
translation, we propose a patch-based model using shared latent variables from
a Gaussian mixture model. We compare our image translation framework to several
state-of-the-art methods on cross-domain image translation and show our model
does a better job preserving fine structures. The superior performance of our
model is verified by performing two tasks with the translated images -
detection and segmentation of aortic plaques and pancreas segmentation. We
expect the utility of our framework will extend to other problems beyond
segmentation due to the improved quality of the generated images and enhanced
ability to preserve small structures.",,,arXiv,,,2020-07-14,2020,,,,,,All OA, Green,Preprint,"Zhu, Yingying; Tang, Youbao; Tang, Yuxing; Elton, Daniel C.; Lee, Sungwon; Pickhardt, Perry J.; Summers, Ronald M.","Zhu, Yingying (); Tang, Youbao (); Tang, Yuxing (); Elton, Daniel C. (); Lee, Sungwon (); Pickhardt, Perry J. (); Summers, Ronald M. ()",,"Zhu, Yingying (); Tang, Youbao (); Tang, Yuxing (); Elton, Daniel C. (); Lee, Sungwon (); Pickhardt, Perry J. (); Summers, Ronald M. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129359287,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
740,pub.1153217255,10.18178/joig.10.4.166-171,,,A Comparison of Applying Image Processing and Deep Learning in Acne Region Extraction,,,,Journal of Image and Graphics,,,2022,2022,2022,2022,10,4,,All OA, Hybrid,Article,"Zhang, Chengrui; Huang, Guangyao; Yao, Kai; Leach, Mark; Sun, Jie; Huang, Kaizhu; Zhou, Xiaoyun; Yuan, Liqiong","Zhang, Chengrui (); Huang, Guangyao (); Yao, Kai (); Leach, Mark (); Sun, Jie (); Huang, Kaizhu (); Zhou, Xiaoyun (); Yuan, Liqiong ()",,"Zhang, Chengrui (); Huang, Guangyao (); Yao, Kai (); Leach, Mark (); Sun, Jie (); Huang, Kaizhu (); Zhou, Xiaoyun (); Yuan, Liqiong ()",0,0,,,https://doi.org/10.18178/joig.10.4.166-171,https://app.dimensions.ai/details/publication/pub.1153217255,,,,,,,,,,,,
740,pub.1151146291,10.48550/arxiv.2209.08256,,,Can segmentation models be trained with fully synthetically generated  data?,"In order to achieve good performance and generalisability, medical image
segmentation models should be trained on sizeable datasets with sufficient
variability. Due to ethics and governance restrictions, and the costs
associated with labelling data, scientific development is often stifled, with
models trained and tested on limited data. Data augmentation is often used to
artificially increase the variability in the data distribution and improve
model generalisability. Recent works have explored deep generative models for
image synthesis, as such an approach would enable the generation of an
effectively infinite amount of varied data, addressing the generalisability and
data access problems. However, many proposed solutions limit the user's control
over what is generated. In this work, we propose brainSPADE, a model which
combines a synthetic diffusion-based label generator with a semantic image
generator. Our model can produce fully synthetic brain labels on-demand, with
or without pathology of interest, and then generate a corresponding MRI image
of an arbitrary guided style. Experiments show that brainSPADE synthetic data
can be used to train segmentation models with performance comparable to that of
models trained on real data.",,,arXiv,,,2022-09-17,2022,,,,,,All OA, Green,Preprint,"Fernandez, Virginia; Pinaya, Walter Hugo Lopez; Borges, Pedro; Tudosiu, Petru-Daniel; Graham, Mark S; Vercauteren, Tom; Cardoso, M Jorge","Fernandez, Virginia (King's College London); Pinaya, Walter Hugo Lopez (King's College London); Borges, Pedro (King's College London); Tudosiu, Petru-Daniel (King's College London); Graham, Mark S (King's College London); Vercauteren, Tom (King's College London); Cardoso, M Jorge ()",,"Fernandez, Virginia (King's College London); Pinaya, Walter Hugo Lopez (King's College London); Borges, Pedro (King's College London); Tudosiu, Petru-Daniel (King's College London); Graham, Mark S (King's College London); Vercauteren, Tom (King's College London); Cardoso, M Jorge ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151146291,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
740,pub.1154035211,10.1109/isia55826.2022.9993505,,,A Deep Learning-based 3D CNN for Automated COVID-19 Lung Lesions Segmentation from 3D Chest CT Scans,"This paper presents an automated COVID-19 lung lesions segmentation method based on a deep three-dimensional convolutional neural network model which automatically detects and extracts multifocal, bilateral and peripheral lung lesions from chest 3D-CT scans. The proposed CNN model is based on a modified 11-layer U-net architecture and employs a loss function that combines Dice coefficient and Cross-Entropy. It has been tested and evaluated on Covid-19-20_v2 training dataset containing a total of 199 3D-CT scans of different subjects with COVID-19 lesions representing different sizes, shapes and locations in CT images. The obtained results have proven to be satisfactory and objective, as well as similar and close to ground truth data provided by medical experts. On these challenging CT data, the proposed CNN obtained average scores of 0.7639, 0.8129 and 0.9986 corresponding to Dice Similarity Coefficient, Sensitivity and Specificity metrics respectively.","This study is part of two Algerian research projects, PRFU project (No. C00L07ES160520210002) and ATRSSV project directed by Dr. A. Kermi and Prof. M.T. Khadir respectively, which are financed by the Algerian Ministry of Higher Education and Scientific Research. We would like to thank the COVID-19-20 challenge organizing members for providing us the entire training dataset.","This study is part of two Algerian research projects, PRFU project (No. C00L07ES160520210002) and ATRSSV project directed by Dr. A. Kermi and Prof. M.T. Khadir respectively, which are financed by the Algerian Ministry of Higher Education and Scientific Research. We would like to thank the COVID-19-20 challenge organizing members for providing us the entire training dataset.",,2022 5th International Symposium on Informatics and its Applications (ISIA),,2022-11-30,2022,,2022-11-30,0,,1-5,Closed,Proceeding,"Kermi, Adel; Djennelbaroud, Hadj Cheikh; Khadir, Mohamed Tarek","Kermi, Adel (LMCS Laboratory, Ecole nationale SupÃ©rieure d'Informatique (ESI-Algiers), Algiers, Algeria); Djennelbaroud, Hadj Cheikh (LMCS Laboratory, Ecole nationale SupÃ©rieure d'Informatique (ESI-Algiers), Algiers, Algeria); Khadir, Mohamed Tarek (LabGed Laboratory, University Badji-Mokhtar of Annaba, Dept. of Computer Sciences, Annaba, Algeria)",,"Kermi, Adel (Ãcole Nationale SupÃ©rieure d'Informatique); Djennelbaroud, Hadj Cheikh (Ãcole Nationale SupÃ©rieure d'Informatique); Khadir, Mohamed Tarek (Badji Mokhtar University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154035211,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
740,pub.1152620540,10.1109/etcm56276.2022.9935708,,,Distance-based Loss Weightings for Improving Retinal Tissue Segmentation using Fully Convolutional Neural Networks,"This study proposes a novel loss weighting approach to circumvent the negative effect of class imbalance in optical coherence tomography (OCT) segmentation using fully convolutional neural networks. The proposed weighting builds on the Eu-clidean distance transform to assign pixel-wise weights adaptively to balance the contribution of the minority class to the loss value. Unlike weighting approaches used in state-of-the-art methods for OCT segmentation, the proposed method is task agnostic, data-driven, and parameter-free. The proposed weighting was evaluated on a benchmark OCT dataset consisting of OCT B-scans showing severe macular pathology and contrasted against weighting approaches commonly used in image segmentation. Results of the evaluation show the effectiveness of the proposed method, which improved the segmentation performance of an FCN trained with the cross-entropy loss and outperformed comparative weightings.",,"This work was funded by the SecretarÃ­a de EducaciÃ³n Superior, Cien-cia, TecnologÃ­a e InnovaciÃ³n (Senescyt), Ecuador and in part by the FundaÃ§Ã£o para a CiÃªncia e a Tecnologia (FCT/MCTES), Portugal, through national funds and when applicable co-funded by EU funds under Projects UIDB/EEA/50008/2020, UIDP/50008/2020 and LA/P/0109/2020.",,2022 IEEE Sixth Ecuador Technical Chapters Meeting (ETCM),,2022-10-14,2022,,2022-10-14,0,,1-6,Closed,Proceeding,"CazaÃ±as-GordÃ³n, Alex; Parra-Mora, Esther; da Silva Cruz, LuÃ­s A.","CazaÃ±as-GordÃ³n, Alex (Instituto de TelecomunicaÃ§Ãµes, University of Coimbra, Department of Electrical and Computer Engineering, Coimbra, Portugal); Parra-Mora, Esther (Instituto de TelecomunicaÃ§Ãµes, University of Coimbra, Department of Electrical and Computer Engineering, Coimbra, Portugal); da Silva Cruz, LuÃ­s A. (Instituto de TelecomunicaÃ§Ãµes, University of Coimbra, Department of Electrical and Computer Engineering, Coimbra, Portugal)",,"CazaÃ±as-GordÃ³n, Alex (University of Coimbra); Parra-Mora, Esther (University of Coimbra); da Silva Cruz, LuÃ­s A. (University of Coimbra)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152620540,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
740,pub.1151032985,10.1007/978-3-031-16443-9_15,,,MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation,"Convolutional neural networks (CNNs) have achieved remarkable segmentation accuracy on benchmark datasets where training and test sets are from the same domain, yet their performance can degrade significantly on unseen domains, which hinders the deployment of CNNs in many clinical scenarios. Most existing works improve model out-of-domain (OOD) robustness by collecting multi-domain datasets for training, which is expensive and may not always be feasible due to privacy and logistical issues. In this work, we focus on improving model robustness using a single-domain dataset only. We propose a novel data augmentation framework called MaxStyle, which maximizes the effectiveness of style augmentation for model OOD performance. It attaches an auxiliary style-augmented image decoder to a segmentation network for robust feature learning and data augmentation. Importantly, MaxStyle augments data with improved image style diversity and hardness, by expanding the style space with noise and searching for the worst-case style composition of latent features via adversarial training. With extensive experiments on multiple public cardiac and prostate MR datasets, we demonstrate that MaxStyle leads to significantly improved out-of-distribution robustness against unseen corruptions as well as common distribution shifts across multiple, different, unseen sites and unknown image sequences under both low- and high-training data settings. The code can be found at https://github.com/cherise215/MaxStyle.","This work was supported by two EPSRC Programme Grants (EP/P001009/1, EP/W01842X/1) and the UKRI Innovate UK Grant (No.104691).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13435,,151-161,All OA, Green,Chapter,"Chen, Chen; Li, Zeju; Ouyang, Cheng; Sinclair, Matthew; Bai, Wenjia; Rueckert, Daniel","Chen, Chen (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Li, Zeju (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Ouyang, Cheng (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Sinclair, Matthew (BioMedIA Group, Department of Computing, Imperial College London, London, UK; HeartFlow, Mountain View, USA); Bai, Wenjia (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Data Science Institute, Imperial College London, London, UK; Department of Brain Sciences, Imperial College London, London, UK); Rueckert, Daniel (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Klinikum rechts der Isar, Technical University of Munich, Munich, Germany)","Chen, Chen (Imperial College London)","Chen, Chen (Imperial College London); Li, Zeju (Imperial College London); Ouyang, Cheng (Imperial College London); Sinclair, Matthew (Imperial College London; HeartFlow (United States)); Bai, Wenjia (Imperial College London; Imperial College London; Imperial College London); Rueckert, Daniel (Imperial College London; Rechts der Isar Hospital; Technical University of Munich)",0,0,,,http://arxiv.org/pdf/2206.01737,https://app.dimensions.ai/details/publication/pub.1151032985,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
740,pub.1145638845,10.1109/tencon54134.2021.9707278,,,Deep learning model-based segmentation of medical diseases from MRI and CT images,"Medical image segmentation is quite challenging field. Deep Learning (DL) based Unet model is used for medical image segmentation. The Unet architecture is based on encoder and decoder which is the most successful method. Unet based methods still have a drawback that is not able to fully utilize the output features of the node's convolutional units. This paper presented deep learning model-base segmentation of medical diseases from MRI and CT scan images data with the help of 2D-Unet and 3D-Unet model. The model using package nibabel for reading, visualizing (itk, itkweidgets, ipywidgts), and 3D-Unet method for classification and segmentation of MRI and CT scan images. The proposed system has been tested on Medical Segmentation Decathion (MSD) datasets for the data of Brain, Spleen and Heart. The performance metrices has been analysis by F1-score (F1), Intersection over Union (IOU), and Dice Factor coefficient (DFC) on Brain, Spleen and Heart datasets. The proposed model is outperformed as comparison with some recent network model.",,,,TENCON 2021 - 2021 IEEE Region 10 Conference (TENCON),,2021-12-10,2021,,2021-12-10,0,,608-613,Closed,Proceeding,"Murmu, Anita; Kumar, Piyush","Murmu, Anita (Department of Computer Science and Engineering, National Institute of Technology, Patna, 800005, India); Kumar, Piyush (Department of Computer Science and Engineering, National Institute of Technology, Patna, 800005, India)",,"Murmu, Anita (National Institute of Technology Patna); Kumar, Piyush (National Institute of Technology Patna)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1145638845,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,,
740,pub.1141301953,10.1007/978-3-030-87193-2_2,,,TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation,"Medical image segmentation - the prerequisite of numerous clinical needs - has been significantly prospered by recent advances in convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range relation, and existing cures, resorting to building deep encoders along with aggressive downsampling operations, leads to redundant deepened networks and loss of localized details. Hence, the segmentation task awaits a better solution to improve the efficiency of modeling global contexts while maintaining a strong grasp of low-level details. In this paper, we propose a novel parallel-in-branch architecture, TransFuse, to address this challenge. TransFuse combines Transformers and CNNs in a parallel style, where both global dependency and low-level spatial details can be efficiently captured in a much shallower manner. Besides, a novel fusion technique - BiFusion module is created to efficiently fuse the multi-level features from both branches. Extensive experiments demonstrate that TransFuse achieves the newest state-of-the-art results on both 2D and 3D medical image sets including polyp, skin lesion, hip, and prostate segmentation, with significant parameter decrease and inference speed improvement.","We gratefully thank Weijun Wang, MD, Zhefeng Chen, MD, Chuan He, MD, Zhengyu Xu, Huaikun Xu for serving as our medical advisors on hip segmentation project.",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention â MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12901,,14-24,All OA, Green,Chapter,"Zhang, Yundong; Liu, Huiye; Hu, Qiang","Zhang, Yundong (Rayicer, Suzhou, China); Liu, Huiye (Rayicer, Suzhou, China; Georgia Institute of Technology, Atlanta, GA, USA); Hu, Qiang (Rayicer, Suzhou, China)","Liu, Huiye (; Georgia Institute of Technology)","Zhang, Yundong (); Liu, Huiye (Georgia Institute of Technology); Hu, Qiang ()",203,203,,175.83,http://arxiv.org/pdf/2102.08005,https://app.dimensions.ai/details/publication/pub.1141301953,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4611 Machine Learning",,,,,,,,,,,
740,pub.1131140757,10.48550/arxiv.2009.11160,,,Label-Efficient Multi-Task Segmentation using Contrastive Learning,"Obtaining annotations for 3D medical images is expensive and time-consuming,
despite its importance for automating segmentation tasks. Although multi-task
learning is considered an effective method for training segmentation models
using small amounts of annotated data, a systematic understanding of various
subtasks is still lacking. In this study, we propose a multi-task segmentation
model with a contrastive learning based subtask and compare its performance
with other multi-task models, varying the number of labeled data for training.
We further extend our model so that it can utilize unlabeled data through the
regularization branch in a semi-supervised manner. We experimentally show that
our proposed method outperforms other multi-task methods including the
state-of-the-art fully supervised model when the amount of annotated data is
limited.",,,arXiv,,,2020-09-23,2020,,,,,,All OA, Green,Preprint,"Iwasawa, Junichiro; Hirano, Yuichiro; Sugawara, Yohei","Iwasawa, Junichiro (); Hirano, Yuichiro (); Sugawara, Yohei ()",,"Iwasawa, Junichiro (); Hirano, Yuichiro (); Sugawara, Yohei ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1131140757,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
738,pub.1154711692,10.1007/978-3-031-23911-3_4,,,"Semi-supervised Detection, Identification and Segmentation for Abdominal Organs","Abdominal organ segmentation is an important prerequisite in many medical image analysis applications. Methods based on U-Net have demonstrated their scalability and achieved great success in different organ segmentation tasks. However, the limited number of data and labels hinders the training process of these methods. Moreover, traditional U-Net models based on convolutional neural networks suffer from limited receptive fields. Lacking the ability to model long-term dependencies from a global perspective, these methods are prone to produce false positive predictions. In this paper, we propose a new semi-supervised learning algorithm based on the vision transformer to overcome these challenges. The overall architecture of our method consists of three stages. In the first stage, we tackle the abdomen region location problem via a lightweight segmentation network. In the second stage, we adopt a vision transformer model equipped with a semi-supervised learning strategy to detect different abdominal organs. In the final stage, we attach multiple organ-specific segmentation networks to automatically segment organs from their bounding boxes. We evaluate our method on MICCAI FLARE 2022 challenge dataset. Experimental results demonstrate the effectiveness of our method. Our segmentation results currently achieve 0.897 mean DSC on the leaderboard of FLARE 2022 validation set.",The authors of this paper declare that the segmentation method they implemented for participation in the FLARE 2022 challenge has not used any pre-trained models nor additional datasets other than those provided by the organizers. The proposed solution is fully automatic without any manual intervention.,,Lecture Notes in Computer Science,Fast and Low-Resource Semi-supervised Abdominal Organ Segmentation,,2022,2022,2023-01-21,2022,13816,,35-46,Closed,Chapter,"Sun, Mingze; Jiang, Yankai; Guo, Heng","Sun, Mingze (Alibaba DAMO Academy, Beijing, China); Jiang, Yankai (Alibaba DAMO Academy, Beijing, China); Guo, Heng (Alibaba DAMO Academy, Beijing, China)","Guo, Heng (Alibaba DAMO Academy)","Sun, Mingze (Alibaba DAMO Academy); Jiang, Yankai (Alibaba DAMO Academy); Guo, Heng (Alibaba DAMO Academy)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154711692,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
738,pub.1154062625,10.1109/bigmm55396.2022.00009,,,Improving Interactive Segmentation using a Novel Weighted Loss Function with an Adaptive Click Size and Two-Stream Fusion,"Interactive segmentation has recently attracted at-tention for specialized segmentation tasks where expert input is required to further enhance the segmentation performance. In this work, we propose a novel interactive segmentation framework, where user clicks are dynamically adapted in size based on the current segmentation mask. The clicked regions form a weight map and are fed to a deep neural network together with the image, where the network learns to discriminate important regions through a novel weighted loss function. To evaluate our loss function, a state-of-the-art interactive V-Net (IV-Net) model which utilizes both foreground and background user clicks as the main method of interaction is employed. To further improve on the IV-Net, we propose the use of a two-stream fusion interactive If-Net (TSFIV-Net) which applies multimodal fusion to allow for the propagation of image feature information throughout the architecture. We train and validate both the models on the BCV dataset, while testing on both seen and unseen structures from the MSD dataset to determine the models generalization and segmentation abilities in comparison to the standard IV-Net. Applying adaptive user click sizes increases the overall dice score by 4.86 % and 8.59 % for seen and unseen structures respectively by utilizing only a single user interaction on the IV-Net compared to the original version, and 9.88% and 10.35% on the TSFIV-Net.",,This work was supported by a grant from the Natural Sciences and Engineering Research Council of Canada,,2022 IEEE Eighth International Conference on Multimedia Big Data (BigMM),,2022-12-07,2022,,2022-12-07,0,,7-12,Closed,Proceeding,"Pirabaharan, Ragavie; Khan, Naimul","Pirabaharan, Ragavie (Electrical, Computer and Biomedical Engineering, Toronto Metropolitan University); Khan, Naimul (Electrical, Computer and Biomedical Engineering, Toronto Metropolitan University)",,"Pirabaharan, Ragavie (); Khan, Naimul ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154062625,"46 Information and Computing Sciences; 4605 Data Management and Data Science; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,,
738,pub.1146252302,10.48550/arxiv.2203.05573,,,Self Pre-training with Masked Autoencoders for Medical Image Analysis,"Masked Autoencoder (MAE) has recently been shown to be effective in
pre-training Vision Transformers (ViT) for natural image analysis. By
performing the pretext task of reconstructing the original image from only
partial observations, the encoder, which is a ViT, is encouraged to aggregate
contextual information to infer content in masked image regions. We believe
that this context aggregation ability is also essential to the medical image
domain where each anatomical structure is functionally and mechanically
connected to other structures and regions. However, there is no ImageNet-scale
medical image dataset for pre-training. Thus, in this paper, we investigate a
self pre-training paradigm with MAE for medical images, i.e., models are
pre-trained on the same target dataset. To validate the MAE self pre-training,
we consider three diverse medical image tasks including chest X-ray disease
classification, CT abdomen multi-organ segmentation and MRI brain tumor
segmentation. It turns out MAE self pre-training benefits all the tasks
markedly. Specifically, the mAUC on lung disease classification is increased by
9.4%. The average DSC on brain tumor segmentation is improved from 77.4% to
78.9%. Most interestingly, on the small-scale multi-organ segmentation dataset
(N=30), the average DSC improves from 78.8% to 83.5% and the HD95 is reduced by
60%, indicating its effectiveness in limited data scenarios. The segmentation
and classification results reveal the promising potential of MAE self
pre-training for medical image analysis.",,,arXiv,,,2022-03-10,2022,,,,,,All OA, Green,Preprint,"Zhou, Lei; Liu, Huidong; Bae, Joseph; He, Junjun; Samaras, Dimitris; Prasanna, Prateek","Zhou, Lei (); Liu, Huidong (); Bae, Joseph (); He, Junjun (); Samaras, Dimitris (); Prasanna, Prateek ()",,"Zhou, Lei (); Liu, Huidong (); Bae, Joseph (); He, Junjun (); Samaras, Dimitris (); Prasanna, Prateek ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1146252302,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
738,pub.1142014307,10.48550/arxiv.2110.08322,,,Robustness of different loss functions and their impact on networks  learning capability,"Recent developments in AI have made it ubiquitous, every industry is trying
to adopt some form of intelligent processing of their data. Despite so many
advances in the field, AIs full capability is yet to be exploited by the
industry. Industries that involve some risk factors still remain cautious about
the usage of AI due to the lack of trust in such autonomous systems.
Present-day AI might be very good in a lot of things but it is very bad in
reasoning and this behavior of AI can lead to catastrophic results. Autonomous
cars crashing into a person or a drone getting stuck in a tree are a few
examples where AI decisions lead to catastrophic results. To develop insight
and generate an explanation about the learning capability of AI, we will try to
analyze the working of loss functions. For our case, we will use two sets of
loss functions, generalized loss functions like Binary cross-entropy or BCE and
specialized loss functions like Dice loss or focal loss. Through a series of
experiments, we will establish whether combining different loss functions is
better than using a single loss function and if yes, then what is the reason
behind it. In order to establish the difference between generalized loss and
specialized losses, we will train several models using the above-mentioned
losses and then compare their robustness on adversarial examples. In
particular, we will look at how fast the accuracy of different models decreases
when we change the pixels corresponding to the most salient gradients.",,,arXiv,,,2021-10-15,2021,,,,,,All OA, Green,Preprint,"Rajput, Vishal","Rajput, Vishal ()",,"Rajput, Vishal ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142014307,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
738,pub.1137834685,10.1007/978-3-030-75768-7_31,,,Addressing the Class Imbalance Problem in Medical Image Segmentation via Accelerated Tversky Loss Function,"Image segmentation in the medical domain has gained a lot of research interest in recent years with the advancements in deep learning algorithms and related technologies. Medical image datasets are often imbalanced and to handle the imbalance problem, deep learning models are equipped with modified loss functions to effectively penalize the training weights for false predictions and conduct unbiased learning. Recent works have introduced various loss functions suitable for certain scenarios of segmentation. In this paper, we have explored the existing loss functions that are widely used for medical image segmentation, following which an accelerated Tversky loss (ATL) function is proposed that uses log cosh function to better optimize the gradients. The no-new U-Net (nn-Unet) model is adopted as the base model to validate the behaviour of the loss functions by using the standard benchmark segmentation performance metrics. To establish the robustness and effectiveness of the loss functions, multiple datasets are adopted, where ATL function illustrated better performance with faster convergence and better mask generation.",,,Lecture Notes in Computer Science,Advances in Knowledge Discovery and Data Mining,,2021-05-08,2021,2021-05-08,2021,12714,,390-402,Closed,Chapter,"Nasalwai, Nikhil; Punn, Narinder Singh; Sonbhadra, Sanjay Kumar; Agarwal, Sonali","Nasalwai, Nikhil (Indian Institute of Information Technology Allahabad, Prayagraj, India); Punn, Narinder Singh (Indian Institute of Information Technology Allahabad, Prayagraj, India); Sonbhadra, Sanjay Kumar (Indian Institute of Information Technology Allahabad, Prayagraj, India); Agarwal, Sonali (Indian Institute of Information Technology Allahabad, Prayagraj, India)","Nasalwai, Nikhil (Indian Institute of Information Technology, Allahabad)","Nasalwai, Nikhil (Indian Institute of Information Technology, Allahabad); Punn, Narinder Singh (Indian Institute of Information Technology, Allahabad); Sonbhadra, Sanjay Kumar (Indian Institute of Information Technology, Allahabad); Agarwal, Sonali (Indian Institute of Information Technology, Allahabad)",4,4,,3.27,,https://app.dimensions.ai/details/publication/pub.1137834685,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
738,pub.1131417980,10.1007/978-3-030-61166-8_26,,,Predicting Scores of Medical Imaging Segmentation Methods with Meta-learning,"Deep learning has led to state-of-the-art results for many medical imaging tasks, such as segmentation of different anatomical structures. With the increased numbers of deep learning publications and openly available code, the approach to choosing a model for a new task becomes more complicated, while time and (computational) resources are limited. A possible solution to choosing a model efficiently is meta-learning, a learning method in which prior performance of a model is used to predict the performance for new tasks. We investigate meta-learning for segmentation across ten datasets of different organs and modalities. We propose four ways to represent each dataset by meta-features: one based on statistical features of the images and three are based on deep learning features. We use support vector regression and deep neural networks to learn the relationship between the meta-features and prior model performance. On three external test datasets these methods give Dice scores within 0.10 of the true performance. These results demonstrate the potential of meta-learning in medical imaging.",,,Lecture Notes in Computer Science,Interpretable and Annotation-Efficient Learning for Medical Image Computing,,2020-10-02,2020,2020-10-02,2020,12446,,242-253,All OA, Green,Chapter,"van Sonsbeek, Tom; Cheplygina, Veronika","van Sonsbeek, Tom (Eindhoven University of Technology, Eindhoven, The Netherlands); Cheplygina, Veronika (Eindhoven University of Technology, Eindhoven, The Netherlands)","van Sonsbeek, Tom (Eindhoven University of Technology)","van Sonsbeek, Tom (Eindhoven University of Technology); Cheplygina, Veronika (Eindhoven University of Technology)",2,2,,1.03,http://arxiv.org/pdf/2005.08869,https://app.dimensions.ai/details/publication/pub.1131417980,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
737,pub.1151333024,10.1101/2022.09.24.22280071,,,The FELIX Project: Deep Networks To Detect Pancreatic Neoplasms,"Tens of millions of abdominal images are obtained with computed tomography (CT) in the U.S. each year but pancreatic cancers are sometimes not initially detected in these images. We here describe a suite of algorithms (named FELIX) that can recognize pancreatic lesions from CT images without human input. Using FELIX, >95% of patients with pancreatic ductal adenocarcinomas were detected at a specificity of >95% in patients without pancreatic disease. FELIX may be able to assist radiologists in identifying pancreatic cancers earlier, when surgery and other treatments offer more hope for long-term survival.","This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research, the Virginia and D.K. Ludwig Fund for Cancer Research, the Sol Goldman Charitable Trust, and NIH Grant #CA06973.",This work was funded by the Lustgarten Foundation for Pancreatic Cancer Research.,medRxiv,,,2022-09-25,2022,2022-09-25,,,,2022.09.24.22280071,All OA, Green,Preprint,"Xia, Yingda; Yu, Qihang; Chu, Linda; Kawamoto, Satomi; Park, Seyoun; Liu, Fengze; Chen, Jieneng; Zhu, Zhuotun; Li, Bowen; Zhou, Zongwei; Lu, Yongyi; Wang, Yan; Shen, Wei; Xie, Lingxi; Zhou, Yuyin; Wolfgang, Christopher; Javed, Ammar; Fouladi, Daniel Fadaei; Shayesteh, Shahab; Graves, Jefferson; Blanco, Alejandra; Zinreich, Eva S.; Klauss, Miriam; Mayer, Philipp; Kinny-Koster, Benedict; Kinzler, Kenneth; Hruban, Ralph H.; Vogelstein, Bert; Yuille, Alan L.; Fishman, Elliot K.","Xia, Yingda (); Yu, Qihang (); Chu, Linda (); Kawamoto, Satomi (); Park, Seyoun (); Liu, Fengze (); Chen, Jieneng (); Zhu, Zhuotun (); Li, Bowen (); Zhou, Zongwei (); Lu, Yongyi (); Wang, Yan (); Shen, Wei (); Xie, Lingxi (); Zhou, Yuyin (); Wolfgang, Christopher (); Javed, Ammar (); Fouladi, Daniel Fadaei (); Shayesteh, Shahab (); Graves, Jefferson (); Blanco, Alejandra (); Zinreich, Eva S. (); Klauss, Miriam (); Mayer, Philipp (); Kinny-Koster, Benedict (); Kinzler, Kenneth (); Hruban, Ralph H. (); Vogelstein, Bert (); Yuille, Alan L. (); Fishman, Elliot K. ()","Yuille, Alan L. ; Fishman, Elliot K. ","Xia, Yingda (); Yu, Qihang (); Chu, Linda (); Kawamoto, Satomi (); Park, Seyoun (); Liu, Fengze (); Chen, Jieneng (); Zhu, Zhuotun (); Li, Bowen (); Zhou, Zongwei (); Lu, Yongyi (); Wang, Yan (); Shen, Wei (); Xie, Lingxi (); Zhou, Yuyin (); Wolfgang, Christopher (); Javed, Ammar (); Fouladi, Daniel Fadaei (); Shayesteh, Shahab (); Graves, Jefferson (); Blanco, Alejandra (); Zinreich, Eva S. (); Klauss, Miriam (); Mayer, Philipp (); Kinny-Koster, Benedict (); Kinzler, Kenneth (); Hruban, Ralph H. (); Vogelstein, Bert (); Yuille, Alan L. (); Fishman, Elliot K. ()",0,0,,,https://www.medrxiv.org/content/medrxiv/early/2022/09/25/2022.09.24.22280071.full.pdf,https://app.dimensions.ai/details/publication/pub.1151333024,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
735,pub.1155388493,10.1016/j.patcog.2023.109432,,,3D Medical image segmentation using parallel transformers,"Most recent 3D medical image segmentation methods adopt convolutional neural networks (CNNs) that rely on deep feature representation and achieve adequate performance. However, due to the convolutional architectures having limited receptive fields, they cannot explicitly model the long-range dependencies in the medical image. Recently, Transformer can benefit from global dependencies using self-attention mechanisms and learn highly expressive representations. Some works were designed based on the Transformers, but the existing Transformers suffer from extreme computational and memories, and they cannot take full advantage of the powerful feature representations in 3D medical image segmentation. In this paper, we aim to connect the different resolution streams in parallel and propose a novel network, named Transformer based High Resolution Network (TransHRNet), with an Effective Transformer (EffTrans) block, which has sufficient feature representation even at high feature resolutions. Given a 3D image, the encoder first utilizes CNN to extract the feature representations to capture the local information, and then the different feature maps are reshaped elaborately for tokens that are fed into each Transformer stream in parallel to learn the global information and repeatedly exchange the information across streams. Unfortunately, the proposed framework based on the standard Transformer needs a huge amount of computation, thus we introduce a deep and effective Transformer to deliver better performance with fewer parameters. The proposed TransHRNet is evaluated on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV) dataset that consists of 11 major human organs and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Experimental results show that it performs better than the convolutional and other related Transformer-based methods on the 3D multi-organ segmentation tasks. Code is available at https://github.com/duweidai/TransHRNet.","This work is supported by National Science Foundation of China under Grant No. U19B2037, 61901384, 12026609, 61876150, the Fundamental Research Funds for the Central Universities No. D5000220444, Natural Science Basic Research Program of Shaanxi No. 2023-JC-QN-0685, the Ministry of Science and Technology of China No. 2020AAA0106302 and National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology.",,Pattern Recognition,,,2023-06,2023,,2023-06,138,,109432,Closed,Article,"Yan, Qingsen; Liu, Shengqiang; Xu, Songhua; Dong, Caixia; Li, Zongfang; Shi, Javen Qinfeng; Zhang, Yanning; Dai, Duwei","Yan, Qingsen (School of Computer Science, Northwestern Polytechnical University, 710072, China); Liu, Shengqiang (School of Computer Science, Northwestern Polytechnical University, 710072, China); Xu, Songhua (Institute of Medical Artificial Intelligence, The Second Affiliated Hospital of Xiâan Jiaotong University, Xiâan 710004, China); Dong, Caixia (Institute of Medical Artificial Intelligence, The Second Affiliated Hospital of Xiâan Jiaotong University, Xiâan 710004, China); Li, Zongfang (Institute of Medical Artificial Intelligence, The Second Affiliated Hospital of Xiâan Jiaotong University, Xiâan 710004, China); Shi, Javen Qinfeng (Australian Institute for Machine Learning, University of Adelaide, Adelaide 5005, Australia); Zhang, Yanning (School of Computer Science, Northwestern Polytechnical University, 710072, China); Dai, Duwei (Institute of Medical Artificial Intelligence, The Second Affiliated Hospital of Xiâan Jiaotong University, Xiâan 710004, China)","Dai, Duwei (Second Affiliated Hospital of Xi'an Jiaotong University)","Yan, Qingsen (Northwestern Polytechnical University); Liu, Shengqiang (Northwestern Polytechnical University); Xu, Songhua (Second Affiliated Hospital of Xi'an Jiaotong University); Dong, Caixia (Second Affiliated Hospital of Xi'an Jiaotong University); Li, Zongfang (Second Affiliated Hospital of Xi'an Jiaotong University); Shi, Javen Qinfeng (University of Adelaide); Zhang, Yanning (Northwestern Polytechnical University); Dai, Duwei (Second Affiliated Hospital of Xi'an Jiaotong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155388493,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
735,pub.1153585456,10.21203/rs.3.rs-2327533/v1,,,Towards fully automated Inner Ear Analysis: Deep-Learning-based Joint Segmentation and Landmark Detection Framework,"Automated analysis of the inner ear anatomy in radiological data instead of time-consuming manual assessment is a worthwhile goal that could facilitate preoperative planning and clinical research. We propose a framework encompassing joint semantic segmentation and anatomical landmark detection. A fully automated pipeline with a single, dual-headed volumetric 3D U-Net was implemented, trained and evaluated using manually labeled in-house datasets from cadaveric specimen (N = 44) and clinical practice (N = 10). The model robustness was further evaluated on two independent open-source datasets (N = 23 + 7 scans). For the in-house datasets, Dice scores of 0.965 and 0.9505, intersection-over-union scores of 0.934 and 0.906 and average Hausdorff distances of 0.085 and 0.096 voxel units were achieved. The landmark localization task was performed automatically with an average localization error of 5.36 and 5.77 voxel units. A robust, but reduced performance could be attained for the two open-source datasets. The feasibility of the suggested integrated segmentation and landmark detection framework was experimentally evident by achieving competitive, efficient and consistent performance across a multitude of realistic, clinically relevant datasets and evaluation metrics. Based on these results, the applications in clinical practice and research beyond the data selection utilized in this study can be accomplishable.",,,Research Square,,,2022-12-12,2022,2022-12-12,,,,,All OA, Green,Preprint,"Stebani, Jannik; Blaimer, Martin; Zabler, Simon; Neun, Tilmann; Pelt, DanÃ¯el M.; Rak, Kristen","Stebani, Jannik (Fraunhofer IIS, Magnetic Resonance and X-Ray Imaging Department); Blaimer, Martin (Fraunhofer IIS, Magnetic Resonance and X-Ray Imaging Department); Zabler, Simon (Fraunhofer IIS, Magnetic Resonance and X-Ray Imaging Department); Neun, Tilmann (UniversitÃ¤tsklinikum WÃ¼rzburg, Institute for Diagnostic and Interventional Neuroradiology); Pelt, DanÃ¯el M. (Universiteit Leiden, Leiden Institute of Advanced Computer Science (LIACS)); Rak, Kristen (UniversitÃ¤tsklinikum WÃ¼rzburg, Department of Oto-Rhino-Laryngology, Plastic, Aesthetic and Reconstructive Head and Neck Surgery and the Comprehensive Hearing Center)",,"Stebani, Jannik (); Blaimer, Martin (); Zabler, Simon (); Neun, Tilmann (); Pelt, DanÃ¯el M. (Leiden University); Rak, Kristen ()",0,0,,,https://www.researchsquare.com/article/rs-2327533/latest.pdf,https://app.dimensions.ai/details/publication/pub.1153585456,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
713,pub.1149652590,10.1007/978-3-031-08999-2_22,,,Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images,"Semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple MRI imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs) approaches have become the de facto standard for 3D medical image segmentation. The popular âU-shapedâ network architecture has achieved state-of-the-art performance benchmarks on different 2D and 3D semantic segmentation tasks and across various imaging modalities. However, due to the limited kernel size of convolution layers in FCNNs, their performance of modeling long-range information is sub-optimal, and this can lead to deficiencies in the segmentation of tumors with variable sizes. On the other hand, transformer models have demonstrated excellent capabilities in capturing such long-range information in multiple domains, including natural language processing and computer vision. Inspired by the success of vision transformers and their variants, we propose a novel segmentation model termed Swin UNEt TRansformers (Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder. The swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self-attention and is connected to an FCNN-based decoder at each resolution via skip connections. We have participated in BraTS 2021 segmentation challenge, and our proposed model ranks among the top-performing approaches in the validation phase.Code: https://monai.io/research/swin-unetr.",,,Lecture Notes in Computer Science,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",,2022-07-22,2022,2022-07-22,2022,12962,,272-284,All OA, Green,Chapter,"Hatamizadeh, Ali; Nath, Vishwesh; Tang, Yucheng; Yang, Dong; Roth, Holger R.; Xu, Daguang","Hatamizadeh, Ali (NVIDIA, Santa Clara, USA); Nath, Vishwesh (NVIDIA, Santa Clara, USA); Tang, Yucheng (Vanderbilt University, Nashville, USA); Yang, Dong (NVIDIA, Santa Clara, USA); Roth, Holger R. (NVIDIA, Santa Clara, USA); Xu, Daguang (NVIDIA, Santa Clara, USA)","Hatamizadeh, Ali (Nvidia (United States))","Hatamizadeh, Ali (Nvidia (United States)); Nath, Vishwesh (Nvidia (United States)); Tang, Yucheng (Vanderbilt University); Yang, Dong (Nvidia (United States)); Roth, Holger R. (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",55,55,,,http://arxiv.org/pdf/2201.01266,https://app.dimensions.ai/details/publication/pub.1149652590,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
713,pub.1143637455,10.1007/978-3-030-92185-9_7,,,Adaptive Curriculum Learning for Semi-supervised Segmentation of 3D CT-Scans,"Semi-supervised learning algorithms make use of both labelled training data and unlabelled data. However, the visual domain gap between these sets poses a challenge which prevents deep learning models from obtaining the results they have achieved most especially in the field of medical imaging. Recently, self-training with deep learning has become a powerful approach to leverage labelled training and unlabelled data. However, a challenge of generating noisy pseudo-labels and placing over-confident labelling belief on incorrect classes leads to deviation from the solution. To solve this challenge, the study investigates a curriculum-styled approach for deep semi-supervised segmentation which relaxes and treats pseudo-labels as continuous hidden variables by developing an adaptive pseudo-label generation strategy to jointly optimized the pseudo-label generation and selection process. A regularization scheme is further proposed to smoothen the probability outputs and sharpen the less represented pseudo-label regions. The proposed method was evaluated on three publicly available Computer Tomography (CT) scan benchmarks and extensive experiments on all modules have demonstrated the efficacy of the proposed method.","This work was partially supported by the National Natural Science Foundation of China under Grant No. 61772006, Sub Project of Independent Scientific Research Project under Grant No. ZZKY-ZX-03-02-04, and the Special Fund for Bagui Scholars of Guangxi.",,Lecture Notes in Computer Science,Neural Information Processing,,2021-12-06,2021,2021-12-06,2021,13108,,77-90,Closed,Chapter,"Nartey, Obed Tettey; Yang, Guowu; Agyapong, DorothyÂ ArabaÂ Yakoba; Wu, JinZhao; Sarpong, Asare K.; Frempong, Lady Nadia","Nartey, Obed Tettey (Big Data Research Center, School of Computer Science and Engineering, University of Electronic Science and Technology of China, 611731, Chengdu, China; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China); Yang, Guowu (Big Data Research Center, School of Computer Science and Engineering, University of Electronic Science and Technology of China, 611731, Chengdu, China); Agyapong, DorothyÂ ArabaÂ Yakoba (Biomedical Engineering Program, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana); Wu, JinZhao (The School of Computer Science and Electronic Information, Guangxi University, 530004, Nanning, China); Sarpong, Asare K. (School of Medicine, Department of Radiology and Imaging Sciences, Emory University, 1364 Clitton Rd., 30322, Atlanta, GA, USA); Frempong, Lady Nadia (Biomedical Engineering Program, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana)","Nartey, Obed Tettey (University of Electronic Science and Technology of China; University of Electronic Science and Technology of China)","Nartey, Obed Tettey (University of Electronic Science and Technology of China; University of Electronic Science and Technology of China); Yang, Guowu (University of Electronic Science and Technology of China); Agyapong, DorothyÂ ArabaÂ Yakoba (Kwame Nkrumah University of Science and Technology); Wu, JinZhao (Guangxi University); Sarpong, Asare K. (Emory University); Frempong, Lady Nadia (Kwame Nkrumah University of Science and Technology)",1,1,,0.82,,https://app.dimensions.ai/details/publication/pub.1143637455,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
713,pub.1151155501,10.1007/978-3-031-16980-9_8,,,Can Segmentation Models Be Trained with Fully Synthetically Generated Data?,"In order to achieve good performance and generalisability, medical image segmentation models should be trained on sizeable datasets with sufficient variability. Due to ethics and governance restrictions, and the costs associated with labelling data, scientific development is often stifled, with models trained and tested on limited data. Data augmentation is often used to artificially increase the variability in the data distribution and improve model generalisability. Recent works have explored deep generative models for image synthesis, as such an approach would enable the generation of an effectively infinite amount of varied data, addressing the generalisability and data access problems. However, many proposed solutions limit the userâs control over what is generated. In this work, we propose brainSPADE, a model which combines a synthetic diffusion-based label generator with a semantic image generator. Our model can produce fully synthetic brain labels on-demand, with or without pathology of interest, and then generate a corresponding MRI image of an arbitrary guided style. Experiments show that brainSPADE synthetic data can be used to train segmentation models with performance comparable to that of models trained on real data.",,,Lecture Notes in Computer Science,Simulation and Synthesis in Medical Imaging,,2022-09-21,2022,2022-09-21,2022,13570,,79-90,All OA, Green,Chapter,"Fernandez, Virginia; Pinaya, Walter Hugo Lopez; Borges, Pedro; Tudosiu, Petru-Daniel; Graham, Mark S.; Vercauteren, Tom; Cardoso, M. Jorge","Fernandez, Virginia (Kingâs College London, WC2R 2LS, London, UK); Pinaya, Walter Hugo Lopez (Kingâs College London, WC2R 2LS, London, UK); Borges, Pedro (Kingâs College London, WC2R 2LS, London, UK); Tudosiu, Petru-Daniel (Kingâs College London, WC2R 2LS, London, UK); Graham, Mark S. (Kingâs College London, WC2R 2LS, London, UK); Vercauteren, Tom (Kingâs College London, WC2R 2LS, London, UK); Cardoso, M. Jorge (Kingâs College London, WC2R 2LS, London, UK)","Fernandez, Virginia (King's College London)","Fernandez, Virginia (King's College London); Pinaya, Walter Hugo Lopez (King's College London); Borges, Pedro (King's College London); Tudosiu, Petru-Daniel (King's College London); Graham, Mark S. (King's College London); Vercauteren, Tom (King's College London); Cardoso, M. Jorge (King's College London)",0,0,,,http://arxiv.org/pdf/2209.08256,https://app.dimensions.ai/details/publication/pub.1151155501,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
713,pub.1149727532,10.1007/978-3-031-12053-4_21,,,Fitting Segmentation Networks on Varying Image Resolutions Using Splatting,"Data used in image segmentation are not always defined on the same grid. This is particularly true for medical images, where the resolution, field-of-view and orientation can differ across channels and subjects. Images and labels are therefore commonly resampled onto the same grid, as a pre-processing step. However, the resampling operation introduces partial volume effects and blurring, thereby changing the effective resolution and reducing the contrast between structures. In this paper we propose a splat layer, which automatically handles resolution mismatches in the input data. This layer pushes each image onto a mean space where the forward pass is performed. As the splat operator is the adjoint to the resampling operator, the mean-space prediction can be pulled back to the native label space, where the loss function is computed. Thus, the need for explicit resolution adjustment using interpolation is removed. We show on two publicly available datasets, with simulated and real multi-modal magnetic resonance images, that this model improves segmentation results compared to resampling as a pre-processing step.",,,Lecture Notes in Computer Science,Medical Image Understanding and Analysis,,2022-07-25,2022,2022-07-25,2022,13413,,271-282,All OA, Green,Chapter,"Brudfors, Mikael; Balbastre, YaÃ«l; Ashburner, John; Rees, Geraint; Nachev, Parashkev; Ourselin, SÃ©bastien; Cardoso, M. Jorge","Brudfors, Mikael (School of Biomedical Engineering and Imaging Sciences, KCL, London, UK); Balbastre, YaÃ«l (Athinoula A. Martinos Center for Biomedical Imaging, MGH and HMS, Boston, USA); Ashburner, John (Wellcome Center for Human Neuroimaging, UCL, London, UK); Rees, Geraint (Institute of Cognitive Neuroscience, UCL, London, UK); Nachev, Parashkev (Institute of Neurology, UCL, London, UK); Ourselin, SÃ©bastien (School of Biomedical Engineering and Imaging Sciences, KCL, London, UK); Cardoso, M. Jorge (School of Biomedical Engineering and Imaging Sciences, KCL, London, UK)","Brudfors, Mikael (King's College London)","Brudfors, Mikael (King's College London); Balbastre, YaÃ«l (Athinoula A. Martinos Center for Biomedical Imaging); Ashburner, John (University College London); Rees, Geraint (University College London); Nachev, Parashkev (University College London); Ourselin, SÃ©bastien (King's College London); Cardoso, M. Jorge (King's College London)",0,0,,,http://arxiv.org/pdf/2206.06445,https://app.dimensions.ai/details/publication/pub.1149727532,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
712,pub.1152954396,10.1109/icosec54921.2022.9952105,,,Efficient Brain and Liver Tumor Segmentation using Seagull Optimization Algorithm based Super Pixel Fuzzy Clustering,"Now a days, medical image segmentation has been utilized in many applications with the consideration of computer aided diagnosis system. From that, brain tumour segmentation with MRI image play a main role in disease prediction. Hence, in this paper Seagull Optimization Algorithm Based Super Pixel Fuzzy Clustering (SOA-SFC)is designed for segmentation. The proposed segmentation process is designed with the combination of Super Pixel Fuzzy Clustering and Seagull Optimization Algorithm. In the Super Pixel Fuzzy Clustering, the efficient cluster center is chosen with the assistance of Seagull Optimization Algorithm. Initially, the Super Pixel Fuzzy Clustering objective function is considered with the consideration of fuzzy information extracted from the images of brain. After that, Seagull Optimization Algorithm is utilized towards optimize the cluster center in addition fuzzifier from the clustering method. The projectedtechniquecan be implemented in the MATLAB in additionpresentationiscomputed. The projectedtechniquecan becontrasted with the existing techniqueslike fuzzy c means clustering, k means clustering methods and Chimp Optimization Algorithm Based Type-2 Intuitionistic Fuzzy C-Means Clustering (COA-T2FCM). The projected method can be validated by performance metrices such as Dice similarity coefficient (DSC), Jaccard Similarity Index (JSI), accuracy, sensitivity, and specificity.",,,,2022 3rd International Conference on Smart Electronics and Communication (ICOSEC),,2022-10-22,2022,,2022-10-22,0,,930-940,Closed,Proceeding,"Devi, S. Nandhini; Manoharan, E. Gnana","Devi, S. Nandhini (Department of Electronics and Communication Engineering, Annamalai University, Chidambaram); Manoharan, E. Gnana (Department of Electronics and Communication Engineering, Annamalai University, Chidambaram)","Devi, S. Nandhini (Annamalai University)","Devi, S. Nandhini (Annamalai University); Manoharan, E. Gnana (Annamalai University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152954396,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,,,,,,,,,
711,pub.1148488878,10.48550/arxiv.2206.01737,,,MaxStyle: Adversarial Style Composition for Robust Medical Image  Segmentation,"Convolutional neural networks (CNNs) have achieved remarkable segmentation
accuracy on benchmark datasets where training and test sets are from the same
domain, yet their performance can degrade significantly on unseen domains,
which hinders the deployment of CNNs in many clinical scenarios. Most existing
works improve model out-of-domain (OOD) robustness by collecting multi-domain
datasets for training, which is expensive and may not always be feasible due to
privacy and logistical issues. In this work, we focus on improving model
robustness using a single-domain dataset only. We propose a novel data
augmentation framework called MaxStyle, which maximizes the effectiveness of
style augmentation for model OOD performance. It attaches an auxiliary
style-augmented image decoder to a segmentation network for robust feature
learning and data augmentation. Importantly, MaxStyle augments data with
improved image style diversity and hardness, by expanding the style space with
noise and searching for the worst-case style composition of latent features via
adversarial training. With extensive experiments on multiple public cardiac and
prostate MR datasets, we demonstrate that MaxStyle leads to significantly
improved out-of-distribution robustness against unseen corruptions as well as
common distribution shifts across multiple, different, unseen sites and unknown
image sequences under both low- and high-training data settings. The code can
be found at https://github.com/cherise215/MaxStyle.",,,arXiv,,,2022-06-02,2022,,,,,,All OA, Green,Preprint,"Chen, Chen; Li, Zeju; Ouyang, Cheng; Sinclair, Matt; Bai, Wenjia; Rueckert, Daniel","Chen, Chen (); Li, Zeju (); Ouyang, Cheng (); Sinclair, Matt (); Bai, Wenjia (); Rueckert, Daniel ()",,"Chen, Chen (); Li, Zeju (); Ouyang, Cheng (); Sinclair, Matt (); Bai, Wenjia (); Rueckert, Daniel ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148488878,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
711,pub.1148728310,10.48550/arxiv.2206.06445,,,Fitting Segmentation Networks on Varying Image Resolutions using  Splatting,"Data used in image segmentation are not always defined on the same grid. This
is particularly true for medical images, where the resolution, field-of-view
and orientation can differ across channels and subjects. Images and labels are
therefore commonly resampled onto the same grid, as a pre-processing step.
However, the resampling operation introduces partial volume effects and
blurring, thereby changing the effective resolution and reducing the contrast
between structures. In this paper we propose a splat layer, which automatically
handles resolution mismatches in the input data. This layer pushes each image
onto a mean space where the forward pass is performed. As the splat operator is
the adjoint to the resampling operator, the mean-space prediction can be pulled
back to the native label space, where the loss function is computed. Thus, the
need for explicit resolution adjustment using interpolation is removed. We show
on two publicly available datasets, with simulated and real multi-modal
magnetic resonance images, that this model improves segmentation results
compared to resampling as a pre-processing step.",,,arXiv,,,2022-06-13,2022,,,,,,All OA, Green,Preprint,"Brudfors, Mikael; Balbastre, Yael; Ashburner, John; Rees, Geraint; Nachev, Parashkev; Ourselin, Sebastien; Cardoso, M. Jorge","Brudfors, Mikael (); Balbastre, Yael (); Ashburner, John (); Rees, Geraint (); Nachev, Parashkev (); Ourselin, Sebastien (); Cardoso, M. Jorge ()",,"Brudfors, Mikael (); Balbastre, Yael (); Ashburner, John (); Rees, Geraint (); Nachev, Parashkev (); Ourselin, Sebastien (); Cardoso, M. Jorge ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148728310,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
711,pub.1132271565,10.48550/arxiv.2011.00526,,,Learning Euler's Elastica Model for Medical Image Segmentation,"Image segmentation is a fundamental topic in image processing and has been
studied for many decades. Deep learning-based supervised segmentation models
have achieved state-of-the-art performance but most of them are limited by
using pixel-wise loss functions for training without geometrical constraints.
Inspired by Euler's Elastica model and recent active contour models introduced
into the field of deep learning, we propose a novel active contour with
elastica (ACE) loss function incorporating Elastica (curvature and length) and
region information as geometrically-natural constraints for the image
segmentation tasks. We introduce the mean curvature i.e. the average of all
principal curvatures, as a more effective image prior to representing curvature
in our ACE loss function. Furthermore, based on the definition of the mean
curvature, we propose a fast solution to approximate the ACE loss in
three-dimensional (3D) by using Laplace operators for 3D image segmentation. We
evaluate our ACE loss function on four 2D and 3D natural and biomedical image
datasets. Our results show that the proposed loss function outperforms other
mainstream loss functions on different segmentation networks. Our source code
is available at https://github.com/HiLab-git/ACELoss.",,,arXiv,,,2020-11-01,2020,,,,,,All OA, Green,Preprint,"Chen, Xu; Luo, Xiangde; Zhao, Yitian; Zhang, Shaoting; Wang, Guotai; Zheng, Yalin","Chen, Xu (); Luo, Xiangde (); Zhao, Yitian (); Zhang, Shaoting (); Wang, Guotai (); Zheng, Yalin ()",,"Chen, Xu (); Luo, Xiangde (); Zhao, Yitian (); Zhang, Shaoting (); Wang, Guotai (); Zheng, Yalin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1132271565,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
709,pub.1153777520,10.48550/arxiv.2212.08423,,,Context Label Learning: Improving Background Class Representations in  Semantic Segmentation,"Background samples provide key contextual information for segmenting regions
of interest (ROIs). However, they always cover a diverse set of structures,
causing difficulties for the segmentation model to learn good decision
boundaries with high sensitivity and precision. The issue concerns the highly
heterogeneous nature of the background class, resulting in multi-modal
distributions. Empirically, we find that neural networks trained with
heterogeneous background struggle to map the corresponding contextual samples
to compact clusters in feature space. As a result, the distribution over
background logit activations may shift across the decision boundary, leading to
systematic over-segmentation across different datasets and tasks. In this
study, we propose context label learning (CoLab) to improve the context
representations by decomposing the background class into several subclasses.
Specifically, we train an auxiliary network as a task generator, along with the
primary segmentation model, to automatically generate context labels that
positively affect the ROI segmentation accuracy. Extensive experiments are
conducted on several challenging segmentation tasks and datasets. The results
demonstrate that CoLab can guide the segmentation model to map the logits of
background samples away from the decision boundary, resulting in significantly
improved segmentation accuracy. Code is available.",,,arXiv,,,2022-12-16,2022,,,,,,All OA, Green,Preprint,"Li, Zeju; Kamnitsas, Konstantinos; Ouyang, Cheng; Chen, Chen; Glocker, Ben","Li, Zeju (); Kamnitsas, Konstantinos (); Ouyang, Cheng (); Chen, Chen (); Glocker, Ben ()",,"Li, Zeju (); Kamnitsas, Konstantinos (); Ouyang, Cheng (); Chen, Chen (); Glocker, Ben ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153777520,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
707,pub.1139075124,10.48550/arxiv.2106.10947,,,Leveraging Conditional Generative Models in a General Explanation  Framework of Classifier Decisions,"Providing a human-understandable explanation of classifiers' decisions has
become imperative to generate trust in their use for day-to-day tasks. Although
many works have addressed this problem by generating visual explanation maps,
they often provide noisy and inaccurate results forcing the use of heuristic
regularization unrelated to the classifier in question. In this paper, we
propose a new general perspective of the visual explanation problem overcoming
these limitations. We show that visual explanation can be produced as the
difference between two generated images obtained via two specific conditional
generative models. Both generative models are trained using the classifier to
explain and a database to enforce the following properties: (i) All images
generated by the first generator are classified similarly to the input image,
whereas the second generator's outputs are classified oppositely. (ii)
Generated images belong to the distribution of real images. (iii) The distances
between the input image and the corresponding generated images are minimal so
that the difference between the generated elements only reveals relevant
information for the studied classifier. Using symmetrical and cyclic
constraints, we present two different approximations and implementations of the
general formulation. Experimentally, we demonstrate significant improvements
w.r.t the state-of-the-art on three different public data sets. In particular,
the localization of regions influencing the classifier is consistent with human
annotations.",,,arXiv,,,2021-06-21,2021,,,,,,All OA, Green,Preprint,"Charachon, Martin; CournÃ¨de, Paul-Henry; Hudelot, CÃ©line; Ardon, Roberto","Charachon, Martin (); CournÃ¨de, Paul-Henry (); Hudelot, CÃ©line (); Ardon, Roberto ()",,"Charachon, Martin (); CournÃ¨de, Paul-Henry (); Hudelot, CÃ©line (); Ardon, Roberto ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139075124,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
707,pub.1143519392,10.1016/j.sigpro.2021.108418,,,Learning multi-level structural information for small organ segmentation,"Deep neural networks have achieved great success in medical image segmentation problems such as liver, kidney, the accuracy of which already exceeds the human level. However, small organ segmentation (e.g., pancreas) is still a challenging task. To tackle such problems, extracting and aggregating multi-scale robust features become essentially important. In this paper, we develop a multi-level structural loss by integrating the region, boundary, and pixel-wise information to supervise feature fusion and precise segmentation. The novel pixel-wise term can provide information complementary to the region and boundary loss, which helps to discover more local information from the image. We further develop a multi-branch network with a saliency guidance module to better aggregate the three levels of features. The coarse-to-fine segmentation architecture is adopted to use the prediction on the coarse stage to obtain the bounding box for the fine stage. Comprehensive evaluations are performed on three benchmark datasets, i.e., the NIH pancreas, ISICDM pancreas, and MSD spleen dataset, showing that our models can achieve significant increases in segmentation accuracy compared to several state-of-the-art pancreas and spleen segmentation methods. Furthermore, the ablation study demonstrates the multi-level structural features help both the training stability and the convergence of the coarse-to-fine approach.","The work was partially supported by National Natural Science Foundation of China (NSFC 12071345, 11701418), Major Science and Technology Project of Tianjin 18ZXRHSY00160 and Recruitment Program of Global Young Expert.",,Signal Processing,,,2022-04,2022,,2022-04,193,,108418,Closed,Article,"Liu, Yueyun; Duan, Yuping; Zeng, Tieyong","Liu, Yueyun (Center for Applied Mathematics, Tianjin University, Tianjin, 300072, China); Duan, Yuping (Center for Applied Mathematics, Tianjin University, Tianjin, 300072, China); Zeng, Tieyong (Department of mathematics, The Chinese University of Hong Kong, Shatin, NT, Hong Kong)","Duan, Yuping (Tianjin University)","Liu, Yueyun (Tianjin University); Duan, Yuping (Tianjin University); Zeng, Tieyong (Chinese University of Hong Kong)",7,7,,,,https://app.dimensions.ai/details/publication/pub.1143519392,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
688,pub.1151155500,10.1007/978-3-031-16980-9_7,,,Morphology-Preserving Autoregressive 3D Generative Modelling of the Brain,"Human anatomy, morphology, and associated diseases can be studied using medical imaging data. However, access to medical imaging data is restricted by governance and privacy concerns, data ownership, and the cost of acquisition, thus limiting our ability to understand the human body. A possible solution to this issue is the creation of a model able to learn and then generate synthetic images of the human body conditioned on specific characteristics of relevance (e.g., age, sex, and disease status). Deep generative models, in the form of neural networks, have been recently used to create synthetic 2D images of natural scenes. Still, the ability to produce high-resolution 3D volumetric imaging data with correct anatomical morphology has been hampered by data scarcity and algorithmic and computational limitations. This work proposes a generative model that can be scaled to produce anatomically correct, high-resolution, and realistic images of the human brain, with the necessary quality to allow further downstream analyses. The ability to generate a potentially unlimited amount of data not only enables large-scale studies of human anatomy and pathology without jeopardizing patient privacy, but also significantly advances research in the field of anomaly detection, modality synthesis, learning under limited data, and fair and ethical AI. Code and trained models are available at: https://github.com/AmigoLab/SynthAnatomy.","WHLP, MG, PB, MJC and PN are supported by Wellcome [WT213038/Z/18/Z]. PTD is supported by the EPSRC Research Council, part of the EPSRC DTP [EP/R513064/1]. FV is supported by Wellcome/ EPSRC Centre for Medical Engineering [WT203148/Z/16/Z], Wellcome Flagship Programme [WT213038/Z/18/Z], The London AI Centre for Value-based Healthcare and GE Healthcare. PB is also supported by Wellcome Flagship Programme [WT213038/Z/18/Z] and Wellcome EPSRC CME [WT203148/Z/16/Z]. PN is also supported by the UCLH NIHR Biomedical Research Centre. The models in this work were trained on NVIDIA Cambridge-1, the UKâs largest supercomputer, aimed at accelerating digital biology.",,Lecture Notes in Computer Science,Simulation and Synthesis in Medical Imaging,,2022-09-21,2022,2022-09-21,2022,13570,,66-78,All OA, Green,Chapter,"Tudosiu, Petru-Daniel; Pinaya, Walter Hugo Lopez; Graham, Mark S.; Borges, Pedro; Fernandez, Virginia; Yang, Dai; Appleyard, Jeremy; Novati, Guido; Mehra, Disha; Vella, Mike; Nachev, Parashkev; Ourselin, Sebastien; Cardoso, Jorge","Tudosiu, Petru-Daniel (Department of Biomedical Engineering, School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Pinaya, Walter Hugo Lopez (Department of Biomedical Engineering, School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Graham, Mark S. (Department of Biomedical Engineering, School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Borges, Pedro (Department of Biomedical Engineering, School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Fernandez, Virginia (Department of Biomedical Engineering, School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Yang, Dai (NVIDIA, Santa Clara, USA); Appleyard, Jeremy (NVIDIA, Santa Clara, USA); Novati, Guido (DeepMind, London, UK); Mehra, Disha (NVIDIA, Santa Clara, USA); Vella, Mike (Oxford Nanopore Technologies, Gosling Building, Oxford Science Park, Edmund Halley Road, Littlemore, OX4 4DQ, Oxford, UK); Nachev, Parashkev (Queen Square Institute of Neurology, University College London, London, UK); Ourselin, Sebastien (Department of Biomedical Engineering, School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK); Cardoso, Jorge (Department of Biomedical Engineering, School of Biomedical Engineering & Imaging Sciences, Kingâs College London, London, UK)","Tudosiu, Petru-Daniel (King's College London)","Tudosiu, Petru-Daniel (King's College London); Pinaya, Walter Hugo Lopez (King's College London); Graham, Mark S. (King's College London); Borges, Pedro (King's College London); Fernandez, Virginia (King's College London); Yang, Dai (Nvidia (United States)); Appleyard, Jeremy (Nvidia (United States)); Novati, Guido (DeepMind (United Kingdom)); Mehra, Disha (Nvidia (United States)); Vella, Mike (Oxford Nanopore Technologies (United Kingdom)); Nachev, Parashkev (University College London); Ourselin, Sebastien (King's College London); Cardoso, Jorge (King's College London)",0,0,,,http://arxiv.org/pdf/2209.03177,https://app.dimensions.ai/details/publication/pub.1151155500,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
687,pub.1150846852,10.48550/arxiv.2209.03177,,,Morphology-preserving Autoregressive 3D Generative Modelling of the  Brain,"Human anatomy, morphology, and associated diseases can be studied using
medical imaging data. However, access to medical imaging data is restricted by
governance and privacy concerns, data ownership, and the cost of acquisition,
thus limiting our ability to understand the human body. A possible solution to
this issue is the creation of a model able to learn and then generate synthetic
images of the human body conditioned on specific characteristics of relevance
(e.g., age, sex, and disease status). Deep generative models, in the form of
neural networks, have been recently used to create synthetic 2D images of
natural scenes. Still, the ability to produce high-resolution 3D volumetric
imaging data with correct anatomical morphology has been hampered by data
scarcity and algorithmic and computational limitations. This work proposes a
generative model that can be scaled to produce anatomically correct,
high-resolution, and realistic images of the human brain, with the necessary
quality to allow further downstream analyses. The ability to generate a
potentially unlimited amount of data not only enables large-scale studies of
human anatomy and pathology without jeopardizing patient privacy, but also
significantly advances research in the field of anomaly detection, modality
synthesis, learning under limited data, and fair and ethical AI. Code and
trained models are available at: https://github.com/AmigoLab/SynthAnatomy.",,,arXiv,,,2022-09-07,2022,,,,,,All OA, Green,Preprint,"Tudosiu, Petru-Daniel; Pinaya, Walter Hugo Lopez; Graham, Mark S.; Borges, Pedro; Fernandez, Virginia; Yang, Dai; Appleyard, Jeremy; Novati, Guido; Mehra, Disha; Vella, Mike; Nachev, Parashkev; Ourselin, Sebastien; Cardoso, Jorge","Tudosiu, Petru-Daniel (); Pinaya, Walter Hugo Lopez (); Graham, Mark S. (); Borges, Pedro (); Fernandez, Virginia (); Yang, Dai (); Appleyard, Jeremy (); Novati, Guido (); Mehra, Disha (); Vella, Mike (); Nachev, Parashkev (); Ourselin, Sebastien (); Cardoso, Jorge ()",,"Tudosiu, Petru-Daniel (); Pinaya, Walter Hugo Lopez (); Graham, Mark S. (); Borges, Pedro (); Fernandez, Virginia (); Yang, Dai (); Appleyard, Jeremy (); Novati, Guido (); Mehra, Disha (); Vella, Mike (); Nachev, Parashkev (); Ourselin, Sebastien (); Cardoso, Jorge ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150846852,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
687,pub.1149727542,10.1007/978-3-031-12053-4_30,,,A Deep-Learning Lesion Segmentation Model that Addresses Class Imbalance and Expected Low Probability Tissue Abnormalities in Pre and Postoperative Liver MRI,"Class imbalance in various forms is a common challenge in machine learning (ML) applied to medical imaging. One of these forms is the presence of low probability, but unsurprising, tissue abnormalities as a result of e.g. implants and surgery. Assessments from automated methods can be impeded if the ML system cannot address these abnormalities. A context where this issue arises is segmentation of lesions within the liver when postoperative scans are possible inputs to the model, since surgical clips and postoperative seromas can distort measures such as future liver remnant volume if they are not correctly identified. To this end, we developed a deep learning segmentation model with classes that expliciltly include surgery-related structures: liver parenchyma, lesions, surgical clips, and postoperative seromas. Given a heavy class imbalance in this task, we deployed an asymmetric focal loss function and hysteresis thresholding post-processing. We applied our model to T1-weighted MRI data, reporting average Dice scores of 0.96, 0.57, 0.71, and 0.84 for the four classes, respectively. Finally, we tested the modelâs potential in a semi-automatic workflow, finding a user-interaction speedup and an increased inter-rater agreement compared to fully manual delineations. To our knowledge, this is the first study to investigate an automated lesion segmentation model for postoperative MRI with both surgical clips and seromas as explicit classes, and the first work to explore an asymmetric focal loss function for segmentation in liver cancer.",,,Lecture Notes in Computer Science,Medical Image Understanding and Analysis,,2022-07-25,2022,2022-07-25,2022,13413,,398-411,Closed,Chapter,"Vogt, Nora; Arya, Zobair; NÃºÃ±ez, Luis; Hobson, Kezia; Connell, John; Brady, Sir Michael; Aljabar, Paul","Vogt, Nora (Perspectum Ltd., Oxford, UK); Arya, Zobair (Perspectum Ltd., Oxford, UK); NÃºÃ±ez, Luis (Perspectum Ltd., Oxford, UK); Hobson, Kezia (Perspectum Ltd., Oxford, UK); Connell, John (Perspectum Ltd., Oxford, UK); Brady, Sir Michael (Perspectum Ltd., Oxford, UK); Aljabar, Paul (Perspectum Ltd., Oxford, UK)","Arya, Zobair ","Vogt, Nora (); Arya, Zobair (); NÃºÃ±ez, Luis (); Hobson, Kezia (); Connell, John (); Brady, Sir Michael (); Aljabar, Paul ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149727542,46 Information and Computing Sciences,,,,,,,,,,,,
687,pub.1144438234,10.48550/arxiv.2201.01266,,,Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors  in MRI Images,"Semantic segmentation of brain tumors is a fundamental medical image analysis
task involving multiple MRI imaging modalities that can assist clinicians in
diagnosing the patient and successively studying the progression of the
malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs)
approaches have become the de facto standard for 3D medical image segmentation.
The popular ""U-shaped"" network architecture has achieved state-of-the-art
performance benchmarks on different 2D and 3D semantic segmentation tasks and
across various imaging modalities. However, due to the limited kernel size of
convolution layers in FCNNs, their performance of modeling long-range
information is sub-optimal, and this can lead to deficiencies in the
segmentation of tumors with variable sizes. On the other hand, transformer
models have demonstrated excellent capabilities in capturing such long-range
information in multiple domains, including natural language processing and
computer vision. Inspired by the success of vision transformers and their
variants, we propose a novel segmentation model termed Swin UNEt TRansformers
(Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is
reformulated as a sequence to sequence prediction problem wherein multi-modal
input data is projected into a 1D sequence of embedding and used as an input to
a hierarchical Swin transformer as the encoder. The swin transformer encoder
extracts features at five different resolutions by utilizing shifted windows
for computing self-attention and is connected to an FCNN-based decoder at each
resolution via skip connections. We have participated in BraTS 2021
segmentation challenge, and our proposed model ranks among the top-performing
approaches in the validation phase. Code: https://monai.io/research/swin-unetr",,,arXiv,,,2022-01-04,2022,,,,,,All OA, Green,Preprint,"Hatamizadeh, Ali; Nath, Vishwesh; Tang, Yucheng; Yang, Dong; Roth, Holger; Xu, Daguang","Hatamizadeh, Ali (); Nath, Vishwesh (); Tang, Yucheng (); Yang, Dong (); Roth, Holger (); Xu, Daguang ()",,"Hatamizadeh, Ali (); Nath, Vishwesh (); Tang, Yucheng (); Yang, Dong (); Roth, Holger (); Xu, Daguang ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144438234,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
687,pub.1143690917,10.1109/cisp-bmei53629.2021.9624442,,,Survey on machine learning applied to medical image analysis,"This paper presents a selective survey on recent advances in machine learning applied to medical imaging. It aims to highlight both innovations that increase the performance of the models and methods that ensure certainty, interpretability and robustness of the trained models. The paper focuses particularly on new concepts such as attention modules that allow to gather specific features considering global context. Its second main focus is given to domain adaptation methods to enhance model robustness to distribution shifts. Finally, we discuss uncertainty estimation and interpretability methods to evaluate confidence in a trained model.",This research project is supported by the French Clinical Research Infrastructure Network on Venous Thrombo-Embolism (FCRIN INNOVTE). The authors would like also to acknowledge Brest University Hospital and ENSTA Bretagne for their supports,,,"2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",,2021-10-25,2021,,2021-10-25,0,,1-6,Closed,Proceeding,"Olivier, AurÃ©lien; Hoffmann, ClÃ©ment; Mansour, Ali; Bressollette, Luc; Clement, Benoit","Olivier, AurÃ©lien (Lab-STICC UMR CNRS 6285, ENSTA Bretagne, Brest, France); Hoffmann, ClÃ©ment (EA3878 CIC INSERM 1412, CHRU Cavale Blanche, Brest, France); Mansour, Ali (Lab-STICC UMR CNRS 6285, ENSTA Bretagne, Brest, France); Bressollette, Luc (EA3878 CIC INSERM 1412, CHRU Cavale Blanche, Brest, France); Clement, Benoit (College of Science and Engineering, Flinders University, Adelaide, Australia)",,"Olivier, AurÃ©lien (National Institute of Advanced Technologies of Brittany); Hoffmann, ClÃ©ment (); Mansour, Ali (National Institute of Advanced Technologies of Brittany); Bressollette, Luc (); Clement, Benoit (Flinders University)",1,1,,0.82,,https://app.dimensions.ai/details/publication/pub.1143690917,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
685,pub.1141723010,10.48550/arxiv.2110.03352,,,Optimized U-Net for Brain Tumor Segmentation,"We propose an optimized U-Net architecture for a brain tumor segmentation
task in the BraTS21 challenge. To find the optimal model architecture and the
learning schedule, we have run an extensive ablation study to test: deep
supervision loss, Focal loss, decoder attention, drop block, and residual
connections. Additionally, we have searched for the optimal depth of the U-Net
encoder, number of convolutional channels and post-processing strategy. Our
method won the validation phase and took third place in the test phase. We have
open-sourced the code to reproduce our BraTS21 submission at the NVIDIA Deep
Learning Examples GitHub Repository.",,,arXiv,,,2021-10-07,2021,,,,,,All OA, Green,Preprint,"Futrega, MichaÅ; Milesi, Alexandre; Marcinkiewicz, Michal; Ribalta, Pablo","Futrega, MichaÅ (); Milesi, Alexandre (); Marcinkiewicz, Michal (); Ribalta, Pablo ()",,"Futrega, MichaÅ (); Milesi, Alexandre (); Marcinkiewicz, Michal (); Ribalta, Pablo ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141723010,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
685,pub.1143526239,10.48550/arxiv.2111.15409,,,Fully Automatic Deep Learning Framework for Pancreatic Ductal  Adenocarcinoma Detection on Computed Tomography,"Early detection improves prognosis in pancreatic ductal adenocarcinoma (PDAC)
but is challenging as lesions are often small and poorly defined on
contrast-enhanced computed tomography scans (CE-CT). Deep learning can
facilitate PDAC diagnosis, however current models still fail to identify small
(<2cm) lesions. In this study, state-of-the-art deep learning models were used
to develop an automatic framework for PDAC detection, focusing on small
lesions. Additionally, the impact of integrating surrounding anatomy was
investigated. CE-CT scans from a cohort of 119 pathology-proven PDAC patients
and a cohort of 123 patients without PDAC were used to train a nnUnet for
automatic lesion detection and segmentation (nnUnet_T). Two additional nnUnets
were trained to investigate the impact of anatomy integration: (1) segmenting
the pancreas and tumor (nnUnet_TP), (2) segmenting the pancreas, tumor, and
multiple surrounding anatomical structures (nnUnet_MS). An external, publicly
available test set was used to compare the performance of the three networks.
The nnUnet_MS achieved the best performance, with an area under the receiver
operating characteristic curve of 0.91 for the whole test set and 0.88 for
tumors <2cm, showing that state-of-the-art deep learning can detect small PDAC
and benefits from anatomy information.",,,arXiv,,,2021-11-30,2021,,,,,,All OA, Green,Preprint,"Alves, NatÃ¡lia; Schuurmans, Megan; Litjens, Geke; Bosma, Joeran S.; Hermans, John; Huisman, Henkjan","Alves, NatÃ¡lia (); Schuurmans, Megan (); Litjens, Geke (); Bosma, Joeran S. (); Hermans, John (); Huisman, Henkjan ()",,"Alves, NatÃ¡lia (); Schuurmans, Megan (); Litjens, Geke (); Bosma, Joeran S. (); Hermans, John (); Huisman, Henkjan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143526239,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
683,pub.1143782854,10.48550/arxiv.2112.04489,,,"Learn2Reg: comprehensive multi-task medical image registration  challenge, dataset and evaluation in the era of deep learning","Image registration is a fundamental medical image analysis task, and a wide
variety of approaches have been proposed. However, only a few studies have
comprehensively compared medical image registration approaches on a wide range
of clinically relevant tasks. This limits the development of registration
methods, the adoption of research advances into practice, and a fair benchmark
across competing approaches. The Learn2Reg challenge addresses these
limitations by providing a multi-task medical image registration data set for
comprehensive characterisation of deformable registration algorithms. A
continuous evaluation will be possible at
https://learn2reg.grand-challenge.org. Learn2Reg covers a wide range of
anatomies (brain, abdomen, and thorax), modalities (ultrasound, CT, MR),
availability of annotations, as well as intra- and inter-patient registration
evaluation. We established an easily accessible framework for training and
validation of 3D registration methods, which enabled the compilation of results
of over 65 individual method submissions from more than 20 unique teams. We
used a complementary set of metrics, including robustness, accuracy,
plausibility, and runtime, enabling unique insight into the current
state-of-the-art of medical image registration. This paper describes datasets,
tasks, evaluation methods and results of the challenge, as well as results of
further analysis of transferability to new datasets, the importance of label
supervision, and resulting bias. While no single approach worked best across
all tasks, many methodological aspects could be identified that push the
performance of medical image registration to new state-of-the-art performance.
Furthermore, we demystified the common belief that conventional registration
methods have to be much slower than deep-learning-based methods.",,,arXiv,,,2021-12-08,2021,,,,,,All OA, Green,Preprint,"Hering, Alessa; Hansen, Lasse; Mok, Tony C. W.; Chung, Albert C. S.; Siebert, Hanna; HÃ¤ger, Stephanie; Lange, Annkristin; Kuckertz, Sven; Heldmann, Stefan; Shao, Wei; Vesal, Sulaiman; Rusu, Mirabela; Sonn, Geoffrey; Estienne, ThÃ©o; Vakalopoulou, Maria; Han, Luyi; Huang, Yunzhi; Yap, Pew-Thian; Brudfors, Mikael; Balbastre, YaÃ«l; Joutard, Samuel; Modat, Marc; Lifshitz, Gal; Raviv, Dan; Lv, Jinxin; Li, Qiang; Jaouen, Vincent; Visvikis, Dimitris; Fourcade, Constance; Rubeaux, Mathieu; Pan, Wentao; Xu, Zhe; Jian, Bailiang; De Benetti, Francesca; Wodzinski, Marek; Gunnarsson, Niklas; SjÃ¶lund, Jens; Grzech, Daniel; Qiu, Huaqi; Li, Zeju; Thorley, Alexander; Duan, Jinming; GroÃbrÃ¶hmer, Christoph; Hoopes, Andrew; Reinertsen, Ingerid; Xiao, Yiming; Landman, Bennett; Huo, Yuankai; Murphy, Keelin; Lessmann, Nikolas; van Ginneken, Bram; Dalca, Adrian V.; Heinrich, Mattias P.","Hering, Alessa (); Hansen, Lasse (); Mok, Tony C. W. (); Chung, Albert C. S. (); Siebert, Hanna (); HÃ¤ger, Stephanie (); Lange, Annkristin (); Kuckertz, Sven (); Heldmann, Stefan (); Shao, Wei (); Vesal, Sulaiman (); Rusu, Mirabela (); Sonn, Geoffrey (); Estienne, ThÃ©o (); Vakalopoulou, Maria (); Han, Luyi (); Huang, Yunzhi (); Yap, Pew-Thian (); Brudfors, Mikael (); Balbastre, YaÃ«l (); Joutard, Samuel (); Modat, Marc (); Lifshitz, Gal (); Raviv, Dan (); Lv, Jinxin (); Li, Qiang (); Jaouen, Vincent (); Visvikis, Dimitris (); Fourcade, Constance (); Rubeaux, Mathieu (); Pan, Wentao (); Xu, Zhe (); Jian, Bailiang (); De Benetti, Francesca (); Wodzinski, Marek (); Gunnarsson, Niklas (); SjÃ¶lund, Jens (); Grzech, Daniel (); Qiu, Huaqi (); Li, Zeju (); Thorley, Alexander (); Duan, Jinming (); GroÃbrÃ¶hmer, Christoph (); Hoopes, Andrew (); Reinertsen, Ingerid (); Xiao, Yiming (); Landman, Bennett (); Huo, Yuankai (); Murphy, Keelin (); Lessmann, Nikolas (); van Ginneken, Bram (); Dalca, Adrian V. (); Heinrich, Mattias P. ()",,"Hering, Alessa (); Hansen, Lasse (); Mok, Tony C. W. (); Chung, Albert C. S. (); Siebert, Hanna (); HÃ¤ger, Stephanie (); Lange, Annkristin (); Kuckertz, Sven (); Heldmann, Stefan (); Shao, Wei (); Vesal, Sulaiman (); Rusu, Mirabela (); Sonn, Geoffrey (); Estienne, ThÃ©o (); Vakalopoulou, Maria (); Han, Luyi (); Huang, Yunzhi (); Yap, Pew-Thian (); Brudfors, Mikael (); Balbastre, YaÃ«l (); Joutard, Samuel (); Modat, Marc (); Lifshitz, Gal (); Raviv, Dan (); Lv, Jinxin (); Li, Qiang (); Jaouen, Vincent (); Visvikis, Dimitris (); Fourcade, Constance (); Rubeaux, Mathieu (); Pan, Wentao (); Xu, Zhe (); Jian, Bailiang (); De Benetti, Francesca (); Wodzinski, Marek (); Gunnarsson, Niklas (); SjÃ¶lund, Jens (); Grzech, Daniel (); Qiu, Huaqi (); Li, Zeju (); Thorley, Alexander (); Duan, Jinming (); GroÃbrÃ¶hmer, Christoph (); Hoopes, Andrew (); Reinertsen, Ingerid (); Xiao, Yiming (); Landman, Bennett (); Huo, Yuankai (); Murphy, Keelin (); Lessmann, Nikolas (); van Ginneken, Bram (); Dalca, Adrian V. (); Heinrich, Mattias P. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143782854,32 Biomedical and Clinical Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
663,pub.1129637238,10.24132/jwscg.2020.28.5,,,Strategies for Training Deep Learning Models in Medical Domains with Small Reference Datasets,,,,Journal of WSCG,,,2020,2020,2020,,28,1-2,37-46,All OA, Gold,Article,"Zwettler, Gerald A; Holmes, David R; Backfrieder, Werner","Zwettler, Gerald A (); Holmes, David R (); Backfrieder, Werner ()",,"Zwettler, Gerald A (); Holmes, David R (); Backfrieder, Werner ()",0,0,,,https://doi.org/10.24132/jwscg.2020.28.5,https://app.dimensions.ai/details/publication/pub.1129637238,,,,,,,,,,,,
662,pub.1154053689,10.48550/arxiv.2212.13504,,,DAE-Former: Dual Attention-guided Efficient Transformer for Medical  Image Segmentation,"Transformers have recently gained attention in the computer vision domain due
to their ability to model long-range dependencies. However, the self-attention
mechanism, which is the core part of the Transformer model, usually suffers
from quadratic computational complexity with respect to the number of tokens.
Many architectures attempt to reduce model complexity by limiting the
self-attention mechanism to local regions or by redesigning the tokenization
process. In this paper, we propose DAE-Former, a novel method that seeks to
provide an alternative perspective by efficiently designing the self-attention
mechanism. More specifically, we reformulate the self-attention mechanism to
capture both spatial and channel relations across the whole feature dimension
while staying computationally efficient. Furthermore, we redesign the skip
connection path by including the cross-attention module to ensure the feature
reusability and enhance the localization power. Our method outperforms
state-of-the-art methods on multi-organ cardiac and skin lesion segmentation
datasets without requiring pre-training weights. The code is publicly available
at https://github.com/mindflow-institue/DAEFormer.",,,arXiv,,,2022-12-27,2022,,,,,,All OA, Green,Preprint,"Azad, Reza; Arimond, RenÃ©; Aghdam, Ehsan Khodapanah; Kazerouni, Amirhossein; Merhof, Dorit","Azad, Reza (); Arimond, RenÃ© (); Aghdam, Ehsan Khodapanah (); Kazerouni, Amirhossein (); Merhof, Dorit ()",,"Azad, Reza (); Arimond, RenÃ© (); Aghdam, Ehsan Khodapanah (); Kazerouni, Amirhossein (); Merhof, Dorit ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154053689,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
662,pub.1141558164,10.48550/arxiv.2109.14879,,,Robust Segmentation Models using an Uncertainty Slice Sampling Based  Annotation Workflow,"Semantic segmentation neural networks require pixel-level annotations in
large quantities to achieve a good performance. In the medical domain, such
annotations are expensive, because they are time-consuming and require expert
knowledge. Active learning optimizes the annotation effort by devising
strategies to select cases for labeling that are most informative to the model.
In this work, we propose an uncertainty slice sampling (USS) strategy for
semantic segmentation of 3D medical volumes that selects 2D image slices for
annotation and compare it with various other strategies. We demonstrate the
efficiency of USS on a CT liver segmentation task using multi-site data. After
five iterations, the training data resulting from USS consisted of 2410 slices
(4% of all slices in the data pool) compared to 8121 (13%), 8641 (14%), and
3730 (6%) for uncertainty volume (UVS), random volume (RVS), and random slice
(RSS) sampling, respectively. Despite being trained on the smallest amount of
data, the model based on the USS strategy evaluated on 234 test volumes
significantly outperformed models trained according to other strategies and
achieved a mean Dice index of 0.964, a relative volume error of 4.2%, a mean
surface distance of 1.35 mm, and a Hausdorff distance of 23.4 mm. This was only
slightly inferior to 0.967, 3.8%, 1.18 mm, and 22.9 mm achieved by a model
trained on all available data, but the robustness analysis using the 5th
percentile of Dice and the 95th percentile of the remaining metrics
demonstrated that USS resulted not only in the most robust model compared to
other sampling schemes, but also outperformed the model trained on all data
according to Dice (0.946 vs. 0.945) and mean surface distance (1.92 mm vs. 2.03
mm).",,,arXiv,,,2021-09-30,2021,,,,,,All OA, Green,Preprint,"Chlebus, Grzegorz; Schenk, Andrea; Hahn, Horst K.; van Ginneken, Bram; Meine, Hans","Chlebus, Grzegorz (); Schenk, Andrea (); Hahn, Horst K. (); van Ginneken, Bram (); Meine, Hans ()",,"Chlebus, Grzegorz (); Schenk, Andrea (); Hahn, Horst K. (); van Ginneken, Bram (); Meine, Hans ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141558164,32 Biomedical and Clinical Sciences, 46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
662,pub.1148380484,10.48550/arxiv.2206.00566,,,The Fully Convolutional Transformer for Medical Image Segmentation,"We propose a novel transformer model, capable of segmenting medical images of
varying modalities. Challenges posed by the fine grained nature of medical
image analysis mean that the adaptation of the transformer for their analysis
is still at nascent stages. The overwhelming success of the UNet lay in its
ability to appreciate the fine-grained nature of the segmentation task, an
ability which existing transformer based models do not currently posses. To
address this shortcoming, we propose The Fully Convolutional Transformer (FCT),
which builds on the proven ability of Convolutional Neural Networks to learn
effective image representations, and combines them with the ability of
Transformers to effectively capture long-term dependencies in its inputs. The
FCT is the first fully convolutional Transformer model in medical imaging
literature. It processes its input in two stages, where first, it learns to
extract long range semantic dependencies from the input image, and then learns
to capture hierarchical global attributes from the features. FCT is compact,
accurate and robust. Our results show that it outperforms all existing
transformer architectures by large margins across multiple medical image
segmentation datasets of varying data modalities without the need for any
pre-training. FCT outperforms its immediate competitor on the ACDC dataset by
1.3%, on the Synapse dataset by 4.4%, on the Spleen dataset by 1.2% and on ISIC
2017 dataset by 1.1% on the dice metric, with up to five times fewer
parameters. Our code, environments and models will be available via GitHub.",,,arXiv,,,2022-06-01,2022,,,,,,All OA, Green,Preprint,"Tragakis, Athanasios; Kaul, Chaitanya; Murray-Smith, Roderick; Husmeier, Dirk","Tragakis, Athanasios (); Kaul, Chaitanya (); Murray-Smith, Roderick (); Husmeier, Dirk ()",,"Tragakis, Athanasios (); Kaul, Chaitanya (); Murray-Smith, Roderick (); Husmeier, Dirk ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1148380484,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
662,pub.1151568951,10.48550/arxiv.2210.00646,,,Pixel-global Self-supervised Learning with Uncertainty-aware Context  Stabilizer,"We developed a novel SSL approach to capture global consistency and
pixel-level local consistencies between differently augmented views of the same
images to accommodate downstream discriminative and dense predictive tasks. We
adopted the teacher-student architecture used in previous contrastive SSL
methods. In our method, the global consistency is enforced by aggregating the
compressed representations of augmented views of the same image. The
pixel-level consistency is enforced by pursuing similar representations for the
same pixel in differently augmented views. Importantly, we introduced an
uncertainty-aware context stabilizer to adaptively preserve the context gap
created by the two views from different augmentations. Moreover, we used Monte
Carlo dropout in the stabilizer to measure uncertainty and adaptively balance
the discrepancy between the representations of the same pixels in different
views.",,,arXiv,,,2022-10-02,2022,,,,,,All OA, Green,Preprint,"Zhang, Zhuangzhuang; Zhang, Weixiong","Zhang, Zhuangzhuang (); Zhang, Weixiong ()",,"Zhang, Zhuangzhuang (); Zhang, Weixiong ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151568951,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
660,pub.1149763500,10.48550/arxiv.2207.11553,,,High-Resolution Swin Transformer for Automatic Medical Image  Segmentation,"The Resolution of feature maps is critical for medical image segmentation.
Most of the existing Transformer-based networks for medical image segmentation
are U-Net-like architecture that contains an encoder that utilizes a sequence
of Transformer blocks to convert the input medical image from high-resolution
representation into low-resolution feature maps and a decoder that gradually
recovers the high-resolution representation from low-resolution feature maps.
Unlike previous studies, in this paper, we utilize the network design style
from the High-Resolution Network (HRNet), replace the convolutional layers with
Transformer blocks, and continuously exchange information from the different
resolution feature maps that are generated by Transformer blocks. The newly
Transformer-based network presented in this paper is denoted as High-Resolution
Swin Transformer Network (HRSTNet). Extensive experiments illustrate that
HRSTNet can achieve comparable performance with the state-of-the-art
Transformer-based U-Net-like architecture on Brain Tumor Segmentation(BraTS)
2021 and the liver dataset from Medical Segmentation Decathlon. The code of
HRSTNet will be publicly available at https://github.com/auroua/HRSTNet.",,,arXiv,,,2022-07-23,2022,,,,,,All OA, Green,Preprint,"Wei, Chen; Ren, Shenghan; Guo, Kaitai; Hu, Haihong; Liang, Jimin","Wei, Chen (); Ren, Shenghan (); Guo, Kaitai (); Hu, Haihong (); Liang, Jimin ()",,"Wei, Chen (); Ren, Shenghan (); Guo, Kaitai (); Hu, Haihong (); Liang, Jimin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149763500,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
640,pub.1154444496,10.1101/2023.01.10.22279679,,,Deep learning-based Segmentation of Multi-site Disease in Ovarian Cancer,"Abstract  Purpose To determine if pelvic/ovarian and omental lesions of ovarian cancer can be reliably segmented on computed tomography (CT) using fully automated deep learning-based methods.   Materials and Methods A deep learning model for the two most common disease sites of high grade serous ovarian cancer lesions (pelvis/ovaries and omentum) was developed and compared against the well-established âno-new-Netâ (nnU-Net) framework and unrevised trainee radiologist segmentations. A total of 451 pre-treatment and post neoadjuvant chemotherapy (NACT) CT scans collected from four different institutions were used for training (n=276), hyper-parameter tuning (n=104) and testing (n=71) of the methods. The performance was evaluated using the Dice similarity coefficient (DSC) and compared using a Wilcoxon test on paired results   Results Our model outperforms the nnU-Net framework by a significant margin for both disease (validation: p=1Ã10-4,1.5Ã10-6, test: p=0.004, 0.005) and it does not perform significantly different from a trainee radiologist for the pelvic/ovarian lesions (p=0.392). On an independent test set (n=71), the model achieves a performance of 72Â±19 mean DSC for the pelvic/ovarian and 64Â±24 for the omental lesions.   Conclusion Automated ovarian cancer segmentation on CT using deep neural networks is feasible and achieves performance close to a trainee-level radiologist for pelvic/ovarian lesions.   Summary Deep learning-based models were used to assess whether fully automated segmentation is feasible for the main two disease sites in high grade serous ovarian cancer.   Key Points   First automated approach for pelvic/ovarian and omental ovarian cancer lesion segmentation on CT images.   Automated segmentation of ovarian cancer lesions can be comparable with manual segmentation of trainee radiologists with three years of experience in oncological and gynecological imaging.   Careful hyper-parameter tuning can provide models significantly outperforming strong state-of-the-art baselines.","The authors would like to thank Fabian Isensee for both making the nnU-Net library freely available and easy to use for new datasets. This work was partially supported by The Mark Foundation for Cancer Research and Cancer Research UK Cambridge Centre [C9685/A25177], the Wellcome Trust Innovator Award [RG98755] and the CRUK National Cancer Imaging Translational Accelerator (NCITA) [C42780/A27066]. Additional support was also provided by the National Institute of Health Research (NIHR) Cambridge Biomedical Research Centre (BRC-1215-20014). The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, or the Department of Health and Social Care. This project has been funded in whole or in part with Federal funds from the National Cancer institute, National Institutes of Health, Department of Health and Human Services, under Contract No. 75N91019D00024. CBS acknowledges support from the Leverhulme Trust project on âBreaking the non-convexity barrierâ, the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, the EPSRC grants EP/S026045/1 and EP/T003553/1, EP/N014588/1, EP/T017961/1, European Union Horizon 2020 research and innovation programmes under the Marie Skodowska-Curie grant agreement No. 777826 NoMADS and No. 691070 CHiPS, the Cantab Capital Institute for the Mathematics of Information and the Alan Turing Institute. The work by Ãktem was supported by the Swedish Foundation of Strategic Research under Grants AM13-0049. Microsoft Radiomics was provided to the Addenbrookeâs Hospital (Cambridge University Hospitals NHS Foundation Trust, Cambridge, UK) by the Microsoft InnerEye project.","This work was partially supported by The Mark Foundation for Cancer Research and Cancer Research UK Cambridge Centre [C9685/A25177], the Wellcome Trust Innovator Award [RG98755] and the CRUK National Cancer Imaging Translational Accelerator (NCITA) [C42780/A27066]. Additional support was also provided by the National Institute of Health Research (NIHR) Cambridge Biomedical Research Centre (BRC-1215-20014). The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, or the Department of Health and Social Care. This project has been funded in whole or in part with Federal funds from the National Cancer institute, National Institutes of Health, Department of Health and Human Services, under Contract No. 75N91019D00024. CBS acknowledges support from the Leverhulme Trust project on ""Breaking the non-convexity barrier"", the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, the EPSRC grants EP/S026045/1 and EP/T003553/1, EP/N014588/1, EP/T017961/1, European Union Horizon 2020 research and innovation programmes under the Marie Skodowska-Curie grant agreement No. 777826 NoMADS and No. 691070 CHiPS, the Cantab Capital Institute for the Mathematics of Information and the Alan Turing Institute. The work by Oektem was supported by the Swedish Foundation of Strategic Research under Grants AM13-0049.",medRxiv,,,2023-01-11,2023,2023-01-11,,,,2023.01.10.22279679,All OA, Green,Preprint,"Buddenkotte, Thomas; Rundo, Leonardo; Woitek, Ramona; Sanchez, Lorena Escudero; Beer, Lucian; Crispin-Ortuzar, Mireia; Etmann, Christian; Mukherjee, Subhadip; Bura, Vlad; McCague, Cathal; Sahin, Hilal; Pintican, Roxana; Zerunian, Marta; Allajbeu, Iris; Singh, Naveena; Anju, Sahdev; Havrilesky, Laura; Cohn, David E.; Bateman, Nicholas W.; Conrads, Thomas P.; Darcy, Kathleen M.; Maxwell, G. Larry; Freymann, John B.; Ãktem, Ozan; Brenton, James D.; Sala, Evis; SchÃ¶nlieb, Carola-Bibiane","Buddenkotte, Thomas (Department of Applied Mathematics and Theoretical Physics, University of Cambridge, United Kingdom); Rundo, Leonardo (Department of Radiology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom); Woitek, Ramona (Department of Radiology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom; Department of Medicine, Danube Private University, Krems, Austria); Sanchez, Lorena Escudero (Department of Radiology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom); Beer, Lucian (Department of Radiology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom; Department of Biomedical Imaging and Image-guided Therapy, Medical University Vienna, Austria); Crispin-Ortuzar, Mireia (Cancer Research UK Cambridge Institute, University of Cambridge, United Kingdom; Department of Oncology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom); Etmann, Christian (Department of Applied Mathematics and Theoretical Physics, University of Cambridge, United Kingdom); Mukherjee, Subhadip (Department of Applied Mathematics and Theoretical Physics, University of Cambridge, United Kingdom); Bura, Vlad (Department of Radiology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom; Department of Radiology and Medical Imaging, County Clinical Emergency Hospital, Cluj-Napoca, Romania); McCague, Cathal (Department of Radiology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom); Sahin, Hilal (Department of Radiology, Tepecik Training and Research Hospital, Izmir, Turkey; Department of Radiology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom); Pintican, Roxana (Department of Radiology and Medical Imaging, County Clinical Emergency Hospital, Cluj-Napoca, Romania; Department of Radiology, Iuliu Haâ¡ieganu University of Medicine and Pharmacy, Cluj-Napoca, Romania); Zerunian, Marta (Department of Medical-Surgical and Translational Medicine-Radiology Unit, Sapienza University of Rome, SantâAndrea Hospital, Rome, Italy); Allajbeu, Iris (Department of Radiology, University of Cambridge, Cambridge, United Kingdom); Singh, Naveena (Department of Clinical Pathology, Barts Health NHS Trust, London, UK); Anju, Sahdev (Department of Radiology, Barts Health NHS Trust, London, UK); Havrilesky, Laura (Duke University Medical Center, Durham, NC, USA); Cohn, David E. (Departmant of Obstetrics and Gynecology, Division of Gynecologic Oncology, Ohio State University Comprehensive Cancer Center, Ohio State University College of Medicine, Columbus, OH, USA); Bateman, Nicholas W. (Department of Obstetrics and Gynecology, Gynecologic Cancer Center of Excellence, Walter Reed National Military Medical Center, Uniformed Services University of the Health Sciences, Bethesda, MD, USA; The John P. Murtha Cancer Center, Walter Reed National Military Medical Center, Uniformed Services University, Bethesda, MD, USA); Conrads, Thomas P. (Department of Obstetrics and Gynecology, Gynecologic Cancer Center of Excellence, Walter Reed National Military Medical Center, Uniformed Services University of the Health Sciences, Bethesda, MD, USA; The John P. Murtha Cancer Center, Walter Reed National Military Medical Center, Uniformed Services University, Bethesda, MD, USA; Department of Obstetrics and Gynecology, Inova Fairfax Medical Campus, Falls Church, VA, USA; Inova Center for Personalized Health, Inova Schar Cancer Institute, Falls Church, VA, USA); Darcy, Kathleen M. (Department of Obstetrics and Gynecology, Gynecologic Cancer Center of Excellence, Walter Reed National Military Medical Center, Uniformed Services University of the Health Sciences, Bethesda, MD, USA; The John P. Murtha Cancer Center, Walter Reed National Military Medical Center, Uniformed Services University, Bethesda, MD, USA); Maxwell, G. Larry (Department of Obstetrics and Gynecology, Gynecologic Cancer Center of Excellence, Walter Reed National Military Medical Center, Uniformed Services University of the Health Sciences, Bethesda, MD, USA; The John P. Murtha Cancer Center, Walter Reed National Military Medical Center, Uniformed Services University, Bethesda, MD, USA; Department of Obstetrics and Gynecology, Inova Fairfax Medical Campus, Falls Church, VA, USA); Freymann, John B. (Cancer Imaging Informatics Lab, Frederick National Laboratory for Cancer Research, Frederick, MD, USA); Ãktem, Ozan (Department of Mathematics, KTH Royal Institute of Technology, Sweden); Brenton, James D. (Cancer Research UK Cambridge Institute, University of Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom); Sala, Evis (Department of Radiology, University of Cambridge, Cambridge, United Kingdom; Cancer Research UK Cambridge Centre, University of Cambridge, Cambridge, United Kingdom; Dipartimento di Scienze Radiologiche ed Ematologiche, Universita Cattolica del Sacro Cuore, Rome, Italy; Dipartimento Diagnostica per Immagini, Radioterapia Oncologica ed Ematologia, Policlinico Universitario A. Gemelli IRCCS, Rome, Italy); SchÃ¶nlieb, Carola-Bibiane (Department of Applied Mathematics and Theoretical Physics, University of Cambridge, United Kingdom)","Sala, Evis (University of Cambridge; Cancer Research UK Cambridge Center; Catholic University of the Sacred Heart; Istituti di Ricovero e Cura a Carattere Scientifico)","Buddenkotte, Thomas (University of Cambridge); Rundo, Leonardo (University of Cambridge; Cancer Research UK Cambridge Center); Woitek, Ramona (University of Cambridge; Cancer Research UK Cambridge Center; Danube Private University); Sanchez, Lorena Escudero (University of Cambridge; Cancer Research UK Cambridge Center); Beer, Lucian (University of Cambridge; Cancer Research UK Cambridge Center; Medical University of Vienna); Crispin-Ortuzar, Mireia (Cancer Research UK Cambridge Center; University of Cambridge; Cancer Research UK Cambridge Center); Etmann, Christian (University of Cambridge); Mukherjee, Subhadip (University of Cambridge); Bura, Vlad (University of Cambridge; Cancer Research UK Cambridge Center); McCague, Cathal (University of Cambridge; Cancer Research UK Cambridge Center); Sahin, Hilal (Izmir Tepecik EÄitim ve AraÅtÄ±rma Hastanesi; University of Cambridge; Cancer Research UK Cambridge Center); Pintican, Roxana (Iuliu HaÈieganu University of Medicine and Pharmacy); Zerunian, Marta (Sapienza University of Rome); Allajbeu, Iris (University of Cambridge); Singh, Naveena (Barts Health NHS Trust); Anju, Sahdev (Barts Health NHS Trust); Havrilesky, Laura (Duke University Hospital); Cohn, David E. (The James Cancer Hospital); Bateman, Nicholas W. (Walter Reed National Military Medical Center; Uniformed Services University of the Health Sciences; Uniformed Services University of the Health Sciences; Walter Reed National Military Medical Center); Conrads, Thomas P. (Walter Reed National Military Medical Center; Uniformed Services University of the Health Sciences; Uniformed Services University of the Health Sciences; Walter Reed National Military Medical Center; Inova Fairfax Hospital; Inova Health System); Darcy, Kathleen M. (Walter Reed National Military Medical Center; Uniformed Services University of the Health Sciences; Uniformed Services University of the Health Sciences; Walter Reed National Military Medical Center); Maxwell, G. Larry (Walter Reed National Military Medical Center; Uniformed Services University of the Health Sciences; Uniformed Services University of the Health Sciences; Walter Reed National Military Medical Center; Inova Fairfax Hospital); Freymann, John B. (Frederick National Laboratory for Cancer Research); Ãktem, Ozan (Royal Institute of Technology); Brenton, James D. (Cancer Research UK Cambridge Center; Cancer Research UK Cambridge Center); Sala, Evis (University of Cambridge; Cancer Research UK Cambridge Center; Catholic University of the Sacred Heart; Istituti di Ricovero e Cura a Carattere Scientifico); SchÃ¶nlieb, Carola-Bibiane (University of Cambridge)",0,0,,,https://www.medrxiv.org/content/medrxiv/early/2023/01/11/2023.01.10.22279679.full.pdf,https://app.dimensions.ai/details/publication/pub.1154444496,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
639,pub.1136692163,10.48550/arxiv.2103.13578,,,Test-Time Training for Deformable Multi-Scale Image Registration,"Registration is a fundamental task in medical robotics and is often a crucial
step for many downstream tasks such as motion analysis, intra-operative
tracking and image segmentation. Popular registration methods such as ANTs and
NiftyReg optimize objective functions for each pair of images from scratch,
which are time-consuming for 3D and sequential images with complex
deformations. Recently, deep learning-based registration approaches such as
VoxelMorph have been emerging and achieve competitive performance. In this
work, we construct a test-time training for deep deformable image registration
to improve the generalization ability of conventional learning-based
registration model. We design multi-scale deep networks to consecutively model
the residual deformations, which is effective for high variational
deformations. Extensive experiments validate the effectiveness of multi-scale
deep registration with test-time training based on Dice coefficient for image
segmentation and mean square error (MSE), normalized local cross-correlation
(NLCC) for tissue dense tracking tasks. Two videos are in
https://www.youtube.com/watch?v=NvLrCaqCiAE and
https://www.youtube.com/watch?v=pEA6ZmtTNuQ",,,arXiv,,,2021-03-24,2021,,,,,,All OA, Green,Preprint,"Zhu, Wentao; Huang, Yufang; Xu, Daguang; Qian, Zhen; Fan, Wei; Xie, Xiaohui","Zhu, Wentao (); Huang, Yufang (); Xu, Daguang (); Qian, Zhen (); Fan, Wei (); Xie, Xiaohui ()",,"Zhu, Wentao (); Huang, Yufang (); Xu, Daguang (); Qian, Zhen (); Fan, Wei (); Xie, Xiaohui ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136692163,46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
639,pub.1132266935,10.1007/978-3-030-58452-8_9,,,Synthesize Then Compare: Detecting Failures and Anomalies for Semantic Segmentation,"The ability to detect failures and anomalies are fundamental requirements for building reliable systems for computer vision applications, especially safety-critical applications of semantic segmentation, such as autonomous driving and medical image analysis. In this paper, we systematically study failure and anomaly detection for semantic segmentation and propose a unified framework, consisting of two modules, to address these two related problems. The first module is an image synthesis module, which generates a synthesized image from a segmentation layout map, and the second is a comparison module, which computes the difference between the synthesized image and the input image. We validate our framework on three challenging datasets and improve the state-of-the-arts by large margins, i.e., 6% AUPR-Error on Cityscapes, 7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on StreetHazards anomaly segmentation.","This work was supported by NSF BCS-1827427, the Lustgarten Foundation for Pancreatic Cancer Research and NSFC No. 61672336. We also thank the constructive suggestions from Dr. Chenxi Liu, Qing Liu and Huiyu Wang.",,Lecture Notes in Computer Science,Computer Vision â ECCV 2020,,2020-11-03,2020,2020-11-03,2020,12346,,145-161,All OA, Green,Chapter,"Xia, Yingda; Zhang, Yi; Liu, Fengze; Shen, Wei; Yuille, Alan L.","Xia, Yingda (Johns Hopkins University, Baltimore, USA); Zhang, Yi (Johns Hopkins University, Baltimore, USA); Liu, Fengze (Johns Hopkins University, Baltimore, USA); Shen, Wei (Johns Hopkins University, Baltimore, USA); Yuille, Alan L. (Johns Hopkins University, Baltimore, USA)","Shen, Wei (Johns Hopkins University)","Xia, Yingda (Johns Hopkins University); Zhang, Yi (Johns Hopkins University); Liu, Fengze (Johns Hopkins University); Shen, Wei (Johns Hopkins University); Yuille, Alan L. (Johns Hopkins University)",47,44,,19.99,http://arxiv.org/pdf/2003.08440,https://app.dimensions.ai/details/publication/pub.1132266935,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
639,pub.1153255558,10.1587/transinf.2022edp7058,,,Model-Agnostic Multi-Domain Learning with Domain-Specific Adapters for Action Recognition,"In this paper, we propose a multi-domain learning model for action recognition. The proposed method inserts domain-specific adapters between layers of domain-independent layers of a backbone network. Unlike a multi-head network that switches classification heads only, our model switches not only the heads, but also the adapters for facilitating to learn feature representations universal to multiple domains. Unlike prior works, the proposed method is model-agnostic and doesn't assume model structures unlike prior works. Experimental results on three popular action recognition datasets (HMDB51, UCF101, and Kinetics-400) demonstrate that the proposed method is more effective than a multi-head architecture and more efficient than separately training models for each domain.",,,IEICE Transactions on Information and Systems,,,2022-12-01,2022,,2022-12-01,E105.D,12,2119-2126,All OA, Gold,Article,"Kazuki, OMI; KIMATA, Jun; TAMAKI, Toru","Kazuki, OMI (Nagoya Institute of Technology); KIMATA, Jun (Nagoya Institute of Technology); TAMAKI, Toru (Nagoya Institute of Technology)",,"Kazuki, OMI (Nagoya Institute of Technology); KIMATA, Jun (Nagoya Institute of Technology); TAMAKI, Toru (Nagoya Institute of Technology)",2,2,,,https://www.jstage.jst.go.jp/article/transinf/E105.D/12/E105.D_2022EDP7058/_pdf,https://app.dimensions.ai/details/publication/pub.1153255558,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
639,pub.1142020010,10.1109/icra48506.2021.9561808,,,Test-Time Training for Deformable Multi-Scale Image Registration,"Registration is a fundamental task in medical robotics and is often a crucial step for many downstream tasks such as motion analysis, intra-operative tracking and image segmentation. Popular registration methods such as ANTs and NiftyReg optimize objective functions for each pair of images from scratch, which are time-consuming for 3D and sequential images with complex deformations. Recently, deep learning-based registration approaches such as VoxelMorph have been emerging and achieve competitive performance. In this work, we construct a test-time training for deep deformable image registration to improve the generalization ability of conventional learning-based registration model. We design multi-scale deep networks to consecutively model the residual deformations, which is effective for high variational deformations. Extensive experiments validate the effectiveness of multi-scale deep registration with test-time training based on Dice coefficient for image segmentation and mean square error (MSE), normalized local cross-correlation (NLCC) for tissue dense tracking tasks.",,,,2021 IEEE International Conference on Robotics and Automation (ICRA),,2021-06-05,2021,,2021-06-05,0,,13618-13625,All OA, Green,Proceeding,"Zhu, Wentao; Huang, Yufang; Xu, Daguang; Qian, Zhen; Fan, Wei; Xie, Xiaohui","Zhu, Wentao (Kuaishou Technology); Huang, Yufang (Cornell University); Xu, Daguang (NVIDIA); Qian, Zhen (Tencent); Fan, Wei (Tencent); Xie, Xiaohui (University of California, Irvine)","Xie, Xiaohui (University of California, Irvine)","Zhu, Wentao (); Huang, Yufang (Cornell University); Xu, Daguang (Nvidia (United States)); Qian, Zhen (); Fan, Wei (); Xie, Xiaohui (University of California, Irvine)",6,6,,7.51,http://arxiv.org/pdf/2103.13578,https://app.dimensions.ai/details/publication/pub.1142020010,40 Engineering, 46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
638,pub.1155825406,10.2139/ssrn.4372888,,,Hierarchical Deep Auto-Encoding (Hdae): Effective Information Fusion for Simultaneous Reconstruction and Segmentation of Regionsof-Interest (Roi) from Sub-Sampled Magnetic Resonance (Mr) Images,"â¢ Our Information Fusion (IF) strategy is specially designed for magnetic resonance images (MRI)â¢ Called Hierarchical Deep Autoencoding (HDAE), it learns compact and noise-free MRI representationsâ¢ Anatomical structure and contrast details are preserved based on a low-cost techniqueâ¢ A relevant contribution is provided, advancing the state-of-the-art conventional U-Net",,,SSRN Electronic Journal,,,2023,2023,2023,,,,,All OA, Green,Preprint,"Guido, Rodrigo","Guido, Rodrigo (affiliation not provided to SSRN)",,"Guido, Rodrigo ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155825406,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
638,pub.1145639385,10.1109/wacv51458.2022.00162,,,Uncertainty Learning towards Unsupervised Deformable Medical Image Registration,"Uncertainty estimation in medical image registration enables surgeons to evaluate the operative risk based on the trustworthiness of the registered image data thus of paramount importance for practical clinical applications. Despite the recent promising results obtained with deep unsupervised learning-based registration methods, reasoning about uncertainty of unsupervised registration models remains largely unexplored. In this work, we propose a predictive module to learn the registration and uncertainty in correspondence simultaneously. Our framework introduces empirical randomness and registration error based uncertainty prediction. We systematically assess the performances on two MRI datasets with different ensemble paradigms. Experimental results highlight that our proposed framework significantly improves the registration accuracy and uncertainty compared with the baseline.",,,,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),,2022-01-08,2022,,2022-01-08,0,,1555-1564,Closed,Proceeding,"Gong, Xuan; Khaidem, Luckyson; Zhu, Wentao; Zhang, Baochang; Doermann, David","Gong, Xuan (University at Buffalo); Khaidem, Luckyson (University at Buffalo); Zhu, Wentao (Kuaishou Technology); Zhang, Baochang (Beihang University); Doermann, David (University at Buffalo)","Zhang, Baochang (Beihang University)","Gong, Xuan (University at Buffalo, State University of New York); Khaidem, Luckyson (University at Buffalo, State University of New York); Zhu, Wentao (); Zhang, Baochang (Beihang University); Doermann, David (University at Buffalo, State University of New York)",4,4,,,,https://app.dimensions.ai/details/publication/pub.1145639385,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
637,pub.1127625259,10.1109/wacv45572.2020.9093506,,,NeurReg: Neural Registration and Its Application to Image Segmentation,"Registration is a fundamental task in medical image analysis which can be applied to several tasks including image segmentation, intra-operative tracking, multi-modal image alignment, and motion analysis. Popular registration tools such as ANTs and NiftyReg optimize an objective function for each pair of images from scratch which is time-consuming for large images with complicated deformation. Facilitated by the rapid progress of deep learning, learning-based approaches such as VoxelMorph have been emerging for image registration. These approaches can achieve competitive performance in a fraction of a second on advanced GPUs. In this work, we construct a neural registration framework, called NeurReg, with a hybrid loss of displacement fields and data similarity, which substantially improves the current state-of-the-art of registrations. Within the framework, we simulate various transformations by a registration simulator which generates fixed image and displacement field ground truth for training. Furthermore, we design three segmentation frameworks based on the proposed registration framework: 1) atlas-based segmentation, 2) joint learning of both segmentation and registration tasks, and 3) multi-task learning with atlas-based segmentation as an intermediate feature. Extensive experimental results validate the effectiveness of the proposed NeurReg framework based on various metrics: the endpoint error (EPE) of the predicted displacement field, mean square error (MSE), normalized local cross-correlation (NLCC), mutual information (MI), Dice coefficient, uncertainty estimation, and the interpretability of the segmentation. The proposed NeurReg improves registration accuracy with fast inference speed, which can greatly accelerate related medical image analysis tasks.",,,,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,2020-03-05,2020,,2020-03-05,0,,3606-3615,All OA, Green,Proceeding,"Zhu, Wentao; Myronenko, Andriy; Xu, Ziyue; Li, Wenqi; Roth, Holger; Huang, Yufang; Milletari, Fausto; Xu, Daguang","Zhu, Wentao (NVIDIA); Myronenko, Andriy (NVIDIA); Xu, Ziyue (NVIDIA); Li, Wenqi (NVIDIA); Roth, Holger (NVIDIA); Huang, Yufang (Cornell University); Milletari, Fausto (NVIDIA); Xu, Daguang (NVIDIA)","Zhu, Wentao (Nvidia (United States))","Zhu, Wentao (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Xu, Ziyue (Nvidia (United States)); Li, Wenqi (Nvidia (United States)); Roth, Holger (Nvidia (United States)); Huang, Yufang (Cornell University); Milletari, Fausto (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",22,15,,11.34,http://arxiv.org/pdf/1910.01763,https://app.dimensions.ai/details/publication/pub.1127625259,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
637,pub.1125773116,10.48550/arxiv.2003.08440,,,Synthesize then Compare: Detecting Failures and Anomalies for Semantic  Segmentation,"The ability to detect failures and anomalies are fundamental requirements for
building reliable systems for computer vision applications, especially
safety-critical applications of semantic segmentation, such as autonomous
driving and medical image analysis. In this paper, we systematically study
failure and anomaly detection for semantic segmentation and propose a unified
framework, consisting of two modules, to address these two related problems.
The first module is an image synthesis module, which generates a synthesized
image from a segmentation layout map, and the second is a comparison module,
which computes the difference between the synthesized image and the input
image. We validate our framework on three challenging datasets and improve the
state-of-the-arts by large margins, \emph{i.e.}, 6% AUPR-Error on Cityscapes,
7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on
StreetHazards anomaly segmentation.",,,arXiv,,,2020-03-18,2020,,,,,,All OA, Green,Preprint,"Xia, Yingda; Zhang, Yi; Liu, Fengze; Shen, Wei; Yuille, Alan","Xia, Yingda (); Zhang, Yi (); Liu, Fengze (); Shen, Wei (); Yuille, Alan ()",,"Xia, Yingda (); Zhang, Yi (); Liu, Fengze (); Shen, Wei (); Yuille, Alan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1125773116,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
637,pub.1155160425,10.1109/wacv56688.2023.00365,,,The Fully Convolutional Transformer for Medical Image Segmentation,"We propose a novel transformer, capable of segmenting medical images of varying modalities. Challenges posed by the fine-grained nature of medical image analysis mean that the adaptation of the transformer for their analysis is still at nascent stages. The overwhelming success of the UNet lay in its ability to appreciate the fine-grained nature of the segmentation task, an ability which existing transformer based models do not currently posses. To address this shortcoming, we propose The Fully Convolutional Transformer (FCT), which builds on the proven ability of Convolutional Neural Networks to learn effective image representations, and combines them with the ability of Transformers to effectively capture long-term dependencies in its inputs. The FCT is the first fully convolutional Transformer model in medical imaging literature. It processes its input in two stages, where first, it learns to extract long range semantic dependencies from the input image, and then learns to capture hierarchical global attributes from the features. FCT is compact, accurate and robust. Our results show that it outperforms all existing transformer architectures by large margins across multiple medical image segmentation datasets of varying data modalities without the need for any pre-training. FCT outperforms its immediate competitor on the ACDC dataset by 1.3%, on the Synapse dataset by 4.4%, on the Spleen dataset by 1.2% and on ISIC 2017 dataset by 1.1% on the dice metric, with up to five times fewer parameters. On the ACDC Post-2017-MICCAI-Challenge online test set, our model sets a new state-of-the-art on unseen MRI test cases out-performing large ensemble models as well as nnUNet with considerably fewer parameters. Our code, environments and models will be available via GitHubâ .","C.K. and R.M-S. were supported by UKRI project 104690, iCAIRD, funded by Innovate UK, and from EPSRC grant EP/M01326X/1, QuantIC. R.M-S. and D.H. were also supported by EPSRC grant EP/R018634/1, Closed-loop Data Science. D.H. was also supported by EPSRC grant EP/T017899/1.","C.K. and R.M-S. were supported by UKRI project 104690, iCAIRD, funded by Innovate UK, and from EPSRC grant EP/M01326X/1, QuantIC. R.M-S. and D.H. were also supported by EPSRC grant EP/R018634/1, Closed-loop Data Science. D.H. was also supported by EPSRC grant EP/T017899/1.",,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),,2023-01-07,2023,,2023-01-07,0,,3649-3658,All OA, Green,Proceeding,"Tragakis, Athanasios; Kaul, Chaitanya; Murray-Smith, Roderick; Husmeier, Dirk","Tragakis, Athanasios (Mathematics and Statistics, University of Glasgow, United Kingdom, G12 8QW); Kaul, Chaitanya (School of Computing Science, University of Glasgow, United Kingdom, G12 8RZ); Murray-Smith, Roderick (School of Computing Science, University of Glasgow, United Kingdom, G12 8RZ); Husmeier, Dirk (Mathematics and Statistics, University of Glasgow, United Kingdom, G12 8QW)","Tragakis, Athanasios (University of Glasgow)","Tragakis, Athanasios (University of Glasgow); Kaul, Chaitanya (University of Glasgow); Murray-Smith, Roderick (University of Glasgow); Husmeier, Dirk (University of Glasgow)",1,1,,,http://arxiv.org/pdf/2206.00566,https://app.dimensions.ai/details/publication/pub.1155160425,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
637,pub.1152757518,10.1007/s40032-022-00894-w,,,Error Minimization in Pre-surgical Model of Brain Tumor for 3-D Printing,"Brain cancer treatment options vary in many ways and include surgical procedures, radiotherapy, and chemotherapy. The neurosurgeons typically uses 2-D MRI images for preoperative planning and execution of surgery. The determination of the size of the tumor and its exact location is of utmost necessity for planning the safe surgery so that the normal brain tissues should not be harmed. If this information is available before the surgery, the problems could be minimized. 3-D printing of tumors extracted from medical images also had an inbuilt problem of STL error. The surface models obtained from the software are not 3-D printing ready and have multiple errors such as holes, the inverted orientation of triangles, shells, and border edges. Because of such errors in the STL mesh, faulty or erroneous 3-D printed parts get produced. If such parts are used by the neurosurgeons for preoperative planning of surgery the normal healthy tissues of the brain could permanently get damaged and will harm the normal functioning of the patient and in the worst-case scenario could be life-threatening to the patient. To avoid such problems an attempt has been made in the present study. Medical imaging (MRI) datasets of brain tumors are used to create 3-D printed brain tumors. For extracting the tumor from images an open-source software, 3-D slicer has been used. The surface models directly obtained from the 3-D slicer are then optimized for errors in Autodesk NetFabb software. The surface models before and after optimization are 3-D printed using Makerbot Replicator 3-D printer and are compared. The comparison showed that the physical models directly printed from the surface models obtained from 3-D slicer are missing important information regarding tumor exact size and shape. The optimized surface models obtained from Autodesk Netfabb are having refined mesh with STL errors minimized and thus the physical models printed from these surface models are more accurate and are preserving the exact shape and size of the tumor.","This paper is a revised and expanded version of an article entitled, ââError minimization in pre-surgical model of brain tumor for 3-D printingââ presented in âRecent Advances in Materials, Manufacturing and Machine Learning (RAMMML-2022)â Conference, held at âYeshwantrao Chavan College of Engineeringâ, Nagpur, India, during April 26â27, 2022.",The authors have not disclosed any funding.,Journal of The Institution of Engineers (India): Series C,,,2022-11-15,2022,2022-11-15,2023-02,104,1,101-111,Closed,Article,"Mahatme, Chetan; Giri, Jayant","Mahatme, Chetan (Yeshwantrao Chavan College of Engineering, 441110, Nagpur, India); Giri, Jayant (Yeshwantrao Chavan College of Engineering, 441110, Nagpur, India)","Mahatme, Chetan (Rashtrasant Tukadoji Maharaj Nagpur University)","Mahatme, Chetan (Rashtrasant Tukadoji Maharaj Nagpur University); Giri, Jayant (Rashtrasant Tukadoji Maharaj Nagpur University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152757518,40 Engineering, 4017 Mechanical Engineering,,,,,,,,,,,
635,pub.1125840947,10.48550/arxiv.2003.10299,,,Robust Medical Instrument Segmentation Challenge 2019,"Intraoperative tracking of laparoscopic instruments is often a prerequisite
for computer and robotic-assisted interventions. While numerous methods for
detecting, segmenting and tracking of medical instruments based on endoscopic
video images have been proposed in the literature, key limitations remain to be
addressed: Firstly, robustness, that is, the reliable performance of
state-of-the-art methods when run on challenging images (e.g. in the presence
of blood, smoke or motion artifacts). Secondly, generalization; algorithms
trained for a specific intervention in a specific hospital should generalize to
other interventions or institutions.
  In an effort to promote solutions for these limitations, we organized the
Robust Medical Instrument Segmentation (ROBUST-MIS) challenge as an
international benchmarking competition with a specific focus on the robustness
and generalization capabilities of algorithms. For the first time in the field
of endoscopic image processing, our challenge included a task on binary
segmentation and also addressed multi-instance detection and segmentation. The
challenge was based on a surgical data set comprising 10,040 annotated images
acquired from a total of 30 surgical procedures from three different types of
surgery. The validation of the competing methods for the three tasks (binary
segmentation, multi-instance detection and multi-instance segmentation) was
performed in three different stages with an increasing domain gap between the
training and the test data. The results confirm the initial hypothesis, namely
that algorithm performance degrades with an increasing domain gap. While the
average detection and segmentation quality of the best-performing algorithms is
high, future research should concentrate on detection and segmentation of
small, crossing, moving and transparent instrument(s) (parts).",,,arXiv,,,2020-03-23,2020,,,,,,All OA, Green,Preprint,"Ross, Tobias; Reinke, Annika; Full, Peter M.; Wagner, Martin; Kenngott, Hannes; Apitz, Martin; Hempe, Hellena; Filimon, Diana Mindroc; Scholz, Patrick; Tran, Thuy Nuong; Bruno, Pierangela; ArbelÃ¡ez, Pablo; Bian, Gui-Bin; Bodenstedt, Sebastian; Bolmgren, Jon LindstrÃ¶m; Bravo-SÃ¡nchez, Laura; Chen, Hua-Bin; GonzÃ¡lez, Cristina; Guo, Dong; Halvorsen, PÃ¥l; Heng, Pheng-Ann; Hosgor, Enes; Hou, Zeng-Guang; Isensee, Fabian; Jha, Debesh; Jiang, Tingting; Jin, Yueming; Kirtac, Kadir; Kletz, Sabrina; Leger, Stefan; Li, Zhixuan; Maier-Hein, Klaus H.; Ni, Zhen-Liang; Riegler, Michael A.; Schoeffmann, Klaus; Shi, Ruohua; Speidel, Stefanie; Stenzel, Michael; Twick, Isabell; Wang, Gutai; Wang, Jiacheng; Wang, Liansheng; Wang, Lu; Zhang, Yujie; Zhou, Yan-Jie; Zhu, Lei; Wiesenfarth, Manuel; Kopp-Schneider, Annette; MÃ¼ller-Stich, Beat P.; Maier-Hein, Lena","Ross, Tobias (); Reinke, Annika (); Full, Peter M. (); Wagner, Martin (); Kenngott, Hannes (); Apitz, Martin (); Hempe, Hellena (); Filimon, Diana Mindroc (); Scholz, Patrick (); Tran, Thuy Nuong (); Bruno, Pierangela (); ArbelÃ¡ez, Pablo (); Bian, Gui-Bin (); Bodenstedt, Sebastian (); Bolmgren, Jon LindstrÃ¶m (); Bravo-SÃ¡nchez, Laura (); Chen, Hua-Bin (); GonzÃ¡lez, Cristina (); Guo, Dong (); Halvorsen, PÃ¥l (); Heng, Pheng-Ann (); Hosgor, Enes (); Hou, Zeng-Guang (); Isensee, Fabian (); Jha, Debesh (); Jiang, Tingting (); Jin, Yueming (); Kirtac, Kadir (); Kletz, Sabrina (); Leger, Stefan (); Li, Zhixuan (); Maier-Hein, Klaus H. (); Ni, Zhen-Liang (); Riegler, Michael A. (); Schoeffmann, Klaus (); Shi, Ruohua (); Speidel, Stefanie (); Stenzel, Michael (); Twick, Isabell (); Wang, Gutai (); Wang, Jiacheng (); Wang, Liansheng (); Wang, Lu (); Zhang, Yujie (); Zhou, Yan-Jie (); Zhu, Lei (); Wiesenfarth, Manuel (); Kopp-Schneider, Annette (); MÃ¼ller-Stich, Beat P. (); Maier-Hein, Lena ()",,"Ross, Tobias (); Reinke, Annika (); Full, Peter M. (); Wagner, Martin (); Kenngott, Hannes (); Apitz, Martin (); Hempe, Hellena (); Filimon, Diana Mindroc (); Scholz, Patrick (); Tran, Thuy Nuong (); Bruno, Pierangela (); ArbelÃ¡ez, Pablo (); Bian, Gui-Bin (); Bodenstedt, Sebastian (); Bolmgren, Jon LindstrÃ¶m (); Bravo-SÃ¡nchez, Laura (); Chen, Hua-Bin (); GonzÃ¡lez, Cristina (); Guo, Dong (); Halvorsen, PÃ¥l (); Heng, Pheng-Ann (); Hosgor, Enes (); Hou, Zeng-Guang (); Isensee, Fabian (); Jha, Debesh (); Jiang, Tingting (); Jin, Yueming (); Kirtac, Kadir (); Kletz, Sabrina (); Leger, Stefan (); Li, Zhixuan (); Maier-Hein, Klaus H. (); Ni, Zhen-Liang (); Riegler, Michael A. (); Schoeffmann, Klaus (); Shi, Ruohua (); Speidel, Stefanie (); Stenzel, Michael (); Twick, Isabell (); Wang, Gutai (); Wang, Jiacheng (); Wang, Liansheng (); Wang, Lu (); Zhang, Yujie (); Zhou, Yan-Jie (); Zhu, Lei (); Wiesenfarth, Manuel (); Kopp-Schneider, Annette (); MÃ¼ller-Stich, Beat P. (); Maier-Hein, Lena ()",2,2,,0.95,,https://app.dimensions.ai/details/publication/pub.1125840947,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
618,pub.1144062096,10.48550/arxiv.2112.10325,,,Incremental Cross-view Mutual Distillation for Self-supervised Medical  CT Synthesis,"Due to the constraints of the imaging device and high cost in operation time,
computer tomography (CT) scans are usually acquired with low intra-slice
resolution. Improving the intra-slice resolution is beneficial to the disease
diagnosis for both human experts and computer-aided systems. To this end, this
paper builds a novel medical slice synthesis to increase the between-slice
resolution. Considering that the ground-truth intermediate medical slices are
always absent in clinical practice, we introduce the incremental cross-view
mutual distillation strategy to accomplish this task in the self-supervised
learning manner. Specifically, we model this problem from three different
views: slice-wise interpolation from axial view and pixel-wise interpolation
from coronal and sagittal views. Under this circumstance, the models learned
from different views can distill valuable knowledge to guide the learning
processes of each other. We can repeat this process to make the models
synthesize intermediate slice data with increasing inter-slice resolution. To
demonstrate the effectiveness of the proposed approach, we conduct
comprehensive experiments on a large-scale CT dataset. Quantitative and
qualitative comparison results show that our method outperforms
state-of-the-art algorithms by clear margins.",,,arXiv,,,2021-12-19,2021,,,,,,All OA, Green,Preprint,"Fang, Chaowei; Wang, Liang; Zhang, Dingwen; Xu, Jun; Yuan, Yixuan; Han, Junwei","Fang, Chaowei (); Wang, Liang (); Zhang, Dingwen (); Xu, Jun (); Yuan, Yixuan (); Han, Junwei ()",,"Fang, Chaowei (); Wang, Liang (); Zhang, Dingwen (); Xu, Jun (); Yuan, Yixuan (); Han, Junwei ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144062096,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
618,pub.1151381608,10.1109/cvpr52688.2022.02002,,,Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis,"Due to the constraints of the imaging device and high cost in operation time, computer tomography (CT) scans are usually acquired with low within-slice resolution. Improving the inter-slice resolution is beneficial to the disease diagnosis for both human experts and computer-aided systems. To this end, this paper builds a novel medical slice synthesis to increase the inter-slice resolution. Considering that the groundtruth intermediate medical slices are always absent in clinical practice, we introduce the incremental cross-view mutual distillation strategy to accomplish this task in the self-supervised learning manner. Specifically, we model this problem from three different views: slice-wise interpolation from axial view and pixel-wise interpolation from coronal and sagittal views. Under this circumstance, the models learned from different views can distill valuable knowledge to guide the learning processes of each other. We can repeat this process to make the models synthesize intermediate slice data with increasing between-slice resolution. To demonstrate the effectiveness of the proposed approach, we conduct comprehensive experiments on a large-scale$CT$dataset. Quantitative and qualitative comparison results show that our method outperforms state-of-the-art algorithms by clear margins.","This work was supported in part by Key-Area Research and Development Program of Guangdong Province (No. 2021B0101200001), in part by the National Natural Science Foundation of China (No. 62003256, 61876140, 62027813, U1801265, and U21B2048), in part by Open Research Projects of Zhejiang Lab (No. 2019kD0AD01/010), and in part by MindSpore which is a new deep learning computing framework**https://www.mindspore.cn/.","This work was supported in part by Key-Area Research and Development Program of Guangdong Province (No. 2021B0101200001), in part by the National Natural Science Foundation of China (No. 62003256, 61876140, 62027813, U1801265, and U21B2048), in part by Open Research Projects of Zhejiang Lab (No. 2019kD0AD01/010), and in part by MindSpore which is a new deep learning computing framework*.",,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2022-06-24,2022,,2022-06-24,0,,20645-20654,All OA, Green,Proceeding,"Fang, Chaowei; Wang, Liang; Zhang, Dingwen; Xu, Jun; Yuan, Yixuan; Han, Junwei","Fang, Chaowei (Xidian University); Wang, Liang (Xidian University); Zhang, Dingwen (Northwestern Polytechnical University; Hefei Comprehensive National Science Center); Xu, Jun (Nankai University); Yuan, Yixuan (City University of Hong Kong); Han, Junwei (Northwestern Polytechnical University; Hefei Comprehensive National Science Center)","Zhang, Dingwen (Northwestern Polytechnical University; )","Fang, Chaowei (Xidian University); Wang, Liang (Xidian University); Zhang, Dingwen (Northwestern Polytechnical University); Xu, Jun (Nankai University); Yuan, Yixuan (City University of Hong Kong); Han, Junwei (Northwestern Polytechnical University)",3,3,,,http://arxiv.org/pdf/2112.10325,https://app.dimensions.ai/details/publication/pub.1151381608,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
617,pub.1144479822,10.1109/access.2022.3141021,,,Robust Segmentation Models Using an Uncertainty Slice Sampling-Based Annotation Workflow,"Semantic segmentation neural networks require pixel-level annotations in large quantities to achieve a good performance. In the medical domain, such annotations are expensive because they are time-consuming and require expert knowledge. Active learning optimizes the annotation effort by devising strategies to select cases for labeling that are the most informative to the model. In this work, we propose an uncertainty slice sampling (USS) strategy for the semantic segmentation of 3D medical volumes that selects 2D image slices for annotation and we compare it with various other strategies. We demonstrate the efficiency of USS on a CT liver segmentation task using multisite data. After five iterations, the training data resulting from USS consisted of 2410 slices (4% of all slices in the data pool) compared to 8121(13%), 8641(14%), and 3730(6%) slices for uncertainty volume (UVS), random volume (RVS), and random slice (RSS) sampling, respectively. Despite being trained on the smallest amount of data, the model based on the USS strategy evaluated on 234 test volumes significantly outperformed models trained according to the UVS, RVS, and RSS strategies and achieved a mean Dice index of 0.964, a relative volume error of 4.2%, a mean surface distance of 1.35mm, and a Hausdorff distance of 23.4mm. This was only slightly inferior to 0.967, 3.8%, 1.18mm, and 22.9mm achieved by a model trained on all available data. Our robustness analysis using the 5th percentile of Dice and the 95th percentile of the remaining metrics demonstrated that USS not only resulted in the most robust model compared to other strategies, but also outperformed the model trained on all data according to the 5th percentile of Dice (0.946 vs. 0.945) and the 95th percentile of mean surface distance (1.92mm vs. 2.03mm).","This work was supported by the Fraunhofer-Gesellschaft. The authors would like to thank our clinical partners from Yokohama City University, Yokohama, Japan, StÃ¤dtisches Klinikum Dresden, Dresden, Germany, and Radboud University Clinical Center, Nijmegen, the Netherlands for providing the imaging data used in this study, also would like to thank the organizers of the LiTS and the CHAOS competitions for making the training data publicly available, and also would like to thank Christiane Engel and Andrea Koller for providing manual segmentations.",,IEEE Access,,,2022-01-01,2022,2022-01-06,2022-01-01,10,,4728-4738,All OA, Gold,Article,"Chlebus, Grzegorz; Schenk, Andrea; Hahn, Horst K.; Van Ginneken, Bram; Meine, Hans","Chlebus, Grzegorz (Fraunhofer Institute for Digital Medicine MEVIS, 28359, Bremen, Germany; Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6525, Nijmegen, The Netherlands); Schenk, Andrea (Fraunhofer Institute for Digital Medicine MEVIS, 28359, Bremen, Germany); Hahn, Horst K. (Fraunhofer Institute for Digital Medicine MEVIS, 28359, Bremen, Germany; Department of Computer Science and Electrical Engineering, Jacobs University, 28759, Bremen, Germany); Van Ginneken, Bram (Fraunhofer Institute for Digital Medicine MEVIS, 28359, Bremen, Germany; Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6525, Nijmegen, The Netherlands); Meine, Hans (Fraunhofer Institute for Digital Medicine MEVIS, 28359, Bremen, Germany; Medical Image Computing Group, University of Bremen, 28359, Bremen, Germany)","Chlebus, Grzegorz (Fraunhofer Institute for Digital Medicine; Radboud University Nijmegen Medical Centre)","Chlebus, Grzegorz (Fraunhofer Institute for Digital Medicine; Radboud University Nijmegen Medical Centre); Schenk, Andrea (Fraunhofer Institute for Digital Medicine); Hahn, Horst K. (Fraunhofer Institute for Digital Medicine; Jacobs University); Van Ginneken, Bram (Fraunhofer Institute for Digital Medicine; Radboud University Nijmegen Medical Centre); Meine, Hans (Fraunhofer Institute for Digital Medicine; University of Bremen)",3,3,,,https://ieeexplore.ieee.org/ielx7/6287639/9668973/09672086.pdf,https://app.dimensions.ai/details/publication/pub.1144479822,46 Information and Computing Sciences,,,,,,,,,,,
617,pub.1134892752,10.1007/s12204-021-2264-x,,,Rethinking the Dice Loss for Deep Learning Lesion Segmentation in Medical Images,"Deep learning is widely used for lesion segmentation in medical images due to its breakthrough performance. Loss functions are critical in a deep learning pipeline, and they play important roles in segmenting performance. Dice loss is the most commonly used loss function in medical image segmentation, but it also has some disadvantages. In this paper, we discuss the advantages and disadvantages of the Dice loss function, and group the extensions of the Dice loss according to its improved purpose. The performances of some extensions are compared according to core references. Because different loss functions have different performances in different tasks, automatic loss function selection will be the potential direction in the future.",,,Journal of Shanghai Jiaotong University (Science),,,2021-01-26,2021,2021-01-26,2021-02,26,1,93-102,Closed,Article,"Zhang, Yue; Liu, Shijie; Li, Chunlai; Wang, Jianyu","Zhang, Yue (Key Laboratory of Space Active Opto-Electronics Technology, Shanghai Institute of Technical Physics, Chinese Academy of Sciences, 200083, Shanghai, China; University of Chinese Academy of Sciences, 100049, Beijing, China); Liu, Shijie (Key Laboratory of Space Active Opto-Electronics Technology, Shanghai Institute of Technical Physics, Chinese Academy of Sciences, 200083, Shanghai, China; University of Chinese Academy of Sciences, 100049, Beijing, China; Hangzhou Institute for Advanced Study, University of Chinese Academy of Sciences, 310024, Hangzhou, China); Li, Chunlai (Key Laboratory of Space Active Opto-Electronics Technology, Shanghai Institute of Technical Physics, Chinese Academy of Sciences, 200083, Shanghai, China); Wang, Jianyu (Key Laboratory of Space Active Opto-Electronics Technology, Shanghai Institute of Technical Physics, Chinese Academy of Sciences, 200083, Shanghai, China; University of Chinese Academy of Sciences, 100049, Beijing, China; Hangzhou Institute for Advanced Study, University of Chinese Academy of Sciences, 310024, Hangzhou, China)","Wang, Jianyu (Shanghai Institute of Technical Physics; University of Chinese Academy of Sciences; University of Chinese Academy of Sciences)","Zhang, Yue (Shanghai Institute of Technical Physics; University of Chinese Academy of Sciences); Liu, Shijie (Shanghai Institute of Technical Physics; University of Chinese Academy of Sciences; University of Chinese Academy of Sciences); Li, Chunlai (Shanghai Institute of Technical Physics); Wang, Jianyu (Shanghai Institute of Technical Physics; University of Chinese Academy of Sciences; University of Chinese Academy of Sciences)",12,12,,8.27,,https://app.dimensions.ai/details/publication/pub.1134892752,40 Engineering, 4015 Maritime Engineering,,,,,,,,,,,
617,pub.1149999855,10.1101/2022.08.01.22278193,,,Deep Learning for Automatic Segmentation of Vestibular Schwannoma: A Retrospective Study from Multi-Centre Routine MRI,"Abstract  Objective Automatic segmentation of vestibular schwannoma (VS) from routine clinical MRI can improve clinical workflow, facilitate treatment decisions, and assist patient management. Previously, excellent automatic segmentation results were achieved on datasets of standardised MRI images acquired for stereotactic surgery planning. However, diagnostic clinical datasets are generally more diverse and pose a larger challenge to automatic segmentation algorithms. Here, we show that automatic segmentation of VS on such datasets is also possible with high accuracy.   Methods We acquired a large multi-centre routine clinical (MC-RC) dataset of 168 patients with a single sporadic VS who were referred from 10 medical sites and consecutively seen at a single centre. Up to three longitudinal MRI exams were selected for each patient. Selection rules based on image modality, resolution orientation, and acquisition timepoint were defined to automatically select contrast-enhanced T1-weighted (ceT1w) images (n=130) and T2-weighted images (n=379). Manual ground truth segmentations were obtained in an iterative process in which segmentations were: 1) produced or amended by a specialized company; and 2) reviewed by one of three trained radiologists; and 3) validated by an expert team. Inter- and intra-observer reliability was assessed on a subset of 10 ceT1w and 41 T2w images. The MC-RC dataset was split randomly into 3 nonoverlapping sets for model training, hyperparameter-tuning and testing in proportions 70/10/20%. We applied deep learning to train our VS segmentation model, based on convolutional neural networks (CNN) within the nnU-Net framework.   Results Our model achieved excellent Dice scores when evaluated on the MC-RC testing set as well as the public testing set. On the MC-RC testing set, Dice scores were 90.8Â±4.5% for ceT1w, 86.1Â±11.6% for T2w and 82.3Â±18.4% for a combined ceT1w+T2w input.   Conclusions We developed a model for automatic VS segmentation on diverse multi-centre clinical datasets. The results show that the performance of the framework is comparable to that of human annotators. In contrast, a model trained a publicly available dataset acquired for Gamma Knife stereotactic radiosurgery did not perform well on the MC-RC testing set. The application of our model has the potential to greatly facilitate the management of patients in clinical practice. Our pre-trained segmentation models are made available online. Moreover, we are in the process of making the MC-RC dataset publicly available.",The authors would like to thank Dr Andrew Worth and Gregory Millington for their contributions to the generation of the segmentation ground truth.,"This work was supported by Wellcome Trust (203145Z/16/Z, 203148/Z/16/Z, WT106882), EPSRC (NS/A000050/1, NS/A000049/1) and MRC (MC/PC/180520) funding. Additional funding was provided by Medtronic. TV is also supported by a Medtronic/Royal Academy of Engineering Research Chair (RCSRF1819/7/34).",medRxiv,,,2022-08-02,2022,2022-08-02,,,,2022.08.01.22278193,All OA, Green,Preprint,"Kujawa, Aaron; Dorent, Reuben; Connor, Steve; Thomson, Suki; Ivory, Marina; Vahedi, Ali; Guilhem, Emily; Bradford, Robert; Kitchen, Neil; Bisdas, Sotirios; Ourselin, Sebastien; Vercauteren, Tom; Shapey, Jonathan","Kujawa, Aaron (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, United Kingdom); Dorent, Reuben (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, United Kingdom); Connor, Steve (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, United Kingdom; Department of Neuroradiology, Kingâs College Hospital, London, United Kingdom; Department of Radiology, Guyâs and St Thomasâ Hospital, London, United Kingdom); Thomson, Suki (Department of Neuroradiology, Kingâs College Hospital, London, United Kingdom); Ivory, Marina (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, United Kingdom); Vahedi, Ali (Department of Neuroradiology, Kingâs College Hospital, London, United Kingdom); Guilhem, Emily (Department of Neuroradiology, Kingâs College Hospital, London, United Kingdom); Bradford, Robert (Queen Square Radiosurgery Centre (Gamma Knife), National Hospital for Neurology and Neurosurgery, London, United Kingdom; Department of Neurosurgery, National Hospital for Neurology and Neurosurgery, London, United Kingdom); Kitchen, Neil (Queen Square Radiosurgery Centre (Gamma Knife), National Hospital for Neurology and Neurosurgery, London, United Kingdom; Department of Neurosurgery, National Hospital for Neurology and Neurosurgery, London, United Kingdom); Bisdas, Sotirios (Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, London, United Kingdom); Ourselin, Sebastien (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, United Kingdom); Vercauteren, Tom (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, United Kingdom); Shapey, Jonathan (School of Biomedical Engineering and Imaging Sciences, Kingâs College London, London, United Kingdom; Department of Neurosurgery, Kingâs College Hospital, London, United Kingdom)","Kujawa, Aaron (King's College London)","Kujawa, Aaron (King's College London); Dorent, Reuben (King's College London); Connor, Steve (King's College London; King's College Hospital; St Thomas' Hospital); Thomson, Suki (King's College Hospital); Ivory, Marina (King's College London); Vahedi, Ali (King's College Hospital); Guilhem, Emily (King's College Hospital); Bradford, Robert (National Hospital for Neurology and Neurosurgery; National Hospital for Neurology and Neurosurgery); Kitchen, Neil (National Hospital for Neurology and Neurosurgery; National Hospital for Neurology and Neurosurgery); Bisdas, Sotirios (National Hospital for Neurology and Neurosurgery); Ourselin, Sebastien (King's College London); Vercauteren, Tom (King's College London); Shapey, Jonathan (King's College London; King's College Hospital)",0,0,,,https://www.medrxiv.org/content/medrxiv/early/2022/08/02/2022.08.01.22278193.full.pdf,https://app.dimensions.ai/details/publication/pub.1149999855,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
617,pub.1131140595,10.48550/arxiv.2009.10987,,,Learning Non-Unique Segmentation with Reward-Penalty Dice Loss,"Semantic segmentation is one of the key problems in the field of computer
vision, as it enables computer image understanding. However, most research and
applications of semantic segmentation focus on addressing unique segmentation
problems, where there is only one gold standard segmentation result for every
input image. This may not be true in some problems, e.g., medical applications.
We may have non-unique segmentation annotations as different surgeons may
perform successful surgeries for the same patient in slightly different ways.
To comprehensively learn non-unique segmentation tasks, we propose the
reward-penalty Dice loss (RPDL) function as the optimization objective for deep
convolutional neural networks (DCNN). RPDL is capable of helping DCNN learn
non-unique segmentation by enhancing common regions and penalizing outside
ones. Experimental results show that RPDL improves the performance of DCNN
models by up to 18.4% compared with other loss functions on our collected
surgical dataset.",,,arXiv,,,2020-09-23,2020,,,,,,All OA, Green,Preprint,"He, Jiabo; Erfani, Sarah; Wijewickrema, Sudanthi; O'Leary, Stephen; Ramamohanarao, Kotagiri","He, Jiabo (); Erfani, Sarah (); Wijewickrema, Sudanthi (); O'Leary, Stephen (); Ramamohanarao, Kotagiri ()",,"He, Jiabo (); Erfani, Sarah (); Wijewickrema, Sudanthi (); O'Leary, Stephen (); Ramamohanarao, Kotagiri ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1131140595,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
616,pub.1131294254,10.1109/ijcnn48605.2020.9206940,,,Learning Non-Unique Segmentation with Reward-Penalty Dice Loss,"Semantic segmentation is one of the key problems in the field of computer vision, as it enables computer image understanding. However, most research and applications of semantic segmentation focus on addressing unique segmentation problems, where there is only one gold standard segmentation result for every input image. This may not be true in some problems, e.g., medical applications. We may have non-unique segmentation annotations as different surgeons may perform successful surgeries for the same patient in slightly different ways. To comprehensively learn non-unique segmentation tasks, we propose the reward-penalty Dice loss (RPDL) function as the optimization objective for deep convolutional neural networks (DCNN). RPDL is capable of helping DCNN learn non-unique segmentation by enhancing common regions and penalizing outside ones. Experimental results show that RPDL improves the performance of DCNN models by up to 18.4% compared with other loss functions on our collected surgical dataset.","We truly appreciated great cooperation with 7 anonymous surgeons at the Royal Victorian Eye and Ear Hospital, who helped us perform, validate and grade CM surgeries in the VRTBS simulator. We were grateful for 9 anonymous patients who provided their CT-scan temporal bone images for our research. This research was supported by the Melbourne Research Scholarship. This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200. We truly appreciated great cooperation with 7 anonymous surgeons at the Royal Victorian Eye and Ear Hospital, who helped us perform, validate and grade CM surgeries in the VRTBS simulator. We were grateful for 9 anonymous patients who provided their CT-scan temporal bone images for our research. This research was supported by the Melbourne Research Scholarship. This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200.",,,2020 International Joint Conference on Neural Networks (IJCNN),,2020-07-24,2020,,2020-07-24,0,,1-8,All OA, Green,Proceeding,"He, Jiabo; Erfani, Sarah; Wijewickrema, Sudanthi; OâLeary, Stephen; Ramamohanarao, Kotagiri","He, Jiabo (School of Computing and Information Systems, The University of Melbourne, Australia); Erfani, Sarah (School of Computing and Information Systems, The University of Melbourne, Australia); Wijewickrema, Sudanthi (Department of Otolaryngology, The University of Melbourne, Australia); OâLeary, Stephen (Department of Otolaryngology, The University of Melbourne, Australia); Ramamohanarao, Kotagiri (School of Computing and Information Systems, The University of Melbourne, Australia)","He, Jiabo (University of Melbourne)","He, Jiabo (University of Melbourne); Erfani, Sarah (University of Melbourne); Wijewickrema, Sudanthi (University of Melbourne); OâLeary, Stephen (University of Melbourne); Ramamohanarao, Kotagiri (University of Melbourne)",0,0,,0.0,http://arxiv.org/pdf/2009.10987,https://app.dimensions.ai/details/publication/pub.1131294254,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
616,pub.1150184021,10.2478/ausi-2022-0004,,,U-Net architecture variants for brain tumor segmentation of histogram corrected images,"Abstract In this paper we propose to create an end-to-end brain tumor segmentation system that applies three variants of the well-known U-Net convolutional neural networks. In our results we obtain and analyse the detection performances of U-Net, VGG16-UNet and ResNet-UNet on the BraTS2020 training dataset. Further, we inspect the behavior of the ensemble model obtained as the weighted response of the three CNN models. We introduce essential preprocessing and post-processing steps so as to improve the detection performances. The original images were corrected and the different intensity ranges were transformed into the 8-bit grayscale domain to uniformize the tissue intensities, while preserving the original histogram shapes. For post-processing we apply region connectedness onto the whole tumor and conversion of background pixels into necrosis inside the whole tumor. As a result, we present the Dice scores of our system obtained for WT (whole tumor), TC (tumor core) and ET (enhanced tumor) on the BraTS2020 training dataset.",,,Acta Universitatis Sapientiae Informatica,,,2022-08-01,2022,2022-08-12,2022-08-01,14,1,49-74,All OA, Gold,Article,"Lefkovits, SzidÃ³nia; Lefkovits, LÃ¡szlÃ³","Lefkovits, SzidÃ³nia (George Emil Palade University of Medicine, Pharmacy, Science, and Technology TÃ¢rgu MureÅ, Department of Electrical Engineering and Information Technology); Lefkovits, LÃ¡szlÃ³ (Sapientia Hungarian University of Transylvania, Cluj-Napoca Department of Electrical Engineering, TÃ¢rgu MureÅ)",,"Lefkovits, SzidÃ³nia (); Lefkovits, LÃ¡szlÃ³ (Sapientia Hungarian University of Transylvania)",1,1,,,https://sciendo.com/pdf/10.2478/ausi-2022-0004,https://app.dimensions.ai/details/publication/pub.1150184021,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
616,pub.1142246242,10.48550/arxiv.2110.14795,,,MedMNIST v2 -- A large-scale lightweight benchmark for 2D and 3D  biomedical image classification,"We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of
standardized biomedical images, including 12 datasets for 2D and 6 datasets for
3D. All images are pre-processed into a small size of 28x28 (2D) or 28x28x28
(3D) with the corresponding classification labels so that no background
knowledge is required for users. Covering primary data modalities in biomedical
images, MedMNIST v2 is designed to perform classification on lightweight 2D and
3D images with various dataset scales (from 100 to 100,000) and diverse tasks
(binary/multi-class, ordinal regression, and multi-label). The resulting
dataset, consisting of 708,069 2D images and 10,214 3D images in total, could
support numerous research / educational purposes in biomedical image analysis,
computer vision, and machine learning. We benchmark several baseline methods on
MedMNIST v2, including 2D / 3D neural networks and open-source / commercial
AutoML tools. The data and code are publicly available at
https://medmnist.com/.",,,arXiv,,,2021-10-27,2021,,,,,,All OA, Green,Preprint,"Yang, Jiancheng; Shi, Rui; Wei, Donglai; Liu, Zequan; Zhao, Lin; Ke, Bilian; Pfister, Hanspeter; Ni, Bingbing","Yang, Jiancheng (); Shi, Rui (); Wei, Donglai (); Liu, Zequan (); Zhao, Lin (); Ke, Bilian (); Pfister, Hanspeter (); Ni, Bingbing ()",,"Yang, Jiancheng (); Shi, Rui (); Wei, Donglai (); Liu, Zequan (); Zhao, Lin (); Ke, Bilian (); Pfister, Hanspeter (); Ni, Bingbing ()",1,1,,0.82,,https://app.dimensions.ai/details/publication/pub.1142246242,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
615,pub.1152715054,10.48550/arxiv.2211.06004,,,A Comprehensive Survey of Transformers for Computer Vision,"As a special type of transformer, Vision Transformers (ViTs) are used to
various computer vision applications (CV), such as image recognition. There are
several potential problems with convolutional neural networks (CNNs) that can
be solved with ViTs. For image coding tasks like compression, super-resolution,
segmentation, and denoising, different variants of the ViTs are used. The
purpose of this survey is to present the first application of ViTs in CV. The
survey is the first of its kind on ViTs for CVs to the best of our knowledge.
In the first step, we classify different CV applications where ViTs are
applicable. CV applications include image classification, object detection,
image segmentation, image compression, image super-resolution, image denoising,
and anomaly detection. Our next step is to review the state-of-the-art in each
category and list the available models. Following that, we present a detailed
analysis and comparison of each model and list its pros and cons. After that,
we present our insights and lessons learned for each category. Moreover, we
discuss several open research challenges and future research directions.",,,arXiv,,,2022-11-11,2022,,,,,,All OA, Green,Preprint,"Jamil, Sonain; Piran, Md. Jalil; Kwon, Oh-Jin","Jamil, Sonain (); Piran, Md. Jalil (); Kwon, Oh-Jin ()",,"Jamil, Sonain (); Piran, Md. Jalil (); Kwon, Oh-Jin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152715054,40 Engineering, 4006 Communications Engineering, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,
615,pub.1142083248,10.48550/arxiv.2110.10403,,,AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation,"Recent advances in transformer-based models have drawn attention to exploring
these techniques in medical image segmentation, especially in conjunction with
the U-Net model (or its variants), which has shown great success in medical
image segmentation, under both 2D and 3D settings. Current 2D based methods
either directly replace convolutional layers with pure transformers or consider
a transformer as an additional intermediate encoder between the encoder and
decoder of U-Net. However, these approaches only consider the attention
encoding within one single slice and do not utilize the axial-axis information
naturally provided by a 3D volume. In the 3D setting, convolution on volumetric
data and transformers both consume large GPU memory. One has to either
downsample the image or use cropped local patches to reduce GPU memory usage,
which limits its performance. In this paper, we propose Axial Fusion
Transformer UNet (AFTer-UNet), which takes both advantages of convolutional
layers' capability of extracting detailed features and transformers' strength
on long sequence modeling. It considers both intra-slice and inter-slice
long-range cues to guide the segmentation. Meanwhile, it has fewer parameters
and takes less GPU memory to train than the previous transformer-based models.
Extensive experiments on three multi-organ segmentation datasets demonstrate
that our method outperforms current state-of-the-art methods.",,,arXiv,,,2021-10-20,2021,,,,,,All OA, Green,Preprint,"Yan, Xiangyi; Tang, Hao; Sun, Shanlin; Ma, Haoyu; Kong, Deying; Xie, Xiaohui","Yan, Xiangyi (); Tang, Hao (); Sun, Shanlin (); Ma, Haoyu (); Kong, Deying (); Xie, Xiaohui ()",,"Yan, Xiangyi (); Tang, Hao (); Sun, Shanlin (); Ma, Haoyu (); Kong, Deying (); Xie, Xiaohui ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142083248,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
612,pub.1155070302,10.1016/j.bea.2023.100076,,,Applications of deep learning in disease diagnosis of chest radiographs: A survey on materials and methods,"Recent advances in deep learning have given rise to high performance in image analysis operations in healthcare. Lung diseases are of particular interest, as most can be identified using non-invasive image modalities. Deep learning techniques such as convolutional neural networks, convolution autoencoders, and graph convolutional networks have been implemented in several pulmonary disease identification applications, e.g., lung nodule classification, Covid-19, and pneumonia detection. Various sources of medical images such as X-rays, computed tomography scans, magnetic resonance imaging, and positron emission tomography scans make deep learning techniques favorable to identify lung diseases with great accuracy. This paper discusses state-of-the-art methods that use deep learning on various medical imaging modalities to detect and classify diseases in the lungs. A description of a few publicly available databases is included in this study, along with some distinct deep learning techniques developed in recent times. Furthermore, several challenges and open research areas for pulmonary disease diagnosis using deep learning are discussed. The objective of this work is to direct researchers in the field of diagnosis of lung diseases.","This work has been funded by the Council of Canada (NSERC) Discovery Grants (RGPIN - 2018 - 05523) and (RGPIN - 2019 - 04696), and the University of Windsor, Office of Research Services and Innovation.",,Biomedical Engineering Advances,,,2023-06,2023,,2023-06,5,,100076,All OA, Hybrid,Article,"Modak, Sudipta; Abdel-Raheem, Esam; Rueda, Luis","Modak, Sudipta (University of Windsor, Department of Electrical and Computer Engineering, 401 Sunset Ave, Windsor, ON, N9B 3P4, Canada); Abdel-Raheem, Esam (University of Windsor, Department of Electrical and Computer Engineering, 401 Sunset Ave, Windsor, ON, N9B 3P4, Canada); Rueda, Luis (University of Windsor, School of Computer Science, 401 Sunset Ave, Windsor, ON, N9B 3P4, Canada)","Modak, Sudipta (University of Windsor)","Modak, Sudipta (University of Windsor); Abdel-Raheem, Esam (University of Windsor); Rueda, Luis (University of Windsor)",0,0,,,https://doi.org/10.1016/j.bea.2023.100076,https://app.dimensions.ai/details/publication/pub.1155070302,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,
600,pub.1146382890,10.1016/j.neucom.2021.08.157,,,"Liver, kidney and spleen segmentation from CT scans and MRI with deep learning: A survey","Deep Learning approaches for automatic segmentation of organs from CT scans and MRI are providing promising results, leading towards a revolution in the radiologistsâ workflow. Precise delineations of abdominal organs boundaries reveal fundamental for a variety of purposes: surgical planning, volumetric estimation (e.g. Total Kidney Volume â TKV â assessment in Autosomal Dominant Polycystic Kidney Disease â ADPKD), diagnosis and monitoring of pathologies. Fundamental imaging techniques exploited for these tasks are Computed Tomography (CT) and Magnetic Resonance Imaging (MRI), which enable clinicians to perform 3D analyses of all Regions of Interests (ROIs). In the realm of existing methods for segmentation and classification of these zones, Convolutional Neural Networks (CNNs) are emerging as the reference approach. In the last five years an enormous research effort has been done about the possibility of applying CNNs in Medical Imaging, resulting in more than 8000 documents on Scopus and more than 80000 results on Google Scholar. The high accuracy provided by those systems cannot be denied as motivation of all obtained results, though there are still problems to be addressed with. In this survey, major article databases, as Scopus, for instance, were systematically investigated for different kinds of Deep Learning approaches in segmentation of abdominal organs with a particular focus on liver, kidney and spleen. In this work, approaches are accurately classified, both by relevance of each organ (for instance, segmentation of liver has specific properties, if compared to other organs) and by type of computational approach, as well as the architecture of the employed network. For this purpose, a case study of segmentation for each of these organs is presented.",,,Neurocomputing,,,2022-06,2022,,2022-06,490,,30-53,Closed,Article,"Altini, Nicola; Prencipe, Berardino; Cascarano, Giacomo Donato; Brunetti, Antonio; Brunetti, Gioacchino; Triggiani, Vito; Carnimeo, Leonarda; Marino, Francescomaria; Guerriero, Andrea; Villani, Laura; Scardapane, Arnaldo; Bevilacqua, Vitoantonio","Altini, Nicola (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, Italy); Prencipe, Berardino (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, Italy); Cascarano, Giacomo Donato (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, Italy; Apulian Bioengineering srl, Via delle Violette, 14 - 70026 Modugno (BA), Italy); Brunetti, Antonio (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, Italy; Apulian Bioengineering srl, Via delle Violette, 14 - 70026 Modugno (BA), Italy); Brunetti, Gioacchino (Masmec Biomed SpA, Via delle Violette, 14 - 70026 Modugno (BA), Italy); Triggiani, Vito (Masmec Biomed SpA, Via delle Violette, 14 - 70026 Modugno (BA), Italy); Carnimeo, Leonarda (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, Italy); Marino, Francescomaria (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, Italy); Guerriero, Andrea (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, Italy); Villani, Laura (Interdisciplinary Department of Medicine, Section of Diagnostic Imaging, University of Bari Medical School, 70124 Bari, Italy); Scardapane, Arnaldo (Interdisciplinary Department of Medicine, Section of Diagnostic Imaging, University of Bari Medical School, 70124 Bari, Italy); Bevilacqua, Vitoantonio (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, Italy; Apulian Bioengineering srl, Via delle Violette, 14 - 70026 Modugno (BA), Italy)","Bevilacqua, Vitoantonio (Polytechnic University of Bari; )","Altini, Nicola (Polytechnic University of Bari); Prencipe, Berardino (Polytechnic University of Bari); Cascarano, Giacomo Donato (Polytechnic University of Bari); Brunetti, Antonio (Polytechnic University of Bari); Brunetti, Gioacchino (); Triggiani, Vito (); Carnimeo, Leonarda (Polytechnic University of Bari); Marino, Francescomaria (Polytechnic University of Bari); Guerriero, Andrea (Polytechnic University of Bari); Villani, Laura (University of Bari Aldo Moro); Scardapane, Arnaldo (University of Bari Aldo Moro); Bevilacqua, Vitoantonio (Polytechnic University of Bari)",9,9,,,,https://app.dimensions.ai/details/publication/pub.1146382890,46 Information and Computing Sciences,,,,,,,,,,,,
598,pub.1147813927,10.48550/arxiv.2205.05009,,,Using Deep Learning-based Features Extracted from CT scans to Predict  Outcomes in COVID-19 Patients,"The COVID-19 pandemic has had a considerable impact on day-to-day life.
Tackling the disease by providing the necessary resources to the affected is of
paramount importance. However, estimation of the required resources is not a
trivial task given the number of factors which determine the requirement. This
issue can be addressed by predicting the probability that an infected patient
requires Intensive Care Unit (ICU) support and the importance of each of the
factors that influence it. Moreover, to assist the doctors in determining the
patients at high risk of fatality, the probability of death is also calculated.
For determining both the patient outcomes (ICU admission and death), a novel
methodology is proposed by combining multi-modal features, extracted from
Computed Tomography (CT) scans and Electronic Health Record (EHR) data. Deep
learning models are leveraged to extract quantitative features from CT scans.
These features combined with those directly read from the EHR database are fed
into machine learning models to eventually output the probabilities of patient
outcomes. This work demonstrates both the ability to apply a broad set of deep
learning methods for general quantification of Chest CT scans and the ability
to link these quantitative metrics to patient outcomes. The effectiveness of
the proposed method is shown by testing it on an internally curated dataset,
achieving a mean area under Receiver operating characteristic curve (AUC) of
0.77 on ICU admission prediction and a mean AUC of 0.73 on death prediction
using the best performing classifiers.",,,arXiv,,,2022-05-10,2022,,,,,,All OA, Green,Preprint,"Nuthalapati, Sai Vidyaranya; Vizcaychipi, Marcela; Shah, Pallav; Chudzik, Piotr; Leow, Chee Hau; Yousefi, Paria; Selim, Ahmed; Tait, Keiran; Irving, Ben","Nuthalapati, Sai Vidyaranya (); Vizcaychipi, Marcela (); Shah, Pallav (); Chudzik, Piotr (); Leow, Chee Hau (); Yousefi, Paria (); Selim, Ahmed (); Tait, Keiran (); Irving, Ben ()",,"Nuthalapati, Sai Vidyaranya (); Vizcaychipi, Marcela (); Shah, Pallav (); Chudzik, Piotr (); Leow, Chee Hau (); Yousefi, Paria (); Selim, Ahmed (); Tait, Keiran (); Irving, Ben ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1147813927,42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences,3 Good Health and Well Being,,,,,,,,
598,pub.1152130707,10.1007/978-3-031-19803-8_26,,,Auto-FedRL: Federated Hyperparameter Optimization for Multi-institutional Medical Image Segmentation,"Federated learning (FL) is a distributed machine learning technique that enables collaborative model training while avoiding explicit data sharing. The inherent privacy-preserving property of FL algorithms makes them especially attractive to the medical field. However, in case of heterogeneous client data distributions, standard FL methods are unstable and require intensive hyperparameter tuning to achieve optimal performance. Conventional hyperparameter optimization algorithms are impractical in real-world FL applications as they involve numerous training trials, which are often not affordable with limited compute budgets. In this work, we propose an efficient reinforcement learningÂ (RL)-based federated hyperparameter optimization algorithm, termed Auto-FedRL, in which an online RL agent can dynamically adjust hyperparameters of each client based on the current training progress. Extensive experiments are conducted to investigate different search strategies and RL agents. The effectiveness of the proposed method is validated on a heterogeneous data split of the CIFAR-10 dataset as well as two real-world medical image segmentation datasets for COVID-19 lesion segmentation in chest CT and pancreas segmentation in abdominal CT.",,,Lecture Notes in Computer Science,Computer Vision â ECCV 2022,,2022-10-23,2022,2022-10-23,2022,13681,,437-455,All OA, Green,Chapter,"Guo, Pengfei; Yang, Dong; Hatamizadeh, Ali; Xu, An; Xu, Ziyue; Li, Wenqi; Zhao, Can; Xu, Daguang; Harmon, Stephanie; Turkbey, Evrim; Turkbey, Baris; Wood, Bradford; Patella, Francesca; Stellato, Elvira; Carrafiello, Gianpaolo; Patel, Vishal M.; Roth, Holger R.","Guo, Pengfei (Johns Hopkins University, Baltimore, USA); Yang, Dong (NVIDIA, Santa Clara, USA); Hatamizadeh, Ali (NVIDIA, Santa Clara, USA); Xu, An (University of Pittsburgh, Pittsburgh, USA); Xu, Ziyue (NVIDIA, Santa Clara, USA); Li, Wenqi (NVIDIA, Santa Clara, USA); Zhao, Can (NVIDIA, Santa Clara, USA); Xu, Daguang (NVIDIA, Santa Clara, USA); Harmon, Stephanie (National Cancer Institute, Bethesda, USA); Turkbey, Evrim (National Institutes of Health, Bethesda, USA); Turkbey, Baris (National Cancer Institute, Bethesda, USA); Wood, Bradford (National Institutes of Health, Bethesda, USA); Patella, Francesca (ASST Santi Paolo e Carlo, Milan, Italy); Stellato, Elvira (University of Milan, Milan, Italy); Carrafiello, Gianpaolo (University of Milan, Milan, Italy); Patel, Vishal M. (Johns Hopkins University, Baltimore, USA); Roth, Holger R. (NVIDIA, Santa Clara, USA)","Guo, Pengfei (Johns Hopkins University)","Guo, Pengfei (Johns Hopkins University); Yang, Dong (Nvidia (United States)); Hatamizadeh, Ali (Nvidia (United States)); Xu, An (University of Pittsburgh); Xu, Ziyue (Nvidia (United States)); Li, Wenqi (Nvidia (United States)); Zhao, Can (Nvidia (United States)); Xu, Daguang (Nvidia (United States)); Harmon, Stephanie (National Cancer Institute); Turkbey, Evrim (National Institutes of Health); Turkbey, Baris (National Cancer Institute); Wood, Bradford (National Institutes of Health); Patella, Francesca (); Stellato, Elvira (University of Milan); Carrafiello, Gianpaolo (University of Milan); Patel, Vishal M. (Johns Hopkins University); Roth, Holger R. (Nvidia (United States))",4,4,,,http://arxiv.org/pdf/2203.06338,https://app.dimensions.ai/details/publication/pub.1152130707,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
598,pub.1151381626,10.1109/cvpr52688.2022.02020,,,Closing the Generalization Gap of Cross-silo Federated Medical Image Segmentation,"Cross-silo federated learning (FL) has attracted much attention in medical imaging analysis with deep learning in recent years as it can resolve the critical issues of insufficient data, data privacy, and training efficiency. However, there can be a generalization gap between the model trained from FL and the one from centralized training. This important issue comes from the non-iid data distribution of the local data in the participating clients and is well-known as client drift. In this work, we propose a novel training frame-work FedSM to avoid the client drift issue and successfully close the generalization gap compared with the centralized training for medical image segmentation tasks for the first time. We also propose a novel personalized FL objective formulation and a new method SoftPull to solve it in our proposed framework FedSM. We conduct rigorous theoretical analysis to guarantee its convergence for optimizing the non-convex smooth objective function. Real-world medical image segmentation experiments using deep FL validate the motivations and effectiveness of our proposed method.",,"A.X. and H.H. were partially supported by NSF IIS 1845666, 1852606, 1838627, 1837956, 1956002, IIA 2040588. Implementation of this work is available at https://github.com/NVIDIA/NVFlare/examples/FedSM",,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2022-06-24,2022,,2022-06-24,0,,20834-20843,All OA, Green,Proceeding,"Xu, An; Li, Wenqi; Guo, Pengfei; Yang, Dong; Roth, Holger; Hatamizadeh, Ali; Zhao, Can; Xu, Daguang; Huang, Heng; Xu, Ziyue","Xu, An (Univ. of Pittsburgh); Li, Wenqi (NVIDIA); Guo, Pengfei (Johns Hopkins University); Yang, Dong (NVIDIA); Roth, Holger (NVIDIA); Hatamizadeh, Ali (NVIDIA); Zhao, Can (NVIDIA); Xu, Daguang (NVIDIA); Huang, Heng (Univ. of Pittsburgh); Xu, Ziyue (NVIDIA)",,"Xu, An (University of Pittsburgh); Li, Wenqi (Nvidia (United States)); Guo, Pengfei (Johns Hopkins University); Yang, Dong (Nvidia (United States)); Roth, Holger (Nvidia (United States)); Hatamizadeh, Ali (Nvidia (United States)); Zhao, Can (Nvidia (United States)); Xu, Daguang (Nvidia (United States)); Huang, Heng (University of Pittsburgh); Xu, Ziyue (Nvidia (United States))",4,4,,,http://arxiv.org/pdf/2203.10144,https://app.dimensions.ai/details/publication/pub.1151381626,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
598,pub.1146283119,10.48550/arxiv.2203.06338,,,Auto-FedRL: Federated Hyperparameter Optimization for  Multi-institutional Medical Image Segmentation,"Federated learning (FL) is a distributed machine learning technique that
enables collaborative model training while avoiding explicit data sharing. The
inherent privacy-preserving property of FL algorithms makes them especially
attractive to the medical field. However, in case of heterogeneous client data
distributions, standard FL methods are unstable and require intensive
hyperparameter tuning to achieve optimal performance. Conventional
hyperparameter optimization algorithms are impractical in real-world FL
applications as they involve numerous training trials, which are often not
affordable with limited compute budgets. In this work, we propose an efficient
reinforcement learning (RL)-based federated hyperparameter optimization
algorithm, termed Auto-FedRL, in which an online RL agent can dynamically
adjust hyperparameters of each client based on the current training progress.
Extensive experiments are conducted to investigate different search strategies
and RL agents. The effectiveness of the proposed method is validated on a
heterogeneous data split of the CIFAR-10 dataset as well as two real-world
medical image segmentation datasets for COVID-19 lesion segmentation in chest
CT and pancreas segmentation in abdominal CT.",,,arXiv,,,2022-03-11,2022,,,,,,All OA, Green,Preprint,"Guo, Pengfei; Yang, Dong; Hatamizadeh, Ali; Xu, An; Xu, Ziyue; Li, Wenqi; Zhao, Can; Xu, Daguang; Harmon, Stephanie; Turkbey, Evrim; Turkbey, Baris; Wood, Bradford; Patella, Francesca; Stellato, Elvira; Carrafiello, Gianpaolo; Patel, Vishal M.; Roth, Holger R.","Guo, Pengfei (); Yang, Dong (); Hatamizadeh, Ali (); Xu, An (); Xu, Ziyue (); Li, Wenqi (); Zhao, Can (); Xu, Daguang (); Harmon, Stephanie (); Turkbey, Evrim (); Turkbey, Baris (); Wood, Bradford (); Patella, Francesca (); Stellato, Elvira (); Carrafiello, Gianpaolo (); Patel, Vishal M. (); Roth, Holger R. ()",,"Guo, Pengfei (); Yang, Dong (); Hatamizadeh, Ali (); Xu, An (); Xu, Ziyue (); Li, Wenqi (); Zhao, Can (); Xu, Daguang (); Harmon, Stephanie (); Turkbey, Evrim (); Turkbey, Baris (); Wood, Bradford (); Patella, Francesca (); Stellato, Elvira (); Carrafiello, Gianpaolo (); Patel, Vishal M. (); Roth, Holger R. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146283119,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
596,pub.1142235195,10.1007/s00371-021-02315-y,,,PRNet: polar regression network for medical image segmentation,"High-performance segmentation of medical images, though important for automatic clinical diagnosis, remains a challenging problem. In this paper, we propose a novel polar regression network (PRNet) for the segmentation of oval-shaped targets in medical images. Through polar representation, the segmentation problem is formulated and decomposed into two sub-problems of center map estimation and ray length regression. The center map estimation is supervised by the probability center map, which is pre-generated by the contour-based algorithm. The ray length regression further leverages the center-attention polar loss and projection distance loss, in order to emphasize pixels in high probability in the center mask and capitalize on the implicit shape information. Experimental results demonstrate the effectiveness of our approach on the segmentation of oval targets in medical images.",,The authors did not receive support from any organization for the submitted work.,The Visual Computer,,,2021-10-28,2021,2021-10-28,2023-01,39,1,87-98,Closed,Article,"Qian, Xiaoxiao; Quan, Hongyan; Wu, Min","Qian, Xiaoxiao (School of Software Engineering, East China Normal University, 200062, Shanghai, China); Quan, Hongyan (School of Computer Science and Technology, East China Normal University, 200062, Shanghai, China); Wu, Min (School of Software Engineering, East China Normal University, 200062, Shanghai, China)","Quan, Hongyan (East China Normal University)","Qian, Xiaoxiao (East China Normal University); Quan, Hongyan (East China Normal University); Wu, Min (East China Normal University)",1,1,,0.77,,https://app.dimensions.ai/details/publication/pub.1142235195,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
595,pub.1145639555,10.1109/wacv51458.2022.00333,,,AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation,"Recent advances in transformer-based models have drawn attention to exploring these techniques in medical image segmentation, especially in conjunction with the UNet model (or its variants), which has shown great success in medical image segmentation, under both 2D and 3D settings. Current 2D based methods either directly replace convolutional layers with pure transformers or consider a transformer as an additional intermediate encoder between the encoder and decoder of U-Net. However, these approaches only consider the attention encoding within one single slice and do not utilize the axial-axis information naturally provided by a 3D volume. In the 3D setting, convolution on volumetric data and transformers both consume large GPU memory. One has to either downsample the image or use cropped local patches to reduce GPU memory usage, which limits its performance. In this paper, we propose Axial Fusion Transformer UNet (AFTer-UNet), which takes both advantages of convolutional layersâ capability of extracting detailed features and transformersâ strength on long sequence modeling. It considers both intra-slice and inter-slice long-range cues to guide the segmentation. Meanwhile, it has fewer parameters and takes less GPU memory to train than the previous transformer-based models. Extensive experiments on three multi-organ segmentation datasets demonstrate that our method outperforms current state-of-the-art methods.",,,,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),,2022-01-08,2022,,2022-01-08,0,,3270-3280,All OA, Green,Proceeding,"Yan, Xiangyi; Tang, Hao; Sun, Shanlin; Ma, Haoyu; Kong, Deying; Xie, Xiaohui","Yan, Xiangyi (University of California, Irvine); Tang, Hao (University of California, Irvine); Sun, Shanlin (University of California, Irvine); Ma, Haoyu (University of California, Irvine); Kong, Deying (University of California, Irvine); Xie, Xiaohui (University of California, Irvine)","Yan, Xiangyi (University of California, Irvine)","Yan, Xiangyi (University of California, Irvine); Tang, Hao (University of California, Irvine); Sun, Shanlin (University of California, Irvine); Ma, Haoyu (University of California, Irvine); Kong, Deying (University of California, Irvine); Xie, Xiaohui (University of California, Irvine)",35,35,,,http://arxiv.org/pdf/2110.10403,https://app.dimensions.ai/details/publication/pub.1145639555,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
581,pub.1153624193,10.1016/j.artmed.2022.102476,,,Uncertainty-guided mutual consistency learning for semi-supervised medical image segmentation,"Medical image segmentation is a fundamental and critical step in many clinical approaches. Semi-supervised learning has been widely applied to medical image segmentation tasks since it alleviates the heavy burden of acquiring expert-examined annotations and takes the advantage of unlabeled data which is much easier to acquire. Although consistency learning has been proven to be an effective approach by enforcing an invariance of predictions under different distributions, existing approaches cannot make full use of region-level shape constraint and boundary-level distance information from unlabeled data. In this paper, we propose a novel uncertainty-guided mutual consistency learning framework to effectively exploit unlabeled data by integrating intra-task consistency learning from up-to-date predictions for self-ensembling and cross-task consistency learning from task-level regularization to exploit geometric shape information. The framework is guided by the estimated segmentation uncertainty of models to select out relatively certain predictions for consistency learning, so as to effectively exploit more reliable information from unlabeled data. Experiments on two publicly available benchmark datasets showed that: (1) Our proposed method can achieve significant performance improvement by leveraging unlabeled data, with up to 4.13% and 9.82% in Dice coefficient compared to supervised baseline on left atrium segmentation and brain tumor segmentation, respectively. (2) Compared with other semi-supervised segmentation methods, our proposed method achieve better segmentation performance under the same backbone network and task settings on both datasets, demonstrating the effectiveness and robustness of our method and potential transferability for other medical image segmentation tasks.","This work is supported in part by the National Key Research and Development Program of China (2016YFF0201002), and in part by the University Synergy Innovation Program of Anhui Province (GXXT-2019-044).",,Artificial Intelligence in Medicine,,,2023-04,2023,,2023-04,138,,102476,All OA, Green,Article,"Zhang, Yichi; Jiao, Rushi; Liao, Qingcheng; Li, Dongyang; Zhang, Jicong","Zhang, Yichi (School of Biological Science and Medical Engineering, Beihang University, Beijing, China); Jiao, Rushi (School of Biological Science and Medical Engineering, Beihang University, Beijing, China); Liao, Qingcheng (School of Biological Science and Medical Engineering, Beihang University, Beijing, China); Li, Dongyang (School of Biological Science and Medical Engineering, Beihang University, Beijing, China); Zhang, Jicong (School of Biological Science and Medical Engineering, Beihang University, Beijing, China; Hefei Innovation Research Institute, Beihang University, Hefei, China; Beijing Advanced Innovation Centre for Biomedical Engineering, Beijing, China)","Zhang, Jicong (Beihang University; Beihang University; )","Zhang, Yichi (Beihang University); Jiao, Rushi (Beihang University); Liao, Qingcheng (Beihang University); Li, Dongyang (Beihang University); Zhang, Jicong (Beihang University; Beihang University)",0,0,,,http://arxiv.org/pdf/2112.02508,https://app.dimensions.ai/details/publication/pub.1153624193,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
580,pub.1152268495,10.21203/rs.3.rs-2147455/v1,,,Energy-efficient high-fidelity image reconstruction with memristor arrays for medical diagnosis,"Medical imaging is an important tool to make accurate medical diagnosis and disease intervention. Current medical image reconstruction algorithms mainly run on Si-based digital processors with von Neumann architecture, which faces critical challenges to process massive amount of data for high-speed and high-quality imaging. Here, we present a memristive image reconstructor (MIR) to greatly accelerate image reconstruction with discrete Fourier transformation (DFT) by computing-in-memory (CIM) with memristor. To implement DFT on memristor arrays efficiently, we proposed a high-accuracy quasi-analogue mapping (QAM) method and generic complex matrix transfer (CMT) scheme, to improve the mapping precision and transfer efficiency, respectively. With these two strategies, we used MIR to demonstrate high-fidelity magnetic resonance imaging (MRI) and computed tomography (CT) image reconstructions, achieving software-equivalent qualities with peak signal-to-noise ratios (PSNR) of 40.88 dB and 22.38 dB, respectively. The reconstructed images were then segmented using a popular nnU-Net algorithm to further evaluate the reconstruction quality. For the MRI task, the final DICE scores were 0.979 and 0.980 for MIR and software, respectively; while for the CT task, the DICE scores were 0.977 and 0.985 for MIR and software, respectively. These results validated the feasibility of using memristor-reconstructed images for medical diagnosis. Furthermore, our MIR also exhibited more than 153Ã and 79Ã improvements in energy efficiency and normalized image reconstruction speed, respectively, compared to graphics processing unit (GPU). This work demonstrates MIR as a promising platform for high-fidelity image reconstruction for future medical diagnosis, and also largely extends the application of memristor-based CIM beyond artificial neural networks.",,,Research Square,,,2022-10-27,2022,2022-10-27,,,,,All OA, Green,Preprint,"Zhao, Han; Liu, Zhengwu; Tang, Jianshi; Gao, Bin; Qin, Qi; Li, Jiaming; Zhou, Ying; Yao, Peng; Xi, Yue; Lin, Yudeng; Qian, He; Wu, Huaqiang","Zhao, Han (Tsinghua University); Liu, Zhengwu (Tsinghua University); Tang, Jianshi (Tsinghua University); Gao, Bin (Tsinghua University); Qin, Qi (); Li, Jiaming (Tsinghua University); Zhou, Ying (Tsinghua University); Yao, Peng (Tsinghua University); Xi, Yue (Tsinghua University); Lin, Yudeng (Tsinghua University); Qian, He (Tsinghua University); Wu, Huaqiang (Tsinghua University)",,"Zhao, Han (Tsinghua University); Liu, Zhengwu (Tsinghua University); Tang, Jianshi (Tsinghua University); Gao, Bin (Tsinghua University); Qin, Qi (); Li, Jiaming (Tsinghua University); Zhou, Ying (Tsinghua University); Yao, Peng (Tsinghua University); Xi, Yue (Tsinghua University); Lin, Yudeng (Tsinghua University); Qian, He (Tsinghua University); Wu, Huaqiang (Tsinghua University)",0,0,,,https://www.researchsquare.com/article/rs-2147455/latest.pdf,https://app.dimensions.ai/details/publication/pub.1152268495,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,7 Affordable and Clean Energy,,,,,,,,,
580,pub.1151027708,10.1016/j.bspc.2022.104173,,,RTUNet: Residual transformer UNet specifically for pancreas segmentation,"Accurate pancreas segmentation is crucial for the diagnostic assessment of pancreatic cancer. However, large position changes, high variability in shape and size, and the extremely blurred boundary make the task of pancreas segmentation challenging. To alleviate these challenges, we propose the residual transformer UNet (RTUNet) to fit the nature of the pancreas. Specifically, a residual transformer block is implemented to extract multi-scale features from a global perspective which captures high variabilities in the pancreas position. In addition, a dual convolutional down-sampling strategy is leveraged to obtain precise shape and size features of the pancreas in a large receptive field which prevents the loss of information. We finally propose a dice hausdorff distance loss that makes the network focus on the pancreas boundary. Through extensive experiments on the public NIH dataset, we achieved a dice similarity coefficient (DSC) of 86.25%, which outperforms the state-of-the-art DSC of 85.49%. In addition, our method surpasses the baselines by more than 3.0% on DSC and improves the min DSC by 2.93%. Furthermore, ablation studies are also performed to prove the effectiveness of each proposed module.","This work was supported by the National Natural Science Foundation of China (61772242, 62276116, 61976106, 61572239), the China Postdoctoral Science Foundation (2017M611737), the Six Talent Peaks Project in Jiangsu Province (DZXX-122), the Jiangsu Province emergency management science and technology project (YJGL-TG-2020-8), and the key research and development plan of Zhenjiang City (SH2020011).",,Biomedical Signal Processing and Control,,,2023-01,2023,,2023-01,79,,104173,Closed,Article,"Qiu, Chengjian; Liu, Zhe; Song, Yuqing; Yin, Jing; Han, Kai; Zhu, Yan; Liu, Yi; Sheng, Victor S.","Qiu, Chengjian (School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China); Liu, Zhe (School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China); Song, Yuqing (School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China); Yin, Jing (School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China); Han, Kai (School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China); Zhu, Yan (Department of Radiology, Affiliated Hospital of Jiangsu University, Zhenjiang 212001, China); Liu, Yi (School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China); Sheng, Victor S. (Department of Computer Science,Texas Tech University, Lubbock, TX, USA)","Liu, Zhe (Jiangsu University)","Qiu, Chengjian (Jiangsu University); Liu, Zhe (Jiangsu University); Song, Yuqing (Jiangsu University); Yin, Jing (Jiangsu University); Han, Kai (Jiangsu University); Zhu, Yan (Affiliated Hospital of Jiangsu University); Liu, Yi (Jiangsu University); Sheng, Victor S. (Texas Tech University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1151027708,"30 Agricultural, Veterinary and Food Sciences; 3006 Food Sciences; 40 Engineering; 4003 Biomedical Engineering",,,,,,,,,,,,
580,pub.1143667336,10.48550/arxiv.2112.02508,,,Uncertainty-Guided Mutual Consistency Learning for Semi-Supervised  Medical Image Segmentation,"Medical image segmentation is a fundamental and critical step in many
clinical approaches. Semi-supervised learning has been widely applied to
medical image segmentation tasks since it alleviates the heavy burden of
acquiring expert-examined annotations and takes the advantage of unlabeled data
which is much easier to acquire. Although consistency learning has been proven
to be an effective approach by enforcing an invariance of predictions under
different distributions, existing approaches cannot make full use of
region-level shape constraint and boundary-level distance information from
unlabeled data. In this paper, we propose a novel uncertainty-guided mutual
consistency learning framework to effectively exploit unlabeled data by
integrating intra-task consistency learning from up-to-date predictions for
self-ensembling and cross-task consistency learning from task-level
regularization to exploit geometric shape information. The framework is guided
by the estimated segmentation uncertainty of models to select out relatively
certain predictions for consistency learning, so as to effectively exploit more
reliable information from unlabeled data. Experiments on two publicly available
benchmark datasets showed that: 1) Our proposed method can achieve significant
performance improvement by leveraging unlabeled data, with up to 4.13% and
9.82% in Dice coefficient compared to supervised baseline on left atrium
segmentation and brain tumor segmentation, respectively. 2) Compared with other
semi-supervised segmentation methods, our proposed method achieve better
segmentation performance under the same backbone network and task settings on
both datasets, demonstrating the effectiveness and robustness of our method and
potential transferability for other medical image segmentation tasks.",,,arXiv,,,2021-12-05,2021,,,,,,All OA, Green,Preprint,"Zhang, Yichi; Jiao, Rushi; Liao, Qingcheng; Li, Dongyang; Zhang, Jicong","Zhang, Yichi (); Jiao, Rushi (); Liao, Qingcheng (); Li, Dongyang (); Zhang, Jicong ()",,"Zhang, Yichi (); Jiao, Rushi (); Liao, Qingcheng (); Li, Dongyang (); Zhang, Jicong ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143667336,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
579,pub.1148728432,10.48550/arxiv.2206.07156,,,Federated Multi-organ Segmentation with Partially Labeled Data,"Federated learning is an emerging paradigm allowing large-scale decentralized
learning without sharing data across different data owners, which helps address
the concern of data privacy in medical image analysis. However, the requirement
for label consistency across clients by the existing methods largely narrows
its application scope. In practice, each clinical site may only annotate
certain organs of interest with partial or no overlap with other sites.
Incorporating such partially labeled data into a unified federation is an
unexplored problem with clinical significance and urgency. This work tackles
the challenge by using a novel federated multi-encoding U-Net (Fed-MENU) method
for multi-organ segmentation. In our method, a multi-encoding U-Net (MENU-Net)
is proposed to extract organ-specific features through different encoding
sub-networks. Each sub-network can be seen as an expert of a specific organ and
trained for that client. Moreover, to encourage the organ-specific features
extracted by different sub-networks to be informative and distinctive, we
regularize the training of the MENU-Net by designing an auxiliary generic
decoder (AGD). Extensive experiments on four public datasets show that our
Fed-MENU method can effectively obtain a federated learning model using the
partially labeled datasets with superior performance to other models trained by
either localized or centralized learning methods. Source code will be made
publicly available at the time of paper publication.",,,arXiv,,,2022-06-14,2022,,,,,,All OA, Green,Preprint,"Xu, Xuanang; Yan, Pingkun","Xu, Xuanang (); Yan, Pingkun ()",,"Xu, Xuanang (); Yan, Pingkun ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148728432,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
579,pub.1148981827,10.1007/s00530-022-00963-1,,,FPF-Net: feature propagation and fusion based on attention mechanism for pancreas segmentation,"Automatic organ segmentation is a prerequisite step for computer-assisted diagnosis (CAD) in clinical application, which can assist in diabetes inspection, organic cancer diagnosis, surgical planning, etc. However, segmenting tiny organs like the pancreas is very challenging. Despite the success of convolutional neural networks (CNN) in automatic pancreas segmentation, the loss of the shape features impedes progress in clinical applications. Therefore, a novel pancreas segmentation network is proposed to extract features in a propagation and fusion manner, named FPF-Net. Firstly, the low-level features and high-level features are combined progressively to preserve and propagate the shape features of the pancreas. Secondly, instead of context-unaware addition or concatenation, we adopt attentional feature fusion (AFF) to alleviate the problems caused by the shape diversity and small size of the pancreas. Finally, a module consisting of Coordinate and multi-scale spatial attention (CMSA) is designed to exploit long-range dependencies and multi-scale spatial features. This module is used to extract salient information for pancreas segmentation. Experimental results validated on two pancreas datasets and a spleen dataset justify the superiority and generalization ability of our method and guarantee the reliability of our approach in clinical application.","This research is supported by the National Key Research and Development Program of China (2018YFB0804202, 2018YFB0804203), Regional Joint Fund of NSFC (U19A2057), the National Natural Science Foundation of China (61876070), Jilin University âInterdisciplinary Integration and Innovationâ Young Scholars Free Exploration Project (JLUXKJC2021QZ01), Jilin Province Science and Technology Development Plan Project (20190303134SF), Anhui University Collaborative Innovation Project Subproject (GXXT-2021-008).",,Multimedia Systems,,,2022-06-26,2022,2022-06-26,2023-04,29,2,525-538,Closed,Article,"Chen, Haipeng; Liu, Yunjie; Shi, Zenan","Chen, Haipeng (College of Computer Science and Technology, Jilin University, 130012, Changchun, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, 130012, Changchun, China); Liu, Yunjie (College of Computer Science and Technology, Jilin University, 130012, Changchun, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, 130012, Changchun, China); Shi, Zenan (College of Computer Science and Technology, Jilin University, 130012, Changchun, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, 130012, Changchun, China)","Shi, Zenan (Jilin University; Jilin University)","Chen, Haipeng (Jilin University; Jilin University); Liu, Yunjie (Jilin University; Jilin University); Shi, Zenan (Jilin University; Jilin University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148981827,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,,
579,pub.1148746402,10.31265/usps.195,,,hAIst Conference: Health-related Artificial Intelligence in Stavanger,,,,Papers from UiS,,,2022-06-07,2022,2022-06-07,2022-06,40,,,Closed,Monograph,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1148746402,,,,,,,,,,,,,
579,pub.1133613695,10.48550/arxiv.2012.09279,,,Spatial Context-Aware Self-Attention Model For Multi-Organ Segmentation,"Multi-organ segmentation is one of most successful applications of deep
learning in medical image analysis. Deep convolutional neural nets (CNNs) have
shown great promise in achieving clinically applicable image segmentation
performance on CT or MRI images. State-of-the-art CNN segmentation models apply
either 2D or 3D convolutions on input images, with pros and cons associated
with each method: 2D convolution is fast, less memory-intensive but inadequate
for extracting 3D contextual information from volumetric images, while the
opposite is true for 3D convolution. To fit a 3D CNN model on CT or MRI images
on commodity GPUs, one usually has to either downsample input images or use
cropped local regions as inputs, which limits the utility of 3D models for
multi-organ segmentation. In this work, we propose a new framework for
combining 3D and 2D models, in which the segmentation is realized through
high-resolution 2D convolutions, but guided by spatial contextual information
extracted from a low-resolution 3D model. We implement a self-attention
mechanism to control which 3D features should be used to guide 2D segmentation.
Our model is light on memory usage but fully equipped to take 3D contextual
information into account. Experiments on multiple organ segmentation datasets
demonstrate that by taking advantage of both 2D and 3D models, our method is
consistently outperforms existing 2D and 3D models in organ segmentation
accuracy, while being able to directly take raw whole-volume image data as
inputs.",,,arXiv,,,2020-12-16,2020,,,,,,All OA, Green,Preprint,"Tang, Hao; Liu, Xingwei; Han, Kun; Sun, Shanlin; Bai, Narisu; Chen, Xuming; Qian, Huang; Liu, Yong; Xie, Xiaohui","Tang, Hao (); Liu, Xingwei (); Han, Kun (); Sun, Shanlin (); Bai, Narisu (); Chen, Xuming (); Qian, Huang (); Liu, Yong (); Xie, Xiaohui ()",,"Tang, Hao (); Liu, Xingwei (); Han, Kun (); Sun, Shanlin (); Bai, Narisu (); Chen, Xuming (); Qian, Huang (); Liu, Yong (); Xie, Xiaohui ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1133613695,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
579,pub.1125772758,10.48550/arxiv.2003.08119,,,The Future of Digital Health with Federated Learning,"Data-driven Machine Learning has emerged as a promising approach for building
accurate and robust statistical models from medical data, which is collected in
huge volumes by modern healthcare systems. Existing medical data is not fully
exploited by ML primarily because it sits in data silos and privacy concerns
restrict access to this data. However, without access to sufficient data, ML
will be prevented from reaching its full potential and, ultimately, from making
the transition from research to clinical practice. This paper considers key
factors contributing to this issue, explores how Federated Learning (FL) may
provide a solution for the future of digital health and highlights the
challenges and considerations that need to be addressed.",,,arXiv,,,2020-03-18,2020,,,,,,All OA, Green,Preprint,"Rieke, Nicola; Hancox, Jonny; Li, Wenqi; Milletari, Fausto; Roth, Holger; Albarqouni, Shadi; Bakas, Spyridon; Galtier, Mathieu N.; Landman, Bennett; Maier-Hein, Klaus; Ourselin, Sebastien; Sheller, Micah; Summers, Ronald M.; Trask, Andrew; Xu, Daguang; Baust, Maximilian; Cardoso, M. Jorge","Rieke, Nicola (); Hancox, Jonny (); Li, Wenqi (); Milletari, Fausto (); Roth, Holger (); Albarqouni, Shadi (); Bakas, Spyridon (); Galtier, Mathieu N. (); Landman, Bennett (); Maier-Hein, Klaus (); Ourselin, Sebastien (); Sheller, Micah (); Summers, Ronald M. (); Trask, Andrew (); Xu, Daguang (); Baust, Maximilian (); Cardoso, M. Jorge ()",,"Rieke, Nicola (); Hancox, Jonny (); Li, Wenqi (); Milletari, Fausto (); Roth, Holger (); Albarqouni, Shadi (); Bakas, Spyridon (); Galtier, Mathieu N. (); Landman, Bennett (); Maier-Hein, Klaus (); Ourselin, Sebastien (); Sheller, Micah (); Summers, Ronald M. (); Trask, Andrew (); Xu, Daguang (); Baust, Maximilian (); Cardoso, M. Jorge ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1125772758,46 Information and Computing Sciences, 4604 Cybersecurity and Privacy,3 Good Health and Well Being,,,,,,,,,
577,pub.1133114345,10.48550/arxiv.2011.12640,,,PGL: Prior-Guided Local Self-supervised Learning for 3D Medical Image  Segmentation,"It has been widely recognized that the success of deep learning in image
segmentation relies overwhelmingly on a myriad amount of densely annotated
training data, which, however, are difficult to obtain due to the tremendous
labor and expertise required, particularly for annotating 3D medical images.
Although self-supervised learning (SSL) has shown great potential to address
this issue, most SSL approaches focus only on image-level global consistency,
but ignore the local consistency which plays a pivotal role in capturing
structural information for dense prediction tasks such as segmentation. In this
paper, we propose a PriorGuided Local (PGL) self-supervised model that learns
the region-wise local consistency in the latent feature space. Specifically, we
use the spatial transformations, which produce different augmented views of the
same image, as a prior to deduce the location relation between two views, which
is then used to align the feature maps of the same local region but being
extracted on two views. Next, we construct a local consistency loss to minimize
the voxel-wise discrepancy between the aligned feature maps. Thus, our PGL
model learns the distinctive representations of local regions, and hence is
able to retain structural information. This ability is conducive to downstream
segmentation tasks. We conducted an extensive evaluation on four public
computerized tomography (CT) datasets that cover 11 kinds of major human organs
and two tumors. The results indicate that using pre-trained PGL model to
initialize a downstream network leads to a substantial performance improvement
over both random initialization and the initialization with global
consistency-based models. Code and pre-trained weights will be made available
at: https://git.io/PGL.",,,arXiv,,,2020-11-25,2020,,,,,,All OA, Green,Preprint,"Xie, Yutong; Zhang, Jianpeng; Liao, Zehui; Xia, Yong; Shen, Chunhua","Xie, Yutong (); Zhang, Jianpeng (); Liao, Zehui (); Xia, Yong (); Shen, Chunhua ()",,"Xie, Yutong (); Zhang, Jianpeng (); Liao, Zehui (); Xia, Yong (); Shen, Chunhua ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1133114345,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
577,pub.1131241731,10.48550/arxiv.2009.11988,,,Going to Extremes: Weakly Supervised Medical Image Segmentation,"Medical image annotation is a major hurdle for developing precise and robust
machine learning models. Annotation is expensive, time-consuming, and often
requires expert knowledge, particularly in the medical field. Here, we suggest
using minimal user interaction in the form of extreme point clicks to train a
segmentation model which, in effect, can be used to speed up medical image
annotation. An initial segmentation is generated based on the extreme points
utilizing the random walker algorithm. This initial segmentation is then used
as a noisy supervision signal to train a fully convolutional network that can
segment the organ of interest, based on the provided user clicks. Through
experimentation on several medical imaging datasets, we show that the
predictions of the network can be refined using several rounds of training with
the prediction from the same weakly annotated data. Further improvements are
shown utilizing the clicked points within a custom-designed loss and attention
mechanism. Our approach has the potential to speed up the process of generating
new training datasets for the development of new machine learning and deep
learning-based models for, but not exclusively, medical image analysis.",,,arXiv,,,2020-09-24,2020,,,,,,All OA, Green,Preprint,"Roth, Holger R; Yang, Dong; Xu, Ziyue; Wang, Xiaosong; Xu, Daguang","Roth, Holger R (); Yang, Dong (); Xu, Ziyue (); Wang, Xiaosong (); Xu, Daguang ()",,"Roth, Holger R (); Yang, Dong (); Xu, Ziyue (); Wang, Xiaosong (); Xu, Daguang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1131241731,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
577,pub.1123314326,10.1016/j.neucom.2019.12.036,,,Polar coordinate sampling-based segmentation of overlapping cervical cells using attention U-Net and random walk,"Segmentation of nuclei and cytoplasm inside the cellular clumps in cervical smear images is a difficult task because of the poor contrast and unpredictable shape of cytoplasm. This article addresses a new framework based on Attention U-Net (ATT U-Net) network and graph-based Random Walk (RW) to extract both nucleus and cytoplasm of each individual cell within an image of overlapping cervical cells. The proposed approach starts by separating nuclei from the cellular clumps through ATT U-Net architecture. Then, we remove fake nuclei that are usually much smaller than real nuclei. For each nucleus, a polar coordinate sampling matrix is generated. Each element in this matrix is realized by converting the image pixel from Cartesian coordinates to polar coordinates. After that, converted images would serve as the input of ATT U-Net for predicting cytoplasm. And finally, Graph-based RW is applied to extract the contour of cytoplasm. Because the features of cytoplasm boundaries in predicted maps are so obvious that the segmentation of every individual cell, including overlapping area, worked well under RW. We evaluate our framework on ISBI 2014 Challenge Dataset. The results reveal that our approach improves the performance on extracting individual cell from heavy overlapping cell clumps.","The authors would like to thank the anonymous reviewers and the associate editor for their insightful comments that significantly improved the quality of this paper. This work was supported by the National Nature Science Foundation of China under Grant 61872143, Natural Science Foundation of Shanghai under Grant 19ZR1413400.",,Neurocomputing,,,2020-03,2020,,2020-03,383,,212-223,Closed,Article,"Zhang, Han; Zhu, Hongqing; Ling, Xiaofeng","Zhang, Han (School of Information Science and Engineering, East China University of Science and Technology, Shanghai 200237, China); Zhu, Hongqing (School of Information Science and Engineering, East China University of Science and Technology, Shanghai 200237, China); Ling, Xiaofeng (School of Information Science and Engineering, East China University of Science and Technology, Shanghai 200237, China)","Zhu, Hongqing (East China University of Science and Technology)","Zhang, Han (East China University of Science and Technology); Zhu, Hongqing (East China University of Science and Technology); Ling, Xiaofeng (East China University of Science and Technology)",9,9,,5.47,,https://app.dimensions.ai/details/publication/pub.1123314326,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,,
576,pub.1152129051,10.1007/978-3-031-19803-8_24,,,Graph-Constrained Contrastive Regularization for Semi-weakly Volumetric Segmentation,"Semantic volume segmentation suffers from the requirement of having voxel-wise annotated ground-truth data, which requires immense effort to obtain. In this work, we investigate how models can be trained from sparsely annotated volumes, i.e.Â volumes with only individual slices annotated. By formulating the scenario as a semi-weakly supervised problem where only some regions in the volume are annotated, we obtain surprising results: expensive dense volumetric annotations can be replaced by cheap, partially labeled volumes with limited impact on accuracy if the hypothesis space of valid models gets properly constrained during training. With our Contrastive Constrained Regularization (Con2R), we demonstrate that 3D convolutional models can be trained with less than 4%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4\%$$\end{document} of only two dimensional ground-truth labels and still reach up to 88%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$88\%$$\end{document} accuracy of fully supervised baseline models with dense volumetric annotations. To get insights into Con2Rs success, we study how strong semi-supervised algorithms transfer to our new volumetric semi-weakly supervised setting. In this manner, we explore retinal fluid and brain tumor segmentation and give a detailed look into accuracy progression for scenarios with extremely scarce labels.",,,Lecture Notes in Computer Science,Computer Vision â ECCV 2022,,2022-10-23,2022,2022-10-23,2022,13681,,401-419,Closed,Chapter,"ReiÃ, Simon; Seibold, Constantin; Freytag, Alexander; Rodner, Erik; Stiefelhagen, Rainer","ReiÃ, Simon (Karlsruhe Institute of Technology, 76131, Karlsruhe, Germany); Seibold, Constantin (Karlsruhe Institute of Technology, 76131, Karlsruhe, Germany); Freytag, Alexander (Carl Zeiss AG, 07745, Jena, Germany); Rodner, Erik (University of Applied Sciences Berlin, 12459, Berlin, Germany); Stiefelhagen, Rainer (Karlsruhe Institute of Technology, 76131, Karlsruhe, Germany)","ReiÃ, Simon (Karlsruhe Institute of Technology)","ReiÃ, Simon (Karlsruhe Institute of Technology); Seibold, Constantin (Karlsruhe Institute of Technology); Freytag, Alexander (Carl Zeiss (Germany)); Rodner, Erik (HTW Berlin - University of Applied Sciences); Stiefelhagen, Rainer (Karlsruhe Institute of Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152129051,46 Information and Computing Sciences,,,,,,,,,,,,
576,pub.1126022699,10.48550/arxiv.2003.12798,,,CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks,"3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene
understanding, such as video analysis and volumetric image recognition.
However, 3D networks can easily lead to over-parameterization which incurs
expensive computation cost. In this paper, we propose Channel-wise Automatic
KErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard
3D convolutions into a set of economic operations e.g., 1D, 2D convolutions.
Unlike previous methods, CAKES performs channel-wise kernel shrinkage, which
enjoys the following benefits: 1) enabling operations deployed in every layer
to be heterogeneous, so that they can extract diverse and complementary
information to benefit the learning process; and 2) allowing for an efficient
and flexible replacement design, which can be generalized to both
spatial-temporal and volumetric data. Further, we propose a new search space
based on CAKES, so that the replacement configuration can be determined
automatically for simplifying 3D networks. CAKES shows superior performance to
other methods with similar model size, and it also achieves comparable
performance to state-of-the-art with much fewer parameters and computational
costs on tasks including 3D medical imaging segmentation and video action
recognition. Codes and models are available at
https://github.com/yucornetto/CAKES",,,arXiv,,,2020-03-28,2020,,,,,,All OA, Green,Preprint,"Yu, Qihang; Li, Yingwei; Mei, Jieru; Zhou, Yuyin; Yuille, Alan L.","Yu, Qihang (); Li, Yingwei (); Mei, Jieru (); Zhou, Yuyin (); Yuille, Alan L. ()",,"Yu, Qihang (); Li, Yingwei (); Mei, Jieru (); Zhou, Yuyin (); Yuille, Alan L. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1126022699,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
547,pub.1143055508,10.48550/arxiv.2111.12525,,,Causality-inspired Single-source Domain Generalization for Medical Image  Segmentation,"Deep learning models usually suffer from domain shift issues, where models
trained on one source domain do not generalize well to other unseen domains. In
this work, we investigate the single-source domain generalization problem:
training a deep network that is robust to unseen domains, under the condition
that training data is only available from one source domain, which is common in
medical imaging applications. We tackle this problem in the context of
cross-domain medical image segmentation. Under this scenario, domain shifts are
mainly caused by different acquisition processes. We propose a simple
causality-inspired data augmentation approach to expose a segmentation model to
synthesized domain-shifted training examples. Specifically, 1) to make the deep
model robust to discrepancies in image intensities and textures, we employ a
family of randomly-weighted shallow networks. They augment training images
using diverse appearance transformations. 2) Further we show that spurious
correlations among objects in an image are detrimental to domain robustness.
These correlations might be taken by the network as domain-specific clues for
making predictions, and they may break on unseen domains. We remove these
spurious correlations via causal intervention. This is achieved by resampling
the appearances of potentially correlated objects independently. The proposed
approach is validated on three cross-domain segmentation tasks: cross-modality
(CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI
segmentation, and cross-center prostate MRI segmentation. The proposed approach
yields consistent performance gains compared with competitive methods when
tested on unseen domains.",,,arXiv,,,2021-11-24,2021,,,,,,All OA, Green,Preprint,"Ouyang, Cheng; Chen, Chen; Li, Surui; Li, Zeju; Qin, Chen; Bai, Wenjia; Rueckert, Daniel","Ouyang, Cheng (); Chen, Chen (); Li, Surui (); Li, Zeju (); Qin, Chen (); Bai, Wenjia (); Rueckert, Daniel ()",,"Ouyang, Cheng (); Chen, Chen (); Li, Surui (); Li, Zeju (); Qin, Chen (); Bai, Wenjia (); Rueckert, Daniel ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143055508,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
545,pub.1140969721,10.48550/arxiv.2109.03230,,,Self-supervised Tumor Segmentation through Layer Decomposition,"In this paper, we target self-supervised representation learning for
zero-shot tumor segmentation. We make the following contributions: First, we
advocate a zero-shot setting, where models from pre-training should be directly
applicable for the downstream task, without using any manual annotations.
Second, we take inspiration from ""layer-decomposition"", and innovate on the
training regime with simulated tumor data. Third, we conduct extensive ablation
studies to analyse the critical components in data simulation, and validate the
necessity of different proxy tasks. We demonstrate that, with sufficient
texture randomization in simulation, model trained on synthetic data can
effortlessly generalise to segment real tumor data. Forth, our approach
achieves superior results for zero-shot tumor segmentation on different
downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for
liver tumor segmentation. While evaluating the model transferability for tumor
segmentation under a low-annotation regime, the proposed approach also
outperforms all existing self-supervised approaches, opening up the usage of
self-supervised learning in practical scenarios.",,,arXiv,,,2021-09-07,2021,,,,,,All OA, Green,Preprint,"Zhang, Xiaoman; Xie, Weidi; Huang, Chaoqin; Wang, Yanfeng; Zhang, Ya; Chen, Xin; Tian, Qi","Zhang, Xiaoman (); Xie, Weidi (); Huang, Chaoqin (); Wang, Yanfeng (); Zhang, Ya (); Chen, Xin (); Tian, Qi ()",,"Zhang, Xiaoman (); Xie, Weidi (); Huang, Chaoqin (); Wang, Yanfeng (); Zhang, Ya (); Chen, Xin (); Tian, Qi ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140969721,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
545,pub.1138562115,10.3390/make3020026,,,Going to Extremes: Weakly Supervised Medical Image Segmentation,"Medical image annotation is a major hurdle for developing precise and robust machine-learning models. Annotation is expensive, time-consuming, and often requires expert knowledge, particularly in the medical field. Here, we suggest using minimal user interaction in the form of extreme point clicks to train a segmentation model which, in effect, can be used to speed up medical image annotation. An initial segmentation is generated based on the extreme points using the random walker algorithm. This initial segmentation is then used as a noisy supervision signal to train a fully convolutional network that can segment the organ of interest, based on the provided user clicks. Through experimentation on several medical imaging datasets, we show that the predictions of the network can be refined using several rounds of training with the prediction from the same weakly annotated data. Further improvements are shown using the clicked points within a custom-designed loss and attention mechanism. Our approach has the potential to speed up the process of generating new training datasets for the development of new machine-learning and deep-learning-based models for, but not exclusively, medical image analysis.",,This research received no external funding.,Machine Learning and Knowledge Extraction,,,2021-06-02,2021,2021-06-02,,3,2,507-524,All OA, Gold,Article,"Roth, Holger R.; Yang, Dong; Xu, Ziyue; Wang, Xiaosong; Xu, Daguang","Roth, Holger R. (); Yang, Dong (); Xu, Ziyue (); Wang, Xiaosong (); Xu, Daguang ()","Roth, Holger R. ; Xu, Daguang ","Roth, Holger R. (); Yang, Dong (); Xu, Ziyue (); Wang, Xiaosong (); Xu, Daguang ()",14,14,,11.46,https://www.mdpi.com/2504-4990/3/2/26/pdf?version=1622625267,https://app.dimensions.ai/details/publication/pub.1138562115,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
545,pub.1148312910,10.21203/rs.3.rs-1705779/v1,,,A large public dataset of annotated clinical MRIs of patients with acute stroke and linked metadata,"To extract meaningful and reproducible models of brain function from stroke images, for both clinical and research proposes, is a daunting task severely hindered by the great variability of lesion frequency and patterns. Large datasets are therefore imperative, as well as fully automated image post-processing tools to analyze them. The development of such tools, particularly with artificial intelligence, is highly dependent on the availability of large datasets to model training and testing. We present a public dataset of 2,888 multimodal clinical MRIs of patients with acute and early subacute stroke, with manual lesion segmentation, and metadata. The dataset provides high quality, large scale, human-supervised knowledge to feed artificial intelligence models and enable further development of tools to automate several tasks that currently rely on human labor, such as lesion segmentation, labeling, calculation of disease-relevant scores, and lesion-based studies relating function to frequency lesion maps",,,Research Square,,,2022-05-31,2022,2022-05-31,,,,,All OA, Green,Preprint,"Liu, Chin-Fu; Leigh, Richard; Johnson, Brenda; Urrutia, Victor; Hsu, Johnny; Xu, Xin; Li, Xin; Mori, Susumu; Hillis, Argye; Faria, Andreia","Liu, Chin-Fu (Jonhs Hopkins Unoversity); Leigh, Richard (Johns Hopkins University); Johnson, Brenda (Johns Hopkins University); Urrutia, Victor (Johns Hopkins University); Hsu, Johnny (Johns Hopkins University); Xu, Xin (Johns Hopkins University); Li, Xin (Johns Hopkins University); Mori, Susumu (Johns Hopkins University); Hillis, Argye (Johns Hopkins University); Faria, Andreia (Johns Hopkins University)",,"Liu, Chin-Fu (); Leigh, Richard (Johns Hopkins University); Johnson, Brenda (Johns Hopkins University); Urrutia, Victor (Johns Hopkins University); Hsu, Johnny (Johns Hopkins University); Xu, Xin (Johns Hopkins University); Li, Xin (Johns Hopkins University); Mori, Susumu (Johns Hopkins University); Hillis, Argye (Johns Hopkins University); Faria, Andreia (Johns Hopkins University)",0,0,,,https://www.researchsquare.com/article/rs-1705779/latest.pdf,https://app.dimensions.ai/details/publication/pub.1148312910,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
545,pub.1144375420,10.21203/rs.3.rs-1215123/v1,,,A Novel 2-Phase U-net Algorithm Combined with Optimal Mass Transportation for 3D Brain Tumor Detection and Segmentation,"Utilizing the optimal mass transportation (OMT) technique to convert an irregular 3D brain image into a cube, a required input format for the U-net algorithm, is a brand new idea for medical imaging research. We develop a cubic volume-measure-preserving OMT (V-OMT) model for the implementation of this conversion. The contrast-enhanced histogram equalization grayscale of fluid attenuated inversion recovery (FLAIR) in a brain image creates the corresponding density function. We then propose an effective two-phase U-net algorithm combined with the V-OMT algorithm for training and validation. First, we use the U-net and V-OMT algorithms to precisely predict the whole tumor (WT) region. Second, we expand this predicted WT region with dilation and create a smooth function by convoluting the step-like function associated with the WT region in the brain image with a 5Ã5Ã5 blur tensor. Then, a new V-OMT algorithm with mesh refinement is constructed to allow the U-net algorithm to effectively train Net1--Net3 models. Finally, we propose ensemble voting postprocessing to validate the final labels of brain images. We randomly choose 1000 and 251 brain samples from theBraTS 2021 training dataset, which contains 1251 samples, for training and validation, respectively. The Dice scores of the WT, tumor core (TC) and enhanced tumor (ET) regions for validation computed by Net1--Net3 were 0.93705, 0.90617 and 0.87470, respectively. A significant improvement in brain tumor detection and segmentation with higher accuracy is achieved.",,,Research Square,,,2022-01-03,2022,2022-01-03,,,,,All OA, Green,Preprint,"Lin, Wen-Wei; Lin, Jia-Wei; Huang, Tsung-Ming; Li, Tiexiang; Yueh, Mei-Heng; Yau, Shing-Tung","Lin, Wen-Wei (National Yang Ming Chiao Tung University); Lin, Jia-Wei (National Yang Ming Chiao Tung University); Huang, Tsung-Ming (National Taiwan Normal University); Li, Tiexiang (Southeast University); Yueh, Mei-Heng (National Taiwan Normal University); Yau, Shing-Tung (Harvard University)",,"Lin, Wen-Wei (National Yang Ming Chiao Tung University); Lin, Jia-Wei (National Yang Ming Chiao Tung University); Huang, Tsung-Ming (National Taiwan Normal University); Li, Tiexiang (Southeast University); Yueh, Mei-Heng (National Taiwan Normal University); Yau, Shing-Tung (Harvard University)",1,1,,,https://www.researchsquare.com/article/rs-1215123/latest.pdf,https://app.dimensions.ai/details/publication/pub.1144375420,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
544,pub.1155644749,10.1109/tmi.2023.3247941,,,Shape-aware Joint Distribution Alignment for Cross-domain Image Segmentation,"We present an unsupervised domain adaptation method for image segmentation which aligns high-order statistics, computed for the source and target domains, encoding domain-invariant spatial relationships between segmentation classes. Our method first estimates the joint distribution of predictions for pair of pixels whose relative position corresponds to a given spatial displacement. Domain adaptation is then achieved by aligning the joint distributions of source and target images, computed for a set of displacements. Two enhancements of this method are proposed. The first one uses an efficient multi-scale strategy that enables capturing long-range relationships in the statistics. The second one extends the joint distribution alignment loss to features in intermediate layers of the network by computing their cross-correlation. We test our method on the task of unpaired multi-modal cardiac segmentation using the Multi-Modality Whole Heart Segmentation Challenge dataset and prostate segmentation task where images from two datasets are taken as data in different domains. Our results show the advantages of our method compared to recent approaches for cross-domain image segmentation. Code is available at https://github.com/WangPing521/Domain_adaptation_shape_prior.",,,IEEE Transactions on Medical Imaging,,,2023-02-22,2023,2023-02-22,,PP,99,1-1,Closed,Article,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (Software and IT department, &#x00C9;cole de technologie sup&#x00E9;rieure(ETS), Montreal, Canada); Peng, Jizong (Software and IT department, ETS, Montreal, Canada); Pedersoli, Marco (Software and IT department, ETS, Montreal, Canada); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, China); Zhang, Caiming (School of Software, Shandong University, Jinan, China); Desrosiers, Christian (Software and IT department, ETS, Montreal, Canada)",,"Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (Shandong University); Zhang, Caiming (Shandong University); Desrosiers, Christian ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155644749,46 Information and Computing Sciences, 4601 Applied Computing,,,,,,,,,,,
544,pub.1128748075,10.48550/arxiv.2006.13877,,,Does Non-COVID19 Lung Lesion Help? Investigating Transferability in  COVID-19 CT Image Segmentation,"Coronavirus disease 2019 (COVID-19) is a highly contagious virus spreading
all around the world. Deep learning has been adopted as an effective technique
to aid COVID-19 detection and segmentation from computed tomography (CT)
images. The major challenge lies in the inadequate public COVID-19 datasets.
Recently, transfer learning has become a widely used technique that leverages
the knowledge gained while solving one problem and applying it to a different
but related problem. However, it remains unclear whether various non-COVID19
lung lesions could contribute to segmenting COVID-19 infection areas and how to
better conduct this transfer procedure. This paper provides a way to understand
the transferability of non-COVID19 lung lesions. Based on a publicly available
COVID-19 CT dataset and three public non-COVID19 datasets, we evaluate four
transfer learning methods using 3D U-Net as a standard encoder-decoder method.
The results reveal the benefits of transferring knowledge from non-COVID19 lung
lesions, and learning from multiple lung lesion datasets can extract more
general features, leading to accurate and robust pre-trained models. We further
show the capability of the encoder to learn feature representations of lung
lesions, which improves segmentation accuracy and facilitates training
convergence. In addition, our proposed Hybrid-encoder learning method
incorporates transferred lung lesion features from non-COVID19 datasets
effectively and achieves significant improvement. These findings promote new
insights into transfer learning for COVID-19 CT image segmentation, which can
also be further generalized to other medical tasks.",,,arXiv,,,2020-06-23,2020,,,,,,All OA, Green,Preprint,"Wang, Yixin; Zhang, Yao; Liu, Yang; Tian, Jiang; Zhong, Cheng; Shi, Zhongchao; Zhang, Yang; He, Zhiqiang","Wang, Yixin (); Zhang, Yao (); Liu, Yang (); Tian, Jiang (); Zhong, Cheng (); Shi, Zhongchao (); Zhang, Yang (); He, Zhiqiang ()",,"Wang, Yixin (); Zhang, Yao (); Liu, Yang (); Tian, Jiang (); Zhong, Cheng (); Shi, Zhongchao (); Zhang, Yang (); He, Zhiqiang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1128748075,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
544,pub.1125773012,10.48550/arxiv.2003.07923,,,3D medical image segmentation with labeled and unlabeled data using  autoencoders at the example of liver segmentation in CT images,"Automatic segmentation of anatomical structures with convolutional neural
networks (CNNs) constitutes a large portion of research in medical image
analysis. The majority of CNN-based methods rely on an abundance of labeled
data for proper training. Labeled medical data is often scarce, but unlabeled
data is more widely available. This necessitates approaches that go beyond
traditional supervised learning and leverage unlabeled data for segmentation
tasks. This work investigates the potential of autoencoder-extracted features
to improve segmentation with a CNN. Two strategies were considered. First,
transfer learning where pretrained autoencoder features were used as
initialization for the convolutional layers in the segmentation network.
Second, multi-task learning where the tasks of segmentation and feature
extraction, by means of input reconstruction, were learned and optimized
simultaneously. A convolutional autoencoder was used to extract features from
unlabeled data and a multi-scale, fully convolutional CNN was used to perform
the target task of 3D liver segmentation in CT images. For both strategies,
experiments were conducted with varying amounts of labeled and unlabeled
training data. The proposed learning strategies improved results in $75\%$ of
the experiments compared to training from scratch and increased the dice score
by up to $0.040$ and $0.024$ for a ratio of unlabeled to labeled training data
of about $32 : 1$ and $12.5 : 1$, respectively. The results indicate that both
training strategies are more effective with a large ratio of unlabeled to
labeled training data.",,,arXiv,,,2020-03-17,2020,,,,,,All OA, Green,Preprint,"Sital, Cheryl; Brosch, Tom; Tio, Dominique; Raaijmakers, Alexander; Weese, JÃ¼rgen","Sital, Cheryl (); Brosch, Tom (); Tio, Dominique (); Raaijmakers, Alexander (); Weese, JÃ¼rgen ()",,"Sital, Cheryl (); Brosch, Tom (); Tio, Dominique (); Raaijmakers, Alexander (); Weese, JÃ¼rgen ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1125773012,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
543,pub.1153827106,10.1002/ima.22836,,,Multiâplanar 3D knee MRI segmentation via UNet inspired architectures,"The UNet has become the golden standard method for the segmentation of 2D medical images that any new method must be validated against. In recent years, a number of variations to the seminal UNet have been proposed with promising results in the papers introducing them. However, there is no clear consensus if any of these architectures generalize as well and the UNet currently remains the methodological golden standard. For the segmentation of 3D scans, UNetâinspired methods are also dominant, but there is a larger variety across applications. By evaluating the architectures in a different dimensionality, embedded in a different method, and for a different task, we aimed to evaluate if any of these UNet alternatives are promising as a new golden standard that generalizes even better than the UNet. The purpose of this study was to compare UNet inspired models for generalized 3D segmentation. To efficiently segment the 3D scans, we employed each UNet variant architecture as the central 2D segmentation core in the multiâplanar UNet 3D segmentation method that previously demonstrated excellent generalization in the MICCAI Segmentation Decathlon. It would strongly support a claim of generalizability, if a promising UNetâvariant consistently outperforms the UNet in this quite different setting. The experimental results show that the multiâplanarâbased UNet2+ (MPUNet2+) method outperforms other variants including the original multiâplanar UNet (MPUNet).","Work on the LIRA cohort was supported by an unrestricted grant from Novo Nordisk A/S including active and placebo medicine, and by the Cambridge Weight Plan donating dietary supplements. In addition, the work on the LIRA cohort was supported by a core grant from the Oak Foundation (OCAYâ13â309) given to the Parker Institute, Copenhagen University Hospital, Bispebjerg and Frederiksberg. We also would like to acknowledge the staff of the Department of Radiology BispebjergâFrederiksberg and the patients in support of collecting the LIRA cohort. The funders had no influence on the design or conduct of the study and were not involved in data collection or analysis, in the writing of the manuscript, or in the decision to submit it for publication. Work on the LIRA cohort was supported by an unrestricted grant from Novo Nordisk A/S including active and placebo medicine, and by the Cambridge Weight Plan donating dietary supplements. In addition, the work on the LIRA cohort was supported by a core grant from the Oak Foundation (OCAYâ13â309) given to the Parker Institute, Copenhagen University Hospital, Bispebjerg and Frederiksberg. We also would like to acknowledge the staff of the Department of Radiology BispebjergâFrederiksberg and the patients in support of collecting the LIRA cohort. The funders had no influence on the design or conduct of the study and were not involved in data collection or analysis, in the writing of the manuscript, or in the decision to submit it for publication.","Funding information In this study, the work on the LIRA cohort was supported by an unrestricted grant from Novo Nordisk A/S including active and placebo medicine, and by the Cambridge Weight Plan donating dietary supplements. In addition, the work on the LIRA cohort was supported by a core grant from the Oak Foundation (OCAYâ13â309) given to the Parker Institute, Copenhagen University Hospital, Bispebjerg and Frederiksberg. The funders had no influence on the design or conduct of the study and were not involved in",International Journal of Imaging Systems and Technology,,,2022-12-20,2022,2022-12-20,,,,,Closed,Article,"Sengar, Sandeep Singh; Meulengracht, Christopher; Boesen, Mikael Ploug; Overgaard, Anders FÃ¸hrby; Gudbergsen, Henrik; Nybing, Janus Damm; Perslev, Mathias; Dam, Erik BjÃ¸rnager","Sengar, Sandeep Singh (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark); Meulengracht, Christopher (Copenhagen University Hospital, Bispebjerg and Frederiksberg, Copenhagen, Denmark); Boesen, Mikael Ploug (Copenhagen University Hospital, Bispebjerg and Frederiksberg, Copenhagen, Denmark); Overgaard, Anders FÃ¸hrby (Copenhagen University Hospital, Bispebjerg and Frederiksberg, Copenhagen, Denmark); Gudbergsen, Henrik (Copenhagen University Hospital, Bispebjerg and Frederiksberg, Copenhagen, Denmark); Nybing, Janus Damm (Copenhagen University Hospital, Bispebjerg and Frederiksberg, Copenhagen, Denmark); Perslev, Mathias (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark); Dam, Erik BjÃ¸rnager (Department of Computer Science, University of Copenhagen, Copenhagen, Denmark)","Sengar, Sandeep Singh (University of Copenhagen)","Sengar, Sandeep Singh (University of Copenhagen); Meulengracht, Christopher (Copenhagen University Hospital); Boesen, Mikael Ploug (Copenhagen University Hospital); Overgaard, Anders FÃ¸hrby (Copenhagen University Hospital); Gudbergsen, Henrik (Copenhagen University Hospital); Nybing, Janus Damm (Copenhagen University Hospital); Perslev, Mathias (University of Copenhagen); Dam, Erik BjÃ¸rnager (University of Copenhagen)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153827106,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,,
543,pub.1131970407,10.48550/arxiv.2010.11008,,,What is Wrong with Continual Learning in Medical Image Segmentation?,"Continual learning protocols are attracting increasing attention from the
medical imaging community. In continual environments, datasets acquired under
different conditions arrive sequentially; and each is only available for a
limited period of time. Given the inherent privacy risks associated with
medical data, this setup reflects the reality of deployment for deep learning
diagnostic radiology systems. Many techniques exist to learn continuously for
image classification, and several have been adapted to semantic segmentation.
Yet most struggle to accumulate knowledge in a meaningful manner. Instead, they
focus on preventing the problem of catastrophic forgetting, even when this
reduces model plasticity and thereon burdens the training process. This puts
into question whether the additional overhead of knowledge preservation is
worth it - particularly for medical image segmentation, where computation
requirements are already high - or if maintaining separate models would be a
better solution. We propose UNEG, a simple and widely applicable multi-model
benchmark that maintains separate segmentation and autoencoder networks for
each training stage. The autoencoder is built from the same architecture as the
segmentation network, which in our case is a full-resolution nnU-Net, to bypass
any additional design decisions. During inference, the reconstruction error is
used to select the most appropriate segmenter for each test image. Open this
concept, we develop a fair evaluation scheme for different continual learning
settings that moves beyond the prevention of catastrophic forgetting. Our
results across three regions of interest (prostate, hippocampus, and right
ventricle) show that UNEG outperforms several continual learning methods,
reinforcing the need for strong baselines in continual learning research.",,,arXiv,,,2020-10-21,2020,,,,,,All OA, Green,Preprint,"Gonzalez, Camila; Lemke, Nick; Sakas, Georgios; Mukhopadhyay, Anirban","Gonzalez, Camila (); Lemke, Nick (); Sakas, Georgios (); Mukhopadhyay, Anirban ()",,"Gonzalez, Camila (); Lemke, Nick (); Sakas, Georgios (); Mukhopadhyay, Anirban ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1131970407,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
543,pub.1154796648,10.1155/2023/3617318,,,An End-to-End Data-Adaptive Pancreas Segmentation System with an Image Quality Control Toolbox,"With the development of radiology and computer technology, diagnosis by medical imaging is heading toward precision and automation. Due to complex anatomy around the pancreatic tissue and high demands for clinical experience, the assisted pancreas segmentation system will greatly promote clinical efficiency. However, the existing segmentation model suffers from poor generalization among images from multiple hospitals. In this paper, we propose an end-to-end data-adaptive pancreas segmentation system to tackle the problems of lack of annotations and model generalizability. The system employs adversarial learning to transfer features from labeled domains to unlabeled domains, seeking a dynamic balance between domain discrimination and unsupervised segmentation. The image quality control toolbox is embedded in the system, which standardizes image quality in terms of intensity, field of view, and so on, to decrease heterogeneity among image domains. In addition, the system implements a data-adaptive process end-to-end without complex operations by doctors. The experiments are conducted on an annotated public dataset and an unannotated in-hospital dataset. The results indicate that after data adaptation, the segmentation performance measured by the dice similarity coefficient on unlabeled images improves from 58.79% to 75.43%, with a gain of 16.64%. Furthermore, the system preserves quantitatively structured information such as the pancreasâ size and volume, as well as objective and accurate visualized images, which assists clinicians in diagnosing and formulating treatment plans in a timely and accurate manner.","This work was supported in part by the National Natural Science Foundation of China (Nos. 12101571, 82172069, and 81702332), the Major Scientific Project of Zhejiang Lab (No. 2020ND8AD01), the Zhejiang Provincial Natural Science Foundation of China (No. LQ20H180001), and the Zhejiang Provincial Key Research and Development Program (No. 2020C03117).",,Journal of Healthcare Engineering,,,2023-01-24,2023,,2023-01-24,2023,,1-12,All OA, Gold,Article,"Zhu, Yan; Hu, Peijun; Li, Xiang; Tian, Yu; Bai, Xueli; Liang, Tingbo; Li, Jingsong","Zhu, Yan (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou 310027, China, meb.gov.tr); Hu, Peijun (Research Center for Healthcare Data Science, Zhejiang Lab, Hangzhou 311100, China, zhejianglab.com); Li, Xiang (Department of Hepatobiliary and Pancreatic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou 310006, China, zju.edu.cn; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou 310006, China, zju.edu.cn); Tian, Yu (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou 310027, China, meb.gov.tr); Bai, Xueli (Department of Hepatobiliary and Pancreatic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou 310006, China, zju.edu.cn; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou 310006, China, zju.edu.cn); Liang, Tingbo (Department of Hepatobiliary and Pancreatic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou 310006, China, zju.edu.cn; Zhejiang Provincial Key Laboratory of Pancreatic Disease, Hangzhou 310006, China, zju.edu.cn); Li, Jingsong (Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou 310027, China, meb.gov.tr; Research Center for Healthcare Data Science, Zhejiang Lab, Hangzhou 311100, China, zhejianglab.com)","Li, Jingsong (Zhejiang University; Zhejiang Lab)","Zhu, Yan (Zhejiang University); Hu, Peijun (Zhejiang Lab); Li, Xiang (Zhejiang University; First Affiliated Hospital Zhejiang University); Tian, Yu (Zhejiang University); Bai, Xueli (Zhejiang University; First Affiliated Hospital Zhejiang University); Liang, Tingbo (Zhejiang University; First Affiliated Hospital Zhejiang University); Li, Jingsong (Zhejiang University; Zhejiang Lab)",0,0,,,https://downloads.hindawi.com/journals/jhe/2023/3617318.pdf,https://app.dimensions.ai/details/publication/pub.1154796648,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,
543,pub.1153402310,10.48550/arxiv.2212.01703,,,Active learning using adaptable task-based prioritisation,"Supervised machine learning-based medical image computing applications
necessitate expert label curation, while unlabelled image data might be
relatively abundant. Active learning methods aim to prioritise a subset of
available image data for expert annotation, for label-efficient model training.
We develop a controller neural network that measures priority of images in a
sequence of batches, as in batch-mode active learning, for multi-class
segmentation tasks. The controller is optimised by rewarding positive
task-specific performance gain, within a Markov decision process (MDP)
environment that also optimises the task predictor. In this work, the task
predictor is a segmentation network. A meta-reinforcement learning algorithm is
proposed with multiple MDPs, such that the pre-trained controller can be
adapted to a new MDP that contains data from different institutes and/or
requires segmentation of different organs or structures within the abdomen. We
present experimental results using multiple CT datasets from more than one
thousand patients, with segmentation tasks of nine different abdominal organs,
to demonstrate the efficacy of the learnt prioritisation controller function
and its cross-institute and cross-organ adaptability. We show that the proposed
adaptable prioritisation metric yields converging segmentation accuracy for the
novel class of kidney, unseen in training, using between approximately 40\% to
60\% of labels otherwise required with other heuristic or random prioritisation
metrics. For clinical datasets of limited size, the proposed adaptable
prioritisation offers a performance improvement of 22.6\% and 10.2\% in Dice
score, for tasks of kidney and liver vessel segmentation, respectively,
compared to random prioritisation and alternative active sampling strategies.",,,arXiv,,,2022-12-03,2022,,,,,,All OA, Green,Preprint,"Saeed, Shaheer U.; Ramalhinho, JoÃ£o; Pinnock, Mark; Shen, Ziyi; Fu, Yunguan; MontaÃ±a-Brown, Nina; Bonmati, Ester; Barratt, Dean C.; Pereira, Stephen P.; Davidson, Brian; Clarkson, Matthew J.; Hu, Yipeng","Saeed, Shaheer U. (); Ramalhinho, JoÃ£o (); Pinnock, Mark (); Shen, Ziyi (); Fu, Yunguan (); MontaÃ±a-Brown, Nina (); Bonmati, Ester (); Barratt, Dean C. (); Pereira, Stephen P. (); Davidson, Brian (); Clarkson, Matthew J. (); Hu, Yipeng ()",,"Saeed, Shaheer U. (); Ramalhinho, JoÃ£o (); Pinnock, Mark (); Shen, Ziyi (); Fu, Yunguan (); MontaÃ±a-Brown, Nina (); Bonmati, Ester (); Barratt, Dean C. (); Pereira, Stephen P. (); Davidson, Brian (); Clarkson, Matthew J. (); Hu, Yipeng ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153402310,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,
543,pub.1151981070,10.48550/arxiv.2210.09309,,,RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical  Centerline Extraction,"Automatic rib labeling and anatomical centerline extraction are common
prerequisites for various clinical applications. Prior studies either use
in-house datasets that are inaccessible to communities, or focus on rib
segmentation that neglects the clinical significance of rib labeling. To
address these issues, we extend our prior dataset (RibSeg) on the binary rib
segmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CT
scans (15,466 individual ribs in total) and annotations manually inspected by
experts for rib labeling and anatomical centerline extraction. Based on the
RibSeg v2, we develop a pipeline including deep learning-based methods for rib
labeling, and a skeletonization-based method for centerline extraction. To
improve computational efficiency, we propose a sparse point cloud
representation of CT scans and compare it with standard dense voxel grids.
Moreover, we design and analyze evaluation metrics to address the key
challenges of each task. Our dataset, code, and model are available online to
facilitate open research at https://github.com/M3DV/RibSeg",,,arXiv,,,2022-10-17,2022,,,,,,All OA, Green,Preprint,"Jin, Liang; Gu, Shixuan; Wei, Donglai; Kuang, Kaiming; Pfister, Hanspeter; Ni, Bingbing; Yang, Jiancheng; Li, Ming","Jin, Liang (); Gu, Shixuan (); Wei, Donglai (); Kuang, Kaiming (); Pfister, Hanspeter (); Ni, Bingbing (); Yang, Jiancheng (); Li, Ming ()",,"Jin, Liang (); Gu, Shixuan (); Wei, Donglai (); Kuang, Kaiming (); Pfister, Hanspeter (); Ni, Bingbing (); Yang, Jiancheng (); Li, Ming ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151981070,46 Information and Computing Sciences, 4608 Human-Centred Computing,,,,,,,,,,
543,pub.1151954973,10.1016/j.dsp.2022.103784,,,3D PSwinBTS: An efficient transformer-based Unet using 3D parallel shifted windows for brain tumor segmentation,"Computer-Aided Decision is a vital component in modern smart medical care. A fast and accurate automated MRI brain tumor segmentation method is critical for clinical diagnosis and treatment of brain cancer. Convolutional Neural Network-based segmentation method has achieved impressive performance in medical image segmentation with powerful local representation capacities. Nevertheless, they have limitations in modeling global or long-range contextual interactions and spatial dependencies, which are critical for medical image segmentation. In this work, we presented an efficient and lightweight transformer-based Unet for automatic MRI brain tumor segmentation named 3D PSwinBTS, which utilize 3D Parallel Shifted Window-based Transformer module to extract long-range contextual information. Moreover, we utilize semantic supervision to introduce semantic priors in the encoder of 3D PSwinBTS for efficient semantic modeling. To demonstrate the superiority of our proposed method, we compared the performance of our proposed method with state-of-the-art methods on BraTS 2021 dataset, BraTS 2020 dataset and MSD brain tumor dataset. The results demonstrate that our 3D PSwinBTS achieved remarkable performance while computational complexity remains attractive.","This work was supported by the Natural Science Foundation of Jiangxi Province, China (No. 20202BABL202028).",,Digital Signal Processing,,,2022-11,2022,,2022-11,131,,103784,Closed,Article,"Liang, Junjie; Yang, Cihui; Zeng, Lingguo","Liang, Junjie (School of Information Engineering, Nanchang Hangkong University, No. 696 Fenghe South Avenue, Nanchang, 330063, China); Yang, Cihui (School of Information Engineering, Nanchang Hangkong University, No. 696 Fenghe South Avenue, Nanchang, 330063, China); Zeng, Lingguo (School of Information Engineering, Nanchang Hangkong University, No. 696 Fenghe South Avenue, Nanchang, 330063, China)","Yang, Cihui (Nanchang Hangkong University)","Liang, Junjie (Nanchang Hangkong University); Yang, Cihui (Nanchang Hangkong University); Zeng, Lingguo (Nanchang Hangkong University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1151954973,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,,
542,pub.1148758824,10.48550/arxiv.2206.08023,,,AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile  Medical Image Segmentation,"Despite the considerable progress in automatic abdominal multi-organ
segmentation from CT/MRI scans in recent years, a comprehensive evaluation of
the models' capabilities is hampered by the lack of a large-scale benchmark
from diverse clinical scenarios. Constraint by the high cost of collecting and
labeling 3D medical data, most of the deep learning models to date are driven
by datasets with a limited number of organs of interest or samples, which still
limits the power of modern deep models and makes it difficult to provide a
fully comprehensive and fair estimate of various methods. To mitigate the
limitations, we present AMOS, a large-scale, diverse, clinical dataset for
abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected
from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease
patients, each with voxel-level annotations of 15 abdominal organs, providing
challenging examples and test-bed for studying robust segmentation algorithms
under diverse targets and scenarios. We further benchmark several
state-of-the-art medical segmentation models to evaluate the status of the
existing methods on this new challenging dataset. We have made our datasets,
benchmark servers, and baselines publicly available, and hope to inspire future
research. Information can be found at https://amos22.grand-challenge.org.",,,arXiv,,,2022-06-16,2022,,,,,,All OA, Green,Preprint,"Ji, Yuanfeng; Bai, Haotian; Yang, Jie; Ge, Chongjian; Zhu, Ye; Zhang, Ruimao; Li, Zhen; Zhang, Lingyan; Ma, Wanling; Wan, Xiang; Luo, Ping","Ji, Yuanfeng (); Bai, Haotian (); Yang, Jie (); Ge, Chongjian (); Zhu, Ye (); Zhang, Ruimao (); Li, Zhen (); Zhang, Lingyan (); Ma, Wanling (); Wan, Xiang (); Luo, Ping ()",,"Ji, Yuanfeng (); Bai, Haotian (); Yang, Jie (); Ge, Chongjian (); Zhu, Ye (); Zhang, Ruimao (); Li, Zhen (); Zhang, Lingyan (); Ma, Wanling (); Wan, Xiang (); Luo, Ping ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148758824,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
515,pub.1152337937,10.1016/j.neucom.2022.10.060,,,TD-Net: Trans-Deformer network for automatic pancreas segmentation,"Accurate and efficient pancreas segmentation is the basis for subsequent diagnosis and qualitative treatment of pancreatic cancer. Segmenting the pancreas from abdominal CT images is a challenging task because the morphology of the pancreas varies greatly among different individuals and may be affected by problems such as the unbalanced category and blurred boundaries. This paper proposes a two-stage Trans-Deformer network to solve these problems of pancreas segmentation. To be specific, we first use 2D Unet for coarse segmentation to generate candidate regions of the pancreas. In the fine segmentation stage, we propose to integrate deformable convolution into Vision Transformer (VIT) for solving the deformation problem of the pancreas. For the problem of blurred boundaries caused by low contrast in the pancreas, a multi-input module based on wavelet decomposition is proposed to make our network pay more attention to high-frequency texture information. In addition, we propose using the Scale Inter-active Fusion (SIF) module to merge local features and global features. Our method was evaluated on the public NIH dataset including 82 abdominal contrast-enhanced CT volumes and the public MSD dataset including 281 abdominal contrast-enhanced CT volumes via fourfold cross-validation. We have achieved the average Dice Similarity Coefficient (DSC) values of 89.89Â Â±Â 1.82Â % on the NIH dataset, and 91.22Â Â±Â 1.37Â % on the MSD dataset, outperforming other exiting state-of-the-art pancreas segmentation methods.","This work was supported in part by the Science and Technology Commission of Shanghai Municipality (20DZ2254400, 21DZ2200600, 20DZ2261200), National Scientific Foundation of China (82170110), Fujian Province Department of Science and Technology (2022D014).",,Neurocomputing,,,2023-01,2023,,2023-01,517,,279-293,Closed,Article,"Dai, Shunbo; Zhu, Yu; Jiang, Xiaoben; Yu, Fuli; Lin, Jiajun; Yang, Dawei","Dai, Shunbo (School of Information Science and Engineering, East China University of Science and Technology, Shanghai 200237, China); Zhu, Yu (School of Information Science and Engineering, East China University of Science and Technology, Shanghai 200237, China; Shanghai Engineering Research Center of Internet of Things for Respiratory Medicine, China); Jiang, Xiaoben (School of Information Science and Engineering, East China University of Science and Technology, Shanghai 200237, China); Yu, Fuli (School of Information Science and Engineering, East China University of Science and Technology, Shanghai 200237, China); Lin, Jiajun (School of Information Science and Engineering, East China University of Science and Technology, Shanghai 200237, China); Yang, Dawei (Department of Pulmonary and Critical Care Medicine, Zhongshan Hospital, Fudan University, Shanghai 200032, China; Shanghai Engineering Research Center of Internet of Things for Respiratory Medicine, China)","Zhu, Yu (East China University of Science and Technology; ); Yang, Dawei (Zhongshan Hospital; Fudan University; )","Dai, Shunbo (East China University of Science and Technology); Zhu, Yu (East China University of Science and Technology); Jiang, Xiaoben (East China University of Science and Technology); Yu, Fuli (East China University of Science and Technology); Lin, Jiajun (East China University of Science and Technology); Yang, Dawei (Zhongshan Hospital; Fudan University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1152337937,46 Information and Computing Sciences,,,,,,,,,,,,
515,pub.1143055106,10.48550/arxiv.2111.12123,,,"MICS : Multi-steps, Inverse Consistency and Symmetric deep learning  registration network","Deformable registration consists of finding the best dense correspondence
between two different images. Many algorithms have been published, but the
clinical application was made difficult by the high calculation time needed to
solve the optimisation problem. Deep learning overtook this limitation by
taking advantage of GPU calculation and the learning process. However, many
deep learning methods do not take into account desirable properties respected
by classical algorithms.
  In this paper, we present MICS, a novel deep learning algorithm for medical
imaging registration. As registration is an ill-posed problem, we focused our
algorithm on the respect of different properties: inverse consistency, symmetry
and orientation conservation. We also combined our algorithm with a multi-step
strategy to refine and improve the deformation grid. While many approaches
applied registration to brain MRI, we explored a more challenging body
localisation: abdominal CT. Finally, we evaluated our method on a dataset used
during the Learn2Reg challenge, allowing a fair comparison with published
methods.",,,arXiv,,,2021-11-23,2021,,,,,,All OA, Green,Preprint,"Estienne, ThÃ©o; Vakalopoulou, Maria; Battistella, Enzo; Henry, Theophraste; Lerousseau, Marvin; Leroy, Amaury; Paragios, Nikos; Deutsch, Eric","Estienne, ThÃ©o (); Vakalopoulou, Maria (); Battistella, Enzo (); Henry, Theophraste (); Lerousseau, Marvin (); Leroy, Amaury (); Paragios, Nikos (); Deutsch, Eric ()",,"Estienne, ThÃ©o (); Vakalopoulou, Maria (); Battistella, Enzo (); Henry, Theophraste (); Lerousseau, Marvin (); Leroy, Amaury (); Paragios, Nikos (); Deutsch, Eric ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143055106,46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
515,pub.1139631047,10.48550/arxiv.2107.04537,,,Modality specific U-Net variants for biomedical image segmentation: A  survey,"With the advent of advancements in deep learning approaches, such as deep
convolution neural network, residual neural network, adversarial network; U-Net
architectures are most widely utilized in biomedical image segmentation to
address the automation in identification and detection of the target regions or
sub-regions. In recent studies, U-Net based approaches have illustrated
state-of-the-art performance in different applications for the development of
computer-aided diagnosis systems for early diagnosis and treatment of diseases
such as brain tumor, lung cancer, alzheimer, breast cancer, etc., using various
modalities. This article contributes in presenting the success of these
approaches by describing the U-Net framework, followed by the comprehensive
analysis of the U-Net variants by performing 1) inter-modality, and 2)
intra-modality categorization to establish better insights into the associated
challenges and solutions. Besides, this article also highlights the
contribution of U-Net based frameworks in the ongoing pandemic, severe acute
respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.
Finally, the strengths and similarities of these U-Net variants are analysed
along with the challenges involved in biomedical image segmentation to uncover
promising future research directions in this area.",,,arXiv,,,2021-07-09,2021,,,,,,All OA, Green,Preprint,"Punn, Narinder Singh; Agarwal, Sonali","Punn, Narinder Singh (); Agarwal, Sonali ()",,"Punn, Narinder Singh (); Agarwal, Sonali ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139631047,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
514,pub.1155386519,10.3389/fcomp.2023.1071174,,,Lowering the computational barrier: Partially Bayesian neural networks for transparency in medical imaging AI,"Deep Neural Networks (DNNs) can provide clinicians with fast and accurate predictions that are highly valuable for high-stakes medical decision-making, such as in brain tumor segmentation and treatment planning. However, these models largely lack transparency about the uncertainty in their predictions, potentially giving clinicians a false sense of reliability that may lead to grave consequences in patient care. Growing calls for Transparent and Responsible AI have promoted Uncertainty Quantification (UQ) to capture and communicate uncertainty in a systematic and principled manner. However, traditional Bayesian UQ methods remain prohibitively costly for large, million-dimensional tumor segmentation DNNs such as the U-Net. In this work, we discuss a computationally-efficient UQ approach via the partially Bayesian neural networks (pBNN). In pBNN, only a single layer, strategically selected based on gradient-based sensitivity analysis, is targeted for Bayesian inference. We illustrate the effectiveness of pBNN in capturing the full uncertainty for a 7.8-million parameter U-Net. We also demonstrate how practitioners and model developers can use the pBNN's predictions to better understand the model's capabilities and behavior.","Experiments were performed on Armis2 HPC Clusters provided by UM&#x27;s Precision Health Initiative. Advice on clinical perspective on gliomas was provided by Dr. Danielle Bitterman and Nicholas Wang. Additionally, we thank Santhoshi Krishnan, Jane Im, and Sumit Asthana for providing valuable editing feedback.","SP, XH, and AR were partially supported by the University of Michigan (UM) Michigan Institute for Computational Discovery and Engineering (MICDE) Catalyst Grant Enabling Tractable Uncertainty Quantification for High-Dimensional Predictive AI Systems in Computational Medicine. AR was supported by CCSG Bioinformatics Shared Resource 5 P30 CA046592, a Research Scholar Grant from the American Cancer Society (RSG-16-005-01), and a UM Precision Health Investigator Award to AR (along with L. Rozek and M. Sartor). SP and AR were also partially supported by the NCI Grant R37-CA214955. JH, XH, and NB were supported in part by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, under Award Numbers DE-SC0021397 and DE-SC0021398. This paper was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.",Frontiers in Computer Science,,,2023-02-15,2023,2023-02-15,,5,,1071174,Closed,Article,"Prabhudesai, Snehal; Hauth, Jeremiah; Guo, Dingkun; Rao, Arvind; Banovic, Nikola; Huan, Xun","Prabhudesai, Snehal (Department of Computer Science and Engineering, University of Michigan, Ann Arbor, MI, United States); Hauth, Jeremiah (Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, United States); Guo, Dingkun (Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States); Rao, Arvind (Department of Computational Medicine and Bioinformatics, Michigan Medicine, Ann Arbor, MI, United States; Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States); Banovic, Nikola (Department of Computer Science and Engineering, University of Michigan, Ann Arbor, MI, United States); Huan, Xun (Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, United States)","Prabhudesai, Snehal (University of MichiganâAnn Arbor)","Prabhudesai, Snehal (University of MichiganâAnn Arbor); Hauth, Jeremiah (University of MichiganâAnn Arbor); Guo, Dingkun (Carnegie Mellon University); Rao, Arvind (Michigan Medicine; University of MichiganâAnn Arbor); Banovic, Nikola (University of MichiganâAnn Arbor); Huan, Xun (University of MichiganâAnn Arbor)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155386519,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
514,pub.1144437923,10.48550/arxiv.2201.00942,,,External Attention Assisted Multi-Phase Splenic Vascular Injury  Segmentation with Limited Data,"The spleen is one of the most commonly injured solid organs in blunt
abdominal trauma. The development of automatic segmentation systems from
multi-phase CT for splenic vascular injury can augment severity grading for
improving clinical decision support and outcome prediction. However, accurate
segmentation of splenic vascular injury is challenging for the following
reasons: 1) Splenic vascular injury can be highly variant in shape, texture,
size, and overall appearance; and 2) Data acquisition is a complex and
expensive procedure that requires intensive efforts from both data scientists
and radiologists, which makes large-scale well-annotated datasets hard to
acquire in general.
  In light of these challenges, we hereby design a novel framework for
multi-phase splenic vascular injury segmentation, especially with limited data.
On the one hand, we propose to leverage external data to mine pseudo splenic
masks as the spatial attention, dubbed external attention, for guiding the
segmentation of splenic vascular injury. On the other hand, we develop a
synthetic phase augmentation module, which builds upon generative adversarial
networks, for populating the internal data by fully leveraging the relation
between different phases. By jointly enforcing external attention and
populating internal data representation during training, our proposed method
outperforms other competing methods and substantially improves the popular
DeepLab-v3+ baseline by more than 7% in terms of average DSC, which confirms
its effectiveness.",,,arXiv,,,2022-01-03,2022,,,,,,All OA, Green,Preprint,"Zhou, Yuyin; Dreizin, David; Wang, Yan; Liu, Fengze; Shen, Wei; Yuille, Alan L.","Zhou, Yuyin (); Dreizin, David (); Wang, Yan (); Liu, Fengze (); Shen, Wei (); Yuille, Alan L. ()",,"Zhou, Yuyin (); Dreizin, David (); Wang, Yan (); Liu, Fengze (); Shen, Wei (); Yuille, Alan L. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144437923,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
514,pub.1143966320,10.1016/j.neucom.2021.12.045,,,3D Md-Unet: A novel model of multi-dataset collaboration for medical image segmentation,"Image segmentation is widely used in the medical field. Convolutional neural network has become more diverse and effective in recent years. However, at present, most networks are designed for a single dataset (i.e., a single organ or target). The designed network is only suitable for a single dataset, and its accuracy is very different (especially small-size image datasets). In response to this problem, a collaborative network can be designed to simultaneously extract the specific and common features of a multi-dataset (i.e., multiple organs or targets). The network can be used for multi-dataset segmentation and help to balance the segmentation performance of different datasets, especially to improve the accuracy of small-size image datasets. By exploring the adapters modified by the convolution kernels, the adaptive weight update strategy and the network branched structure, the paper proposes a multi-dataset collaborative image segmentation network, called Md-Unet, which integrates a shared-specific adapter (SSA), an asymmetric similarity loss function with the proposed adaptive weight update strategy, and a dual-branch. Experimental results showed that compared with the baseline 3D U2Net, the accuracy of the module using the SSA was improved by 3.7%, using several loss functions with the proposed adaptive weight update strategy was improved by 0.64%â30.63%, and using dual-branch integrated architecture was improved by 17.47%. Moreover, Md-Unet had a significant improvement on small-size image datasets compared with single-dataset models.","This study was supported by the National Natural Science Foundation of China (No. U1811461), the State Key Development Program of China (No. 2018YFC0116904), and the Major Science and Technology Planning Project of Guangdong Province of China (No. 810229511112).",,Neurocomputing,,,2022-07,2022,,2022-07,492,,530-544,Closed,Article,"Lin, Manying; Cai, Qingling; Zhou, Jun","Lin, Manying (School of Intelligent Systems Engineering in Sun Yat-sen University, No. 132, Outer Ring East Road, University Town, Panyu District, Guangzhou City, Guangdong Province, China); Cai, Qingling (School of Intelligent Systems Engineering in Sun Yat-sen University, No. 132, Outer Ring East Road, University Town, Panyu District, Guangzhou City, Guangdong Province, China); Zhou, Jun (School of Intelligent Systems Engineering in Sun Yat-sen University, No. 132, Outer Ring East Road, University Town, Panyu District, Guangzhou City, Guangdong Province, China)","Cai, Qingling (Sun Yat-sen University)","Lin, Manying (Sun Yat-sen University); Cai, Qingling (Sun Yat-sen University); Zhou, Jun (Sun Yat-sen University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1143966320,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,,
514,pub.1154995257,10.1109/jbhi.2023.3240844,,,Self-supervised Tumor Segmentation with Sim2Real Adaptation,"This paper targets on self-supervised tumor segmentation. We make the following contributions: (i) we take inspiration from the observation that tumors are often characterised independently of their contexts, we propose a novel proxy task âlayer-decompositionâ, that closely matches the goal of the downstream task, and design a scalable pipeline for generating synthetic tumor data for pre-training; (ii) we propose a two-stage Sim2Real training regime for unsupervised tumor segmentation, where we first pre-train a model with simulated tumors, and then adopt a self-training strategy for downstream data adaptation; (iii) when evaluating on different tumor segmentation benchmarks, e.g.Â BraTS2018 for brain tumor segmentation and LiTS2017 for liver tumor segmentation, our approach achieves state-of-the-art segmentation performance under the unsupervised setting. While transferring the model for tumor segmentation under a low-annotation regime, the proposed approach also outperforms all existing self-supervised approaches; (iv) we conduct extensive ablation studies to analyse the critical components in data simulation, and validate the necessity of different proxy tasks. We demonstrate that, with sufficient texture randomization in simulation, model trained on synthetic data can effortlessly generalise to datasets with real tumors.",,,IEEE Journal of Biomedical and Health Informatics,,,2023-01-31,2023,2023-01-31,,PP,99,1-13,Closed,Article,"Zhang, Xiaoman; Xie, Weidi; Huang, Chaoqin; Zhang, Ya; Chen, Xin; Tian, Qi; Wang, Yanfeng","Zhang, Xiaoman (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China); Xie, Weidi (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China); Huang, Chaoqin (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China); Zhang, Ya (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China); Chen, Xin (Huawei Cloud, Shanghai, China); Tian, Qi (Huawei Cloud, Shanghai, China); Wang, Yanfeng (Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China)",,"Zhang, Xiaoman (Shanghai Jiao Tong University); Xie, Weidi (Shanghai Jiao Tong University); Huang, Chaoqin (Shanghai Jiao Tong University); Zhang, Ya (Shanghai Jiao Tong University); Chen, Xin (); Tian, Qi (); Wang, Yanfeng (Shanghai Jiao Tong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154995257,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
514,pub.1153364406,10.48550/arxiv.2212.01082,,,Membership Inference Attacks Against Semantic Segmentation Models,"Membership inference attacks aim to infer whether a data record has been used
to train a target model by observing its predictions. In sensitive domains such
as healthcare, this can constitute a severe privacy violation. In this work we
attempt to address the existing knowledge gap by conducting an exhaustive study
of membership inference attacks and defences in the domain of semantic image
segmentation. Our findings indicate that for certain threat models, these
learning settings can be considerably more vulnerable than the previously
considered classification settings. We additionally investigate a threat model
where a dishonest adversary can perform model poisoning to aid their inference
and evaluate the effects that these adaptations have on the success of
membership inference attacks. We quantitatively evaluate the attacks on a
number of popular model architectures across a variety of semantic segmentation
tasks, demonstrating that membership inference attacks in this domain can
achieve a high success rate and defending against them may result in
unfavourable privacy-utility trade-offs or increased computational costs.",,,arXiv,,,2022-12-02,2022,,,,,,All OA, Green,Preprint,"Chobola, Tomas; Usynin, Dmitrii; Kaissis, Georgios","Chobola, Tomas (); Usynin, Dmitrii (); Kaissis, Georgios ()",,"Chobola, Tomas (); Usynin, Dmitrii (); Kaissis, Georgios ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153364406,46 Information and Computing Sciences, 4604 Cybersecurity and Privacy,,,,,,,,,,
514,pub.1141151083,10.48550/arxiv.2109.07138,,,Patch-based Medical Image Segmentation using Matrix Product State Tensor  Networks,"Tensor networks are efficient factorisations of high-dimensional tensors into
a network of lower-order tensors. They have been most commonly used to model
entanglement in quantum many-body systems and more recently are witnessing
increased applications in supervised machine learning. In this work, we
formulate image segmentation in a supervised setting with tensor networks. The
key idea is to first lift the pixels in image patches to exponentially
high-dimensional feature spaces and using a linear decision hyper-plane to
classify the input pixels into foreground and background classes. The
high-dimensional linear model itself is approximated using the matrix product
state (MPS) tensor network. The MPS is weight-shared between the
non-overlapping image patches resulting in our strided tensor network model.
The performance of the proposed model is evaluated on three 2D- and one 3D-
biomedical imaging datasets. The performance of the proposed tensor network
segmentation model is compared with relevant baseline methods. In the 2D
experiments, the tensor network model yields competitive performance compared
to the baseline methods while being more resource efficient.",,,arXiv,,,2021-09-15,2021,,,,,,All OA, Green,Preprint,"Selvan, Raghavendra; Dam, Erik B; Flensborg, SÃ¸ren Alexander; Petersen, Jens","Selvan, Raghavendra (); Dam, Erik B (); Flensborg, SÃ¸ren Alexander (); Petersen, Jens ()",,"Selvan, Raghavendra (); Dam, Erik B (); Flensborg, SÃ¸ren Alexander (); Petersen, Jens ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141151083,46 Information and Computing Sciences, 4611 Machine Learning, 51 Physical Sciences,,,,,,,,,
513,pub.1133007539,10.1016/j.patcog.2020.107762,,,Automatic pancreas segmentation based on lightweight DCNN modules and spatial prior propagation,"Nowadays, pancreas segmentation in CT scans has gained more and more attention for computer-assisted diagnosis of inflammation (pancreatitis) or cancer. Despite the thrilling success of deep convolutional neural networks (DCNNs) in automatic pancreas segmentation, the heavy computational complexity of such networks impedes the deployment in clinical applications. To alleviate this issue, this paper establishes a novel end-to-end DCNN model for pursuing high-accurate automatic pancreas segmentation but with low computational cost. Specifically, built upon a simplified FCN architecture, we propose two novel network modules, named as the scale-transferrable feature fusion module (STFFM) and prior propagation module (PPM), respectively, for pancreas segmentation. Equipped with the scale-transferrable operation, STFFM can learn rich fusion features but with very lightweight network architecture. By dynamically adapting the spatial prior to the input slice data as well as the deep feature maps, PPM enables the network model to explore informative spatial priors for pancreas segmentation. Comprehensive experiments on the NIH dataset and the MSD dataset are conducted to evaluate the proposed approach. The obtained experimental results demonstrate that our approach can effectively reduce the computational cost and simultaneously archive the outperforming performance when compared to the state-of-the-art methods.","This work was supported in part by the National Natural Science Foundation of China under Grants 61876140 and 61773301, and the China Postdoctoral Support Scheme for Innovative Talents under Grant BX20180236.",,Pattern Recognition,,,2021-06,2021,,2021-06,114,,107762,Closed,Article,"Zhang, Dingwen; Zhang, Jiajia; Zhang, Qiang; Han, Jungong; Zhang, Shu; Han, Junwei","Zhang, Dingwen (School of Mechano-Electronic Engineering, Xidian University, No. 2 South Taibai Road, Xiâan, Shaanxi 710071, China); Zhang, Jiajia (School of Mechano-Electronic Engineering, Xidian University, No. 2 South Taibai Road, Xiâan, Shaanxi 710071, China); Zhang, Qiang (School of Mechano-Electronic Engineering, Xidian University, No. 2 South Taibai Road, Xiâan, Shaanxi 710071, China); Han, Jungong (Computer Science Department, Aberystwyth University, Ceredigion, SY23 3FL, UK); Zhang, Shu (Deepwise AI Lab, NO. 8 Haidian Street, Haidian District, Beijing, 100080, China); Han, Junwei (School of Automation, Northwestern Polytechnical University, 127 West Youyi Road, Beilin District, Xiâan Shaanxi, 710072, PR China)","Zhang, Qiang (Xidian University); Han, Jungong (Aberystwyth University)","Zhang, Dingwen (Xidian University); Zhang, Jiajia (Xidian University); Zhang, Qiang (Xidian University); Han, Jungong (Aberystwyth University); Zhang, Shu (); Han, Junwei (Northwestern Polytechnical University)",36,36,,28.49,,https://app.dimensions.ai/details/publication/pub.1133007539,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,,
512,pub.1137047872,10.48550/arxiv.2104.02847,,,Deep Implicit Statistical Shape Models for 3D Medical Image Delineation,"3D delineation of anatomical structures is a cardinal goal in medical imaging
analysis. Prior to deep learning, statistical shape models that imposed
anatomical constraints and produced high quality surfaces were a core
technology. Prior to deep learning, statistical shape models that imposed
anatomical constraints and produced high quality surfaces were a core
technology. Today fully-convolutional networks (FCNs), while dominant, do not
offer these capabilities. We present deep implicit statistical shape models
(DISSMs), a new approach to delineation that marries the representation power
of convolutional neural networks (CNNs) with the robustness of SSMs. DISSMs use
a deep implicit surface representation to produce a compact and descriptive
shape latent space that permits statistical models of anatomical variance. To
reliably fit anatomically plausible shapes to an image, we introduce a novel
rigid and non-rigid pose estimation pipeline that is modelled as a Markov
decision process(MDP). We outline a training regime that includes inverted
episodic training and a deep realization of marginal space learning (MSL).
Intra-dataset experiments on the task of pathological liver segmentation
demonstrate that DISSMs can perform more robustly than three leading FCN
models, including nnU-Net: reducing the mean Hausdorff distance (HD) by
7.7-14.3mm and improving the worst case Dice-Sorensen coefficient (DSC) by
1.2-2.3%. More critically, cross-dataset experiments on a dataset directly
reflecting clinical deployment scenarios demonstrate that DISSMs improve the
mean DSC and HD by 3.5-5.9% and 12.3-24.5mm, respectively, and the worst-case
DSC by 5.4-7.3%. These improvements are over and above any benefits from
representing delineations with high-quality surface.",,,arXiv,,,2021-04-06,2021,,,,,,All OA, Green,Preprint,"Raju, Ashwin; Miao, Shun; Jin, Dakai; Lu, Le; Huang, Junzhou; Harrison, Adam P.","Raju, Ashwin (); Miao, Shun (); Jin, Dakai (); Lu, Le (); Huang, Junzhou (); Harrison, Adam P. ()",,"Raju, Ashwin (); Miao, Shun (); Jin, Dakai (); Lu, Le (); Huang, Junzhou (); Harrison, Adam P. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1137047872,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
512,pub.1136528992,10.1002/cpe.6276,,,A review of research on coâtraining,"Coâtraining algorithm is one of the main methods of semiâsupervised learning in machine learning, which explores the effective information in unlabeled data by multiâlearner collaboration. Based on the development of coâtraining algorithm, the research work in recent years was further summarized in this article. In particular, three main steps of relevant coâtraining algorithms are introduced: view acquisition, learners' differentiation, and label confidence estimation. Finally, we summarized the problems existing in the current coâtraining methods, gave some suggestions for improvement, and looked forward to the future development direction of the coâtraining algorithm.","This work was supported by the National Natural Science Foundation of China (Grant No. 61901436), the National Natural Science Foundation of China (No. 61972040), and the Premium Funding Project for Academic Human Resources Development in Beijing Union University (No. BPHR2020AZ03).","Funding information the National Natural Science Foundation of China, 61901436; 61972040; the Premium Funding Project for Academic Human Resources Development in Beijing Union University, BPHR2020AZ03",Concurrency and Computation Practice and Experience,,,2021-03-20,2021,2021-03-20,,,,,Closed,Article,"Ning, Xin; Wang, Xinran; Xu, Shaohui; Cai, Weiwei; Zhang, Liping; Yu, Lina; Li, Wenfa","Ning, Xin (Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; Cognitive Computing Technology Joint Laboratory, Wave Group, Beijing, China; Shenzhen Wave Kingdom Co., Ltd., Shenzhen, China); Wang, Xinran (Beijing University of Posts and Telecommunications, Beijing, China); Xu, Shaohui (Cognitive Computing Technology Joint Laboratory, Wave Group, Beijing, China; Shenzhen Wave Kingdom Co., Ltd., Shenzhen, China); Cai, Weiwei (School of Logistics and Transportation, Central South University of Forestry and Technology, Changsha, China); Zhang, Liping (Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; Cognitive Computing Technology Joint Laboratory, Wave Group, Beijing, China); Yu, Lina (Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China); Li, Wenfa (College of Robotics, Beijing Union University, Beijing, China)","Yu, Lina (Institute of Semiconductors); Li, Wenfa (Beijing Union University)","Ning, Xin (Institute of Semiconductors); Wang, Xinran (Beijing University of Posts and Telecommunications); Xu, Shaohui (); Cai, Weiwei (Central South University of Forestry and Technology); Zhang, Liping (Institute of Semiconductors); Yu, Lina (Institute of Semiconductors); Li, Wenfa (Beijing Union University)",59,59,,48.29,,https://app.dimensions.ai/details/publication/pub.1136528992,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
512,pub.1127553805,10.48550/arxiv.2005.05856,,,Probabilistic Semantic Segmentation Refinement by Monte Carlo Region  Growing,"Semantic segmentation with fine-grained pixel-level accuracy is a fundamental
component of a variety of computer vision applications. However, despite the
large improvements provided by recent advances in the architectures of
convolutional neural networks, segmentations provided by modern
state-of-the-art methods still show limited boundary adherence. We introduce a
fully unsupervised post-processing algorithm that exploits Monte Carlo sampling
and pixel similarities to propagate high-confidence pixel labels into regions
of low-confidence classification. Our algorithm, which we call probabilistic
Region Growing Refinement (pRGR), is based on a rigorous mathematical
foundation in which clusters are modelled as multivariate normally distributed
sets of pixels. Exploiting concepts of Bayesian estimation and variance
reduction techniques, pRGR performs multiple refinement iterations at varied
receptive fields sizes, while updating cluster statistics to adapt to local
image features. Experiments using multiple modern semantic segmentation
networks and benchmark datasets demonstrate the effectiveness of our approach
for the refinement of segmentation predictions at different levels of
coarseness, as well as the suitability of the variance estimates obtained in
the Monte Carlo iterations as uncertainty measures that are highly correlated
with segmentation accuracy.",,,arXiv,,,2020-05-12,2020,,,,,,All OA, Green,Preprint,"Dias, Philipe A.; Medeiros, Henry","Dias, Philipe A. (); Medeiros, Henry ()",,"Dias, Philipe A. (); Medeiros, Henry ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1127553805,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
510,pub.1143054548,10.1007/s11633-021-1313-0,,,Supervised and Semi-supervised Methods for Abdominal Organ Segmentation: A Review,"Abdominal organ segmentation is the segregation of a single or multiple abdominal organ(s) into semantic image segments of pixels identified with homogeneous features such as color and texture, and intensity. The abdominal organ(s) condition is mostly connected with greater morbidity and mortality. Most patients often have asymptomatic abdominal conditions and symptoms, which are often recognized late; hence the abdomen has been the third most common cause of damage to the human body. That notwithstanding, there may be improved outcomes where the condition of an abdominal organ is detected earlier. Over the years, supervised and semi-supervised machine learning methods have been used to segment abdominal organ(s) in order to detect the organ(s) condition. The supervised methods perform well when the used training data represents the target data, but the methods require large manually annotated data and have adaptation problems. The semi-supervised methods are fast but record poor performance than the supervised if assumptions about the data fail to hold. Current state-of-the-art methods of supervised segmentation are largely based on deep learning techniques due to their good accuracy and success in real world applications. Though it requires a large amount of training data for automatic feature extraction, deep learning can hardly be used. As regards the semi-supervised methods of segmentation, self-training and graph-based techniques have attracted much research attention. Self-training can be used with any classifier but does not have a mechanism to rectify mistakes early. Graph-based techniques thrive on their convexity, scalability, and effectiveness in application but have an out-of-sample problem. In this review paper, a study has been carried out on supervised and semi-supervised methods of performing abdominal organ segmentation. An observation of the current approaches, connection and gaps are identified, and prospective future research opportunities are enumerated.","This work was supported by National Natural Science Foundation of China (Nos. 61772242, 61976106 and 61572239), the China Postdoctoral Science Foundation (No. 2017M611737), the Six Talent Peaks Project in Jiangsu Province (No. DZXX-122), and the Key Special Project of Health and Family Planning Science and Technology in Zhenjiang City (No. SHW2017019). The authors would like to thank the Radiologists of the Medical Imaging Department of Affiliated Hospital of Jiangsu University.",,Machine Intelligence Research,,,2021-11-25,2021,2021-11-25,2021-12,18,6,887-914,Closed,Article,"Senkyire, Isaac Baffour; Liu, Zhe","Senkyire, Isaac Baffour (School of Computer Science and Communication Engineering, Jiangsu University, 212013, Zhenjiang, China; Computer Science Department, Ghana Communication Technology University, Accra, Ghana); Liu, Zhe (School of Computer Science and Communication Engineering, Jiangsu University, 212013, Zhenjiang, China)","Liu, Zhe (Jiangsu University)","Senkyire, Isaac Baffour (Jiangsu University); Liu, Zhe (Jiangsu University)",3,3,,2.46,,https://app.dimensions.ai/details/publication/pub.1143054548,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
488,pub.1154197453,10.48550/arxiv.2301.00785,,,CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection,"An increasing number of public datasets have shown a marked clinical impact
on assessing anatomical structures. However, each of the datasets is small,
partially labeled, and rarely investigates severe tumor subjects. Moreover,
current models are limited to segmenting specific organs/tumors, which can not
be extended to novel domains and classes. To tackle these limitations, we
introduce embedding learned from Contrastive Language-Image Pre-training (CLIP)
to segmentation models, dubbed the CLIP-Driven Universal Model. The Universal
Model can better segment 25 organs and 6 types of tumors by exploiting the
semantic relationship between abdominal structures. The model is developed from
an assembly of 14 datasets with 3,410 CT scans and evaluated on 6,162 external
CT scans from 3 datasets. We achieve the state-of-the-art results on Beyond The
Cranial Vault (BTCV). Compared with dataset-specific models, the Universal
Model is computationally more efficient (6x faster), generalizes better to CT
scans from varying sites, and shows stronger transfer learning performance on
novel tasks. The design of CLIP embedding enables the Universal Model to be
easily extended to new classes without catastrophically forgetting the
previously learned classes.",,,arXiv,,,2023-01-02,2023,,,,,,All OA, Green,Preprint,"Liu, Jie; Zhang, Yixiao; Chen, Jie-Neng; Xiao, Junfei; Lu, Yongyi; Landman, Bennett A.; Yuan, Yixuan; Yuille, Alan; Tang, Yucheng; Zhou, Zongwei","Liu, Jie (); Zhang, Yixiao (); Chen, Jie-Neng (); Xiao, Junfei (); Lu, Yongyi (); Landman, Bennett A. (); Yuan, Yixuan (); Yuille, Alan (); Tang, Yucheng (); Zhou, Zongwei ()",,"Liu, Jie (); Zhang, Yixiao (); Chen, Jie-Neng (); Xiao, Junfei (); Lu, Yongyi (); Landman, Bennett A. (); Yuan, Yixuan (); Yuille, Alan (); Tang, Yucheng (); Zhou, Zongwei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154197453,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
487,pub.1154351173,10.48550/arxiv.2301.02554,,,MSCDA: Multi-level Semantic-guided Contrast Improves Unsupervised Domain  Adaptation for Breast MRI Segmentation in Small Datasets,"Deep learning (DL) applied to breast tissue segmentation in magnetic
resonance imaging (MRI) has received increased attention in the last decade,
however, the domain shift which arises from different vendors, acquisition
protocols, and biological heterogeneity, remains an important but challenging
obstacle on the path towards clinical implementation. Recently, unsupervised
domain adaptation (UDA) methods have attempted to mitigate this problem by
incorporating self-training with contrastive learning. To better exploit the
underlying semantic information of the image at different levels, we propose a
Multi-level Semantic-guided Contrastive Domain Adaptation (MSCDA) framework to
align the feature representation between domains. In particular, we extend the
contrastive loss by incorporating pixel-to-pixel, pixel-to-centroid, and
centroid-to-centroid contrasts to integrate semantic information of images. We
utilize a category-wise cross-domain sampling strategy to sample anchors from
target images and build a hybrid memory bank to store samples from source
images. Two breast MRI datasets were retrospectively collected: The source
dataset contains non-contrast MRI examinations from 11 healthy volunteers and
the target dataset contains contrast-enhanced MRI examinations of 134 invasive
breast cancer patients. We set up experiments from source T2W image to target
dynamic contrast-enhanced (DCE)-T1W image (T2W-to-T1W) and from source T1W
image to target T2W image (T1W-to-T2W). The proposed method achieved Dice
similarity coefficient (DSC) of 89.2\% and 84.0\% in T2W-to-T1W and T1W-to-T2W,
respectively, outperforming state-of-the-art methods. Notably, good performance
is still achieved with a smaller source dataset, proving that our framework is
label-efficient.",,,arXiv,,,2023-01-04,2023,,,,,,All OA, Green,Preprint,"Kuang, Sheng; Woodruff, Henry C.; Granzier, Renee; van Nijnatten, Thiemo J. A.; Lobbes, Marc B. I.; Smidt, Marjolein L.; Lambin, Philippe; Mehrkanoon, Siamak","Kuang, Sheng (); Woodruff, Henry C. (); Granzier, Renee (); van Nijnatten, Thiemo J. A. (); Lobbes, Marc B. I. (); Smidt, Marjolein L. (); Lambin, Philippe (); Mehrkanoon, Siamak ()",,"Kuang, Sheng (); Woodruff, Henry C. (); Granzier, Renee (); van Nijnatten, Thiemo J. A. (); Lobbes, Marc B. I. (); Smidt, Marjolein L. (); Lambin, Philippe (); Mehrkanoon, Siamak ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154351173,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
486,pub.1154955270,10.48550/arxiv.2301.12291,,,"Towards a Single Unified Model for Effective Detection, Segmentation,  and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans","Human readers or radiologists routinely perform full-body multi-organ
multi-disease detection and diagnosis in clinical practice, while most medical
AI systems are built to focus on single organs with a narrow list of a few
diseases. This might severely limit AI's clinical adoption. A certain number of
AI models need to be assembled non-trivially to match the diagnostic process of
a human reading a CT scan. In this paper, we construct a Unified Tumor
Transformer (UniT) model to detect (tumor existence and location) and diagnose
(tumor characteristics) eight major cancer-prevalent organs in CT scans. UniT
is a query-based Mask Transformer model with the output of multi-organ and
multi-tumor semantic segmentation. We decouple the object queries into organ
queries, detection queries and diagnosis queries, and further establish
hierarchical relationships among the three groups. This clinically-inspired
architecture effectively assists inter- and intra-organ representation learning
of tumors and facilitates the resolution of these complex, anatomically related
multi-organ cancer image reading tasks. UniT is trained end-to-end using a
curated large-scale CT images of 10,042 patients including eight major types of
cancers and occurring non-cancer tumors (all are pathology-confirmed with 3D
tumor masks annotated by radiologists). On the test set of 631 patients, UniT
has demonstrated strong performance under a set of clinically relevant
evaluation metrics, substantially outperforming both multi-organ segmentation
methods and an assembly of eight single-organ expert models in tumor detection,
segmentation, and diagnosis. Such a unified multi-cancer image reading model
(UniT) can significantly reduce the number of false positives produced by
combined multi-system models. This moves one step closer towards a universal
high-performance cancer screening tool.",,,arXiv,,,2023-01-28,2023,,,,,,All OA, Green,Preprint,"Chen, Jieneng; Xia, Yingda; Yao, Jiawen; Yan, Ke; Zhang, Jianpeng; Lu, Le; Wang, Fakai; Zhou, Bo; Qiu, Mingyan; Yu, Qihang; Yuan, Mingze; Fang, Wei; Tang, Yuxing; Xu, Minfeng; Zhou, Jian; Zhao, Yuqian; Wang, Qifeng; Ye, Xianghua; Yin, Xiaoli; Shi, Yu; Chen, Xin; Zhou, Jingren; Yuille, Alan; Liu, Zaiyi; Zhang, Ling","Chen, Jieneng (); Xia, Yingda (); Yao, Jiawen (); Yan, Ke (); Zhang, Jianpeng (); Lu, Le (); Wang, Fakai (); Zhou, Bo (); Qiu, Mingyan (); Yu, Qihang (); Yuan, Mingze (); Fang, Wei (); Tang, Yuxing (); Xu, Minfeng (); Zhou, Jian (); Zhao, Yuqian (); Wang, Qifeng (); Ye, Xianghua (); Yin, Xiaoli (); Shi, Yu (); Chen, Xin (); Zhou, Jingren (); Yuille, Alan (); Liu, Zaiyi (); Zhang, Ling ()",,"Chen, Jieneng (); Xia, Yingda (); Yao, Jiawen (); Yan, Ke (); Zhang, Jianpeng (); Lu, Le (); Wang, Fakai (); Zhou, Bo (); Qiu, Mingyan (); Yu, Qihang (); Yuan, Mingze (); Fang, Wei (); Tang, Yuxing (); Xu, Minfeng (); Zhou, Jian (); Zhao, Yuqian (); Wang, Qifeng (); Ye, Xianghua (); Yin, Xiaoli (); Shi, Yu (); Chen, Xin (); Zhou, Jingren (); Yuille, Alan (); Liu, Zaiyi (); Zhang, Ling ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154955270,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
485,pub.1153402613,10.48550/arxiv.2212.02014,,,Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query  Embedding,"Automatic parsing of human anatomies at instance-level from 3D computed
tomography (CT) scans is a prerequisite step for many clinical applications.
The presence of pathologies, broken structures or limited field-of-view (FOV)
all can make anatomy parsing algorithms vulnerable. In this work, we explore
how to exploit and conduct the prosperous detection-then-segmentation paradigm
in 3D medical data, and propose a steerable, robust, and efficient computing
framework for detection, identification, and segmentation of anatomies in CT
scans. Considering complicated shapes, sizes and orientations of anatomies,
without lose of generality, we present the nine degrees-of-freedom (9-DoF) pose
estimation solution in full 3D space using a novel single-stage,
non-hierarchical forward representation. Our whole framework is executed in a
steerable manner where any anatomy of interest can be directly retrieved to
further boost the inference efficiency. We have validated the proposed method
on three medical imaging parsing tasks of ribs, spine, and abdominal organs.
For rib parsing, CT scans have been annotated at the rib instance-level for
quantitative evaluation, similarly for spine vertebrae and abdominal organs.
Extensive experiments on 9-DoF box detection and rib instance segmentation
demonstrate the effectiveness of our framework (with the identification rate of
97.0% and the segmentation Dice score of 90.9%) in high efficiency, compared
favorably against several strong baselines (e.g., CenterNet, FCOS, and
nnU-Net). For spine identification and segmentation, our method achieves a new
state-of-the-art result on the public CTSpine1K dataset. Last, we report highly
competitive results in multi-organ segmentation at FLARE22 competition. Our
annotations, code and models will be made publicly available at:
https://github.com/alibaba-damo-academy/Med_Query.",,,arXiv,,,2022-12-04,2022,,,,,,All OA, Green,Preprint,"Guo, Heng; Zhang, Jianfeng; Yan, Ke; Lu, Le; Xu, Minfeng","Guo, Heng (); Zhang, Jianfeng (); Yan, Ke (); Lu, Le (); Xu, Minfeng ()",,"Guo, Heng (); Zhang, Jianfeng (); Yan, Ke (); Lu, Le (); Xu, Minfeng ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153402613,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
485,pub.1146477071,10.48550/arxiv.2203.10144,,,Closing the Generalization Gap of Cross-silo Federated Medical Image  Segmentation,"Cross-silo federated learning (FL) has attracted much attention in medical
imaging analysis with deep learning in recent years as it can resolve the
critical issues of insufficient data, data privacy, and training efficiency.
However, there can be a generalization gap between the model trained from FL
and the one from centralized training. This important issue comes from the
non-iid data distribution of the local data in the participating clients and is
well-known as client drift. In this work, we propose a novel training framework
FedSM to avoid the client drift issue and successfully close the generalization
gap compared with the centralized training for medical image segmentation tasks
for the first time. We also propose a novel personalized FL objective
formulation and a new method SoftPull to solve it in our proposed framework
FedSM. We conduct rigorous theoretical analysis to guarantee its convergence
for optimizing the non-convex smooth objective function. Real-world medical
image segmentation experiments using deep FL validate the motivations and
effectiveness of our proposed method.",,,arXiv,,,2022-03-18,2022,,,,,,All OA, Green,Preprint,"Xu, An; Li, Wenqi; Guo, Pengfei; Yang, Dong; Roth, Holger; Hatamizadeh, Ali; Zhao, Can; Xu, Daguang; Huang, Heng; Xu, Ziyue","Xu, An (); Li, Wenqi (); Guo, Pengfei (); Yang, Dong (); Roth, Holger (); Hatamizadeh, Ali (); Zhao, Can (); Xu, Daguang (); Huang, Heng (); Xu, Ziyue ()",,"Xu, An (); Li, Wenqi (); Guo, Pengfei (); Yang, Dong (); Roth, Holger (); Hatamizadeh, Ali (); Zhao, Can (); Xu, Daguang (); Huang, Heng (); Xu, Ziyue ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146477071,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
485,pub.1132271368,10.48550/arxiv.2011.00325,,,Self-paced and self-consistent co-training for semi-supervised image  segmentation,"Deep co-training has recently been proposed as an effective approach for
image segmentation when annotated data is scarce. In this paper, we improve
existing approaches for semi-supervised segmentation with a self-paced and
self-consistent co-training method. To help distillate information from
unlabeled images, we first design a self-paced learning strategy for
co-training that lets jointly-trained neural networks focus on
easier-to-segment regions first, and then gradually consider harder ones.This
is achieved via an end-to-end differentiable loss inthe form of a generalized
Jensen Shannon Divergence(JSD). Moreover, to encourage predictions from
different networks to be both consistent and confident, we enhance this
generalized JSD loss with an uncertainty regularizer based on entropy. The
robustness of individual models is further improved using a self-ensembling
loss that enforces their prediction to be consistent across different training
iterations. We demonstrate the potential of our method on three challenging
image segmentation problems with different image modalities, using small
fraction of labeled data. Results show clear advantages in terms of performance
compared to the standard co-training baselines and recently proposed
state-of-the-art approaches for semi-supervised segmentation",,,arXiv,,,2020-10-31,2020,,,,,,All OA, Green,Preprint,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (); Zhang, Caiming (); Desrosiers, Christian ()",,"Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (); Zhang, Caiming (); Desrosiers, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1132271368,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
485,pub.1136998522,10.48550/arxiv.2104.02488,,,Weakly supervised segmentation with cross-modality equivariant  constraints,"Weakly supervised learning has emerged as an appealing alternative to
alleviate the need for large labeled datasets in semantic segmentation. Most
current approaches exploit class activation maps (CAMs), which can be generated
from image-level annotations. Nevertheless, resulting maps have been
demonstrated to be highly discriminant, failing to serve as optimal proxy
pixel-level labels. We present a novel learning strategy that leverages
self-supervision in a multi-modal image scenario to significantly enhance
original CAMs. In particular, the proposed method is based on two observations.
First, the learning of fully-supervised segmentation networks implicitly
imposes equivariance by means of data augmentation, whereas this implicit
constraint disappears on CAMs generated with image tags. And second, the
commonalities between image modalities can be employed as an efficient
self-supervisory signal, correcting the inconsistency shown by CAMs obtained
across multiple modalities. To effectively train our model, we integrate a
novel loss function that includes a within-modality and a cross-modality
equivariant term to explicitly impose these constraints during training. In
addition, we add a KL-divergence on the class prediction distributions to
facilitate the information exchange between modalities, which, combined with
the equivariant regularizers further improves the performance of our model.
Exhaustive experiments on the popular multi-modal BRATS dataset demonstrate
that our approach outperforms relevant recent literature under the same
learning conditions.",,,arXiv,,,2021-04-06,2021,,,,,,All OA, Green,Preprint,"Patel, Gaurav; Dolz, Jose","Patel, Gaurav (); Dolz, Jose ()",,"Patel, Gaurav (); Dolz, Jose ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136998522,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
485,pub.1130166240,10.48550/arxiv.2008.07030,,,Training CNN Classifiers for Semantic Segmentation using Partially  Annotated Images: with Application on Human Thigh and Calf MRI,"Objective: Medical image datasets with pixel-level labels tend to have a
limited number of organ or tissue label classes annotated, even when the images
have wide anatomical coverage. With supervised learning, multiple classifiers
are usually needed given these partially annotated datasets. In this work, we
propose a set of strategies to train one single classifier in segmenting all
label classes that are heterogeneously annotated across multiple datasets
without moving into semi-supervised learning. Methods: Masks were first created
from each label image through a process we termed presence masking. Three
presence masking modes were evaluated, differing mainly in weightage assigned
to the annotated and unannotated classes. These masks were then applied to the
loss function during training to remove the influence of unannotated classes.
Results: Evaluation against publicly available CT datasets shows that presence
masking is a viable method for training class-generic classifiers. Our
class-generic classifier can perform as well as multiple class-specific
classifiers combined, while the training duration is similar to that required
for one class-specific classifier. Furthermore, the class-generic classifier
can outperform the class-specific classifiers when trained on smaller datasets.
Finally, consistent results are observed from evaluations against human thigh
and calf MRI datasets collected in-house. Conclusion: The evaluation outcomes
show that presence masking is capable of significantly improving both training
and inference efficiency across imaging modalities and anatomical regions.
Improved performance may even be observed on small datasets. Significance:
Presence masking strategies can reduce the computational resources and costs
involved in manual medical image annotations. All codes are publicly available
at https://github.com/wong-ck/DeepSegment.",,,arXiv,,,2020-08-16,2020,,,,,,All OA, Green,Preprint,"Wong, Chun Kit; Marchesseau, Stephanie; Kalimeri, Maria; Yap, Tiang Siew; Teo, Serena S. H.; Krishna, Lingaraj; Franco-ObregÃ³n, Alfredo; Tay, Stacey K. H.; Khoo, Chin Meng; Lee, Philip T. H.; Leow, Melvin K. S.; Totman, John J.; Stephenson, Mary C.","Wong, Chun Kit (); Marchesseau, Stephanie (); Kalimeri, Maria (); Yap, Tiang Siew (); Teo, Serena S. H. (); Krishna, Lingaraj (); Franco-ObregÃ³n, Alfredo (); Tay, Stacey K. H. (); Khoo, Chin Meng (); Lee, Philip T. H. (); Leow, Melvin K. S. (); Totman, John J. (); Stephenson, Mary C. ()",,"Wong, Chun Kit (); Marchesseau, Stephanie (); Kalimeri, Maria (); Yap, Tiang Siew (); Teo, Serena S. H. (); Krishna, Lingaraj (); Franco-ObregÃ³n, Alfredo (); Tay, Stacey K. H. (); Khoo, Chin Meng (); Lee, Philip T. H. (); Leow, Melvin K. S. (); Totman, John J. (); Stephenson, Mary C. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1130166240,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
465,pub.1144818041,10.48550/arxiv.2201.07463,,,"Cortical lesions, central vein sign, and paramagnetic rim lesions in  multiple sclerosis: emerging machine learning techniques and future avenues","The current multiple sclerosis (MS) diagnostic criteria lack specificity, and
this may lead to misdiagnosis, which remains an issue in present-day clinical
practice. In addition, conventional biomarkers only moderately correlate with
MS disease progression. Recently, advanced MS lesional imaging biomarkers such
as cortical lesions (CL), the central vein sign (CVS), and paramagnetic rim
lesions (PRL), visible in specialized magnetic resonance imaging (MRI)
sequences, have shown higher specificity in differential diagnosis. Moreover,
studies have shown that CL and PRL are potential prognostic biomarkers, the
former correlating with cognitive impairments and the latter with early
disability progression. As machine learning-based methods have achieved
extraordinary performance in the assessment of conventional imaging biomarkers,
such as white matter lesion segmentation, several automated or semi-automated
methods have been proposed for CL, CVS, and PRL as well. In the present review,
we first introduce these advanced MS imaging biomarkers and their imaging
methods. Subsequently, we describe the corresponding machine learning-based
methods that were used to tackle these clinical questions, putting them into
context with respect to the challenges they are still facing, including
non-standardized MRI protocols, limited datasets, and moderate inter-rater
variability. We conclude by presenting the current limitations that prevent
their broader deployment and suggesting future research directions.",,,arXiv,,,2022-01-19,2022,,,,,,All OA, Green,Preprint,"La Rosa, Francesco; Wynen, Maxence; Al-Louzi, Omar; Beck, Erin S; Huelnhagen, Till; Maggi, Pietro; Thiran, Jean-Philippe; Kober, Tobias; Shinohara, Russell T; Sati, Pascal; Reich, Daniel S; Granziera, Cristina; Absinta, Martina; Cuadra, Meritxell Bach","La Rosa, Francesco (); Wynen, Maxence (); Al-Louzi, Omar (); Beck, Erin S (); Huelnhagen, Till (); Maggi, Pietro (); Thiran, Jean-Philippe (); Kober, Tobias (); Shinohara, Russell T (); Sati, Pascal (); Reich, Daniel S (); Granziera, Cristina (); Absinta, Martina (); Cuadra, Meritxell Bach ()",,"La Rosa, Francesco (); Wynen, Maxence (); Al-Louzi, Omar (); Beck, Erin S (); Huelnhagen, Till (); Maggi, Pietro (); Thiran, Jean-Philippe (); Kober, Tobias (); Shinohara, Russell T (); Sati, Pascal (); Reich, Daniel S (); Granziera, Cristina (); Absinta, Martina (); Cuadra, Meritxell Bach ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144818041,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
463,pub.1148519292,10.1007/s00530-022-00952-4,,,Automated brain tumor malignancy detection via 3D MRI using adaptive-3-D U-Net and heuristic-based deep neural network,"Using the 3D image from public benchmark sources, the experiment is initiated with pre-processing using skull stripping and contrast enhancement. Further, the segmentation of tumor region is performed by the Adaptive-3-D U-Net (A-3D-U-Net) utilized for the hybridized Butterfly Optimization Algorithm (BOA), and Tunicate Swarm Algorithm (TSA) termed to as ButterflyâTunicate Swarm Algorithm (B-TSA). The optimal segmentation of tumors is based on solving the multi-objective solution concerning âStructured Similarity Index (SSIM), Mean Square Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Dice Coefficientâ. From the segmented tumor region, the numerical features such as âmean, standard deviation, entropy, skewness, kurtosis, energy, contrast, inverse difference moment, directional moment, correlation, coarseness, and texture features like Local Ternary Pattern (LTP), and Local Tetra Pattern (LTrP)â are extracted. In the final stage, the detection of malignancy is performed by heuristic-based deep neural network (HDNN) using the same proposed B-TSA for the parameter optimization. The findings of applying the suggested methodology to 3D-MRI images from the Decathlon dataset demonstrate that the suggested technique is comparable to conventionalÂ methods for brain tumor segmentation.",,,Multimedia Systems,,,2022-06-08,2022,2022-06-08,2022-12,28,6,2247-2273,Closed,Article,"Manoj, K. C.; Dhas, D. Anto Sahaya","Manoj, K. C. (Electronics and Communication Engineering, A P J Abdul Kalam Technological University, Kerala, India); Dhas, D. Anto Sahaya (Department of Electronics and Communication Engineering, Vimal Jyothi Engineering College, Kannur, India)","Manoj, K. C. ","Manoj, K. C. (); Dhas, D. Anto Sahaya (Kannur University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148519292,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4606 Distributed Computing and Systems Software,,,,,,,,,
463,pub.1136235647,10.48550/arxiv.2103.04813,,,Boosting Semi-supervised Image Segmentation with Global and Local Mutual  Information Regularization,"The scarcity of labeled data often impedes the application of deep learning
to the segmentation of medical images. Semi-supervised learning seeks to
overcome this limitation by exploiting unlabeled examples in the learning
process. In this paper, we present a novel semi-supervised segmentation method
that leverages mutual information (MI) on categorical distributions to achieve
both global representation invariance and local smoothness. In this method, we
maximize the MI for intermediate feature embeddings that are taken from both
the encoder and decoder of a segmentation network. We first propose a global MI
loss constraining the encoder to learn an image representation that is
invariant to geometric transformations. Instead of resorting to
computationally-expensive techniques for estimating the MI on continuous
feature embeddings, we use projection heads to map them to a discrete cluster
assignment where MI can be computed efficiently. Our method also includes a
local MI loss to promote spatial consistency in the feature maps of the decoder
and provide a smoother segmentation. Since mutual information does not require
a strict ordering of clusters in two different assignments, we incorporate a
final consistency regularization loss on the output which helps align the
cluster labels throughout the network. We evaluate the method on four
challenging publicly-available datasets for medical image segmentation.
Experimental results show our method to outperform recently-proposed approaches
for semi-supervised segmentation and provide an accuracy near to full
supervision while training with very few annotated images.",,,arXiv,,,2021-03-08,2021,,,,,,All OA, Green,Preprint,"Peng, Jizong; Pedersoli, Marco; Desrosiers, Christian","Peng, Jizong (); Pedersoli, Marco (); Desrosiers, Christian ()",,"Peng, Jizong (); Pedersoli, Marco (); Desrosiers, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136235647,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
461,pub.1135670781,10.48550/arxiv.2102.10680,,,Transferable Visual Words: Exploiting the Semantics of Anatomical  Patterns for Self-supervised Learning,"This paper introduces a new concept called ""transferable visual words""
(TransVW), aiming to achieve annotation efficiency for deep learning in medical
image analysis. Medical imaging--focusing on particular parts of the body for
defined clinical purposes--generates images of great similarity in anatomy
across patients and yields sophisticated anatomical patterns across images,
which are associated with rich semantics about human anatomy and which are
natural visual words. We show that these visual words can be automatically
harvested according to anatomical consistency via self-discovery, and that the
self-discovered visual words can serve as strong yet free supervision signals
for deep models to learn semantics-enriched generic image representation via
self-supervision (self-classification and self-restoration). Our extensive
experiments demonstrate the annotation efficiency of TransVW by offering higher
performance and faster convergence with reduced annotation cost in several
applications. Our TransVW has several important advantages, including (1)
TransVW is a fully autodidactic scheme, which exploits the semantics of visual
words for self-supervised learning, requiring no expert annotation; (2) visual
word learning is an add-on strategy, which complements existing self-supervised
methods, boosting their performance; and (3) the learned image representation
is semantics-enriched models, which have proven to be more robust and
generalizable, saving annotation efforts for a variety of applications through
transfer learning. Our code, pre-trained models, and curated visual words are
available at https://github.com/JLiangLab/TransVW.",,,arXiv,,,2021-02-21,2021,,,,,,All OA, Green,Preprint,"Haghighi, Fatemeh; Taher, Mohammad Reza Hosseinzadeh; Zhou, Zongwei; Gotway, Michael B.; Liang, Jianming","Haghighi, Fatemeh (); Taher, Mohammad Reza Hosseinzadeh (); Zhou, Zongwei (); Gotway, Michael B. (); Liang, Jianming ()",,"Haghighi, Fatemeh (); Taher, Mohammad Reza Hosseinzadeh (); Zhou, Zongwei (); Gotway, Michael B. (); Liang, Jianming ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1135670781,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
441,pub.1145983640,10.1016/j.future.2022.02.020,,,Leveraging conditional generative models in a general explanation framework of classifier decisions,"With the increase in use of machine learning classifiers in several fields, providing human- understandable explanation of their outputs has become an imperative. It is essential to generate trust for day-to-day tasks, especially in the sensible domains as medical imaging. Although many works have addressed this problem by generating visual explanation maps, they often provide noisy and inaccurate results forcing heuristic regularization unrelated to the classifier in question. In this paper, we propose a general perspective of the visual explanation problem overcoming these limitations. We show that visual explanation can be produced as the difference between two generated images obtained via two specific conditional generative models. Both generative models are trained using the classifier to explain and a database to enforce the following properties: (i) All images generated by the first generator are classified similarly to the input image, whereas the second generatorâs outputs are classified oppositely. (ii) All generated images belong to the distribution of real images. (iii) The distances between the input image and the corresponding generated images are minimal so that the difference between the generated elements only reveals relevant information for the studied classifier. Using symmetrical and cyclic constraints, we present two different approximations and implementations of the general formulation. Experimentally, we demonstrate significant improvements with respect to the state-of-the-art on three different public data sets. In particular, the localization of regions influencing the classifier is consistent with human annotations.",,,Future Generation Computer Systems,,,2022-07,2022,,2022-07,132,,223-238,All OA, Hybrid,Article,"Charachon, Martin; CournÃ¨de, Paul-Henry; Hudelot, CÃ©line; Ardon, Roberto","Charachon, Martin (Incepto Medical, 128 Rue la BoÃ©tie, 75008, Paris, France; UniversitÃ© Paris-Saclay, CentraleSupÃ©lec, Lab of Mathematics and Informatics (MICS), 9 rue Joliot Curie, 91192, Gif-sur-Yvette, France); CournÃ¨de, Paul-Henry (UniversitÃ© Paris-Saclay, CentraleSupÃ©lec, Lab of Mathematics and Informatics (MICS), 9 rue Joliot Curie, 91192, Gif-sur-Yvette, France); Hudelot, CÃ©line (UniversitÃ© Paris-Saclay, CentraleSupÃ©lec, Lab of Mathematics and Informatics (MICS), 9 rue Joliot Curie, 91192, Gif-sur-Yvette, France); Ardon, Roberto (Incepto Medical, 128 Rue la BoÃ©tie, 75008, Paris, France)","Charachon, Martin (; CentraleSupÃ©lec)","Charachon, Martin (CentraleSupÃ©lec); CournÃ¨de, Paul-Henry (CentraleSupÃ©lec); Hudelot, CÃ©line (CentraleSupÃ©lec); Ardon, Roberto ()",1,1,,,https://doi.org/10.1016/j.future.2022.02.020,https://app.dimensions.ai/details/publication/pub.1145983640,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
441,pub.1141450796,10.48550/arxiv.2109.12265,,,Label-Assemble: Leveraging Multiple Datasets with Partial Labels,"The success of deep learning relies heavily on large and diverse datasets
with extensive labels, but we often only have access to several small datasets
associated with partial labels. In this paper, we start a new initiative,
""Label-Assemble"", that aims to unleash the full potential of partially labeled
data from an assembly of public datasets. Specifically, we introduce a new
dynamic adapter to encode different visual tasks, which addresses the
challenges of incomparable, heterogeneous, or even conflicting labeling
protocols. We also employ pseudo-labeling and consistency constraints to
harness data with missing labels and to mitigate the domain gap across
datasets. From rigorous evaluations on three natural imaging and six medical
imaging tasks, we discover that learning from ""negative examples"" facilitates
both classification and segmentation of classes of interest. This sheds new
light on the computer-aided diagnosis of rare diseases and emerging pandemics,
wherein ""positive examples"" are hard to collect, yet ""negative examples"" are
relatively easier to assemble. Apart from exceeding prior arts in the ChestXray
benchmark, our model is particularly strong in identifying diseases of minority
classes, yielding over 3-point improvement on average. Remarkably, when using
existing partial labels, our model performance is on-par with that using full
labels, eliminating the need for an additional 40% of annotation costs. Code
will be made available at https://github.com/MrGiovanni/LabelAssemble.",,,arXiv,,,2021-09-24,2021,,,,,,All OA, Green,Preprint,"Kang, Mintong; Lu, Yongyi; Yuille, Alan L.; Zhou, Zongwei","Kang, Mintong (); Lu, Yongyi (); Yuille, Alan L. (); Zhou, Zongwei ()",,"Kang, Mintong (); Lu, Yongyi (); Yuille, Alan L. (); Zhou, Zongwei ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141450796,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
441,pub.1149106454,10.48550/arxiv.2206.14302,,,"Reinforcement Learning in Medical Image Analysis: Concepts,  Applications, Challenges, and Future Directions","Motivation: Medical image analysis involves tasks to assist physicians in
qualitative and quantitative analysis of lesions or anatomical structures,
significantly improving the accuracy and reliability of diagnosis and
prognosis. Traditionally, these tasks are finished by physicians or medical
physicists and lead to two major problems: (i) low efficiency; (ii) biased by
personal experience. In the past decade, many machine learning methods have
been applied to accelerate and automate the image analysis process. Compared to
the enormous deployments of supervised and unsupervised learning models,
attempts to use reinforcement learning in medical image analysis are scarce.
This review article could serve as the stepping-stone for related research.
Significance: From our observation, though reinforcement learning has gradually
gained momentum in recent years, many researchers in the medical analysis field
find it hard to understand and deploy in clinics. One cause is lacking
well-organized review articles targeting readers lacking professional computer
science backgrounds. Rather than providing a comprehensive list of all
reinforcement learning models in medical image analysis, this paper may help
the readers to learn how to formulate and solve their medical image analysis
research as reinforcement learning problems. Approach & Results: We selected
published articles from Google Scholar and PubMed. Considering the scarcity of
related articles, we also included some outstanding newest preprints. The
papers are carefully reviewed and categorized according to the type of image
analysis task. We first review the basic concepts and popular models of
reinforcement learning. Then we explore the applications of reinforcement
learning models in landmark detection. Finally, we conclude the article by
discussing the reviewed reinforcement learning approaches' limitations and
possible improvements.",,,arXiv,,,2022-06-28,2022,,,,,,All OA, Green,Preprint,"Hu, Mingzhe; Zhang, Jiahan; Matkovic, Luke; Liu, Tian; Yang, Xiaofeng","Hu, Mingzhe (); Zhang, Jiahan (); Matkovic, Luke (); Liu, Tian (); Yang, Xiaofeng ()",,"Hu, Mingzhe (); Zhang, Jiahan (); Matkovic, Luke (); Liu, Tian (); Yang, Xiaofeng ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149106454,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4611 Machine Learning,,,,,,,,,
439,pub.1145843999,10.48550/arxiv.2202.12295,,,Factorizer: A Scalable Interpretable Approach to Context Modeling for  Medical Image Segmentation,"Convolutional Neural Networks (CNNs) with U-shaped architectures have
dominated medical image segmentation, which is crucial for various clinical
purposes. However, the inherent locality of convolution makes CNNs fail to
fully exploit global context, essential for better recognition of some
structures, e.g., brain lesions. Transformers have recently proven promising
performance on vision tasks, including semantic segmentation, mainly due to
their capability of modeling long-range dependencies. Nevertheless, the
quadratic complexity of attention makes existing Transformer-based models use
self-attention layers only after somehow reducing the image resolution, which
limits the ability to capture global contexts present at higher resolutions.
Therefore, this work introduces a family of models, dubbed Factorizer, which
leverages the power of low-rank matrix factorization for constructing an
end-to-end segmentation model. Specifically, we propose a linearly scalable
approach to context modeling, formulating Nonnegative Matrix Factorization
(NMF) as a differentiable layer integrated into a U-shaped architecture. The
shifted window technique is also utilized in combination with NMF to
effectively aggregate local information. Factorizers compete favorably with
CNNs and Transformers in terms of accuracy, scalability, and interpretability,
achieving state-of-the-art results on the BraTS dataset for brain tumor
segmentation and ISLES'22 dataset for stroke lesion segmentation. Highly
meaningful NMF components give an additional interpretability advantage to
Factorizers over CNNs and Transformers. Moreover, our ablation studies reveal a
distinctive feature of Factorizers that enables a significant speed-up in
inference for a trained Factorizer without any extra steps and without
sacrificing much accuracy. The code and models are publicly available at
https://github.com/pashtari/factorizer.",,,arXiv,,,2022-02-24,2022,,,,,,All OA, Green,Preprint,"Ashtari, Pooya; Sima, Diana M.; De Lathauwer, Lieven; Sappey-Marinier, Dominique; Maes, Frederik; Van Huffel, Sabine","Ashtari, Pooya (); Sima, Diana M. (); De Lathauwer, Lieven (); Sappey-Marinier, Dominique (); Maes, Frederik (); Van Huffel, Sabine ()",,"Ashtari, Pooya (); Sima, Diana M. (); De Lathauwer, Lieven (); Sappey-Marinier, Dominique (); Maes, Frederik (); Van Huffel, Sabine ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1145843999,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
421,pub.1150264819,10.48550/arxiv.2208.07167,,,Where is VALDO? VAscular Lesions Detection and segmentatiOn challenge at  MICCAI 2021,"Imaging markers of cerebral small vessel disease provide valuable information
on brain health, but their manual assessment is time-consuming and hampered by
substantial intra- and interrater variability. Automated rating may benefit
biomedical research, as well as clinical assessment, but diagnostic reliability
of existing algorithms is unknown. Here, we present the results of the
\textit{VAscular Lesions DetectiOn and Segmentation} (\textit{Where is VALDO?})
challenge that was run as a satellite event at the international conference on
Medical Image Computing and Computer Aided Intervention (MICCAI) 2021. This
challenge aimed to promote the development of methods for automated detection
and segmentation of small and sparse imaging markers of cerebral small vessel
disease, namely enlarged perivascular spaces (EPVS) (Task 1), cerebral
microbleeds (Task 2) and lacunes of presumed vascular origin (Task 3) while
leveraging weak and noisy labels. Overall, 12 teams participated in the
challenge proposing solutions for one or more tasks (4 for Task 1 - EPVS, 9 for
Task 2 - Microbleeds and 6 for Task 3 - Lacunes). Multi-cohort data was used in
both training and evaluation. Results showed a large variability in performance
both across teams and across tasks, with promising results notably for Task 1 -
EPVS and Task 2 - Microbleeds and not practically useful results yet for Task 3
- Lacunes. It also highlighted the performance inconsistency across cases that
may deter use at an individual level, while still proving useful at a
population level.",,,arXiv,,,2022-08-15,2022,,,,,,All OA, Green,Preprint,"Sudre, Carole H.; Van Wijnen, Kimberlin; Dubost, Florian; Adams, Hieab; Atkinson, David; Barkhof, Frederik; Birhanu, Mahlet A.; Bron, Esther E.; Camarasa, Robin; Chaturvedi, Nish; Chen, Yuan; Chen, Zihao; Chen, Shuai; Dou, Qi; Evans, Tavia; Ezhov, Ivan; Gao, Haojun; Sanguesa, Marta Girones; Gispert, Juan Domingo; Anson, Beatriz Gomez; Hughes, Alun D.; Ikram, M. Arfan; Ingala, Silvia; Jaeger, H. Rolf; Kofler, Florian; Kuijf, Hugo J.; Kutnar, Denis; Lee, Minho; Li, Bo; Lorenzini, Luigi; Menze, Bjoern; Molinuevo, Jose Luis; Pan, Yiwei; Puybareau, Elodie; Rehwald, Rafael; Su, Ruisheng; Shi, Pengcheng; Smith, Lorna; Tillin, Therese; Tochon, Guillaume; Urien, Helene; van der Velden, Bas H. M.; van der Velpen, Isabelle F.; Wiestler, Benedikt; Wolters, Frank J.; Yilmaz, Pinar; de Groot, Marius; Vernooij, Meike W.; de Bruijne, Marleen","Sudre, Carole H. (for the ALFA study); Van Wijnen, Kimberlin (for the ALFA study); Dubost, Florian (for the ALFA study); Adams, Hieab (for the ALFA study); Atkinson, David (for the ALFA study); Barkhof, Frederik (for the ALFA study); Birhanu, Mahlet A. (for the ALFA study); Bron, Esther E. (for the ALFA study); Camarasa, Robin (for the ALFA study); Chaturvedi, Nish (for the ALFA study); Chen, Yuan (for the ALFA study); Chen, Zihao (for the ALFA study); Chen, Shuai (for the ALFA study); Dou, Qi (for the ALFA study); Evans, Tavia (for the ALFA study); Ezhov, Ivan (for the ALFA study); Gao, Haojun (for the ALFA study); Sanguesa, Marta Girones (for the ALFA study); Gispert, Juan Domingo (for the ALFA study); Anson, Beatriz Gomez (for the ALFA study); Hughes, Alun D. (for the ALFA study); Ikram, M. Arfan (for the ALFA study); Ingala, Silvia (for the ALFA study); Jaeger, H. Rolf (for the ALFA study); Kofler, Florian (for the ALFA study); Kuijf, Hugo J. (for the ALFA study); Kutnar, Denis (for the ALFA study); Lee, Minho (for the ALFA study); Li, Bo (for the ALFA study); Lorenzini, Luigi (for the ALFA study); Menze, Bjoern (for the ALFA study); Molinuevo, Jose Luis (for the ALFA study); Pan, Yiwei (for the ALFA study); Puybareau, Elodie (for the ALFA study); Rehwald, Rafael (for the ALFA study); Su, Ruisheng (for the ALFA study); Shi, Pengcheng (for the ALFA study); Smith, Lorna (for the ALFA study); Tillin, Therese (for the ALFA study); Tochon, Guillaume (for the ALFA study); Urien, Helene (for the ALFA study); van der Velden, Bas H. M. (for the ALFA study); van der Velpen, Isabelle F. (for the ALFA study); Wiestler, Benedikt (for the ALFA study); Wolters, Frank J. (for the ALFA study); Yilmaz, Pinar (for the ALFA study); de Groot, Marius (for the ALFA study); Vernooij, Meike W. (for the ALFA study); de Bruijne, Marleen (for the ALFA study)",,"Sudre, Carole H. (); Van Wijnen, Kimberlin (); Dubost, Florian (); Adams, Hieab (); Atkinson, David (); Barkhof, Frederik (); Birhanu, Mahlet A. (); Bron, Esther E. (); Camarasa, Robin (); Chaturvedi, Nish (); Chen, Yuan (); Chen, Zihao (); Chen, Shuai (); Dou, Qi (); Evans, Tavia (); Ezhov, Ivan (); Gao, Haojun (); Sanguesa, Marta Girones (); Gispert, Juan Domingo (); Anson, Beatriz Gomez (); Hughes, Alun D. (); Ikram, M. Arfan (); Ingala, Silvia (); Jaeger, H. Rolf (); Kofler, Florian (); Kuijf, Hugo J. (); Kutnar, Denis (); Lee, Minho (); Li, Bo (); Lorenzini, Luigi (); Menze, Bjoern (); Molinuevo, Jose Luis (); Pan, Yiwei (); Puybareau, Elodie (); Rehwald, Rafael (); Su, Ruisheng (); Shi, Pengcheng (); Smith, Lorna (); Tillin, Therese (); Tochon, Guillaume (); Urien, Helene (); van der Velden, Bas H. M. (); van der Velpen, Isabelle F. (); Wiestler, Benedikt (); Wolters, Frank J. (); Yilmaz, Pinar (); de Groot, Marius (); Vernooij, Meike W. (); de Bruijne, Marleen ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1150264819,46 Information and Computing Sciences, 4608 Human-Centred Computing,,,,,,,,,,
419,pub.1147931821,10.48550/arxiv.2205.07516,,,The use of deep learning in interventional radiotherapy (brachytherapy):  a review with a focus on open source and open data,"Deep learning advanced to one of the most important technologies in almost
all medical fields. Especially in areas, related to medical imaging it plays a
big role. However, in interventional radiotherapy (brachytherapy) deep learning
is still in an early phase. In this review, first, we investigated and
scrutinised the role of deep learning in all processes of interventional
radiotherapy and directly related fields. Additionally we summarised the most
recent developments. To reproduce results of deep learning algorithms both
source code and training data must be available. Therefore, a second focus of
this work was on the analysis of the availability of open source, open data and
open models. In our analysis, we were able to show that deep learning plays
already a major role in some areas of interventional radiotherapy, but is still
hardly presented in others. Nevertheless, its impact is increasing with the
years, partly self-propelled but also influenced by closely related fields.
Open source, data and models are growing in number but are still scarce and
unevenly distributed among different research groups. The reluctance in
publishing code, data and models limits reproducibility and restricts
evaluation to mono-institutional datasets. Summarised, deep learning will
change positively the workflow of interventional radiotherapy but there is room
for improvement when it comes to reproducible results and standardised
evaluation methods.",,,arXiv,,,2022-05-16,2022,,,,,,All OA, Green,Preprint,"Fechter, Tobias; Sachpazidis, Ilias; Baltas, Dimos","Fechter, Tobias (); Sachpazidis, Ilias (); Baltas, Dimos ()",,"Fechter, Tobias (); Sachpazidis, Ilias (); Baltas, Dimos ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1147931821,32 Biomedical and Clinical Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
419,pub.1140316668,10.48550/arxiv.2108.03429,,,Enhancing MR Image Segmentation with Realistic Adversarial Data  Augmentation,"The success of neural networks on medical image segmentation tasks typically
relies on large labeled datasets for model training. However, acquiring and
manually labeling a large medical image set is resource-intensive, expensive,
and sometimes impractical due to data sharing and privacy issues. To address
this challenge, we propose AdvChain, a generic adversarial data augmentation
framework, aiming at improving both the diversity and effectiveness of training
data for medical image segmentation tasks. AdvChain augments data with dynamic
data augmentation, generating randomly chained photo-metric and geometric
transformations to resemble realistic yet challenging imaging variations to
expand training data. By jointly optimizing the data augmentation model and a
segmentation network during training, challenging examples are generated to
enhance network generalizability for the downstream task. The proposed
adversarial data augmentation does not rely on generative networks and can be
used as a plug-in module in general segmentation networks. It is
computationally efficient and applicable for both low-shot supervised and
semi-supervised learning. We analyze and evaluate the method on two MR image
segmentation tasks: cardiac segmentation and prostate segmentation with limited
labeled data. Results show that the proposed approach can alleviate the need
for labeled data while improving model generalization ability, indicating its
practical value in medical imaging applications.",,,arXiv,,,2021-08-07,2021,,,,,,All OA, Green,Preprint,"Chen, Chen; Qin, Chen; Ouyang, Cheng; Li, Zeju; Wang, Shuo; Qiu, Huaqi; Chen, Liang; Tarroni, Giacomo; Bai, Wenjia; Rueckert, Daniel","Chen, Chen (); Qin, Chen (); Ouyang, Cheng (); Li, Zeju (); Wang, Shuo (); Qiu, Huaqi (); Chen, Liang (); Tarroni, Giacomo (); Bai, Wenjia (); Rueckert, Daniel ()",,"Chen, Chen (); Qin, Chen (); Ouyang, Cheng (); Li, Zeju (); Wang, Shuo (); Qiu, Huaqi (); Chen, Liang (); Tarroni, Giacomo (); Bai, Wenjia (); Rueckert, Daniel ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140316668,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
403,pub.1142581012,10.15388/damss.12.2021,,,Data Analysis Methods for Software Systems,"Established in 2019, this journal contains various selected articles prepared by the community of Vilnius University.",,,Vilnius University Proceedings,,,2021-11-15,2021,2021-11-15,,17,,1-82,All OA, Gold,Article,"BernataviÄienÄ, Jolita","BernataviÄienÄ, Jolita (Vilnius University, Lithuania)",,"BernataviÄienÄ, Jolita (Vilnius University)",0,0,,0.0,https://www.journals.vu.lt/proceedings/article/download/25028/24294,https://app.dimensions.ai/details/publication/pub.1142581012,46 Information and Computing Sciences, 4612 Software Engineering,,,,,,,,,,
402,pub.1153642233,10.1016/j.inffus.2022.12.013,,,Deep and statistical learning in biomedical imaging: State of the art in 3D MRI brain tumor segmentation,"Clinical diagnosis and treatment decisions rely upon the integration of patient-specific data with clinical reasoning. Cancer presents a unique context that influences treatment decisions, given its diverse forms of disease evolution. Biomedical imaging allows non-invasive assessment of diseases based on visual evaluations, leading to better clinical outcome prediction and therapeutic planning. Early methods of brain cancer characterization predominantly relied upon the statistical modeling of neuroimaging data. Driven by breakthroughs in computer vision, deep learning has become the de facto standard in medical imaging. Integrated statistical and deep learning methods have recently emerged as a new direction in the automation of medical practice unifying multi-disciplinary knowledge in medicine, statistics, and artificial intelligence. In this study, we critically review major statistical, deep learning, and probabilistic deep learning models and their applications in brain imaging research with a focus on MRI-based brain tumor segmentation. These results highlight that model-driven classical statistics and data-driven deep learning is a potent combination for developing automated systems in clinical oncology.",,,Information Fusion,,,2023-04,2023,,2023-04,92,,450-465,All OA, Green,Article,"Fernando, K. Ruwani M.; Tsokos, Chris P.","Fernando, K. Ruwani M. (Department of Mathematics and Statistics, University of South Florida, Tampa, FL, 33620, USA); Tsokos, Chris P. (Department of Mathematics and Statistics, University of South Florida, Tampa, FL, 33620, USA)","Fernando, K. Ruwani M. (University of South Florida)","Fernando, K. Ruwani M. (University of South Florida); Tsokos, Chris P. (University of South Florida)",0,0,,,http://arxiv.org/pdf/2103.05529,https://app.dimensions.ai/details/publication/pub.1153642233,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,3 Good Health and Well Being,,,,,,,
386,pub.1144584589,10.48550/arxiv.2201.02831,,,CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation  techniques for Vestibular Schwannoma and Cochlea Segmentation,"Domain Adaptation (DA) has recently raised strong interests in the medical
imaging community. While a large variety of DA techniques has been proposed for
image segmentation, most of these techniques have been validated either on
private datasets or on small publicly available datasets. Moreover, these
datasets mostly addressed single-class problems. To tackle these limitations,
the Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in
conjunction with the 24th International Conference on Medical Image Computing
and Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large
and multi-class benchmark for unsupervised cross-modality DA. The challenge's
goal is to segment two key brain structures involved in the follow-up and
treatment planning of vestibular schwannoma (VS): the VS and the cochleas.
Currently, the diagnosis and surveillance in patients with VS are performed
using contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in
using non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore,
we created an unsupervised cross-modality segmentation benchmark. The training
set provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105).
The aim was to automatically perform unilateral VS and bilateral cochlea
segmentation on hrT2 as provided in the testing set (N=137). A total of 16
teams submitted their algorithm for the evaluation phase. The level of
performance reached by the top-performing teams is strikingly high (best median
Dice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice -
VS:92.5%; Cochleas:87.7%). All top-performing methods made use of an
image-to-image translation approach to transform the source-domain images into
pseudo-target-domain images. A segmentation network was then trained using
these generated images and the manual annotations provided for the source
image.",,,arXiv,,,2022-01-08,2022,,,,,,All OA, Green,Preprint,"Dorent, Reuben; Kujawa, Aaron; Ivory, Marina; Bakas, Spyridon; Rieke, Nicola; Joutard, Samuel; Glocker, Ben; Cardoso, Jorge; Modat, Marc; Batmanghelich, Kayhan; Belkov, Arseniy; Calisto, Maria Baldeon; Choi, Jae Won; Dawant, Benoit M.; Dong, Hexin; Escalera, Sergio; Fan, Yubo; Hansen, Lasse; Heinrich, Mattias P.; Joshi, Smriti; Kashtanova, Victoriya; Kim, Hyeon Gyu; Kondo, Satoshi; Kruse, Christian N.; Lai-Yuen, Susana K.; Li, Hao; Liu, Han; Ly, Buntheng; Oguz, Ipek; Shin, Hyungseob; Shirokikh, Boris; Su, Zixian; Wang, Guotai; Wu, Jianghao; Xu, Yanwu; Yao, Kai; Zhang, Li; Ourselin, Sebastien; Shapey, Jonathan; Vercauteren, Tom","Dorent, Reuben (); Kujawa, Aaron (); Ivory, Marina (); Bakas, Spyridon (); Rieke, Nicola (); Joutard, Samuel (); Glocker, Ben (); Cardoso, Jorge (); Modat, Marc (); Batmanghelich, Kayhan (); Belkov, Arseniy (); Calisto, Maria Baldeon (); Choi, Jae Won (); Dawant, Benoit M. (); Dong, Hexin (); Escalera, Sergio (); Fan, Yubo (); Hansen, Lasse (); Heinrich, Mattias P. (); Joshi, Smriti (); Kashtanova, Victoriya (); Kim, Hyeon Gyu (); Kondo, Satoshi (); Kruse, Christian N. (); Lai-Yuen, Susana K. (); Li, Hao (); Liu, Han (); Ly, Buntheng (); Oguz, Ipek (); Shin, Hyungseob (); Shirokikh, Boris (); Su, Zixian (); Wang, Guotai (); Wu, Jianghao (); Xu, Yanwu (); Yao, Kai (); Zhang, Li (); Ourselin, Sebastien (); Shapey, Jonathan (); Vercauteren, Tom ()",,"Dorent, Reuben (); Kujawa, Aaron (); Ivory, Marina (); Bakas, Spyridon (); Rieke, Nicola (); Joutard, Samuel (); Glocker, Ben (); Cardoso, Jorge (); Modat, Marc (); Batmanghelich, Kayhan (); Belkov, Arseniy (); Calisto, Maria Baldeon (); Choi, Jae Won (); Dawant, Benoit M. (); Dong, Hexin (); Escalera, Sergio (); Fan, Yubo (); Hansen, Lasse (); Heinrich, Mattias P. (); Joshi, Smriti (); Kashtanova, Victoriya (); Kim, Hyeon Gyu (); Kondo, Satoshi (); Kruse, Christian N. (); Lai-Yuen, Susana K. (); Li, Hao (); Liu, Han (); Ly, Buntheng (); Oguz, Ipek (); Shin, Hyungseob (); Shirokikh, Boris (); Su, Zixian (); Wang, Guotai (); Wu, Jianghao (); Xu, Yanwu (); Yao, Kai (); Zhang, Li (); Ourselin, Sebastien (); Shapey, Jonathan (); Vercauteren, Tom ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144584589,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
375,pub.1151694516,10.1007/978-3-031-18523-6,,,"Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health, Third MICCAI Workshop, DeCaF 2022, and Second MICCAI Workshop, FAIR 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18 and 22, 2022, Proceedings","This book constitutes the refereed proceedings of the Third MICCAI Workshop on Distributed, Collaborative, and Federated Learning, DeCaF 2022, and the Second MICCAI Workshop on Affordable AI and Healthcare, FAIR 2022, held in conjunction with MICCAI 2022, in Singapore in September 2022. FAIR 2022 was held as a hybrid event. DeCaF 2022 accepted 14 papers from the 18 submissions received. The workshop aims at creating a scientific discussion focusing on the comparison, evaluation, and discussion of methodological advancement and practical ideas about machine learning applied to problems where data cannot be stored in centralized databases or where information privacy is a priority. For FAIR 2022, 4 papers from 9 submissions were accepted for publication. The topics of the accepted submissions focus on deep ultrasound segmentation, portable OCT image quality enhancement, self-attention deep networks and knowledge distillation in low-regime setting.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13573,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1151694516,46 Information and Computing Sciences, 4606 Distributed Computing and Systems Software,3 Good Health and Well Being,,,,,,,,,,
358,pub.1144690374,10.1016/b978-0-12-823818-9.00019-5,,,Chapter 8 Machine learning ML for eHealth systems,"Healthcare is at the dawn of a new era of intelligent systems and improved human relationships. The potential of artificial intelligence (AI) and machine learning (ML) technologies to support decision-making, optimize workflows, and free up quality human time is revolutionizing how people deliver and receive care. The success and performance of AI-based expert-level diagnostic systems have inspired unprecedented optimism. However, there are growing concerns about ethics, safety, and equity in the delivery of care. The lack of clarity about how it works and the resulting mistrust has negatively affected the relationship between AI and caregivers and recipients, preventing adoption. This chapter provides a general overview of the various AI learning areas and a detailed introduction to the area of federated learning, a key to the application of machine learning in the future vision of healthcare.",,,,Anomaly Detection and Complex Event Processing over IoT Data Streams,,2022,2022,,2022,,,149-191,Closed,Chapter,"Schneider, Patrick; Xhafa, Fatos","Schneider, Patrick (Universitat Oberta de Catalunya (UOC), Barcelona, Spain); Xhafa, Fatos (Universitat PolitÃ¨cnica de Catalunya (UPC), Barcelona, Spain)",,"Schneider, Patrick (Open University of Catalonia); Xhafa, Fatos (Universitat PolitÃ¨cnica de Catalunya)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1144690374,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4608 Human-Centred Computing,3 Good Health and Well Being,,,,,,,,,
357,pub.1151357320,10.48550/arxiv.2209.12157,,,"Dive into Self-Supervised Learning for Medical Image Analysis: Data,  Models and Tasks","Self-supervised learning (SSL) has achieved remarkable performance on various
medical imaging tasks by dint of priors from massive unlabeled data. However,
for a specific downstream task, there is still a lack of an instruction book on
how to select suitable pretext tasks and implementation details. In this work,
we first review the latest applications of self-supervised methods in the field
of medical imaging analysis. Then, we conduct extensive experiments to explore
four significant issues in SSL for medical imaging, including (1) the effect of
self-supervised pretraining on imbalanced datasets, (2) network architectures,
(3) the applicability of upstream tasks to downstream tasks and (4) the
stacking effect of SSL and commonly used policies for deep learning, including
data resampling and augmentation. Based on the experimental results, potential
guidelines are presented for self-supervised pretraining in medical imaging.
Finally, we discuss future research directions and raise issues to be aware of
when designing new SSL methods and paradigms.",,,arXiv,,,2022-09-25,2022,,,,,,All OA, Green,Preprint,"Zhang, Chuyan; Gu, Yun","Zhang, Chuyan (); Gu, Yun ()",,"Zhang, Chuyan (); Gu, Yun ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151357320,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
357,pub.1144061858,10.48550/arxiv.2112.10074,,,QU-BraTS: MICCAI BraTS 2020 Challenge on Quantifying Uncertainty in  Brain Tumor Segmentation - Analysis of Ranking Scores and Benchmarking  Results,"Deep learning (DL) models have provided state-of-the-art performance in
various medical imaging benchmarking challenges, including the Brain Tumor
Segmentation (BraTS) challenges. However, the task of focal pathology
multi-compartment segmentation (e.g., tumor and lesion sub-regions) is
particularly challenging, and potential errors hinder translating DL models
into clinical workflows. Quantifying the reliability of DL model predictions in
the form of uncertainties could enable clinical review of the most uncertain
regions, thereby building trust and paving the way toward clinical translation.
Several uncertainty estimation methods have recently been introduced for DL
medical image segmentation tasks. Developing scores to evaluate and compare the
performance of uncertainty measures will assist the end-user in making more
informed decisions. In this study, we explore and evaluate a score developed
during the BraTS 2019 and BraTS 2020 task on uncertainty quantification
(QU-BraTS) and designed to assess and rank uncertainty estimates for brain
tumor multi-compartment segmentation. This score (1) rewards uncertainty
estimates that produce high confidence in correct assertions and those that
assign low confidence levels at incorrect assertions, and (2) penalizes
uncertainty measures that lead to a higher percentage of under-confident
correct assertions. We further benchmark the segmentation uncertainties
generated by 14 independent participating teams of QU-BraTS 2020, all of which
also participated in the main BraTS segmentation task. Overall, our findings
confirm the importance and complementary value that uncertainty estimates
provide to segmentation algorithms, highlighting the need for uncertainty
quantification in medical image analyses. Finally, in favor of transparency and
reproducibility, our evaluation code is made publicly available at:
https://github.com/RagMeh11/QU-BraTS.",,,arXiv,,,2021-12-19,2021,,,,,,All OA, Green,Preprint,"Mehta, Raghav; Filos, Angelos; Baid, Ujjwal; Sako, Chiharu; McKinley, Richard; Rebsamen, Michael; Datwyler, Katrin; Meier, Raphael; Radojewski, Piotr; Murugesan, Gowtham Krishnan; Nalawade, Sahil; Ganesh, Chandan; Wagner, Ben; Yu, Fang F.; Fei, Baowei; Madhuranthakam, Ananth J.; Maldjian, Joseph A.; Daza, Laura; Gomez, Catalina; Arbelaez, Pablo; Dai, Chengliang; Wang, Shuo; Reynaud, Hadrien; Mo, Yuan-han; Angelini, Elsa; Guo, Yike; Bai, Wenjia; Banerjee, Subhashis; Pei, Lin-min; AK, Murat; Rosas-Gonzalez, Sarahi; Zemmoura, Ilyess; Tauber, Clovis; Vu, Minh H.; Nyholm, Tufve; Lofstedt, Tommy; Ballestar, Laura Mora; Vilaplana, Veronica; McHugh, Hugh; Talou, Gonzalo Maso; Wang, Alan; Patel, Jay; Chang, Ken; Hoebel, Katharina; Gidwani, Mishka; Arun, Nishanth; Gupta, Sharut; Aggarwal, Mehak; Singh, Praveer; Gerstner, Elizabeth R.; Kalpathy-Cramer, Jayashree; Boutry, Nicolas; Huard, Alexis; Vidyaratne, Lasitha; Rahman, Md Monibor; Iftekharuddin, Khan M.; Chazalon, Joseph; Puybareau, Elodie; Tochon, Guillaume; Ma, Jun; Cabezas, Mariano; Llado, Xavier; Oliver, Arnau; Valencia, Liliana; Valverde, Sergi; Amian, Mehdi; Soltaninejad, Mohammadreza; Myronenko, Andriy; Hatamizadeh, Ali; Feng, Xue; Dou, Quan; Tustison, Nicholas; Meyer, Craig; Shah, Nisarg A.; Talbar, Sanjay; Weber, Marc-Andre; Mahajan, Abhishek; Jakab, Andras; Wiest, Roland; Fathallah-Shaykh, Hassan M.; Nazeri, Arash; Milchenko1, Mikhail; Marcus, Daniel; Kotrotsou, Aikaterini; Colen, Rivka; Freymann, John; Kirby, Justin; Davatzikos, Christos; Menze, Bjoern; Bakas, Spyridon; Gal, Yarin; Arbel, Tal","Mehta, Raghav (); Filos, Angelos (); Baid, Ujjwal (); Sako, Chiharu (); McKinley, Richard (); Rebsamen, Michael (); Datwyler, Katrin (); Meier, Raphael (); Radojewski, Piotr (); Murugesan, Gowtham Krishnan (); Nalawade, Sahil (); Ganesh, Chandan (); Wagner, Ben (); Yu, Fang F. (); Fei, Baowei (); Madhuranthakam, Ananth J. (); Maldjian, Joseph A. (); Daza, Laura (); Gomez, Catalina (); Arbelaez, Pablo (); Dai, Chengliang (); Wang, Shuo (); Reynaud, Hadrien (); Mo, Yuan-han (); Angelini, Elsa (); Guo, Yike (); Bai, Wenjia (); Banerjee, Subhashis (); Pei, Lin-min (); AK, Murat (); Rosas-Gonzalez, Sarahi (); Zemmoura, Ilyess (); Tauber, Clovis (); Vu, Minh H. (); Nyholm, Tufve (); Lofstedt, Tommy (); Ballestar, Laura Mora (); Vilaplana, Veronica (); McHugh, Hugh (); Talou, Gonzalo Maso (); Wang, Alan (); Patel, Jay (); Chang, Ken (); Hoebel, Katharina (); Gidwani, Mishka (); Arun, Nishanth (); Gupta, Sharut (); Aggarwal, Mehak (); Singh, Praveer (); Gerstner, Elizabeth R. (); Kalpathy-Cramer, Jayashree (); Boutry, Nicolas (); Huard, Alexis (); Vidyaratne, Lasitha (); Rahman, Md Monibor (); Iftekharuddin, Khan M. (); Chazalon, Joseph (); Puybareau, Elodie (); Tochon, Guillaume (); Ma, Jun (); Cabezas, Mariano (); Llado, Xavier (); Oliver, Arnau (); Valencia, Liliana (); Valverde, Sergi (); Amian, Mehdi (); Soltaninejad, Mohammadreza (); Myronenko, Andriy (); Hatamizadeh, Ali (); Feng, Xue (); Dou, Quan (); Tustison, Nicholas (); Meyer, Craig (); Shah, Nisarg A. (); Talbar, Sanjay (); Weber, Marc-Andre (); Mahajan, Abhishek (); Jakab, Andras (); Wiest, Roland (); Fathallah-Shaykh, Hassan M. (); Nazeri, Arash (); Milchenko1, Mikhail (); Marcus, Daniel (); Kotrotsou, Aikaterini (); Colen, Rivka (); Freymann, John (); Kirby, Justin (); Davatzikos, Christos (); Menze, Bjoern (); Bakas, Spyridon (); Gal, Yarin (); Arbel, Tal ()",,"Mehta, Raghav (); Filos, Angelos (); Baid, Ujjwal (); Sako, Chiharu (); McKinley, Richard (); Rebsamen, Michael (); Datwyler, Katrin (); Meier, Raphael (); Radojewski, Piotr (); Murugesan, Gowtham Krishnan (); Nalawade, Sahil (); Ganesh, Chandan (); Wagner, Ben (); Yu, Fang F. (); Fei, Baowei (); Madhuranthakam, Ananth J. (); Maldjian, Joseph A. (); Daza, Laura (); Gomez, Catalina (); Arbelaez, Pablo (); Dai, Chengliang (); Wang, Shuo (); Reynaud, Hadrien (); Mo, Yuan-han (); Angelini, Elsa (); Guo, Yike (); Bai, Wenjia (); Banerjee, Subhashis (); Pei, Lin-min (); AK, Murat (); Rosas-Gonzalez, Sarahi (); Zemmoura, Ilyess (); Tauber, Clovis (); Vu, Minh H. (); Nyholm, Tufve (); Lofstedt, Tommy (); Ballestar, Laura Mora (); Vilaplana, Veronica (); McHugh, Hugh (); Talou, Gonzalo Maso (); Wang, Alan (); Patel, Jay (); Chang, Ken (); Hoebel, Katharina (); Gidwani, Mishka (); Arun, Nishanth (); Gupta, Sharut (); Aggarwal, Mehak (); Singh, Praveer (); Gerstner, Elizabeth R. (); Kalpathy-Cramer, Jayashree (); Boutry, Nicolas (); Huard, Alexis (); Vidyaratne, Lasitha (); Rahman, Md Monibor (); Iftekharuddin, Khan M. (); Chazalon, Joseph (); Puybareau, Elodie (); Tochon, Guillaume (); Ma, Jun (); Cabezas, Mariano (); Llado, Xavier (); Oliver, Arnau (); Valencia, Liliana (); Valverde, Sergi (); Amian, Mehdi (); Soltaninejad, Mohammadreza (); Myronenko, Andriy (); Hatamizadeh, Ali (); Feng, Xue (); Dou, Quan (); Tustison, Nicholas (); Meyer, Craig (); Shah, Nisarg A. (); Talbar, Sanjay (); Weber, Marc-Andre (); Mahajan, Abhishek (); Jakab, Andras (); Wiest, Roland (); Fathallah-Shaykh, Hassan M. (); Nazeri, Arash (); Milchenko1, Mikhail (); Marcus, Daniel (); Kotrotsou, Aikaterini (); Colen, Rivka (); Freymann, John (); Kirby, Justin (); Davatzikos, Christos (); Menze, Bjoern (); Bakas, Spyridon (); Gal, Yarin (); Arbel, Tal ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144061858,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,
356,pub.1128472817,10.1007/s10462-020-09854-1,,,Deep semantic segmentation of natural and medical images: a review,"The semantic image segmentation task consists of classifying each pixel of an image into an instance, where each instance corresponds to a class. This task is a part of the concept of scene understanding or better explaining the global context of an image. In the medical image analysis domain, image segmentation can be used for image-guided interventions, radiotherapy, or improved radiological diagnostics. In this review, we categorize the leading deep learning-based medical and non-medical image segmentation solutions into six main groups of deep architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, and multi-task methodsÂ and provide a comprehensive review of the contributions in each of these groups. Further, for each group, we analyze each variant of these groups and discuss the limitations of the current approaches and present potential future research directions for semantic image segmentation.",,,Artificial Intelligence Review,,,2020-06-13,2020,2020-06-13,2021-01,54,1,137-178,All OA, Green,Article,"Asgari Taghanaki, Saeid; Abhishek, Kumar; Cohen, Joseph Paul; Cohen-Adad, Julien; Hamarneh, Ghassan","Asgari Taghanaki, Saeid (School of Computing Science, Simon Fraser University, Burnaby, Canada); Abhishek, Kumar (School of Computing Science, Simon Fraser University, Burnaby, Canada); Cohen, Joseph Paul (Mila, UniversitÃ© de MontrÃ©al, Montreal, Canada); Cohen-Adad, Julien (NeuroPoly Lab, Institute of Biomedical Engineering, Polytechnique MontrÃ©al, Montreal, Canada); Hamarneh, Ghassan (School of Computing Science, Simon Fraser University, Burnaby, Canada)","Asgari Taghanaki, Saeid (Simon Fraser University)","Asgari Taghanaki, Saeid (Simon Fraser University); Abhishek, Kumar (Simon Fraser University); Cohen, Joseph Paul (University of Montreal); Cohen-Adad, Julien (Polytechnique MontrÃ©al); Hamarneh, Ghassan (Simon Fraser University)",295,269,,140.45,http://arxiv.org/pdf/1910.07655,https://app.dimensions.ai/details/publication/pub.1128472817,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
320,pub.1151189611,10.1007/978-3-031-17027-0,,,"Data Augmentation, Labelling, and Imperfections, Second MICCAI Workshop, DALI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings","This book constitutes the refereed proceedings of the Second MICCAI Workshop on Data Augmentation, Labelling, and Imperfections, DALI 2022, held in conjunction with MICCAI 2022, in Singapore in September 2022. DALI 2022 accepted 12 papers from the 22 submissions that were reviewed. The papers focus on rigorous study of medical data related to machine learning systems.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13567,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-17027-0%2F1,https://app.dimensions.ai/details/publication/pub.1151189611,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
309,pub.1142526737,10.48550/arxiv.2111.05408,,,Robust deep learning-based semantic organ segmentation in hyperspectral  images,"Semantic image segmentation is an important prerequisite for
context-awareness and autonomous robotics in surgery. The state of the art has
focused on conventional RGB video data acquired during minimally invasive
surgery, but full-scene semantic segmentation based on spectral imaging data
and obtained during open surgery has received almost no attention to date. To
address this gap in the literature, we are investigating the following research
questions based on hyperspectral imaging (HSI) data of pigs acquired in an open
surgery setting: (1) What is an adequate representation of HSI data for neural
network-based fully automated organ segmentation, especially with respect to
the spatial granularity of the data (pixels vs. superpixels vs. patches vs.
full images)? (2) Is there a benefit of using HSI data compared to other
modalities, namely RGB data and processed HSI data (e.g. tissue parameters like
oxygenation), when performing semantic organ segmentation? According to a
comprehensive validation study based on 506 HSI images from 20 pigs, annotated
with a total of 19 classes, deep learning-based segmentation performance
increases, consistently across modalities, with the spatial context of the
input data. Unprocessed HSI data offers an advantage over RGB data or processed
data from the camera provider, with the advantage increasing with decreasing
size of the input to the neural network. Maximum performance (HSI applied to
whole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04),
which is in the range of the inter-rater variability (DSC of 0.89 ((standard
deviation (SD)) 0.07)). We conclude that HSI could become a powerful image
modality for fully-automatic surgical scene understanding with many advantages
over traditional imaging, including the ability to recover additional
functional tissue information. Code and pre-trained models:
https://github.com/IMSY-DKFZ/htc.",,,arXiv,,,2021-11-09,2021,,,,,,All OA, Green,Preprint,"Seidlitz, Silvia; Sellner, Jan; Odenthal, Jan; Ãzdemir, Berkin; Studier-Fischer, Alexander; KnÃ¶dler, Samuel; Ayala, Leonardo; Adler, Tim J.; Kenngott, Hannes G.; Tizabi, Minu; Wagner, Martin; Nickel, Felix; MÃ¼ller-Stich, Beat P.; Maier-Hein, Lena","Seidlitz, Silvia (Division of Intelligent Medical Systems, German Cancer Research Center; Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany); Sellner, Jan (Division of Intelligent Medical Systems, German Cancer Research Center; Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany); Odenthal, Jan (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany); Ãzdemir, Berkin (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany); Studier-Fischer, Alexander (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany); KnÃ¶dler, Samuel (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany); Ayala, Leonardo (Division of Intelligent Medical Systems, German Cancer Research Center; Medical Faculty, Heidelberg University, Heidelberg, Germany); Adler, Tim J. (Division of Intelligent Medical Systems, German Cancer Research Center; Faculty of Mathematics and Computer Science, Heidelberg University, Germany); Kenngott, Hannes G. (Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany; Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany); Tizabi, Minu (Division of Intelligent Medical Systems, German Cancer Research Center); Wagner, Martin (Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany; Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany); Nickel, Felix (Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany; Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany); MÃ¼ller-Stich, Beat P. (Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany); Maier-Hein, Lena (Division of Intelligent Medical Systems, German Cancer Research Center; Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany; Medical Faculty, Heidelberg University, Heidelberg, Germany; HIP Helmholtz Imaging Platform, German Cancer Research Center; Faculty of Mathematics and Computer Science, Heidelberg University, Germany)",,"Seidlitz, Silvia (); Sellner, Jan (); Odenthal, Jan (University Hospital Heidelberg); Ãzdemir, Berkin (University Hospital Heidelberg; Heidelberg University); Studier-Fischer, Alexander (University Hospital Heidelberg; Heidelberg University); KnÃ¶dler, Samuel (University Hospital Heidelberg; Heidelberg University); Ayala, Leonardo (Heidelberg University); Adler, Tim J. (Heidelberg University); Kenngott, Hannes G. (University Hospital Heidelberg); Tizabi, Minu (); Wagner, Martin (University Hospital Heidelberg; Heidelberg University); Nickel, Felix (University Hospital Heidelberg; Heidelberg University); MÃ¼ller-Stich, Beat P. (University Hospital Heidelberg; Heidelberg University); Maier-Hein, Lena (Heidelberg University; Heidelberg University)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142526737,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,,,,,,,,,
306,pub.1140872386,10.1016/j.imu.2021.100723,,,An overview of deep learning in medical imaging,"Deep learning (DL) is one of the branches of artificial intelligence that has seen exponential growth in recent years. The scientific community has focused its attention on DL due to its versatility, high performance, high generalization capacity, and multidisciplinary uses, among many other qualities. In addition, a large amount of medical data and the development of more powerful computers has also fostered an interest in this area. This paper presents an overview of current deep learning methods, starting from the most straightforward concept but accompanied by the mathematical models that are behind the functionality of this type of intelligence. In the first instance, the fundamental concept of artificial neural networks is introduced, progressively covering convolutional structures, recurrent networks, attention models, up to the current structure known as the Transformer. Secondly, all the basic concepts involved in training and other common elements in the design of the architectures are introduced. Thirdly, some of the key elements in modern networks for medical image classification and segmentation are shown. Subsequently, a review of some applications realized in the last years is shown, where the main features related to DL are highlighted. Finally, the perspectives and future expectations of deep learning are presented.",This research was supported by the research division from INDIGO Technologies (https://indigo.tech/).,,Informatics in Medicine Unlocked,,,2021,2021,,2021,26,,100723,All OA, Gold,Article,"Anaya-Isaza, AndrÃ©s; Mera-JimÃ©nez, Leonel; Zequera-Diaz, Martha","Anaya-Isaza, AndrÃ©s (Pontificia Universidad Javeriana, 110231, BogotÃ¡, Colombia; INDIGO Research, 110221, BogotÃ¡, Colombia); Mera-JimÃ©nez, Leonel (Faculty of Engineering, Bioengineering Program, Universidad de Antioquia, 050010, MedellÃ­n, Colombia; INDIGO Research, 110221, BogotÃ¡, Colombia); Zequera-Diaz, Martha (School of Engineering, BASPI/FootLab (Bioengineering, Signal Analysis, And Image Processing Research Group), Pontificia Universidad Javeriana, 110231, BogotÃ¡, Colombia)","Mera-JimÃ©nez, Leonel (University of Antioquia; )","Anaya-Isaza, AndrÃ©s (Pontificia Universidad Javeriana); Mera-JimÃ©nez, Leonel (University of Antioquia); Zequera-Diaz, Martha (Pontificia Universidad Javeriana)",20,20,,17.5,https://doi.org/10.1016/j.imu.2021.100723,https://app.dimensions.ai/details/publication/pub.1140872386,42 Health Sciences, 4203 Health Services and Systems,,,,,,,,,,
289,pub.1148408535,10.48550/arxiv.2206.01136,,,"Transforming medical imaging with Transformers? A comparative review of  key properties, current progresses, and future perspectives","Transformer, the latest technological advance of deep learning, has gained
prevalence in natural language processing or computer vision. Since medical
imaging bear some resemblance to computer vision, it is natural to inquire
about the status quo of Transformers in medical imaging and ask the question:
can the Transformer models transform medical imaging? In this paper, we attempt
to make a response to the inquiry. After a brief introduction of the
fundamentals of Transformers, especially in comparison with convolutional
neural networks (CNNs), and highlighting key defining properties that
characterize the Transformers, we offer a comprehensive review of the
state-of-the-art Transformer-based approaches for medical imaging and exhibit
current research progresses made in the areas of medical image segmentation,
recognition, detection, registration, reconstruction, enhancement, etc. In
particular, what distinguishes our review lies in its organization based on the
Transformer's key defining properties, which are mostly derived from comparing
the Transformer and CNN, and its type of architecture, which specifies the
manner in which the Transformer and CNN are combined, all helping the readers
to best understand the rationale behind the reviewed approaches. We conclude
with discussions of future perspectives.",,,arXiv,,,2022-06-02,2022,,,,,,All OA, Green,Preprint,"Li, Jun; Chen, Junyu; Tang, Yucheng; Wang, Ce; Landman, Bennett A.; Zhou, S. Kevin","Li, Jun (); Chen, Junyu (); Tang, Yucheng (); Wang, Ce (); Landman, Bennett A. (); Zhou, S. Kevin ()",,"Li, Jun (); Chen, Junyu (); Tang, Yucheng (); Wang, Ce (); Landman, Bennett A. (); Zhou, S. Kevin ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1148408535,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
257,pub.1152919611,10.1016/j.engappai.2022.105532,,,Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations,"Machine learning and computer vision techniques have grown rapidly in recent years due to their automation, suitability, and ability to generate astounding results. Hence, in this paper, we survey the key studies that are published between 2014 and 2022, showcasing the different machine learning algorithms researchers have used to segment the liver, hepatic tumors, and hepatic-vasculature structures. We divide the surveyed studies based on the tissue of interest (hepatic-parenchyma, hepatic-tumors, or hepatic-vessels), highlighting the studies that tackle more than one task simultaneously. Additionally, the machine learning algorithms are classified as either supervised or unsupervised, and they are further partitioned if the amount of work that falls under a certain scheme is significant. Moreover, different datasets and challenges found in literature and websites containing masks of the aforementioned tissues are thoroughly discussed, highlighting the organizersâ original contributions and those of other researchers. Also, the metrics used excessively in the literature are mentioned in our review, stressing their relevance to the task at hand. Finally, critical challenges and future directions are emphasized for innovative researchers to tackle, exposing gaps that need addressing, such as the scarcity of many studies on the vesselsâ segmentation challenge and why their absence needs to be dealt with sooner than later.",This publication was made possible by an Award [GSRA6-2-0521-19034] from Qatar National Research Fund (a member of Qatar Foundation). The contents herein are solely the responsibility of the authors. Open Access funding provided by the Qatar National Library,,Engineering Applications of Artificial Intelligence,,,2023-01,2023,,2023-01,117,,105532,All OA, Hybrid,Article,"Al-Kababji, Ayman; Bensaali, Faycal; Dakua, Sarada Prasad; Himeur, Yassine","Al-Kababji, Ayman (Department of Electrical Engineering, Qatar University, Doha, Qatar); Bensaali, Faycal (Department of Electrical Engineering, Qatar University, Doha, Qatar); Dakua, Sarada Prasad (Department of Surgery, Hamad Medical Corporation, Doha, Qatar); Himeur, Yassine (Department of Electrical Engineering, Qatar University, Doha, Qatar)","Al-Kababji, Ayman (Qatar University)","Al-Kababji, Ayman (Qatar University); Bensaali, Faycal (Qatar University); Dakua, Sarada Prasad (Hamad Medical Corporation); Himeur, Yassine (Qatar University)",1,1,,,https://doi.org/10.1016/j.engappai.2022.105532,https://app.dimensions.ai/details/publication/pub.1152919611,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
250,pub.1151155487,10.1007/978-3-031-16980-9,,,"Simulation and Synthesis in Medical Imaging, 7th International Workshop, SASHIMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings","This book constitutes the refereed proceedings of the 7th International Workshop on Simulation and Synthesis in Medical Imaging, SASHIMI 2022, held in conjunction with MICCAI 2022, in Singapore, Singapore in September 2022.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13570,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16980-9%2F1,https://app.dimensions.ai/details/publication/pub.1151155487,46 Information and Computing Sciences,,,,,,,,,,,
231,pub.1153198901,10.48550/arxiv.2211.14830,,,Medical Image Segmentation Review: The success of U-Net,"Automatic medical image segmentation is a crucial topic in the medical domain
and successively a critical counterpart in the computer-aided diagnosis
paradigm. U-Net is the most widespread image segmentation architecture due to
its flexibility, optimized modular design, and success in all medical image
modalities. Over the years, the U-Net model achieved tremendous attention from
academic and industrial researchers. Several extensions of this network have
been proposed to address the scale and complexity created by medical tasks.
Addressing the deficiency of the naive U-Net model is the foremost step for
vendors to utilize the proper U-Net variant model for their business. Having a
compendium of different variants in one place makes it easier for builders to
identify the relevant research. Also, for ML researchers it will help them
understand the challenges of the biological tasks that challenge the model. To
address this, we discuss the practical aspects of the U-Net model and suggest a
taxonomy to categorize each network variant. Moreover, to measure the
performance of these strategies in a clinical application, we propose fair
evaluations of some unique and famous designs on well-known datasets. We
provide a comprehensive implementation library with trained models for future
research. In addition, for ease of future studies, we created an online list of
U-Net papers with their possible official implementation. All information is
gathered in https://github.com/NITR098/Awesome-U-Net repository.",,,arXiv,,,2022-11-27,2022,,,,,,All OA, Green,Preprint,"Azad, Reza; Aghdam, Ehsan Khodapanah; Rauland, Amelie; Jia, Yiwei; Avval, Atlas Haddadi; Bozorgpour, Afshin; Karimijafarbigloo, Sanaz; Cohen, Joseph Paul; Adeli, Ehsan; Merhof, Dorit","Azad, Reza (); Aghdam, Ehsan Khodapanah (); Rauland, Amelie (); Jia, Yiwei (); Avval, Atlas Haddadi (); Bozorgpour, Afshin (); Karimijafarbigloo, Sanaz (); Cohen, Joseph Paul (); Adeli, Ehsan (); Merhof, Dorit ()",,"Azad, Reza (); Aghdam, Ehsan Khodapanah (); Rauland, Amelie (); Jia, Yiwei (); Avval, Atlas Haddadi (); Bozorgpour, Afshin (); Karimijafarbigloo, Sanaz (); Cohen, Joseph Paul (); Adeli, Ehsan (); Merhof, Dorit ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153198901,46 Information and Computing Sciences, 4609 Information Systems,,,,,,,,,,
196,pub.1131192379,10.1007/978-3-030-60548-3,,,"Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning, Second MICCAI Workshop, DART 2020, and First MICCAI Workshop, DCL 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4â8, 2020, Proceedings","This book constitutes the refereed proceedings of the Second MICCAI Workshop on Domain Adaptation and Representation Transfer, DART 2020, and the First MICCAI Workshop on Distributed and Collaborative Learning, DCL 2020, held in conjunction with MICCAI 2020 in October 2020. The conference was planned to take place in Lima, Peru, but changed to an online format due to the Coronavirus pandemic. For DART 2020, 12 full papers were accepted from 18 submissions. They deal with methodological advancements and ideas that can improve the applicability of machine learning (ML)/deep learning (DL) approaches to clinical settings by making them robust and consistent across different domains. For DCL 2020, the 8 papers included in this book were accepted from a total of 12 submissions. They focus on the comparison, evaluation and discussion of methodological advancement and practical ideas about machine learning applied to problems where data cannot be stored in centralized databases; where information privacy is a priority; where it is necessary to deliver strong guarantees on the amount and nature of private information that may be revealed by the model as a result of training; and where it's necessary to orchestrate, manage and direct clusters of nodes participating in the same learning task.",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12444,,,All OA, Green,Edited Book,,,,,7,7,,3.61,https://link.springer.com/content/pdf/bfm:978-3-030-60548-3/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1131192379,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
193,pub.1148451587,10.48550/arxiv.2206.01653,,,Metrics reloaded: Pitfalls and recommendations for image analysis  validation,"Increasing evidence shows that flaws in machine learning (ML) algorithm
validation are an underestimated global problem. Particularly in automatic
biomedical image analysis, chosen performance metrics often do not reflect the
domain interest, thus failing to adequately measure scientific progress and
hindering translation of ML techniques into practice. To overcome this, our
large international expert consortium created Metrics Reloaded, a comprehensive
framework guiding researchers in the problem-aware selection of metrics.
Following the convergence of ML methodology across application domains, Metrics
Reloaded fosters the convergence of validation methodology. The framework was
developed in a multi-stage Delphi process and is based on the novel concept of
a problem fingerprint - a structured representation of the given problem that
captures all aspects that are relevant for metric selection, from the domain
interest to the properties of the target structure(s), data set and algorithm
output. Based on the problem fingerprint, users are guided through the process
of choosing and applying appropriate validation metrics while being made aware
of potential pitfalls. Metrics Reloaded targets image analysis problems that
can be interpreted as a classification task at image, object or pixel level,
namely image-level classification, object detection, semantic segmentation, and
instance segmentation tasks. To improve the user experience, we implemented the
framework in the Metrics Reloaded online tool, which also provides a point of
access to explore weaknesses, strengths and specific recommendations for the
most common validation metrics. The broad applicability of our framework across
domains is demonstrated by an instantiation for various biological and medical
image analysis use cases.",,,arXiv,,,2022-06-03,2022,,,,,,All OA, Green,Preprint,"Maier-Hein, Lena; Reinke, Annika; Godau, Patrick; Tizabi, Minu D.; BÃ¼ttner, Florian; Christodoulou, Evangelia; Glocker, Ben; Isensee, Fabian; Kleesiek, Jens; Kozubek, Michal; Reyes, Mauricio; Riegler, Michael A.; Wiesenfarth, Manuel; Kavur, Emre; Sudre, Carole H.; Baumgartner, Michael; Eisenmann, Matthias; Heckmann-NÃ¶tzel, Doreen; RÃ¤dsch, A. Tim; Acion, Laura; Antonelli, Michela; Arbel, Tal; Bakas, Spyridon; Benis, Arriel; Blaschko, Matthew; Cardoso, M. Jorge; Cheplygina, Veronika; Cimini, Beth A.; Collins, Gary S.; Farahani, Keyvan; Ferrer, Luciana; Galdran, Adrian; van Ginneken, Bram; Haase, Robert; Hashimoto, Daniel A.; Hoffman, Michael M.; Huisman, Merel; Jannin, Pierre; Kahn, Charles E.; Kainmueller, Dagmar; Kainz, Bernhard; Karargyris, Alexandros; Karthikesalingam, Alan; Kenngott, Hannes; Kofler, Florian; Kopp-Schneider, Annette; Kreshuk, Anna; Kurc, Tahsin; Landman, Bennett A.; Litjens, Geert; Madani, Amin; Maier-Hein, Klaus; Martel, Anne L.; Mattson, Peter; Meijering, Erik; Menze, Bjoern; Moons, Karel G. M.; MÃ¼ller, Henning; Nichyporuk, Brennan; Nickel, Felix; Petersen, Jens; Rajpoot, Nasir; Rieke, Nicola; Saez-Rodriguez, Julio; SÃ¡nchez, Clara I.; Shetty, Shravya; van Smeden, Maarten; Summers, Ronald M.; Taha, Abdel A.; Tiulpin, Aleksei; Tsaftaris, Sotirios A.; Van Calster, Ben; Varoquaux, GaÃ«l; JÃ¤ger, Paul F.","Maier-Hein, Lena (); Reinke, Annika (); Godau, Patrick (); Tizabi, Minu D. (); BÃ¼ttner, Florian (); Christodoulou, Evangelia (); Glocker, Ben (); Isensee, Fabian (); Kleesiek, Jens (); Kozubek, Michal (); Reyes, Mauricio (); Riegler, Michael A. (); Wiesenfarth, Manuel (); Kavur, Emre (); Sudre, Carole H. (); Baumgartner, Michael (); Eisenmann, Matthias (); Heckmann-NÃ¶tzel, Doreen (); RÃ¤dsch, A. Tim (); Acion, Laura (); Antonelli, Michela (); Arbel, Tal (); Bakas, Spyridon (); Benis, Arriel (); Blaschko, Matthew (); Cardoso, M. Jorge (); Cheplygina, Veronika (); Cimini, Beth A. (); Collins, Gary S. (); Farahani, Keyvan (); Ferrer, Luciana (); Galdran, Adrian (); van Ginneken, Bram (); Haase, Robert (); Hashimoto, Daniel A. (); Hoffman, Michael M. (); Huisman, Merel (); Jannin, Pierre (); Kahn, Charles E. (); Kainmueller, Dagmar (); Kainz, Bernhard (); Karargyris, Alexandros (); Karthikesalingam, Alan (); Kenngott, Hannes (); Kofler, Florian (); Kopp-Schneider, Annette (); Kreshuk, Anna (); Kurc, Tahsin (); Landman, Bennett A. (); Litjens, Geert (); Madani, Amin (); Maier-Hein, Klaus (); Martel, Anne L. (); Mattson, Peter (); Meijering, Erik (); Menze, Bjoern (); Moons, Karel G. M. (); MÃ¼ller, Henning (); Nichyporuk, Brennan (); Nickel, Felix (); Petersen, Jens (); Rajpoot, Nasir (); Rieke, Nicola (); Saez-Rodriguez, Julio (); SÃ¡nchez, Clara I. (); Shetty, Shravya (); van Smeden, Maarten (); Summers, Ronald M. (); Taha, Abdel A. (); Tiulpin, Aleksei (); Tsaftaris, Sotirios A. (); Van Calster, Ben (); Varoquaux, GaÃ«l (); JÃ¤ger, Paul F. ()",,"Maier-Hein, Lena (); Reinke, Annika (); Godau, Patrick (); Tizabi, Minu D. (); BÃ¼ttner, Florian (); Christodoulou, Evangelia (); Glocker, Ben (); Isensee, Fabian (); Kleesiek, Jens (); Kozubek, Michal (); Reyes, Mauricio (); Riegler, Michael A. (); Wiesenfarth, Manuel (); Kavur, Emre (); Sudre, Carole H. (); Baumgartner, Michael (); Eisenmann, Matthias (); Heckmann-NÃ¶tzel, Doreen (); RÃ¤dsch, A. Tim (); Acion, Laura (); Antonelli, Michela (); Arbel, Tal (); Bakas, Spyridon (); Benis, Arriel (); Blaschko, Matthew (); Cardoso, M. Jorge (); Cheplygina, Veronika (); Cimini, Beth A. (); Collins, Gary S. (); Farahani, Keyvan (); Ferrer, Luciana (); Galdran, Adrian (); van Ginneken, Bram (); Haase, Robert (); Hashimoto, Daniel A. (); Hoffman, Michael M. (); Huisman, Merel (); Jannin, Pierre (); Kahn, Charles E. (); Kainmueller, Dagmar (); Kainz, Bernhard (); Karargyris, Alexandros (); Karthikesalingam, Alan (); Kenngott, Hannes (); Kofler, Florian (); Kopp-Schneider, Annette (); Kreshuk, Anna (); Kurc, Tahsin (); Landman, Bennett A. (); Litjens, Geert (); Madani, Amin (); Maier-Hein, Klaus (); Martel, Anne L. (); Mattson, Peter (); Meijering, Erik (); Menze, Bjoern (); Moons, Karel G. M. (); MÃ¼ller, Henning (); Nichyporuk, Brennan (); Nickel, Felix (); Petersen, Jens (); Rajpoot, Nasir (); Rieke, Nicola (); Saez-Rodriguez, Julio (); SÃ¡nchez, Clara I. (); Shetty, Shravya (); van Smeden, Maarten (); Summers, Ronald M. (); Taha, Abdel A. (); Tiulpin, Aleksei (); Tsaftaris, Sotirios A. (); Van Calster, Ben (); Varoquaux, GaÃ«l (); JÃ¤ger, Paul F. ()",2,2,,,,https://app.dimensions.ai/details/publication/pub.1148451587,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,,,,,,,,,
183,pub.1153662504,10.1007/978-3-031-21014-3,,,"Machine Learning in Medical Imaging, 13th International Workshop, MLMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings","This book constitutes the proceedings of the 13th International Workshop on Machine Learning in Medical Imaging, MLMI 2022, held in conjunction with MICCAI 2022, in Singapore, in September 2022. The 48 full papers presented in this volume were carefully reviewed and selected from 64 submissions. They focus on major trends and challenges in the above-mentioned area, aiming to identify new-cutting-edge techniques and their uses in medical imaging. Topics dealt with are: deep learning, generative adversarial learning, ensemble learning, sparse learning, multi-task learning, multi-view learning, manifold learning, and reinforcement learning, with their applications to medical image analysis, computer-aided detection and diagnosis, multi-modality fusion, image reconstruction, image retrieval, cellular image analysis, molecular imaging, digital pathology, etc.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13583,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1153662504,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
179,pub.1135198814,10.1038/nde/15487091/2021/18/2,,,Nature Methods,,,,Nature Methods,,,2021,2021,,,18,2,,Closed,Article,,,,,1,1,,,,https://app.dimensions.ai/details/publication/pub.1135198814,31 Biological Sciences,,,,,,,,,,,,
168,pub.1154910295,10.1007/978-3-031-24985-3,,,"Applied Technologies, 4th International Conference, ICAT 2022, Quito, Ecuador, November 23â25, 2022, Revised Selected Papers, Part I","This three-volume set CCIS 1755-1757 constitutes the refereed proceedings of the 4th International Conference on Applied Technologies, ICAT 2022, held in Quito, Ecuador, in November 2022. The 112 full papers included in this book were carefully reviewed and selected from 415 submissions. They were organized in topical sections as follows: human computing and information science, IT financial and business management.",,,Communications in Computer and Information Science,,,2023,2023,,2023,1755,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1154910295,46 Information and Computing Sciences, 4608 Human-Centred Computing,,,,,,,,,,,
165,pub.1141238855,10.1007/978-3-030-87444-5,,,"Interpretability of Machine Intelligence in Medical Image Computing, and Topological Data Analysis and Its Applications for Medical Data, 4th International Workshop, iMIMIC 2021, and 1st International Workshop, TDA4MedicalData 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings","This book constitutes the refereed joint proceedings of the 4th International Workshop on Interpretability of Machine Intelligence in Medical Image Computing, iMIMIC 2020, and the First International Workshop on Topological Data Analysis and Its Applications for Medical Data, TDA4MedicalData 2021, held on September 27, 2021, in conjunction with the 24th International Conference on Medical Imaging and Computer-Assisted Intervention, MICCAI 2021. The 7 full papers presented at iMIMIC 2021 and 5 full papers held at TDA4MedicalData 2021 were carefully reviewed and selected from 12 submissions each. The iMIMIC papers focus on introducing the challenges and opportunities related to the topic of interpretability of machine learning systems in the context of medical imaging and computer assisted intervention. TDA4MedicalData is focusing on using TDA techniques to enhance the performance, generalizability, efficiency, and explainability of the current methods applied to medical data.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12929,,,All OA, Green,Edited Book,,,,,0,0,,0.0,https://link.springer.com/content/pdf/bfm%3A978-3-030-87444-5%2F1,https://app.dimensions.ai/details/publication/pub.1141238855,46 Information and Computing Sciences, 4602 Artificial Intelligence,,,,,,,,,,
164,pub.1151856854,10.1007/978-3-031-18814-5,,,"Multiscale Multimodal Medical Imaging, Third International Workshop, MMMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings","This book constitutes the refereed proceedings of the Third International Workshop on Multiscale Multimodal Medical Imaging, MMMI 2022, held in conjunction with MICCAI 2022 in singapore, in September 2022. The 12 papers presented were carefully reviewed and selected from 18 submissions. The MMMI workshop aims to advance the state of the art in multi-scale multi-modal medical imaging, including algorithm development, implementation of methodology, and experimental studies. The papers focus on medical image analysis and machine learning, especially on machine learning methods for data fusion and multi-score learning.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13594,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1151856854,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
159,pub.1141326795,10.1007/978-3-030-87199-4,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2021, 24th International Conference, Strasbourg, France, September 27âOctober 1, 2021, Proceedings, Part III","The eight-volume set LNCS 12901, 12902, 12903, 12904, 12905, 12906, 12907, and 12908 constitutes the refereed proceedings of the 24th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2021, held in Strasbourg, France, in September/October 2021.* The 531 revised full papers presented were carefully reviewed and selected from 1630 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: image segmentation Part II: machine learning - self-supervised learning; machine learning - semi-supervised learning; and machine learning - weakly supervised learning Part III: machine learning - advances in machine learning theory; machine learning - attention models; machine learning - domain adaptation; machine learning - federated learning; machine learning - interpretability / explainability; and machine learning - uncertainty Part IV: image registration; image-guided interventions and surgery; surgical data science; surgical planning and simulation; surgical skill and work flow analysis; and surgical visualization and mixed, augmented and virtual reality Part V: computer aided diagnosis; integration of imaging with non-imaging biomarkers; and outcome/disease prediction Part VI: image reconstruction; clinical applications - cardiac; and clinical applications - vascular Part VII: clinical applications - abdomen; clinical applications - breast; clinical applications - dermatology; clinical applications - fetal imaging; clinical applications - lung; clinical applications - neuroimaging - brain development; clinical applications - neuroimaging - DWI and tractography; clinical applications - neuroimaging - functional brain networks; clinical applications - neuroimaging â others; and clinical applications - oncology Part VIII: clinical applications - ophthalmology; computational (integrative) pathology; modalities - microscopy; modalities - histopathology; and modalities - ultrasound *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12903,,,All OA, Green,Edited Book,,,,,5,5,,4.09,https://link.springer.com/content/pdf/bfm%3A978-3-030-87199-4%2F1,https://app.dimensions.ai/details/publication/pub.1141326795,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
144,pub.1151032978,10.1007/978-3-031-16443-9,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2022, 25th International Conference, Singapore, September 18â22, 2022, Proceedings, Part V","The eight-volume set LNCS 13431, 13432, 13433, 13434, 13435, 13436, 13437, and 13438 constitutes the refereed proceedings of the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2022, which was held in Singapore in September 2022. The 574 revised full papers presented were carefully reviewed and selected from 1831 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: Brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; heart and lung imaging; dermatology; Part II: Computational (integrative) pathology; computational anatomy and physiology; ophthalmology; fetal imaging; Part III: Breast imaging; colonoscopy; computer aided diagnosis; Part IV: Microscopic image analysis; positron emission tomography; ultrasound imaging; video data analysis; image segmentation I; Part V: Image segmentation II; integration of imaging with non-imaging biomarkers; Part VI: Image registration; image reconstruction; Part VII: Image-Guided interventions and surgery; outcome and disease prediction; surgical data science; surgical planning and simulation; machine learning â domain adaptation and generalization; Part VIII: Machine learning â weakly-supervised learning; machine learning â model interpretation; machine learning â uncertainty; machine learning theory and methodologies.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13435,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16443-9%2F1,https://app.dimensions.ai/details/publication/pub.1151032978,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
130,pub.1149727518,10.1007/978-3-031-12053-4,,,"Medical Image Understanding and Analysis, 26th Annual Conference, MIUA 2022, Cambridge, UK, July 27â29, 2022, Proceedings",Chapter âFCN-Transformer Feature Fusion for Polyp Segmentationâ is available open access under a Creative Commons Attribution 4.0 International License via link.springer.com.,,,Lecture Notes in Computer Science,,,2022,2022,,2022,13413,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-12053-4%2F1,https://app.dimensions.ai/details/publication/pub.1149727518,46 Information and Computing Sciences,,,,,,,,,,,
129,pub.1148730219,10.1007/978-3-031-08277-1,,,"Intelligent Systems and Pattern Recognition, Second International Conference, ISPR 2022, Hammamet, Tunisia, March 24â26, 2022, Revised Selected Papers","This volume constitutes selected papers presented during the Second International Conference on Intelligent Systems and Pattern Recognition, ISPR 2022, held in Hammamet, Tunisia, in March 2022. Due to the COVID-19 pandemic the conference was held online. The 22 full papers and 10 short papers presented were thoroughly reviewed and selected from the 91 submissions. The papers are organized in the following topical sections: computer vision; data mining; pattern recognition; machine and deep learning.",,,Communications in Computer and Information Science,,,2022,2022,,2022,1589,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-08277-1%2F1,https://app.dimensions.ai/details/publication/pub.1148730219,46 Information and Computing Sciences, 4602 Artificial Intelligence,,,,,,,,,,
128,pub.1151668454,10.1007/978-3-031-17899-3,,,"Machine Learning in Clinical Neuroimaging, 5th International Workshop, MLCN 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings","This book constitutes the refereed proceedings of the 5th International Workshop on Machine Learning in Clinical Neuroimaging, MLCN 2022, held in Conjunction with MICCAI 2022, Singapore in September 2022. The book includes 17 papers which were carefully reviewed and selected from 23 full-length submissions. The 5th international workshop on Machine Learning in Clinical Neuroimaging (MLCN2022) aims to bring together the top researchers in both machine learning and clinical neuroscience as well as tech-savvy clinicians to address two main challenges: 1) development of methodological approaches for analyzing complex and heterogeneous neuroimaging data (machine learning track); and 2) filling the translational gap in applying existing machine learning methods in clinical practices (clinical neuroimaging track). The papers are categorzied into topical sub-headings: Morphometry; Diagnostics, and Aging, and Neurodegeneration.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13596,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1151668454,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
128,pub.1142567426,10.1007/978-3-030-90874-4,,,"Clinical Image-Based Procedures, Distributed and Collaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning, 10th Workshop, CLIP 2021, Second Workshop, DCL 2021, First Workshop, LL-COVID19 2021, and First Workshop and Tutorial, PPML 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27 and October 1, 2021, Proceedings","This book constitutes the refereed proceedings of the 10th International Workshop on Clinical Image-Based Procedures, CLIP 2021, Second MICCAI Workshop on Distributed and Collaborative Learning, DCL 2021, First MICCAI Workshop, LL-COVID19, First Secure and Privacy-Preserving Machine Learning for Medical Imaging Workshop and Tutorial, PPML 2021, held in conjunction with MICCAI 2021, in October 2021. The workshops were planned to take place in Strasbourg, France, but were held virtually due to the COVID-19 pandemic. CLIP 2021 accepted 9 papers from the 13 submissions received. It focuses on holistic patient models for personalized healthcare with the goal to bring basic research methods closer to the clinical practice. For DCL 2021, 4 papers from 7 submissions were accepted for publication. They deal with machine learning applied to problems where data cannot be stored in centralized databases and information privacy is a priority. LL-COVID19 2021 accepted 2 papers out of 3 submissions dealing with the use of AI models in clinical practice. And for PPML 2021, 2 papers were accepted from a total of 6 submissions, exploring the use of privacy techniques in the medical imaging community.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12969,,,All OA, Green,Edited Book,,,,,0,0,,0.0,https://link.springer.com/content/pdf/bfm%3A978-3-030-90874-4%2F1,https://app.dimensions.ai/details/publication/pub.1142567426,46 Information and Computing Sciences, 4604 Cybersecurity and Privacy,3 Good Health and Well Being,,,,,,,,,
97,pub.1143570290,10.1007/978-3-030-85990-9,,,"Proceedings of International Conference on Emerging Technologies and Intelligent Systems, ICETIS 2021 Volume 2","This book sheds light on the emerging research trends in intelligent systems and their applications. It mainly focuses on four different themes, including Artificial Intelligence and Soft Computing, Information Security and Networking, Medical Informatics, and Advances in Information Systems. Each chapter contributes to the aforementioned themes by discussing the recent design, developments, and modifications of intelligent systems and their applications.",,,Lecture Notes in Networks and Systems,,,2022,2022,,2022,322,,,All OA, Green,Edited Book,,,,,1,1,,,https://link.springer.com/content/pdf/bfm%3A978-3-030-85990-9%2F1,https://app.dimensions.ai/details/publication/pub.1143570290,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
89,pub.1151032907,10.1007/978-3-031-16440-8,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2022, 25th International Conference, Singapore, September 18â22, 2022, Proceedings, Part IV","The eight-volume set LNCS 13431, 13432, 13433, 13434, 13435, 13436, 13437, and 13438 constitutes the refereed proceedings of the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2022, which was held in Singapore in September 2022. The 574 revised full papers presented were carefully reviewed and selected from 1831 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: Brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; heart and lung imaging; dermatology; Part II: Computational (integrative) pathology; computational anatomy and physiology; ophthalmology; fetal imaging; Part III: Breast imaging; colonoscopy; computer aided diagnosis; Part IV: Microscopic image analysis; positron emission tomography; ultrasound imaging; video data analysis; image segmentation I; Part V: Image segmentation II; integration of imaging with non-imaging biomarkers; Part VI: Image registration; image reconstruction; Part VII: Image-Guided interventions and surgery; outcome and disease prediction; surgical data science; surgical planning and simulation; machine learning â domain adaptation and generalization; Part VIII: Machine learning â weakly-supervised learning; machine learning â model interpretation; machine learning â uncertainty; machine learning theory and methodologies.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13434,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16440-8%2F1,https://app.dimensions.ai/details/publication/pub.1151032907,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
89,pub.1141489930,10.1007/978-3-030-88210-5,,,"Deep Generative Models, and Data Augmentation, Labelling, and Imperfections, First Workshop, DGM4MICCAI 2021, and First Workshop, DALI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October 1, 2021, Proceedings","This book constitutes the refereed proceedings of the First MICCAI Workshop on Deep Generative Models, DG4MICCAI 2021, and the First MICCAI Workshop on Data Augmentation, Labelling, and Imperfections, DALI 2021, held in conjunction with MICCAI 2021, in October 2021. The workshops were planned to take place in Strasbourg, France, but were held virtually due to the COVID-19 pandemic. DG4MICCAI 2021 accepted 12 papers from the 17 submissions received. The workshop focusses on recent algorithmic developments, new results, and promising future directions in Deep Generative Models. Deep generative models such as Generative Adversarial Network (GAN) and Variational Auto-Encoder (VAE) are currently receiving widespread attention from not only the computer vision and machine learning communities, but also in the MIC and CAI community. For DALI 2021, 15 papers from 32 submissions were accepted for publication. They focus on rigorous study of medical data related to machine learning systems.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,13003,,,All OA, Green,Edited Book,,,,,5,5,,4.09,https://link.springer.com/content/pdf/bfm%3A978-3-030-88210-5%2F1,https://app.dimensions.ai/details/publication/pub.1141489930,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
89,pub.1154711669,10.1007/978-3-031-23911-3,,,"Fast and Low-Resource Semi-supervised Abdominal Organ Segmentation, MICCAI 2022 Challenge, FLARE 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings","This book constitutes the proceedings of the MICCAI 2022 Challenge, FLARE 2022, held in Conjunction with MICCAI 2022, in Singapore, on September 22, 2022. The 28 full papers presented in this book were carefully reviewed and selected from 48 submissions. The papers present research and results for abdominal organ segmentation which has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13816,,,Closed,Edited Book,,,,,1,1,,,,https://app.dimensions.ai/details/publication/pub.1154711669,46 Information and Computing Sciences,,,,,,,,,,,,
83,pub.1131661364,10.1007/978-3-030-60799-9,,,"Intelligent Computing Theories and Application, 16th International Conference, ICIC 2020, Bari, Italy, October 2â5, 2020, Proceedings, Part I","This two-volume set of LNCS 12463 and LNCS 12464 constitutes - in conjunction with the volume LNAI 12465 - the refereed proceedings of the 16th International Conference on Intelligent Computing, ICIC 2020, held in Bari, Italy, in October 2020. The 162 full papers of the three proceedings volumes were carefully reviewed and selected from 457 submissions. The ICIC theme unifies the picture of contemporary intelligent computing techniques as an integral concept that highlights the trends in advanced computational intelligence and bridges theoretical research with applications. The theme for this conference is âAdvanced Intelligent Computing Methodologies and Applications.â Papers related to this theme are especially solicited, addressing theories, methodologies, and applications in science and technology.",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12463,,,All OA, Green,Edited Book,,,,,0,0,,0.0,https://link.springer.com/content/pdf/bfm:978-3-030-60799-9/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1131661364,46 Information and Computing Sciences, 4602 Artificial Intelligence,,,,,,,,,,
77,pub.1148543586,10.1002/mp.15769,35678486,,Abstract,,,,Medical Physics,,"Societies, Medical",2022-06-09,2022,2022-06-09,2022-06,49,6,e113-e982,Closed,Article,,,,,1,1,,,,https://app.dimensions.ai/details/publication/pub.1148543586,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
77,pub.1131419042,10.1007/978-3-030-61166-8,,,"Interpretable and Annotation-Efficient Learning for Medical Image Computing, Third International Workshop, iMIMIC 2020, Second International Workshop, MIL3ID 2020, and 5th International Workshop, LABELS 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4â8, 2020, Proceedings","This book constitutes the refereed joint proceedings of the Third International Workshop on Interpretability of Machine Intelligence in Medical Image Computing, iMIMIC 2020, the Second International Workshop on Medical Image Learning with Less Labels and Imperfect Data, MIL3ID 2020, and the 5th International Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis, LABELS 2020, held in conjunction with the 23rd International Conference on Medical Imaging and Computer-Assisted Intervention, MICCAI 2020, in Lima, Peru, in October 2020. The 8 full papers presented at iMIMIC 2020, 11 full papers to MIL3ID 2020, and the 10 full papers presented at LABELS 2020 were carefully reviewed and selected from 16 submissions to iMIMIC, 28 to MIL3ID, and 12 submissions to LABELS. The iMIMIC papers focus on introducing the challenges and opportunities related to the topic of interpretability of machine learning systems in the context of medical imaging and computer assisted intervention. MIL3ID deals with best practices in medical image learning with label scarcity and data imperfection. The LABELS papers present a variety of approaches for dealing with a limited number of labels, from semi-supervised learning to crowdsourcing.",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12446,,,All OA, Green,Edited Book,,,,,9,9,,4.64,https://link.springer.com/content/pdf/bfm%3A978-3-030-61166-8%2F1,https://app.dimensions.ai/details/publication/pub.1131419042,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
72,pub.1151412641,10.1007/978-3-031-16364-7,,,"Computational Intelligence in Data Science, 5th IFIP TC 12 International Conference, ICCIDS 2022, Virtual Event, March 24â26, 2022, Revised Selected Papers","This book constitutes the refereed post-conference proceedings of the Fifth IFIP TC 12 International Conference on Computational Intelligence in Data Science, ICCIDS 2022, held virtually, in March 2022. The 28 revised full papers presented were carefully reviewed and selected from 96 submissions. The papers cover topics such as computational intelligence for text analysis; computational intelligence for image and video analysis; blockchain and data science.",,,IFIP Advances in Information and Communication Technology,,,2022,2022,,2022,654,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16364-7%2F1,https://app.dimensions.ai/details/publication/pub.1151412641,46 Information and Computing Sciences, 4602 Artificial Intelligence,,,,,,,,,,
61,pub.1141301941,10.1007/978-3-030-87193-2,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2021, 24th International Conference, Strasbourg, France, September 27âOctober 1, 2021, Proceedings, Part I","The eight-volume set LNCS 12901, 12902, 12903, 12904, 12905, 12906, 12907, and 12908 constitutes the refereed proceedings of the 24th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2021, held in Strasbourg, France, in September/October 2021.* The 531 revised full papers presented were carefully reviewed and selected from 1630 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: image segmentation Part II: machine learning - self-supervised learning; machine learning - semi-supervised learning; and machine learning - weakly supervised learning Part III: machine learning - advances in machine learning theory; machine learning - attention models; machine learning - domain adaptation; machine learning - federated learning; machine learning - interpretability / explainability; and machine learning - uncertainty Part IV: image registration; image-guided interventions and surgery; surgical data science; surgical planning and simulation; surgical skill and work flow analysis; and surgical visualization and mixed, augmented and virtual reality Part V: computer aided diagnosis; integration of imaging with non-imaging biomarkers; and outcome/disease prediction Part VI: image reconstruction; clinical applications - cardiac; and clinical applications - vascular Part VII: clinical applications - abdomen; clinical applications - breast; clinical applications - dermatology; clinical applications - fetal imaging; clinical applications - lung; clinical applications - neuroimaging - brain development; clinical applications - neuroimaging - DWI and tractography; clinical applications - neuroimaging - functional brain networks; clinical applications - neuroimaging â others; and clinical applications - oncology Part VIII: clinical applications - ophthalmology; computational (integrative) pathology; modalities - microscopy; modalities - histopathology; and modalities - ultrasound *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12901,,,All OA, Green,Edited Book,,,,,4,4,,3.27,https://link.springer.com/content/pdf/bfm%3A978-3-030-87193-2%2F1,https://app.dimensions.ai/details/publication/pub.1141301941,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
55,pub.1152128760,10.1007/978-3-031-19803-8,,,"Computer Vision â ECCV 2022, 17th European Conference, Tel Aviv, Israel, October 23â27, 2022, Proceedings, Part XXI","The 39-volume set, comprising the LNCS books 13661 until 13699, constitutes the refereed proceedings of the 17th European Conference on Computer Vision, ECCV 2022, held in Tel Aviv, Israel, during October 23â27, 2022. The 1645 papers presented in these proceedings were carefully reviewed and selected from a total of 5804 submissions. The papers deal with topics such as computer vision; machine learning; deep neural networks; reinforcement learning; object recognition; image classification; image processing; object detection; semantic segmentation; human pose estimation; 3d reconstruction; stereo vision; computational photography; neural networks; image coding; image reconstruction; object recognition; motion estimation.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13681,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1152128760,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,,
55,pub.1131399582,10.1007/978-3-030-59719-1,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2020, 23rd International Conference, Lima, Peru, October 4â8, 2020, Proceedings, Part IV","The seven-volume set LNCS 12261, 12262, 12263, 12264, 12265, 12266, and 12267 constitutes the refereed proceedings of the 23rd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2020, held in Lima, Peru, in October 2020. The conference was held virtually due to the COVID-19 pandemic. The 542 revised full papers presented were carefully reviewed and selected from 1809 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: machine learning methodologies Part II: image reconstruction; prediction and diagnosis; cross-domain methods and reconstruction; domain adaptation; machine learning applications; generative adversarial networks Part III: CAI applications; image registration; instrumentation and surgical phase detection; navigation and visualization; ultrasound imaging; video image analysis Part IV: segmentation; shape models and landmark detection Part V: biological, optical, microscopic imaging; cell segmentation and stain normalization; histopathology image analysis; opthalmology Part VI: angiography and vessel analysis; breast imaging; colonoscopy; dermatology; fetal imaging; heart and lung imaging; musculoskeletal imaging Part VI: brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; positron emission tomography",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12264,,,All OA, Green,Edited Book,,,,,6,6,,3.09,https://link.springer.com/content/pdf/bfm:978-3-030-59719-1/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1131399582,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
55,pub.1137834660,10.1007/978-3-030-75768-7,,,"Advances in Knowledge Discovery and Data Mining, 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11â14, 2021, Proceedings, Part III","The 3-volume set LNAI 12712-12714 constitutes the proceedings of the 25th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2021, which was held during May 11-14, 2021. The 157 papers included in the proceedings were carefully reviewed and selected from a total of 628 submissions. They were organized in topical sections as follows: Part I: Applications of knowledge discovery and data mining of specialized data; Part II: Classical data mining; data mining theory and principles; recommender systems; and text analytics; Part III: Representation learning and embedding, and learning from data.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12714,,,All OA, Green,Edited Book,,,,,1,1,,0.83,https://link.springer.com/content/pdf/bfm%3A978-3-030-75768-7%2F1,https://app.dimensions.ai/details/publication/pub.1137834660,46 Information and Computing Sciences, 4602 Artificial Intelligence,,,,,,,,,,
50,pub.1136659889,10.1007/978-3-030-72087-2,,,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries, 6th International Workshop, BrainLes 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised Selected Papers, Part II","This two-volume set LNCS 12658 and 12659 constitutes the thoroughly refereed proceedings of the 6th International MICCAI Brainlesion Workshop, BrainLes 2020, the International Multimodal Brain Tumor Segmentation (BraTS) challenge, and the Computational Precision Medicine: Radiology-Pathology Challenge on Brain Tumor Classification (CPM-RadPath) challenge. These were held jointly at the 23rd Medical Image Computing for Computer Assisted Intervention Conference, MICCAI 2020, in Lima, Peru, in October 2020.* The revised selected papers presented in these volumes were organized in the following topical sections: brain lesion image analysis (16 selected papers from 21 submissions); brain tumor image segmentation (69 selected papers from 75 submissions); and computational precision medicine: radiology-pathology challenge on brain tumor classification (6 selected papers from 6 submissions). *The workshop and challenges were held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12659,,,All OA, Green,Edited Book,,,,,1,1,,,https://library.oapen.org/bitstream/20.500.12657/57925/1/978-3-031-08999-2.pdf,https://app.dimensions.ai/details/publication/pub.1136659889,46 Information and Computing Sciences,3 Good Health and Well Being,,,,,,,,,,
46,pub.1136694191,10.1007/978-3-030-72084-1,,,"Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries, 6th International Workshop, BrainLes 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised Selected Papers, Part I","This two-volume set LNCS 12658 and 12659 constitutes the thoroughly refereed proceedings of the 6th International MICCAI Brainlesion Workshop, BrainLes 2020, the International Multimodal Brain Tumor Segmentation (BraTS) challenge, and the Computational Precision Medicine: Radiology-Pathology Challenge on Brain Tumor Classification (CPM-RadPath) challenge. These were held jointly at the 23rd Medical Image Computing for Computer Assisted Intervention Conference, MICCAI 2020, in Lima, Peru, in October 2020.* The revised selected papers presented in these volumes were organized in the following topical sections: brain lesion image analysis (16 selected papers from 21 submissions); brain tumor image segmentation (69 selected papers from 75 submissions); and computational precision medicine: radiology-pathology challenge on brain tumor classification (6 selected papers from 6 submissions). *The workshop and challenges were held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12658,,,All OA, Green,Edited Book,,,,,1,1,,,https://library.oapen.org/bitstream/20.500.12657/57925/1/978-3-031-08999-2.pdf,https://app.dimensions.ai/details/publication/pub.1136694191,46 Information and Computing Sciences,3 Good Health and Well Being,,,,,,,,,,
35,pub.1143637401,10.1007/978-3-030-92185-9,,,"Neural Information Processing, 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8â12, 2021, Proceedings, Part I","The four-volume proceedings LNCS 13108, 13109, 13110, and 13111 constitutes the proceedings of the 28th International Conference on Neural Information Processing, ICONIP 2021, which was held during December 8-12, 2021. The conference was planned to take place in Bali, Indonesia but changed to an online format due to the COVID-19 pandemic. The total of 226 full papers presented in these proceedings was carefully reviewed and selected from 1093 submissions. The papers were organized in topical sections as follows: Part I: Theory and algorithms; Part II: Theory and algorithms; human centred computing; AI and cybersecurity; Part III: Cognitive neurosciences; reliable, robust, and secure machine learning algorithms; theory and applications of natural computing paradigms; advances in deep and shallow machine learning algorithms for biomedical data and imaging; applications; Part IV: Applications.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,13108,,,All OA, Green,Edited Book,,,,,0,0,,0.0,https://link.springer.com/content/pdf/bfm%3A978-3-030-92185-9%2F1,https://app.dimensions.ai/details/publication/pub.1143637401,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
31,pub.1131394427,10.1007/978-3-030-59713-9,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2020, 23rd International Conference, Lima, Peru, October 4â8, 2020, Proceedings, Part II","The seven-volume set LNCS 12261, 12262, 12263, 12264, 12265, 12266, and 12267 constitutes the refereed proceedings of the 23rd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2020, held in Lima, Peru, in October 2020. The conference was held virtually due to the COVID-19 pandemic. The 542 revised full papers presented were carefully reviewed and selected from 1809 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: machine learning methodologies Part II: image reconstruction; prediction and diagnosis; cross-domain methods and reconstruction; domain adaptation; machine learning applications; generative adversarial networks Part III: CAI applications; image registration; instrumentation and surgical phase detection; navigation and visualization; ultrasound imaging; video image analysis Part IV: segmentation; shape models and landmark detection Part V: biological, optical, microscopic imaging; cell segmentation and stain normalization; histopathology image analysis; opthalmology Part VI: angiography and vessel analysis; breast imaging; colonoscopy; dermatology; fetal imaging; heart and lung imaging; musculoskeletal imaging Part VI: brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; positron emission tomography",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12262,,,All OA, Green,Edited Book,,,,,4,4,,2.06,https://link.springer.com/content/pdf/bfm:978-3-030-59713-9/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1131394427,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
31,pub.1151032831,10.1007/978-3-031-16437-8,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2022, 25th International Conference, Singapore, September 18â22, 2022, Proceedings, Part III","The eight-volume set LNCS 13431, 13432, 13433, 13434, 13435, 13436, 13437, and 13438 constitutes the refereed proceedings of the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2022, which was held in Singapore in September 2022. The 574 revised full papers presented were carefully reviewed and selected from 1831 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: Brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; heart and lung imaging; dermatology; Part II: Computational (integrative) pathology; computational anatomy and physiology; ophthalmology; fetal imaging; Part III: Breast imaging; colonoscopy; computer aided diagnosis; Part IV: Microscopic image analysis; positron emission tomography; ultrasound imaging; video data analysis; image segmentation I; Part V: Image segmentation II; integration of imaging with non-imaging biomarkers; Part VI: Image registration; image reconstruction; Part VII: Image-Guided interventions and surgery; outcome and disease prediction; surgical data science; surgical planning and simulation; machine learning â domain adaptation and generalization; Part VIII: Machine learning â weakly-supervised learning; machine learning â model interpretation; machine learning â uncertainty; machine learning theory and methodologies.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13433,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16437-8%2F1,https://app.dimensions.ai/details/publication/pub.1151032831,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
28,pub.1141326998,10.1007/978-3-030-87240-3,,,"Medical Image Computing and Computer Assisted Intervention â MICCAI 2021, 24th International Conference, Strasbourg, France, September 27 â October 1, 2021, Proceedings, Part V","The eight-volume set LNCS 12901, 12902, 12903, 12904, 12905, 12906, 12907, and 12908 constitutes the refereed proceedings of the 24th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2021, held in Strasbourg, France, in September/October 2021.* The 531 revised full papers presented were carefully reviewed and selected from 1630 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: image segmentation Part II: machine learning - self-supervised learning; machine learning - semi-supervised learning; and machine learning - weakly supervised learning Part III: machine learning - advances in machine learning theory; machine learning - attention models; machine learning - domain adaptation; machine learning - federated learning; machine learning - interpretability / explainability; and machine learning - uncertainty Part IV: image registration; image-guided interventions and surgery; surgical data science; surgical planning and simulation; surgical skill and work flow analysis; and surgical visualization and mixed, augmented and virtual reality Part V: computer aided diagnosis; integration of imaging with non-imaging biomarkers; and outcome/disease prediction Part VI: image reconstruction; clinical applications - cardiac; and clinical applications - vascular Part VII: clinical applications - abdomen; clinical applications - breast; clinical applications - dermatology; clinical applications - fetal imaging; clinical applications - lung; clinical applications - neuroimaging - brain development; clinical applications - neuroimaging - DWI and tractography; clinical applications - neuroimaging - functional brain networks; clinical applications - neuroimaging â others; and clinical applications - oncology Part VIII: clinical applications - ophthalmology; computational (integrative) pathology; modalities - microscopy; modalities - histopathology; and modalities - ultrasound *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12905,,,All OA, Green,Edited Book,,,,,4,4,,3.27,https://link.springer.com/content/pdf/bfm%3A978-3-030-87240-3%2F1,https://app.dimensions.ai/details/publication/pub.1141326998,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
28,pub.1132260856,10.1007/978-3-030-58452-8,,,"Computer Vision â ECCV 2020, 16th European Conference, Glasgow, UK, August 23â28, 2020, Proceedings, Part I","The 30-volume set, comprising the LNCS books 12346 until 12375, constitutes the refereed proceedings of the 16th European Conference on Computer Vision, ECCV 2020, which was planned to be held in Glasgow, UK, during August 23-28, 2020. The conference was held virtually due to the COVID-19 pandemic. The 1360 revised papers presented in these proceedings were carefully reviewed and selected from a total of 5025 submissions. The papers deal with topics such as computer vision; machine learning; deep neural networks; reinforcement learning; object recognition; image classification; image processing; object detection; semantic segmentation; human pose estimation; 3d reconstruction; stereo vision; computational photography; neural networks; image coding; image reconstruction; object recognition; motion estimation.",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12346,,,All OA, Green,Edited Book,,,,,14,13,,6.93,https://link.springer.com/content/pdf/bfm%3A978-3-030-58452-8%2F1,https://app.dimensions.ai/details/publication/pub.1132260856,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
