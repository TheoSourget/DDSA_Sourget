"About the data: Exported on Mar 06, 2023. Criteria: '""Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge""' in full data; Publication Year is 2016 or 2017 or 2018 or 2019 or 2020 or 2021 or 2022 or 2015 or 2014. © 2023 Digital Science &amp; Research Solutions Inc. All rights reserved. Parts of this work may also be protected by copyright of content providers and other third parties, which together with all rights of Digital Science, user agrees not to violate. Redistribution / external use of this work (or parts thereof) is prohibited without prior written approval. Please contact info@dimensions.ai for further information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rank,Publication ID,DOI,PMID,PMCID,Title,Abstract,Acknowledgements,Funding,Source title,Anthology title,MeSH terms,Publication Date,PubYear,Publication Date (online),Publication Date (print),Volume,Issue,Pagination,Open Access,Publication Type,Authors,Authors (Raw Affiliation),Corresponding Authors,Authors Affiliations,Times cited,Recent citations,RCR,FCR,Source Linkout,Dimensions URL,Fields of Research (ANZSRC 2020),Sustainable Development Goals,,,,,,,,,,,
8636,pub.1140492705,10.1016/j.euf.2021.07.017,34417153,,The Growing Role for Semantic Segmentation in Urology,"As the quantity and quality of cross-sectional imaging data increase, it is important to be able to make efficient use of the information. Semantic segmentation is an emerging technology that promises to improve the speed, reproducibility, and accuracy of analysis of medical imaging, and to allow visualization methods that were previously impossible. Manual image segmentation often requires expert knowledge and is both time- and cost-prohibitive in many clinical situations. However, automated methods, especially those using deep learning, show promise in alleviating this burden to make segmentation a standard tool for clinical intervention in the future. It is therefore important for clinicians to have a functional understanding of what segmentation is and to be aware of its uses. Here we include a number of examples of ways in which semantic segmentation has been put into practice in urology. PATIENT SUMMARY: This mini-review highlights the growing role of segmentation methods for medical images in urology to inform clinical practice. Segmentation methods show promise in improving the reliability of diagnosis and aiding in visualization, which may become a tool for patient education.",,,European Urology Focus,,"Deep Learning; Humans; Image Processing, Computer-Assisted; Reproducibility of Results; Semantics; Urology",2021-07,2021,2021-08-18,2021-07,7,4,692-695,Closed,Article,"Rickman, Jack; Struyk, Griffin; Simpson, Benjamin; Byun, Benjamin C; Papanikolopoulos, Nikolaos","Rickman, Jack (Minnesota Robotics Institute, University of Minnesota College of Science and Engineering, Minneapolis, MN, USA. Electronic address: rickm014@umn.edu.); Struyk, Griffin (University of Minnesota Medical School, Twin Cities Campus, Minneapolis, MN, USA.); Simpson, Benjamin (University of Minnesota Medical School, Twin Cities Campus, Minneapolis, MN, USA.); Byun, Benjamin C (University of Minnesota Medical School, Twin Cities Campus, Minneapolis, MN, USA.); Papanikolopoulos, Nikolaos (Minnesota Robotics Institute, University of Minnesota College of Science and Engineering, Minneapolis, MN, USA.)","Rickman, Jack (University of Minnesota)","Rickman, Jack (University of Minnesota); Struyk, Griffin (University of Minnesota); Simpson, Benjamin (University of Minnesota); Byun, Benjamin C (University of Minnesota); Papanikolopoulos, Nikolaos (University of Minnesota)",1,1,0.29,0.88,,https://app.dimensions.ai/details/publication/pub.1140492705,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,,
7800,pub.1150860027,10.1109/embc48229.2022.9871334,36085623,,Multi-parametric Magnetic Resonance Imaging Fusion for Automatic Classification of Prostate Cancer,"Computer-aided diagnosis (CAD) of prostate cancer (PCa) using multi-parametric magnetic resonance imaging (mp-MRI) has recently gained great research interest. In this work, a fully automatic CAD pipeline of PCa using mp-MRI data is presented. In order to fully explore the mp-MRI data, we systematically investigate three multi-modal medical image fusion strategies in convolutional neural networks, namely input-level fusion, feature-level fusion, and decision-level fusion. Extensive experiments are conducted on two datasets with different PCa-related diagnostic tasks. We identify a pipeline that works relatively the best for both diagnostic tasks, two important components of which are stacking three adjacent slices as the input and performing decision-level fusion with specific loss weights. Clinical relevance- This work provides a practical method for automated diagnosis of PCa based on multi-parametric MRI.",,"This study was supported by the National Natural Science Foun-dation of China (62071210); the Shenzhen Basic Research Program (JCYJ20200925153847004, JCYJ20190809120205578); the High-level University Fund (G02236002).",Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"Diagnosis, Computer-Assisted; Humans; Magnetic Resonance Imaging; Male; Multiparametric Magnetic Resonance Imaging; Neural Networks, Computer; Prostatic Neoplasms",2022-07,2022,,2022-07,0,,471-474,Closed,Proceeding,"Huang, Weikai; Wang, Xiangyu; Huang, Yijin; Lin, Fan; Tang, Xiaoying","Huang, Weikai (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China); Wang, Xiangyu (Department of Radiology, Shenzhen Second People's Hospital, Shenzhen, China); Huang, Yijin (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China; School of Biomedical Engineering, University of British Columbia, Vancouver, British Columbia, Canada); Lin, Fan (Department of Radiology, Shenzhen Second People's Hospital, Shenzhen, China); Tang, Xiaoying (Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China)","Huang, Weikai (Southern University of Science and Technology); Wang, Xiangyu (Shenzhen Second People's Hospital); Lin, Fan (Shenzhen Second People's Hospital); Tang, Xiaoying (Southern University of Science and Technology)","Huang, Weikai (Southern University of Science and Technology); Wang, Xiangyu (Shenzhen Second People's Hospital); Huang, Yijin (Southern University of Science and Technology; University of British Columbia); Lin, Fan (Shenzhen Second People's Hospital); Tang, Xiaoying (Southern University of Science and Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150860027,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
7787,pub.1143756283,10.1109/embc46164.2021.9630792,34891962,,Deep learning model for automatic prostate segmentation on bicentric T2w images with and without endorectal coil,"Automatic segmentation of the prostate on Magnetic Resonance Imaging (MRI) is one of the topics on which research has focused in recent years as it is a fundamental first step in the building process of a Computer aided diagnosis (CAD) system for cancer detection. Unfortunately, MRI acquired in different centers with different scanners leads to images with different characteristics. In this work, we propose an automatic algorithm for prostate segmentation, based on a U-Net applying transfer learning method in a bi-center setting. First, T2w images with and without endorectal coil from 80 patients acquired at Center A were used as training set and internal validation set. Then, T2w images without endorectal coil from 20 patients acquired at Center B were used as external validation. The reference standard for this study was manual segmentation of the prostate gland performed by an expert operator. The results showed a Dice similarity coefficient >85% in both internal and external validation datasets.Clinical Relevance- This segmentation algorithm could be integrated into a CAD system to optimize computational effort in prostate cancer detection.",The research leading to these results has received funding from the Fondazione AIRC under IG2017 - ID.20398 project – P.I. Regge Daniele and from the European Union’s Horizon 2020 research and innovation program under grant agreement no 952159. The research leading to these results has received funding from the Fondazione AIRC under IG2017 - ID.20398 project – P.I. Regge Daniele and from the European Union’s Horizon 2020 research and innovation program under grant agreement no 952159.,,Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),Algorithms, Deep Learning, Humans, Magnetic Resonance Imaging, Male, Prostate, Prostatic Neoplasms,2021-11,2021,,2021-11,0,,3370-3373,Closed,Proceeding,"Barra, Davide; Nicoletti, Giulia; Defeudis, Arianna; Mazzetti, Simone; Panic, Jovana; Gatti, Marco; Faletti, Riccardo; Russo, Filippo; Regge, Daniele; Giannini, Valentina","Barra, Davide (University of Turin, Department of Surgical Sciences, via Genova 3, 10126, Torino, Italy); Nicoletti, Giulia (University of Turin, Department of Surgical Sciences, via Genova 3, 10126, Torino, Italy); Defeudis, Arianna (University of Turin, Department of Surgical Sciences, via Genova 3, 10126, Torino, Italy); Mazzetti, Simone (University of Turin, Department of Surgical Sciences, via Genova 3, 10126, Torino, Italy); Panic, Jovana (Polytechnic of Turin, Department of Electronics and Telecommunications, Torino, Italy); Gatti, Marco (University of Turin, Department of Surgical Sciences, via Genova 3, 10126, Torino, Italy); Faletti, Riccardo (University of Turin, Department of Surgical Sciences, via Genova 3, 10126, Torino, Italy); Russo, Filippo (Candiolo Cancer Institute, FPO-IRCCS, Strada Provinciale 142, km 3.95, Candiolo (TO), Italy); Regge, Daniele (University of Turin, Department of Surgical Sciences, via Genova 3, 10126, Torino, Italy; Candiolo Cancer Institute, FPO-IRCCS, Strada Provinciale 142, km 3.95, Candiolo (TO), Italy); Giannini, Valentina (University of Turin, Department of Surgical Sciences, via Genova 3, 10126, Torino, Italy)","Barra, Davide (University of Turin)","Barra, Davide (University of Turin); Nicoletti, Giulia (University of Turin); Defeudis, Arianna (University of Turin); Mazzetti, Simone (University of Turin); Panic, Jovana (); Gatti, Marco (University of Turin); Faletti, Riccardo (University of Turin); Russo, Filippo (Candiolo Cancer Institute); Regge, Daniele (University of Turin; Candiolo Cancer Institute); Giannini, Valentina (University of Turin)",3,3,,2.17,,https://app.dimensions.ai/details/publication/pub.1143756283,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,
7774,pub.1152607511,10.1016/j.dib.2022.108739,36426089,PMC9679750,Dataset of prostate MRI annotated for anatomical zones and cancer,"In the present work, we present a publicly available, expert-segmented representative dataset of 158 3.0 Tesla biparametric MRIs [1]. There is an increasing number of studies investigating prostate and prostate carcinoma segmentation using deep learning (DL) with 3D architectures [2], [3], [4], [5], [6], [7]. The development of robust and data-driven DL models for prostate segmentation and assessment is currently limited by the availability of openly available expert-annotated datasets [8], [9], [10]. The dataset contains 3.0 Tesla MRI images of the prostate of patients with suspected prostate cancer. Patients over 50 years of age who had a 3.0 Tesla MRI scan of the prostate that met PI-RADS version 2.1 technical standards were included. All patients received a subsequent biopsy or surgery so that the MRI diagnosis could be verified/matched with the histopathologic diagnosis. For patients who had undergone multiple MRIs, the last MRI, which was less than six months before biopsy/surgery, was included. All patients were examined at a German university hospital (Charité Universitätsmedizin Berlin) between 02/2016 and 01/2020. All MRI were acquired with two 3.0 Tesla MRI scanners (Siemens VIDA and Skyra, Siemens Healthineers, Erlangen, Germany). Axial T2W sequences and axial diffusion-weighted sequences (DWI) with apparent diffusion coefficient maps (ADC) were included in the data set. T2W sequences and ADC maps were annotated by two board-certified radiologists with 6 and 8 years of experience, respectively. For T2W sequences, the central gland (central zone and transitional zone) and peripheral zone were segmented. If areas of suspected prostate cancer (PIRADS score of ≥ 4) were identified on examination, they were segmented in both the T2W sequences and ADC maps. Because restricted diffusion is best seen in DWI images with high b-values, only these images were selected and all images with low b-values were discarded. Data were then anonymized and converted to NIfTI (Neuroimaging Informatics Technology Initiative) format.","LCA is grateful for her participation in the BIH Charité Junior Clinician and Clinician Scientist Program and KKB is grateful for his participation in the BIH Charité Digital Clinician Scientist Program all funded by the Charité — Universitätsmedizin Berlin, the Berlin Institute of Health, and the Deutsche Forschungsgemeinschaft. We also thank the BIH for providing access to their HPC for Research cluster for our analysis. We acknowledge financial support from the Open Access Publication Fund of Charité – Universitätsmedizin Berlin and the German Research Foundation (DFG).",,Data in Brief,,,2022-11-09,2022,2022-11-09,2022-12,45,,108739,All OA, Gold,Article,"Adams, Lisa C.; Makowski, Marcus R.; Engel, Günther; Rattunde, Maximilian; Busch, Felix; Asbach, Patrick; Niehues, Stefan M.; Vinayahalingam, Shankeeth; van Ginneken, Bram; Litjens, Geert; Bressem, Keno K.","Adams, Lisa C. (Charité – Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Hindenburgdamm 30, 12203 Berlin, Germany; Berlin Institute of Health at Charité – Universitätsmedizin Berlin, Charitéplatz 1, 10117 Berlin, Germany); Makowski, Marcus R. (Technical University of Munich, Department of Diagnostic and Interventional Radiology, Faculty of Medicine, Ismaninger Str. 22, 81675, Munich, Germany); Engel, Günther (Charité – Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Hindenburgdamm 30, 12203 Berlin, Germany; Institute for Diagnostic and Interventional Radiology, Georg-August University, Göttingen, Germany); Rattunde, Maximilian (Charité – Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Hindenburgdamm 30, 12203 Berlin, Germany); Busch, Felix (Charité – Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Hindenburgdamm 30, 12203 Berlin, Germany); Asbach, Patrick (Charité – Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Hindenburgdamm 30, 12203 Berlin, Germany); Niehues, Stefan M. (Charité – Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Hindenburgdamm 30, 12203 Berlin, Germany); Vinayahalingam, Shankeeth (Radboud University Medical Center, Nijmegen, GA, The Netherlands); van Ginneken, Bram (Radboud University Medical Center, Nijmegen, GA, The Netherlands); Litjens, Geert (Radboud University Medical Center, Nijmegen, GA, The Netherlands); Bressem, Keno K. (Charité – Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Hindenburgdamm 30, 12203 Berlin, Germany)","Bressem, Keno K. (Charité - University Medicine Berlin)","Adams, Lisa C. (Charité - University Medicine Berlin; Berlin Institute of Health at Charité - Universitätsmedizin Berlin); Makowski, Marcus R. (Technical University of Munich); Engel, Günther (Charité - University Medicine Berlin; University of Göttingen); Rattunde, Maximilian (Charité - University Medicine Berlin); Busch, Felix (Charité - University Medicine Berlin); Asbach, Patrick (Charité - University Medicine Berlin); Niehues, Stefan M. (Charité - University Medicine Berlin); Vinayahalingam, Shankeeth (Radboud University Nijmegen Medical Centre); van Ginneken, Bram (Radboud University Nijmegen Medical Centre); Litjens, Geert (Radboud University Nijmegen Medical Centre); Bressem, Keno K. (Charité - University Medicine Berlin)",0,0,,,https://doi.org/10.1016/j.dib.2022.108739,https://app.dimensions.ai/details/publication/pub.1152607511,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
7252,pub.1138464996,10.1016/j.cmpb.2021.106211,34134076,,PSP net-based automatic segmentation network model for prostate magnetic resonance imaging,"PURPOSE: Prostate cancer is a common cancer. To improve the accuracy of early diagnosis, we propose a prostate Magnetic Resonance Imaging (MRI) segmentation model based on Pyramid Scene Parsing Network (PSP Net).
METHOD: A total of 270 prostate MRI images were collected, and the data set was divided. Contrast limited adaptive histogram equalization (CLAHE) was enhanced in this study. We use the prostate MRI segmentation model based on PSP net, and use segmentation accuracy, under segmentation rate, over segmentation rate and receiver operating characteristic (ROC) curve evaluation index to compare the segmentation effect based on FCN and U-Net.
RESULTS: PSP net has the highest segmentation accuracy of 0.9865, over segmentation rate of 0.0023, under segmentation rate of 0.1111, which is less than FCN and U-Net. The ROC curve of PSP net is closest to the upper left corner, AUC is 0.9427, larger than FCN and U-Net.
CONCLUSION: This paper proves through a large number of experimental results that the prostate MRI automatic segmentation network model based on PSP Net is able to improve the accuracy of segmentation, relieve the workload of doctors, and is worthy of further clinical promotion.",Special appreciation is extended to radiologists of the Fifth Affiliated Hospital of Southern Medical University of Guangzhou for their help in imaging the prostate based on the patients they collected.,,Computer Methods and Programs in Biomedicine,,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostatic Neoplasms",2021-05-29,2021,2021-05-29,2021-08,207,,106211,Closed,Article,"Yan, Lingfei; Liu, Dawei; Xiang, Qi; Luo, Yang; Wang, Tao; Wu, Dali; Chen, Haiping; Zhang, Yu; Li, Qing","Yan, Lingfei (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China. Electronic address: gzgyylf1@163.com.); Liu, Dawei (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China.); Xiang, Qi (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China.); Luo, Yang (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China.); Wang, Tao (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China.); Wu, Dali (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China.); Chen, Haiping (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China.); Zhang, Yu (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China.); Li, Qing (Department of Urology, the Fifth Affiliated Hospital of Southern Medical University, Guangzhou, Guangdong 11100, China.)","Yan, Lingfei (Southern Medical University)","Yan, Lingfei (Southern Medical University); Liu, Dawei (Southern Medical University); Xiang, Qi (Southern Medical University); Luo, Yang (Southern Medical University); Wang, Tao (Southern Medical University); Wu, Dali (Southern Medical University); Chen, Haiping (Southern Medical University); Zhang, Yu (Southern Medical University); Li, Qing (Southern Medical University)",15,15,1.89,10.09,,https://app.dimensions.ai/details/publication/pub.1138464996,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,
7022,pub.1134184364,10.1016/j.compmedimag.2020.101848,33385932,,Automatic brain extraction from 3D fetal MR image with deep learning-based multi-step framework,"Brain extraction is a fundamental prerequisite step in neuroimage analysis for fetus. Due to surrounding maternal tissues and unpredictable movement, brain extraction from fetal Magnetic Resonance (MR) images is a challenging task. In this paper, we propose a novel deep learning-based multi-step framework for brain extraction from 3D fetal MR images. In the first step, a global localization network is applied to estimate probability maps for brain candidates. Connected-component labeling algorithm is applied to eliminate small erroneous components and accurately locate the candidate brain area. In the second step, a local refinement network is implemented in the brain candidate area to obtain fine-grained probability maps. Final extraction results are derived by a fusion network with the two cascaded probability maps obtained from previous two steps. Experimental results demonstrate that our proposed method has superior performance compared with existing deep learning-based methods.",The work described in this paper was partially supported by Natural Science Foundation of Shanghai [grant number: 19ZR1407200] and Natural Science Foundation of Fujian Province [grant number: 2018J01637].,,Computerized Medical Imaging and Graphics,,"Brain; Deep Learning; Fetus; Image Processing, Computer-Assisted; Magnetic Resonance Imaging",2020-12-29,2020,2020-12-29,2021-03,88,,101848,Closed,Article,"Chen, Jian; Fang, Zhenghan; Zhang, Guofu; Ling, Lei; Li, Gang; Zhang, He; Wang, Li","Chen, Jian (School of Electronic, Electrical Engineering and Physics, Fujian University of Technology, Fuzhou, Fujian, 350118, China. Electronic address: jianchen@fjut.edu.cn.); Fang, Zhenghan (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC, 27517, USA.); Zhang, Guofu (Department of Radiology, Obstetrics and Gynecology Hospital, Fudan University, Shanghai, 200011, China.); Ling, Lei (Department of Radiology, Obstetrics and Gynecology Hospital, Fudan University, Shanghai, 200011, China.); Li, Gang (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC, 27517, USA.); Zhang, He (Department of Radiology, Obstetrics and Gynecology Hospital, Fudan University, Shanghai, 200011, China. Electronic address: dr.zhanghe@yahoo.com.); Wang, Li (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC, 27517, USA. Electronic address: li_wang@med.unc.edu.)","Chen, Jian (Fujian University of Technology); Zhang, He (Fudan University); Wang, Li (University of North Carolina at Chapel Hill)","Chen, Jian (Fujian University of Technology); Fang, Zhenghan (University of North Carolina at Chapel Hill); Zhang, Guofu (Fudan University); Ling, Lei (Fudan University); Li, Gang (University of North Carolina at Chapel Hill); Zhang, He (Fudan University); Wang, Li (University of North Carolina at Chapel Hill)",3,3,0.86,1.55,,https://app.dimensions.ai/details/publication/pub.1134184364,40 Engineering, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
6974,pub.1143784254,10.1007/s10334-021-00979-0,34890013,,Automatic prostate segmentation of magnetic resonance imaging using Res-Net,"ObjectivesSegmenting the prostate from magnetic resonance images plays an important role in prostate cancer diagnosis and in evaluating the treatment response. However, the lack of a clear prostate boundary, heterogeneity of prostate tissue, large variety of prostate shape and scarcity of annotated training data makes automatic segmentation a very challenging task. In this work, we proposed a novel two stage segmentation method to automatically segment prostate to support accurate and reproducible results with multisite and multivendor dataset. In the proposed method, we use the combination U-Net with residual blocks.MethodsThe proposed method comprises two stage neural network, first is 2D U-Net, used find the approximate location of prostate, the second is the combination of U-Net and Res-Net used for accurate segmentation of prostate. The network was trained on 116 patient datasets from three publicly available data sources. 80% of data is used for training, 10% for validation, and 10% for testing. The commonly used segmentation evaluation metrics Dice similarity coefficient (DSC), Sensitivity, and Specificity are used for quantitative evaluation of the network.ResultsWith the proposed method average DSC value of 93.8%, Sensitivity value of 94.6% and Specificity of 99.3% was achieved on test datasets.ConclusionsOur experimental results show that the segmentation accuracy can be improved significantly using two stage neural networks.",,,"Magnetic Resonance Materials in Physics, Biology and Medicine",,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate; Prostatic Neoplasms",2021-12-10,2021,2021-12-10,2022-08,35,4,621-630,Closed,Article,"Kumaraswamy, Asha Kuppe; Patil, Chandrashekar M.","Kumaraswamy, Asha Kuppe (Department of Electronics and Communication, Vidyavardhaka College of Engineering, Mysuru, India); Patil, Chandrashekar M. (Department of Electronics and Communication, Vidyavardhaka College of Engineering, Mysuru, India)","Kumaraswamy, Asha Kuppe ","Kumaraswamy, Asha Kuppe (); Patil, Chandrashekar M. ()",1,1,,0.72,,https://app.dimensions.ai/details/publication/pub.1143784254,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
6821,pub.1092356936,10.1109/embc.2017.8037618,29060659,,Multi-View Collaborative Segmentation for Prostate MRI Images,"Prostate delineation from MRI images is a prolonged challenging issue partially due to appearance variations across patients and disease progression. To address these challenges, our proposed collaborative method takes into account the computed multiple label-relevance maps as multiple views for learning the optimal boundary delineation. In our method, we firstly extracted multiple label-relevance maps to represent the affinities between each unlabeled pixel to the pre-defined labels to avoid the selection of handcrafted features. Then these maps were incorporated in a collaborative clustering to learn the adaptive weights for an optimal segmentation which overcomes the seeds selection sensitivity problems. The segmentation results were evaluated over 22 prostate MRI patient studies with respect to dice similarity coefficient (DSC), absolute relative volume difference (ARVD) and average symmetric surface distance (ASSD) (mm). The results and t-Test demonstrated that the proposed method improved the segmentation accuracy and robustness and the improvement was statistically significant.",,,Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),Algorithms, Humans, Magnetic Resonance Imaging, Male, Prostate,2017-07,2017,,2017-07,2017,,3529-3532,Closed,Proceeding,"Wang, Xiuying; Tang, Wensi; Cui, Hui; Zeng, Shan; Feng, David Dagan; Fulham, Michael","Wang, Xiuying (Biomedical and Multimedia Information Technology (BMIT) research group, School of Information Technologies, University of Sydney, Australia); Tang, Wensi (Biomedical and Multimedia Information Technology (BMIT) research group, School of Information Technologies, University of Sydney, Australia); Cui, Hui (Biomedical and Multimedia Information Technology (BMIT) research group, School of Information Technologies, University of Sydney, Australia); Zeng, Shan (School of Mathematics and Computer Science, Wuhan Polytechnic University, China); Feng, David Dagan (BMIT research group, School of Information Technologies, University of Sydney, Australia and Med-X Research Institute, Shanghai Jiao Tong University, China); Fulham, Michael (Faculty of Medicine, University of Sydney, Department of PET and Nuclear Medicine, Royal Prince Alfred Hospital, Australia)",,"Wang, Xiuying (The University of Sydney); Tang, Wensi (The University of Sydney); Cui, Hui (The University of Sydney); Zeng, Shan (Wuhan Polytechnic University); Feng, David Dagan (The University of Sydney; Shanghai Jiao Tong University); Fulham, Michael (The University of Sydney; Royal Prince Alfred Hospital)",2,0,,0.5,,https://app.dimensions.ai/details/publication/pub.1092356936,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,
6767,pub.1142715307,10.1155/2021/4553832,34819951,PMC8608531,An Optimized Approach for Prostate Image Segmentation Using K-Means Clustering Algorithm with Elbow Method,"Prostate cancer disease is one of the common types that cause men's prostate damage all over the world. Prostate-specific membrane antigen (PSMA) expressed by type-II is an extremely attractive style for imaging-based diagnosis of prostate cancer. Clinically, photodynamic therapy (PDT) is used as noninvasive therapy in treatment of several cancers and some other diseases. This paper aims to segment or cluster and analyze pixels of histological and near-infrared (NIR) prostate cancer images acquired by PSMA-targeting PDT low weight molecular agents. Such agents can provide image guidance to resection of the prostate tumors and permit for the subsequent PDT in order to remove remaining or noneradicable cancer cells. The color prostate image segmentation is accomplished using an optimized image segmentation approach. The optimized approach combines the k-means clustering algorithm with elbow method that can give better clustering of pixels through automatically determining the best number of clusters. Clusters' statistics and ratio results of pixels in the segmented images show the applicability of the proposed approach for giving the optimum number of clusters for prostate cancer analysis and diagnosis.","This project was funded by the National Plan for Science, Technology, and Innovation (MAARIFAH) and King Abdulaziz City for Science and Technology, Kingdom of Saudi Arabia, Award number 10-Bio-1905.",,Computational Intelligence and Neuroscience,,Algorithms, Cluster Analysis, Elbow, Humans, Male, Prostatic Neoplasms,2021-11-15,2021,2021-11-15,2021-11-15,2021,,4553832,All OA, Gold,Article,"Sammouda, Rachid; El-Zaart, Ali","Sammouda, Rachid (Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia, ksu.edu.sa); El-Zaart, Ali (Department of Mathematics and Computer Science, Faculty of Sciences, Beirut Arab University, Beirut, Lebanon, bau.edu.lb)","Sammouda, Rachid (King Saud University); El-Zaart, Ali (Beirut Arab University)","Sammouda, Rachid (King Saud University); El-Zaart, Ali (Beirut Arab University)",16,16,2.24,9.7,https://downloads.hindawi.com/journals/cin/2021/4553832.pdf,https://app.dimensions.ai/details/publication/pub.1142715307,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,
6767,pub.1139509130,10.1007/s10916-021-01751-6,34232409,,Versatile Convolutional Networks Applied to Computed Tomography and Magnetic Resonance Image Segmentation,"Medical image segmentation has seen positive developments in recent years but remains challenging with many practical obstacles to overcome. The applications of this task are wide-ranging in many fields of medicine, and used in several imaging modalities which usually require tailored solutions. Deep learning models have gained much attention and have been lately recognized as the most successful for automated segmentation. In this work we show the versatility of this technique by means of a single deep learning architecture capable of successfully performing segmentation on two very different types of imaging: computed tomography and magnetic resonance. The developed model is fully convolutional with an encoder-decoder structure and high-resolution pathways which can process whole three-dimensional volumes at once, and learn directly from the data to find which voxels belong to the regions of interest and localize those against the background. The model was applied to two publicly available datasets achieving equivalent results for both imaging modalities, as well as performing segmentation of different organs in different anatomic regions with comparable success.",,,Journal of Medical Systems,,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Magnetic Resonance Spectroscopy; Neural Networks, Computer; Tomography, X-Ray Computed",2021-07-07,2021,2021-07-07,2021-08,45,8,79,All OA, Green,Article,"Almeida, Gonçalo; Tavares, João Manuel R. S.","Almeida, Gonçalo (Instituto de Ciência e Inovação em Engenharia Mecânica e Engenharia Industrial, Faculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias, s/n, 4200-465, Porto, Portugal); Tavares, João Manuel R. S. (Instituto de Ciência e Inovação em Engenharia Mecânica e Engenharia Industrial, Departamento de Engenharia Mecânica, Faculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias, s/n, 4200-465, Porto, Portugal)","Tavares, João Manuel R. S. (University of Porto)","Almeida, Gonçalo (University of Porto); Tavares, João Manuel R. S. (University of Porto)",2,2,,1.75,https://repositorio-aberto.up.pt/bitstream/10216/134300/2/478860.pdf,https://app.dimensions.ai/details/publication/pub.1139509130,42 Health Sciences, 4203 Health Services and Systems,,,,,,,,,,
6767,pub.1137351668,10.1097/ju.0000000000001783,33878887,PMC8352566,Deep Learning Improves Speed and Accuracy of Prostate Gland Segmentations on Magnetic Resonance Imaging for Targeted Biopsy,"PURPOSE: Targeted biopsy improves prostate cancer diagnosis. Accurate prostate segmentation on magnetic resonance imaging (MRI) is critical for accurate biopsy. Manual gland segmentation is tedious and time-consuming. We sought to develop a deep learning model to rapidly and accurately segment the prostate on MRI and to implement it as part of routine magnetic resonance-ultrasound fusion biopsy in the clinic.
MATERIALS AND METHODS: A total of 905 subjects underwent multiparametric MRI at 29 institutions, followed by magnetic resonance-ultrasound fusion biopsy at 1 institution. A urologic oncology expert segmented the prostate on axial T2-weighted MRI scans. We trained a deep learning model, ProGNet, on 805 cases. We retrospectively tested ProGNet on 100 independent internal and 56 external cases. We prospectively implemented ProGNet as part of the fusion biopsy procedure for 11 patients. We compared ProGNet performance to 2 deep learning networks (U-Net and holistically-nested edge detector) and radiology technicians. The Dice similarity coefficient (DSC) was used to measure overlap with expert segmentations. DSCs were compared using paired t-tests.
RESULTS: ProGNet (DSC=0.92) outperformed U-Net (DSC=0.85, p <0.0001), holistically-nested edge detector (DSC=0.80, p <0.0001), and radiology technicians (DSC=0.89, p <0.0001) in the retrospective internal test set. In the prospective cohort, ProGNet (DSC=0.93) outperformed radiology technicians (DSC=0.90, p <0.0001). ProGNet took just 35 seconds per case (vs 10 minutes for radiology technicians) to yield a clinically utilizable segmentation file.
CONCLUSIONS: This is the first study to employ a deep learning model for prostate gland segmentation for targeted biopsy in routine urological clinical practice, while reporting results and releasing the code online. Prospective and retrospective evaluations revealed increased speed and accuracy.","The authors thank Rajesh Venkataraman for help converting the segmentation files into a Digital Imaging and Communications in Medicine (DICOM) format that can be read by the ProFuse software (Eigen, Grass Valley, California). The authors also acknowledge the efforts of Rhea Liang and Chris LeCastillo of the 3D and Quantitative Imaging Laboratory at Stanford University.",,Investigative Urology,,"Datasets as Topic; Deep Learning; Feasibility Studies; Humans; Image Processing, Computer-Assisted; Image-Guided Biopsy; Magnetic Resonance Imaging, Interventional; Male; Multimodal Imaging; Multiparametric Magnetic Resonance Imaging; Proof of Concept Study; Prospective Studies; Prostate; Prostatic Neoplasms; Reproducibility of Results; Retrospective Studies; Software; Time Factors; Ultrasonography, Interventional",2021-04-21,2021,2021-04-21,2021-09,206,3,604-612,All OA, Hybrid,Article,"Soerensen, Simon John Christoph; Fan, Richard E.; Seetharaman, Arun; Chen, Leo; Shao, Wei; Bhattacharya, Indrani; Kim, Yong-hun; Sood, Rewa; Borre, Michael; Chung, Benjamin I.; To'o, Katherine J.; Rusu, Mirabela; Sonn, Geoffrey A.","Soerensen, Simon John Christoph (Department of Urology, Stanford University School of Medicine, Stanford, California; Department of Urology, Aarhus University Hospital, Aarhus, Denmark); Fan, Richard E. (Department of Urology, Stanford University School of Medicine, Stanford, California); Seetharaman, Arun (Department of Electrical Engineering, Stanford University, Stanford, California); Chen, Leo (Department of Urology, Stanford University School of Medicine, Stanford, California); Shao, Wei (Department of Radiology, Stanford University School of Medicine, Stanford, California); Bhattacharya, Indrani (Department of Radiology, Stanford University School of Medicine, Stanford, California); Kim, Yong-hun (Department of Computer Science, Stanford University, Stanford, California); Sood, Rewa (Department of Electrical Engineering, Stanford University, Stanford, California); Borre, Michael (Department of Urology, Aarhus University Hospital, Aarhus, Denmark); Chung, Benjamin I. (Department of Urology, Stanford University School of Medicine, Stanford, California); To'o, Katherine J. (Veterans Affairs, Palo Alto Health Care System, Palo Alto, California; Department of Radiology, Stanford University School of Medicine, Stanford, California); Rusu, Mirabela (Department of Radiology, Stanford University School of Medicine, Stanford, California); Sonn, Geoffrey A. (Department of Urology, Stanford University School of Medicine, Stanford, California; Department of Radiology, Stanford University School of Medicine, Stanford, California)","Sonn, Geoffrey A. (Stanford University; Stanford University)","Soerensen, Simon John Christoph (Stanford University; Aarhus University Hospital); Fan, Richard E. (Stanford University); Seetharaman, Arun (Stanford University); Chen, Leo (Stanford University); Shao, Wei (Stanford University); Bhattacharya, Indrani (Stanford University); Kim, Yong-hun (Stanford University); Sood, Rewa (Stanford University); Borre, Michael (Aarhus University Hospital); Chung, Benjamin I. (Stanford University); To'o, Katherine J. (VA Palo Alto Health Care System; Stanford University); Rusu, Mirabela (Stanford University); Sonn, Geoffrey A. (Stanford University; Stanford University)",10,10,1.56,7.24,https://doi.org/10.1097/ju.0000000000001783,https://app.dimensions.ai/details/publication/pub.1137351668,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
6595,pub.1029621027,10.1117/12.2082255,26848206,PMC4736748,A supervoxel-based segmentation method for prostate MR images,"Accurate segmentation of the prostate has many applications in prostate cancer diagnosis and therapy. In this paper, we propose a ""Supervoxel"" based method for prostate segmentation. The prostate segmentation problem is considered as assigning a label to each supervoxel. An energy function with data and smoothness terms is used to model the labeling process. The data term estimates the likelihood of a supervoxel belongs to the prostate according to a shape feature. The geometric relationship between two neighboring supervoxels is used to construct a smoothness term. A three-dimensional (3D) graph cut method is used to minimize the energy function in order to segment the prostate. A 3D level set is then used to get a smooth surface based on the output of the graph cut. The performance of the proposed segmentation algorithm was evaluated with respect to the manual segmentation ground truth. The experimental results on 12 prostate volumes showed that the proposed algorithm yields a mean Dice similarity coefficient of 86.9%±3.2%. The segmentation method can be used not only for the prostate but also for other organs.",,,Proceedings of SPIE,Medical Imaging 2015: Image Processing,,2015-03-20,2015,2015-03-20,2015-03-20,9413,,941318-941318-7,All OA, Green,Proceeding,"Tian, Zhiqiang; Liu, LiZhi; Fei, Baowei","Tian, Zhiqiang (Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA.); Liu, LiZhi (Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA.); Fei, Baowei (Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA; Department of Biomedical Engineering, Emory University and Georgia Institute of Technology.)",,"Tian, Zhiqiang (Emory University); Liu, LiZhi (Emory University); Fei, Baowei (Emory University)",11,2,0.26,3.64,https://europepmc.org/articles/pmc4736748?pdf=render,https://app.dimensions.ai/details/publication/pub.1029621027,"40 Engineering; 4006 Communications Engineering; 4009 Electronics, Sensors and Digital Hardware; 51 Physical Sciences; 5102 Atomic, Molecular and Optical Physics",,,,,,,,,,,
6535,pub.1148515064,10.1016/j.asmr.2022.04.020,36033193,PMC9402425,Automated 3D Analysis of Clinical Magnetic Resonance Images Demonstrates Significant Reductions in Cam Morphology Following Arthroscopic Intervention in Contrast to Physiotherapy,"Purpose: To obtain automated measurements of cam volume, surface area, and height from baseline (preintervention) and 12-month magnetic resonance (MR) images acquired from male and female patients allocated to physiotherapy (PT) or arthroscopic surgery (AS) management for femoroacetabular impingement (FAI) in the Australian FASHIoN trial.
Methods: An automated segmentation pipeline (CamMorph) was used to obtain cam morphology data from three-dimensional (3D) MR hip examinations in FAI patients classified with mild, moderate, or major cam volumes. Pairwise comparisons between baseline and 12-month cam volume, surface area, and height data were performed within the PT and AS patient groups using paired t-tests or Wilcoxon signed-rank tests.
Results: A total of 43 patients were included with 15 PT patients (9 males, 6 females) and 28 AS patients (18 males, 10 females) for premanagement and postmanagement cam morphology assessments. Within the PT male and female patient groups, there were no significant differences between baseline and 12-month mean cam volume (male: 1269 vs 1288 mm3, t[16] = -0.39; female: 545 vs 550 mm,3 t[10] = -0.78), surface area (male: 1525 vs 1491 mm2, t[16] = 0.92; female: 885 vs 925 mm,2 t[10] = -0.78), maximum height (male: 4.36 vs 4.32 mm, t[16] = 0.34; female: 3.05 vs 2.96 mm, t[10] = 1.05) and average height (male: 2.18 vs 2.18 mm, t[16] = 0.22; female: 1.4 vs 1.43 mm, t[10] = -0.38). In contrast, within the AS male and female patient groups, there were significant differences between baseline and 12-month cam volume (male: 1343 vs 718 mm3, W = 0.0; female: 499 vs 240 mm3, t[18] = 2.89), surface area (male: 1520 vs 1031 mm2, t(34) = 6.48; female: 782 vs 483 mm2, t(18) = 3.02), maximum-height (male: 4.3 vs 3.42 mm, W = 13.5; female: 2.85 vs 2.24 mm, t(18) = 3.04) and average height (male: 2.17 vs 1.52 mm, W = 3.0; female: 1.4 vs 0.94 mm, W = 3.0). In AS patients, 3D bone models provided good visualization of cam bone mass removal postostectomy.
Conclusions: Automated measurement of cam morphology from baseline (preintervention) and 12-month MR images demonstrated that the cam volume, surface area, maximum-height, and average height were significantly smaller in AS patients following ostectomy, whereas there were no significant differences in these cam measures in PT patients from the Australian FASHIoN study.
Level of Evidence: Level II, cohort study.",This work was supported by the University of Queensland and the Australian E-Health Research Centre under the Multi-Institutional Agreement for the National Health and Medical Research Council Development Grant APP1139868 entitled “MR Hip Intervention and Planning System to Enhance Clinical and Surgical Outcomes”.,"The authors report the following potential conflicts of interest or sources of funding: S.S.C. reports grants from the National Health and Medical Research Council, during the conduct of this study. S.C. reports grants from National Health and Medical Research Council, during the conduct of the study; and is the director of Magnetica Pty Ltd, outside the submitted work. D.J.H. reports personal fees from Pfizer, Lilly, TLCBio, Novartis, Tissuegene, and Biobone, outside the submitted work. J.F. reports grants from National Health and Medical Research Council, during the conduct of the study. C.E. reports grants from National Health and Medical Research Council, during the conduct of the study.",Arthroscopy Sports Medicine and Rehabilitation,,,2022-06-08,2022,2022-06-08,2022-08,4,4,e1353-e1362,All OA, Gold,Article,"Bugeja, Jessica M.; Xia, Ying; Chandra, Shekhar S.; Murphy, Nicholas J.; Eyles, Jillian; Spiers, Libby; Crozier, Stuart; Hunter, David J.; Fripp, Jurgen; Engstrom, Craig","Bugeja, Jessica M. (School of Information Technology and Electrical Engineering, The University of Queensland, Australia; Australian e-Health Research Centre, CSIRO Health and Biosecurity, Australia); Xia, Ying (Australian e-Health Research Centre, CSIRO Health and Biosecurity, Australia); Chandra, Shekhar S. (School of Information Technology and Electrical Engineering, The University of Queensland, Australia); Murphy, Nicholas J. (Kolling Institute of Medical Research, Institute of Bone and Joint Research, University of Sydney, Australia; Department of Orthopaedic Surgery, John Hunter Hospital, Newcastle, Australia); Eyles, Jillian (Kolling Institute of Medical Research, Institute of Bone and Joint Research, University of Sydney, Australia; Department of Rheumatology, Royal North Shore Hospital, St. Leonards, Australia); Spiers, Libby (Centre for Health, Exercise and Sports Medicine, Department of Physiotherapy, University of Melbourne, Melbourne, Australia); Crozier, Stuart (School of Information Technology and Electrical Engineering, The University of Queensland, Australia); Hunter, David J. (Kolling Institute of Medical Research, Institute of Bone and Joint Research, University of Sydney, Australia; Department of Rheumatology, Royal North Shore Hospital, St. Leonards, Australia); Fripp, Jurgen (Australian e-Health Research Centre, CSIRO Health and Biosecurity, Australia; Kolling Institute of Medical Research, Institute of Bone and Joint Research, University of Sydney, Australia; Department of Rheumatology, Royal North Shore Hospital, St. Leonards, Australia); Engstrom, Craig (School of Human Movement Studies, The University of Queensland, Australia)","Bugeja, Jessica M. (University of Queensland; )","Bugeja, Jessica M. (University of Queensland); Xia, Ying (); Chandra, Shekhar S. (University of Queensland); Murphy, Nicholas J. (The University of Sydney; John Hunter Hospital); Eyles, Jillian (The University of Sydney; Royal North Shore Hospital); Spiers, Libby (University of Melbourne); Crozier, Stuart (University of Queensland); Hunter, David J. (The University of Sydney; Royal North Shore Hospital); Fripp, Jurgen (The University of Sydney; Royal North Shore Hospital); Engstrom, Craig (University of Queensland)",1,1,,,http://arthroscopysportsmedicineandrehabilitation.org/article/S2666061X22000621/pdf,https://app.dimensions.ai/details/publication/pub.1148515064,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
6513,pub.1152946691,10.1002/mp.16102,36412164,,Dual attention guided multiscale neural network trained with curriculum learning for noninvasive prediction of Gleason Grade Group from MRI,"BACKGROUND: The Gleason Grade Group (GG) is essential in assessing the malignancy of prostate cancer (PCa) and is typically obtained by invasive biopsy procedures in which sampling errors could lead to inaccurately scored GGs. With the gradually recognized value of bi-parametric magnetic resonance imaging (bpMRI) in PCa, it is beneficial to noninvasively predict GGs from bpMRI for early diagnosis and treatment planning of PCa. However, it is challenging to establish the connection between bpMRI features and GGs.
PURPOSE: In this study, we propose a dual attention-guided multiscale neural network (DAMS-Net) to predict the 5-scored GG from bpMRI and design a training curriculum to further improve the prediction performance.
METHODS: The proposed DAMS-Net incorporates a feature pyramid network (FPN) to fully extract the multiscale features for lesions of varying sizes and a dual attention module to focus on lesion and surrounding regions while avoiding the influence of irrelevant ones. Furthermore, to enhance the differential ability for lesions with the inter-grade similarity and intra-grade variation in bpMRI, the training process employs a specially designed curriculum based on the differences between the radiological evaluations and the ground truth GGs.
RESULTS: Extensive experiments were conducted on a private dataset of 382 patients and the public PROSTATEx-2 dataset. For the private dataset, the experimental results showed that the proposed network performed better than the plain baseline model for GG prediction, achieving a mean quadratic weighted Kappa (Kw ) of 0.4902 and a mean positive predictive value of 0.9098 for predicting clinically significant cancer (PPVGG>1 ). With the application of curriculum learning, the mean Kw and PPVGG>1 further increased to 0.5144 and 0.9118, respectively. For the public dataset, the proposed method achieved state-of-the-art results of 0.5413 Kw and 0.9747 PPVGG>1 .
CONCLUSION: The proposed DAMS-Net trained with curriculum learning can effectively predict GGs from bpMRI, which may assist clinicians in early diagnosis and treatment planning for PCa patients.","This work was supported partly by Key Research and Development Program of Jiangsu (BE2022049‐2, BE2020625, BE2021612), Suzhou Municipal Health and Family Planning Commission&#x27;s Key Diseases Diagnosis and Treatment Program (LCZX202001), Science and Technology Development Project of Suzhou (SKY2022003, SKY2021031), Youth Innovation Promotion Association CAS (2021324), and Medical Research Project of Jiangsu Provincial Health and Family Planning Commission (M2020068).",,Medical Physics,,,2022-11-22,2022,2022-12-11,2022-11-22,,,,Closed,Article,"Hu, Jisu; Shen, Ao; Qiao, Xiaomeng; Zhou, Zhiyong; Qian, Xusheng; Zheng, Yi; Bao, Jie; Wang, Ximing; Dai, Yakang","Hu, Jisu (School of Biomedical Engineering (Suzhou), Division of Life Sciences and Medicine, University of Science and Technology of China, Suzhou, Jiangsu, China; Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, Jiangsu, China); Shen, Ao (Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, Jiangsu, China); Qiao, Xiaomeng (Department of Radiology, The First Affiliated Hospital of Soochow University, Suzhou, Jiangsu, China); Zhou, Zhiyong (Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, Jiangsu, China); Qian, Xusheng (School of Biomedical Engineering (Suzhou), Division of Life Sciences and Medicine, University of Science and Technology of China, Suzhou, Jiangsu, China; Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, Jiangsu, China); Zheng, Yi (Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, Jiangsu, China); Bao, Jie (Department of Radiology, The First Affiliated Hospital of Soochow University, Suzhou, Jiangsu, China); Wang, Ximing (Department of Radiology, The First Affiliated Hospital of Soochow University, Suzhou, Jiangsu, China); Dai, Yakang (Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, Jiangsu, China)","Wang, Ximing (First Affiliated Hospital of Soochow University); Dai, Yakang (Suzhou Institute of Biomedical Engineering and Technology)","Hu, Jisu (University of Science and Technology of China; Suzhou Institute of Biomedical Engineering and Technology); Shen, Ao (Suzhou Institute of Biomedical Engineering and Technology); Qiao, Xiaomeng (First Affiliated Hospital of Soochow University); Zhou, Zhiyong (Suzhou Institute of Biomedical Engineering and Technology); Qian, Xusheng (University of Science and Technology of China; Suzhou Institute of Biomedical Engineering and Technology); Zheng, Yi (Suzhou Institute of Biomedical Engineering and Technology); Bao, Jie (First Affiliated Hospital of Soochow University); Wang, Ximing (First Affiliated Hospital of Soochow University); Dai, Yakang (Suzhou Institute of Biomedical Engineering and Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152946691,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
6510,pub.1137624812,10.1148/rycan.2021200024,33929265,PMC8189171,Segmentation of the Prostate Transition Zone and Peripheral Zone on MR Images with Deep Learning,"Purpose To develop a deep learning model to delineate the transition zone (TZ) and peripheral zone (PZ) of the prostate on MR images. Materials and Methods This retrospective study was composed of patients who underwent a multiparametric prostate MRI and an MRI/transrectal US fusion biopsy between January 2013 and May 2016. A board-certified abdominal radiologist manually segmented the prostate, TZ, and PZ on the entire data set. Included accessions were split into 60% training, 20% validation, and 20% test data sets for model development. Three convolutional neural networks with a U-Net architecture were trained for automatic recognition of the prostate organ, TZ, and PZ. Model performance for segmentation was assessed using Dice scores and Pearson correlation coefficients. Results A total of 242 patients were included (242 MR images; 6292 total images). Models for prostate organ segmentation, TZ segmentation, and PZ segmentation were trained and validated. Using the test data set, for prostate organ segmentation, the mean Dice score was 0.940 (interquartile range, 0.930-0.961), and the Pearson correlation coefficient for volume was 0.981 (95% CI: 0.966, 0.989). For TZ segmentation, the mean Dice score was 0.910 (interquartile range, 0.894-0.938), and the Pearson correlation coefficient for volume was 0.992 (95% CI: 0.985, 0.995). For PZ segmentation, the mean Dice score was 0.774 (interquartile range, 0.727-0.832), and the Pearson correlation coefficient for volume was 0.927 (95% CI: 0.870, 0.957). Conclusion Deep learning with an architecture composed of three U-Nets can accurately segment the prostate, TZ, and PZ. Keywords: MRI, Genital/Reproductive, Prostate, Neural Networks Supplemental material is available for this article. © RSNA, 2021.",Authors declared no funding for this work. convolutional neural network Prostate Imaging Reporting and Data System peripheral zone transrectal US transition zone,,Radiology Imaging Cancer,,Deep Learning, Humans, Magnetic Resonance Imaging, Male, Prostatic Neoplasms, Retrospective Studies,2021-05-01,2021,,2021-05-01,3,3,e200024,All OA, Gold,Article,"Bardis, Michelle; Houshyar, Roozbeh; Chantaduly, Chanon; Tran-Harding, Karen; Ushinsky, Alexander; Chahine, Chantal; Rupasinghe, Mark; Chow, Daniel; Chang, Peter","Bardis, Michelle (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).); Houshyar, Roozbeh (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).); Chantaduly, Chanon (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).); Tran-Harding, Karen (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).); Ushinsky, Alexander (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).); Chahine, Chantal (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).); Rupasinghe, Mark (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).); Chow, Daniel (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).); Chang, Peter (From the Department of Radiological Sciences, University of California, Irvine, 101 The City Drive South, Building 55, Suite 201, Orange, CA 92868 (M.B., R.H., K.T.H., C. Chahine, M.R.); Center for Artificial Intelligence in Diagnostic Medicine, University of California, Irvine, Irvine, Calif (C. Chantaduly, D.C., P.C.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (A.U.).)",,"Bardis, Michelle (University of California, Irvine); Houshyar, Roozbeh (University of California, Irvine); Chantaduly, Chanon (University of California, Irvine); Tran-Harding, Karen (University of California, Irvine); Ushinsky, Alexander (University of California, Irvine); Chahine, Chantal (University of California, Irvine); Rupasinghe, Mark (University of California, Irvine); Chow, Daniel (University of California, Irvine); Chang, Peter (University of California, Irvine)",17,17,3.9,12.3,https://doi.org/10.1148/rycan.2021200024,https://app.dimensions.ai/details/publication/pub.1137624812,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,
6510,pub.1131096223,10.3389/fonc.2020.01762,33102206,PMC7546883,Analysis of Geometric Performance and Dosimetric Impact of Using Automatic Contour Segmentation for Radiotherapy Planning,"Purpose: To analyze geometric discrepancy and dosimetric impact in using contours generated by auto-segmentation (AS) against manually segmented (MS) clinical contours. Methods: A 48-subject prostate atlas was created and another 15 patients were used for testing. Contours were generated using a commercial atlas-based segmentation tool and compared to their clinical MS counterparts. The geometric correlation was evaluated using the Dice similarity coefficient (DSC) and Hausdorff distance (HD). Dosimetric relevance was evaluated for a subset of patients by assessing the DVH differences derived by optimizing plan dose using the AS and MS contours, respectively, and evaluating with respect to each. A paired t-test was employed for statistical comparison. The discrepancy in plan quality with respect to clinical dosimetric endpoints was evaluated. The analysis was repeated for head/neck (HN) with a 31-subject atlas and 15 test cases. Results: Dice agreement between AS and MS differed significantly across structures: from (L:0.92/R: 0.91) for the femoral heads to seminal vesical of 0.38 in the prostate cohort, and from 0.98 for the brain, to 0.36 for the chiasm of the HN group. Despite the geometric disagreement, the paired t-tests showed the lack of statistical evidence for systematic differences in dosimetric plan quality yielded by the AS and MS approach for the prostate cohort. In HN cases, statistically significant differences in dosimetric endpoints were observed in structures with small volumes or elongated shapes such as cord (p = 0.01) and esophagus (p = 0.04). The largest absolute dose difference of 11 Gy was seen in the mean pharynx dose. Conclusion: Varying AS performance among structures suggests a differential approach of using AS on a subset of structures and focus MS on the rest. The discrepancy between geometric and dosimetric-end-point driven evaluation also indicates the clinical utility of AS contours in optimization and evaluating plan quality despite of suboptimal geometrical accuracy.",,,Frontiers in Oncology,,,2020-09-23,2020,2020-09-23,,10,,1762,All OA, Gold,Article,"Cao, Minsong; Stiehl, Bradley; Yu, Victoria Y.; Sheng, Ke; Kishan, Amar U.; Chin, Robert K.; Yang, Yingli; Ruan, Dan","Cao, Minsong (Department of Radiation Oncology, David Geffen School of Medicine, University of California, Los Angeles, Los Angeles, CA, United States); Stiehl, Bradley (Physics & Biology in Medicine Graduate Program, University of California, Los Angeles, Los Angeles, CA, United States); Yu, Victoria Y. (Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, United States); Sheng, Ke (Department of Radiation Oncology, David Geffen School of Medicine, University of California, Los Angeles, Los Angeles, CA, United States); Kishan, Amar U. (Department of Radiation Oncology, David Geffen School of Medicine, University of California, Los Angeles, Los Angeles, CA, United States); Chin, Robert K. (Department of Radiation Oncology, David Geffen School of Medicine, University of California, Los Angeles, Los Angeles, CA, United States); Yang, Yingli (Department of Radiation Oncology, David Geffen School of Medicine, University of California, Los Angeles, Los Angeles, CA, United States); Ruan, Dan (Department of Radiation Oncology, David Geffen School of Medicine, University of California, Los Angeles, Los Angeles, CA, United States)","Cao, Minsong (University of California, Los Angeles)","Cao, Minsong (University of California, Los Angeles); Stiehl, Bradley (University of California, Los Angeles); Yu, Victoria Y. (Memorial Sloan Kettering Cancer Center); Sheng, Ke (University of California, Los Angeles); Kishan, Amar U. (University of California, Los Angeles); Chin, Robert K. (University of California, Los Angeles); Yang, Yingli (University of California, Los Angeles); Ruan, Dan (University of California, Los Angeles)",9,9,1.03,3.83,https://www.frontiersin.org/articles/10.3389/fonc.2020.01762/pdf,https://app.dimensions.ai/details/publication/pub.1131096223,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
6477,pub.1152702856,10.1007/s00330-022-09239-8,36371606,,Deep learning algorithm performs similarly to radiologists in the assessment of prostate volume on MRI,"ObjectivesProstate volume (PV) in combination with prostate specific antigen (PSA) yields PSA density which is an increasingly important biomarker. Calculating PV from MRI is a time-consuming, radiologist-dependent task. The aim of this study was to assess whether a deep learning algorithm can replace PI-RADS 2.1 based ellipsoid formula (EF) for calculating PV.MethodsEight different measures of PV were retrospectively collected for each of 124 patients who underwent radical prostatectomy and preoperative MRI of the prostate (multicenter and multi-scanner MRI’s 1.5 and 3 T). Agreement between volumes obtained from the deep learning algorithm (PVDL) and ellipsoid formula by two radiologists (PVEF1 and PVEF2) was evaluated against the reference standard PV obtained by manual planimetry by an expert radiologist (PVMPE). A sensitivity analysis was performed using a prostatectomy specimen as the reference standard. Inter-reader agreement was evaluated between the radiologists using the ellipsoid formula and between the expert and inexperienced radiologists performing manual planimetry.ResultsPVDL showed better agreement and precision than PVEF1 and PVEF2 using the reference standard PVMPE (mean difference [95% limits of agreement] PVDL: −0.33 [−10.80; 10.14], PVEF1: −3.83 [−19.55; 11.89], PVEF2: −3.05 [−18.55; 12.45]) or the PV determined based on specimen weight (PVDL: −4.22 [−22.52; 14.07], PVEF1: −7.89 [−30.50; 14.73], PVEF2: −6.97 [−30.13; 16.18]). Inter-reader agreement was excellent between the two experienced radiologists using the ellipsoid formula and was good between expert and inexperienced radiologists performing manual planimetry.ConclusionDeep learning algorithm performs similarly to radiologists in the assessment of prostate volume on MRI.Key Points• A commercially available deep learning algorithm performs similarly to radiologists in the assessment of prostate volume on MRI.• The deep-learning algorithm was previously untrained on this heterogenous multicenter day-to-day practice MRI data set.",We gratefully thank secretary Kajsa Trens at the department of translational medicine for helping out during data collection and statistician Andrea Dahl Sturedahl at Forum Söder for statistical support.,"Open access funding provided by Lund University. This study has received funding by grants from Governmental funding for clinical research (ALF), grants for PhD students from Region Skåne and scholarship from Stig and Ragna Gorthon Foundation.",European Radiology,,,2022-11-12,2022,2022-11-12,2022-11-12,,,1-10,All OA, Hybrid,Article,"Thimansson, Erik; Bengtsson, J.; Baubeta, E.; Engman, J.; Flondell-Sité, D.; Bjartell, A.; Zackrisson, S.","Thimansson, Erik (Department of Translational Medicine, Diagnostic Radiology, Lund University, Carl-Bertil Laurells gata 9, SE-205 02, Malmö, Sweden; Department of Radiology, Helsingborg Hospital, Helsingborg, Sweden); Bengtsson, J. (Department of Clinical Sciences, Diagnostic Radiology, Lund University, Lund, Sweden; Department of Imaging and Functional Medicine, Skåne University Hospital, Malmö, Sweden; Department of Imaging and Functional Medicine, Skåne University Hospital, Lund, Sweden); Baubeta, E. (Department of Translational Medicine, Diagnostic Radiology, Lund University, Carl-Bertil Laurells gata 9, SE-205 02, Malmö, Sweden; Department of Imaging and Functional Medicine, Skåne University Hospital, Malmö, Sweden; Department of Imaging and Functional Medicine, Skåne University Hospital, Lund, Sweden); Engman, J. (Department of Translational Medicine, Diagnostic Radiology, Lund University, Carl-Bertil Laurells gata 9, SE-205 02, Malmö, Sweden; Department of Imaging and Functional Medicine, Skåne University Hospital, Malmö, Sweden; Department of Imaging and Functional Medicine, Skåne University Hospital, Lund, Sweden); Flondell-Sité, D. (Department of Translational Medicine, Urological Cancers, Lund University, Malmö, Sweden; Department of Urology, Skåne University Hospital, Malmö, Sweden); Bjartell, A. (Department of Translational Medicine, Urological Cancers, Lund University, Malmö, Sweden; Department of Urology, Skåne University Hospital, Malmö, Sweden); Zackrisson, S. (Department of Translational Medicine, Diagnostic Radiology, Lund University, Carl-Bertil Laurells gata 9, SE-205 02, Malmö, Sweden; Department of Imaging and Functional Medicine, Skåne University Hospital, Malmö, Sweden; Department of Imaging and Functional Medicine, Skåne University Hospital, Lund, Sweden)","Thimansson, Erik (Lund University; Helsingborgs lasarett)","Thimansson, Erik (Lund University; Helsingborgs lasarett); Bengtsson, J. (Lund University; Skåne University Hospital; Skåne University Hospital); Baubeta, E. (Lund University; Skåne University Hospital; Skåne University Hospital); Engman, J. (Lund University; Skåne University Hospital; Skåne University Hospital); Flondell-Sité, D. (Lund University; Skåne University Hospital); Bjartell, A. (Lund University; Skåne University Hospital); Zackrisson, S. (Lund University; Skåne University Hospital; Skåne University Hospital)",0,0,,,https://link.springer.com/content/pdf/10.1007/s00330-022-09239-8.pdf,https://app.dimensions.ai/details/publication/pub.1152702856,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
6376,pub.1038104791,10.1007/s10278-016-9890-0,27363993,PMC5114230,Gland and Zonal Segmentation of Prostate on T2W MR Images,"For many years, prostate segmentation on MR images concerned only the extraction of the entire gland. Currently, in the focal treatment era, there is a continuously increasing need for the separation of the different parts of the organ. In this paper, we propose an automatic segmentation method based on the use of T2W images and atlas images to segment the prostate and to isolate the peripheral and transition zones. The algorithm consists of two stages. First, the target image is registered with each zonal atlas image then the segmentation is obtained by the application of an evidential C-Means clustering. The method was evaluated on a representative and multi-centric image base and yielded mean Dice accuracy values of 0.81, 0.70, and 0.62 for the prostate, the transition zone, and peripheral zone, respectively.",,,Journal of Digital Imaging,,"Algorithms; Databases, Factual; Humans; Magnetic Resonance Imaging; Male; Prostate; Prostatic Neoplasms",2016-06-30,2016,2016-06-30,2016-12,29,6,730-736,All OA, Green,Article,"Chilali, O.; Puech, P.; Lakroum, S.; Diaf, M.; Mordon, S.; Betrouni, N.","Chilali, O. (INSERM, U1189 – ONCO-THAI – Image Assisted Laser Therapy for Oncology, University of Lille, 59000, Lille, France; Automatic Department, Mouloud Mammeri University, Tizi-Ouzou, Algeria); Puech, P. (INSERM, U1189 – ONCO-THAI – Image Assisted Laser Therapy for Oncology, University of Lille, 59000, Lille, France; CHRU Lille, Radiology Department, Claude Huriez Hospital, 59000, Lille, France); Lakroum, S. (INSERM, U1189 – ONCO-THAI – Image Assisted Laser Therapy for Oncology, University of Lille, 59000, Lille, France); Diaf, M. (Automatic Department, Mouloud Mammeri University, Tizi-Ouzou, Algeria); Mordon, S. (INSERM, U1189 – ONCO-THAI – Image Assisted Laser Therapy for Oncology, University of Lille, 59000, Lille, France); Betrouni, N. (INSERM, U1189 – ONCO-THAI – Image Assisted Laser Therapy for Oncology, University of Lille, 59000, Lille, France)","Betrouni, N. (University of Lille)","Chilali, O. (University of Lille; Mouloud Mammeri University of Tizi-Ouzou); Puech, P. (University of Lille; Hôpital Claude Huriez); Lakroum, S. (University of Lille); Diaf, M. (Mouloud Mammeri University of Tizi-Ouzou); Mordon, S. (University of Lille); Betrouni, N. (University of Lille)",25,14,1.1,6.75,https://europepmc.org/articles/pmc5114230?pdf=render,https://app.dimensions.ai/details/publication/pub.1038104791,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
6345,pub.1117944543,10.1109/isbi.2019.8759295,32874427,PMC7457546,Generalizable Multi-Site Training and Testing Of Deep Neural Networks Using Image Normalization,"The ability of medical image analysis deep learning algorithms to generalize across multiple sites is critical for clinical adoption of these methods. Medical imging data, especially MRI, can have highly variable intensity characteristics across different individuals, scanners, and sites. However, it is not practical to train algorithms with data from all imaging equipment sources at all possible sites. Intensity normalization methods offer a potential solution for working with multi-site data. We evaluate five different image normalization methods on training a deep neural network to segment the prostate gland in MRI. Using 600 MRI prostate gland segmentations from two different sites, our results show that both intra-site and inter-site evaluation is critical for assessing the robustness of trained models and that training with single-site data produces models that fail to fully generalize across testing data from sites not included in the training.",This work was supported by National Institute of Health (NIH) National Cancer Institute (NCI) R41 CA224888,,2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro,2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),,2019-04,2019,2019-07-11,2019-04,0,,348-351,All OA, Green,Proceeding,"Onofrey, John A; Casetti-Dinescu, Dana I; Lauritzen, Andreas D; Sarkar, Saradwata; Venkataraman, Rajesh; Fan, Richard E; Sonn, Geoffrey A; Sprenkle, Preston C; Staib, Lawrence H; Papademetris, Xenophon","Onofrey, John A (Department of Radiology & Biomedical Imaging, Yale University, New Haven, CT, USA.); Casetti-Dinescu, Dana I (Department of Radiology & Biomedical Imaging, Yale University, New Haven, CT, USA.); Lauritzen, Andreas D (Department of Radiology & Biomedical Imaging, Yale University, New Haven, CT, USA.); Sarkar, Saradwata (Eigen, Grass Valley, CA, USA.); Venkataraman, Rajesh (Eigen, Grass Valley, CA, USA.); Fan, Richard E (Department of Urology, Stanford University, Palo Alto, CA, USA.); Sonn, Geoffrey A (Department of Urology, Stanford University, Palo Alto, CA, USA.); Sprenkle, Preston C (Department of Urology, Yale University, New Haven, CT, USA.); Staib, Lawrence H (Department of Radiology & Biomedical Imaging, Yale University, New Haven, CT, USA.; Department of Biomedical Engineering, Yale University, New Haven, CT, USA.; Department of Electrical Engineering, Yale University, New Haven, CT, USA.); Papademetris, Xenophon (Department of Radiology & Biomedical Imaging, Yale University, New Haven, CT, USA.; Department of Biomedical Engineering, Yale University, New Haven, CT, USA.)","Onofrey, John A (Yale University); Casetti-Dinescu, Dana I (Yale University); Lauritzen, Andreas D (Yale University); Sarkar, Saradwata ; Venkataraman, Rajesh ; Fan, Richard E (Stanford University); Sonn, Geoffrey A (Stanford University); Sprenkle, Preston C (Yale University); Staib, Lawrence H (Yale University; Yale University; Yale University); Papademetris, Xenophon (Yale University; Yale University)","Onofrey, John A (Yale University); Casetti-Dinescu, Dana I (Yale University); Lauritzen, Andreas D (Yale University); Sarkar, Saradwata (); Venkataraman, Rajesh (); Fan, Richard E (Stanford University); Sonn, Geoffrey A (Stanford University); Sprenkle, Preston C (Yale University); Staib, Lawrence H (Yale University; Yale University; Yale University); Papademetris, Xenophon (Yale University; Yale University)",32,27,1.78,12.26,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7457546,https://app.dimensions.ai/details/publication/pub.1117944543,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
6310,pub.1143898801,10.1016/j.radonc.2021.12.011,34921895,,"Computer-aided segmentation on MRI for prostate radiotherapy, Part I: Quantifying human interobserver variability of the prostate and organs at risk and its impact on radiation dosimetry","BACKGROUND AND PURPOSE: Quantifying the interobserver variability (IoV) of prostate and periprostatic anatomy delineation on prostate MRI is necessary to inform its use for treatment planning, treatment delivery, and treatment quality assessment.
MATERIALS AND METHODS: Twenty five prostate cancer patients underwent MRI-based low-dose-rate prostate brachytherapy (LDRPBT). The patients were scanned with a 3D T2-weighted sequence for treatment planning and a 3D T2/T1-weighted sequence for quality assessment. Seven observers involved with the LDRPBT workflow delineated the prostate, external urinary sphincter (EUS), seminal vesicles, rectum, and bladder on all 50 MRIs. IoV was assessed by measuring contour similarity metrics, differences in organ volumes, and differences in dosimetry parameters between unique observer pairs. Measurements from a group of 3 radiation oncologists (G1) were compared against those from a group consisting of the other 4 clinical observers (G2).
RESULTS: IoV of the prostate was lower for G1 than G2 (Matthew's correlation coefficient [MCC], G1 vs. G2: planning-0.906 vs. 0.870, p < 0.001; postimplant-0.899 vs. 0.861, p < 0.001). IoV of the EUS was highest of all the organs for both groups, but was lower for G1 (MCC, G1 vs. G2: planning-0.659 vs. 0.402, p < 0.001; postimplant-0.684 vs. 0.398, p < 0.001). Large differences in prostate dosimetry parameters were observed (G1 maximum absolute prostate ΔD90: planning-76.223 Gy, postimplant-36.545 Gy; G1 maximum absolute prostate ΔV100: planning-13.927%, postimplant-8.860%).
CONCLUSIONS: While MRI is optimal in the management of prostate cancer with radiation therapy, significant interobserver variability of the prostate and external urinary sphincter still exist.","The authors thank Tamara Locke, Scientific Editor, Research Medical Library, The University of Texas MD Anderson Cancer Center for help editing this article.",,Radiotherapy and Oncology,,"Brachytherapy; Computers; Humans; Magnetic Resonance Imaging; Male; Observer Variation; Organs at Risk; Prostate; Prostatic Neoplasms; Radiometry; Radiotherapy Dosage; Radiotherapy Planning, Computer-Assisted",2021-12-16,2021,2021-12-16,2022-04,169,,124-131,Closed,Article,"Sanders, Jeremiah W; Mok, Henry; Hanania, Alexander N; Venkatesan, Aradhana M; Tang, Chad; Bruno, Teresa L; Thames, Howard D; Kudchadker, Rajat J; Frank, Steven J","Sanders, Jeremiah W (Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, USA. Electronic address: jsanders1@mdanderson.org.); Mok, Henry (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, USA.); Hanania, Alexander N (Department of Radiation Oncology, Baylor College of Medicine, Houston, USA.); Venkatesan, Aradhana M (Department of Diagnostic Radiology, The University of Texas MD Anderson Cancer Center, Houston, USA.); Tang, Chad (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, USA.); Bruno, Teresa L (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, USA.); Thames, Howard D (Department of Biostatistics, The University of Texas MD Anderson Cancer Center, Houston, USA. Electronic address: hthames@gmail.com.); Kudchadker, Rajat J (Department of Radiation Physics, The University of Texas MD Anderson Cancer Center, Houston, USA.); Frank, Steven J (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, USA.)","Sanders, Jeremiah W (The University of Texas MD Anderson Cancer Center)","Sanders, Jeremiah W (The University of Texas MD Anderson Cancer Center); Mok, Henry (The University of Texas MD Anderson Cancer Center); Hanania, Alexander N (Baylor College of Medicine); Venkatesan, Aradhana M (The University of Texas MD Anderson Cancer Center); Tang, Chad (The University of Texas MD Anderson Cancer Center); Bruno, Teresa L (The University of Texas MD Anderson Cancer Center); Thames, Howard D (The University of Texas MD Anderson Cancer Center); Kudchadker, Rajat J (The University of Texas MD Anderson Cancer Center); Frank, Steven J (The University of Texas MD Anderson Cancer Center)",2,2,,1.71,,https://app.dimensions.ai/details/publication/pub.1143898801,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
6254,pub.1144316285,10.1016/j.radonc.2021.12.033,34979213,,"Computer-aided segmentation on MRI for prostate radiotherapy, part II: Comparing human and computer observer populations and the influence of annotator variability on algorithm variability","BACKGROUND AND PURPOSE: Comparing deep learning (DL) algorithms to human interobserver variability, one of the largest sources of noise in human-performed annotations, is necessary to inform the clinical application, use, and quality assurance of DL for prostate radiotherapy.
MATERIALS AND METHODS: One hundred fourteen DL algorithms were developed on 295 prostate MRIs to segment the prostate, external urinary sphincter (EUS), seminal vesicles (SV), rectum, and bladder. Fifty prostate MRIs of 25 patients undergoing MRI-based low-dose-rate prostate brachytherapy were acquired as an independent test set. Groups of DL algorithms were created based on the loss functions used to train them, and the spatial entropy (SE) of their predictions on the 50 test MRIs was computed. Five human observers contoured the 50 test MRIs, and SE maps of their contours were compared with those of the groups of the DL algorithms. Additionally, similarity metrics were computed between DL algorithm predictions and consensus annotations of the 5 human observers' contours of the 50 test MRIs.
RESULTS: A DL algorithm yielded statistically significantly higher similarity metrics for the prostate than did the human observers (H) (prostate Matthew's correlation coefficient, DL vs. H: planning-0.931 vs. 0.903, p < 0.001; postimplant-0.925 vs. 0.892, p < 0.001); the same was true for the 4 organs at risk. The SE maps revealed that the DL algorithms and human annotators were most variable in similar anatomical regions: the prostate-EUS, prostate-SV, prostate-rectum, and prostate-bladder junctions.
CONCLUSIONS: Annotation quality is an important consideration when developing, evaluating, and using DL algorithms clinically.","The authors thank Bryan Tutt, Scientific Editor, Research Medical Library, The University of Texas MD Anderson Cancer Center for help editing this article.",,Radiotherapy and Oncology,,"Algorithms; Computers; Humans; Magnetic Resonance Imaging; Male; Observer Variation; Prostate; Prostatic Neoplasms; Radiotherapy Planning, Computer-Assisted",2021-12-31,2021,2021-12-31,2022-04,169,,132-139,Closed,Article,"Sanders, Jeremiah W; Mok, Henry; Hanania, Alexander N; Venkatesan, Aradhana M; Tang, Chad; Bruno, Teresa L; Thames, Howard D; Kudchadker, Rajat J; Frank, Steven J","Sanders, Jeremiah W (Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, United States. Electronic address: jsanders1@mdanderson.org.); Mok, Henry (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, United States.); Hanania, Alexander N (Department of Radiation Oncology, Baylor College of Medicine, Houston, United States.); Venkatesan, Aradhana M (Department of Diagnostic Radiology, The University of Texas MD Anderson Cancer Center, Houston, United States.); Tang, Chad (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, United States.); Bruno, Teresa L (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, United States.); Thames, Howard D (Department of Biostatistics, The University of Texas MD Anderson Cancer Center, Houston, United States.); Kudchadker, Rajat J (Department of Radiation Physics, The University of Texas MD Anderson Cancer Center, Houston, United States.); Frank, Steven J (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, United States.)","Sanders, Jeremiah W (The University of Texas MD Anderson Cancer Center)","Sanders, Jeremiah W (The University of Texas MD Anderson Cancer Center); Mok, Henry (The University of Texas MD Anderson Cancer Center); Hanania, Alexander N (Baylor College of Medicine); Venkatesan, Aradhana M (The University of Texas MD Anderson Cancer Center); Tang, Chad (The University of Texas MD Anderson Cancer Center); Bruno, Teresa L (The University of Texas MD Anderson Cancer Center); Thames, Howard D (The University of Texas MD Anderson Cancer Center); Kudchadker, Rajat J (The University of Texas MD Anderson Cancer Center); Frank, Steven J (The University of Texas MD Anderson Cancer Center)",3,3,,2.57,,https://app.dimensions.ai/details/publication/pub.1144316285,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
6254,pub.1144194606,10.1016/j.ijrobp.2021.12.153,34963559,PMC8923952,Voxelwise Prediction of Recurrent High-Grade Glioma via Proximity Estimation–Coupled Multidimensional Support Vector Machine,"PURPOSE: To provide early and localized glioblastoma (GBM) recurrence prediction, we introduce a novel postsurgery multiparametric magnetic resonance-based support vector machine (SVM) method coupling with stem cell niche (SCN) proximity estimation.
METHODS AND MATERIALS: This study used postsurgery magnetic resonance imaging (MRI) scans from 50 patients with recurrent GBM, obtained approximately 2 months before clinically diagnosed recurrence. The main prediction pipeline consisted of a proximity-based estimator to identify regions with high risk of recurrence (HRRs) and an SVM classifier to provide voxelwise prediction in HRRs. The HRRs were estimated using the weighted sum of inverse distances to 2 possible origins of recurrence-the SCN and the tumor cavity. Subsequently, multiparametric voxels (from T1, T1 contrast-enhanced, fluid-attenuated inversion recovery, T2, and apparent diffusion coefficient) within the HRR were grouped into recurrent (warped from the clinical diagnosis) and nonrecurrent subregions and fed into the proximity estimation-coupled SVM classifier (SVMPE). The cohort was randomly divided into 40% and 60% for training and testing, respectively. The trained SVMPE was then extrapolated to an earlier time point for earlier recurrence prediction. As an exploratory analysis, the SVMPE predictive cluster sizes and the image intensities from the 5 magnetic resonance sequences were compared across time to assess the progressive subclinical traces.
RESULTS: On 2-month prerecurrence MRI scans from 30 test cohort patients, the SVMPE classifier achieved a recall of 0.80, a precision of 0.69, an F1-score of 0.73, and a mean boundary distance of 7.49 mm. Exploratory analysis at early time points showed spatially consistent but significantly smaller subclinical clusters and significantly increased T1 contrast-enhanced and apparent diffusion coefficient values over time.
CONCLUSIONS: We demonstrated a novel voxelwise early prediction method, SVMPE, for GBM recurrence based on clinical follow-up MR scans. The SVMPE is promising in localizing subclinical traces of recurrence 2 months ahead of clinical diagnosis and may be used to guide more effective personalized early salvage therapy.",,This research has been supported by the 2020 American Association of Physicists Research Seed Funding Grant.,International Journal of Radiation Oncology • Biology • Physics,,"Brain Neoplasms; Glioblastoma; Glioma; Humans; Magnetic Resonance Imaging; Neoplasm Recurrence, Local; Retrospective Studies; Support Vector Machine",2021-12-26,2021,2021-12-26,2022-04,112,5,1279-1287,Closed,Article,"Lao, Yi; Ruan, Dan; Vassantachart, April; Fan, Zhaoyang; Ye, Jason C; Chang, Eric L; Chin, Robert; Kaprealian, Tania; Zada, Gabriel; Shiroishi, Mark S; Sheng, Ke; Yang, Wensha","Lao, Yi (Department of Radiation Oncology, University of California-Los Angeles.); Ruan, Dan (Department of Radiation Oncology, University of California-Los Angeles.); Vassantachart, April (Departments of Radiation Oncology.); Fan, Zhaoyang (Radiology.); Ye, Jason C (Departments of Radiation Oncology.); Chang, Eric L (Departments of Radiation Oncology.); Chin, Robert (Department of Radiation Oncology, University of California-Los Angeles.); Kaprealian, Tania (Department of Radiation Oncology, University of California-Los Angeles.); Zada, Gabriel (Neurosurgery, Keck School of Medicine of the University of Southern California, Los Angeles, Los Angeles, California.); Shiroishi, Mark S (Radiology.); Sheng, Ke (Department of Radiation Oncology, University of California-Los Angeles.); Yang, Wensha (Departments of Radiation Oncology. Electronic address: wensha.yang@med.usc.edu.)","Yang, Wensha ","Lao, Yi (University of California, Los Angeles); Ruan, Dan (University of California, Los Angeles); Vassantachart, April (); Fan, Zhaoyang (); Ye, Jason C (); Chang, Eric L (); Chin, Robert (University of California, Los Angeles); Kaprealian, Tania (University of California, Los Angeles); Zada, Gabriel (University of Southern California); Shiroishi, Mark S (); Sheng, Ke (University of California, Los Angeles); Yang, Wensha ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144194606,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,,
6230,pub.1004123133,10.1007/978-3-319-10404-1_99,25333192,,3D Prostate TRUS Segmentation Using Globally Optimized Volume-Preserving Prior,"An efficient and accurate segmentation of 3D transrectal ultrasound (TRUS) images plays an important role in the planning and treatment of the practical 3D TRUS guided prostate biopsy. However, a meaningful segmentation of 3D TRUS images tends to suffer from US speckles, shadowing and missing edges etc, which make it a challenging task to delineate the correct prostate boundaries. In this paper, we propose a novel convex optimization based approach to extracting the prostate surface from the given 3D TRUS image, while preserving a new global volume-size prior. We, especially, study the proposed combinatorial optimization problem by convex relaxation and introduce its dual continuous max-flow formulation with the new bounded flow conservation constraint, which results in an efficient numerical solver implemented on GPUs. Experimental results using 12 patient 3D TRUS images show that the proposed approach while preserving the volume-size prior yielded a mean DSC of 89.5%±2.4%, a MAD of 1.4±0.6 mm, a MAXD of 5.2±3.2 mm, and a VD of 7.5%±6.2% in ~1 minute, deomonstrating the advantages of both accuracy and efficiency. In addition, the low standard deviation of the segmentation accuracy shows a good reliability of the proposed approach.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014,"Adenocarcinoma; Algorithms; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Male; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Ultrasonography",2014,2014,,2014,17,Pt 1,796-803,All OA, Bronze,Chapter,"Qiu, Wu; Rajchl, Martin; Guo, Fumin; Sun, Yue; Ukwatta, Eranga; Fenster, Aaron; Yuan, Jing","Qiu, Wu (Robarts Research Institute, University of Western Ontario, London, ON, Canada); Rajchl, Martin (Robarts Research Institute, University of Western Ontario, London, ON, Canada); Guo, Fumin (Robarts Research Institute, University of Western Ontario, London, ON, Canada); Sun, Yue (Robarts Research Institute, University of Western Ontario, London, ON, Canada); Ukwatta, Eranga (Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD, United States); Fenster, Aaron (Robarts Research Institute, University of Western Ontario, London, ON, Canada); Yuan, Jing (Robarts Research Institute, University of Western Ontario, London, ON, Canada)",,"Qiu, Wu (Western University); Rajchl, Martin (Western University); Guo, Fumin (Western University); Sun, Yue (Western University); Ukwatta, Eranga (Johns Hopkins University); Fenster, Aaron (Western University); Yuan, Jing (Western University)",6,0,0.13,,https://link.springer.com/content/pdf/10.1007/978-3-319-10404-1_99.pdf,https://app.dimensions.ai/details/publication/pub.1004123133,46 Information and Computing Sciences,,,,,,,,,,,
6173,pub.1084497925,10.1109/embc.2016.7590782,28268408,,Automatic Prostate Segmentation on MR Images with Deep Network and Graph Model,"Automated prostate diagnoses and treatments have gained much attention due to the high mortality rate of prostate cancer. In particular, unsupervised (automatic) prostate segmentation is an active and challenging research. Most conventional works usually utilize handcrafted (low-level) features for prostate segmentation; however they often fail to extract the intrinsic structure of the prostate, especially on images with blurred boundaries. In this paper, we propose a novel automated prostate segmentation model with learned features from deep network. Specifically, we first generate a set of prostate proposals in transverse plane via recognizing the position and coarse estimate of the shape of the prostate on the global prostate image and using the deep network to extract highly effective features for the boundary refinement in a finer scale. With consideration of the correlations among different sequential images, we then construct a graph to select the best prostate proposals from proposal set for its use in 3D prostate segmentation. Experimental evaluation demonstrates that our proposed deep network and graph based method is superior to state-of-the-art couterparts, in terms of both dice similarity coefficient and Hausdorff distance, on public dataset.",,,Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"Algorithms; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Prostate; Prostatic Neoplasms",2016-08,2016,,2016-08,2016,,635-638,All OA, Green,Proceeding,"Yan, Ke; Li, Changyang; Wang, Xiuying; Li, Ang; Yuan, Yuchen; Feng, Dagan; Khadra, Mohamed; Kim, Jinman","Yan, Ke (School of Information Technologies, University of Sydney, Australia); Li, Changyang (School of Information Technologies, University of Sydney, Australia); Wang, Xiuying (School of Information Technologies, University of Sydney, Australia); Li, Ang (School of Information Technologies, University of Sydney, Australia); Yuan, Yuchen (School of Information Technologies, University of Sydney, Australia); Feng, Dagan (School of Information Technologies, University of Sydney, Australia); Khadra, Mohamed (Nepean Hospital, Penrith, Sydney Medical School, University of Sydney, Australia); Kim, Jinman (School of Information Technologies, University of Sydney, Australia)",,"Yan, Ke (The University of Sydney); Li, Changyang (The University of Sydney); Wang, Xiuying (The University of Sydney); Li, Ang (The University of Sydney); Yuan, Yuchen (The University of Sydney); Feng, Dagan (The University of Sydney); Khadra, Mohamed (Nepean Hospital); Kim, Jinman (The University of Sydney)",13,5,0.31,2.62,https://ses.library.usyd.edu.au/bitstream/2123/21701/1/Automatic%20Prostate%20Segmentation%20on%20MR%20Images%20with%20Deep%20Network%20and%20Graph%20Model.pdf,https://app.dimensions.ai/details/publication/pub.1084497925,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
6171,pub.1005055255,10.1117/12.2216424,31452561,PMC6710014,Patch-based label fusion for automatic multi-atlas-based prostate segmentation in MR images,"In this paper, we propose a 3D multi-atlas-based prostate segmentation method for MR images, which utilizes patch-based label fusion strategy. The atlases with the most similar appearance are selected to serve as the best subjects in the label fusion. A local patch-based atlas fusion is performed using voxel weighting based on anatomical signature. This segmentation technique was validated with a clinical study of 13 patients and its accuracy was assessed using the physicians' manual segmentations (gold standard). Dice volumetric overlapping was used to quantify the difference between the automatic and manual segmentation. In summary, we have developed a new prostate MR segmentation approach based on nonlocal patch-based label fusion, demonstrated its clinical feasibility, and validated its accuracy with manual segmentations.",,,Proceedings of SPIE,"Medical Imaging 2016: Image-Guided Procedures, Robotic Interventions, and Modeling",,2016-03-18,2016,2016-03-18,2016-03-18,9786,,978621-978621-7,All OA, Green,Proceeding,"Yang, Xiaofeng; Jani, Ashesh B; Rossi, Peter J; Mao, Hui; Curran, Walter J; Liu, Tian","Yang, Xiaofeng (Department of Radiation Oncology and Winship Cancer Institute.); Jani, Ashesh B (Department of Radiation Oncology and Winship Cancer Institute.); Rossi, Peter J (Department of Radiation Oncology and Winship Cancer Institute.); Mao, Hui (Department of Radiology and Imaging Sciences and Winship Cancer Institute Emory University, Atlanta, GA 30322.); Curran, Walter J (Department of Radiation Oncology and Winship Cancer Institute.); Liu, Tian (Department of Radiation Oncology and Winship Cancer Institute.)",,"Yang, Xiaofeng (Winship Cancer Institute); Jani, Ashesh B (Winship Cancer Institute); Rossi, Peter J (Winship Cancer Institute); Mao, Hui (Winship Cancer Institute); Curran, Walter J (Winship Cancer Institute); Liu, Tian (Winship Cancer Institute)",6,2,0.18,4.69,https://europepmc.org/articles/pmc6710014?pdf=render,https://app.dimensions.ai/details/publication/pub.1005055255,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
6108,pub.1153144850,10.1002/mp.16120,36433795,,An efficient interactive segmentation framework for medical images without pre‐training,"BACKGROUND AND PURPOSE: Accurate and efficient medical image segmentation plays an important role in subsequent clinical applications such as diagnosis and surgical planning. This paper proposes an efficient interactive framework based on a graph convolutional network (GCN) for medical image segmentation.
METHODS: The initial segmentation results showed that a set of boundary control points can be generated for further interactive segmentation. We presented an adaptive interactive manner that allows the user to click on the boundary for fast interaction or drag the erroneous predicted control points for accurate correction. Furthermore, we proposed an interactive segmentation network (referred to as IVIF-GCN) to learn user experience in the interactive process by transforming interactive cues into annotations. In IVIF-GCN, a module of information fusion of image features and vertex position features (IVIF) is proposed to learn the location relationship between the current vertex and the neighboring vertices. Finally, the locations of control points around the interaction point is predicted and updated automatically.
RESULTS: The proposed method achieves mean Dice of 96.6% and 91.3% on PROMISE12 and our in-house nasopharyngeal carcinoma (NPC) test sets, respectively. The experimental results showed that the proposed method outperforms the state-of-the-art segmentation methods.
CONCLUSIONS: The proposed interactive medical image segmentation method can efficiently improve segmentation results for clinical applications in the absence of training data. The GUI tool based on our method is available at https://github.com/Tian-lab/IGMedSeg.","This work was supported in part by NSFC under Grant Nos. 62173269 and 61876148, Natural Science Basic Research Plan in Shaanxi Province of China under Grant No. 2022JM‐324, Key Research and Development Program of Shaanxi Province of China under Grant No. 2020GXLHY‐008, and Social Science Foundation of Shaanxi Province of China under Grant No. 2021K014.",,Medical Physics,,,2022-11-26,2022,2022-12-03,2022-11-26,,,,Closed,Article,"Sun, Lei; Tian, Zhiqiang; Chen, Zhang; Luo, Wenrui; Du, Shaoyi","Sun, Lei (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China); Tian, Zhiqiang (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China); Chen, Zhang (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China); Luo, Wenrui (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China); Du, Shaoyi (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China)","Tian, Zhiqiang (Xi'an Jiaotong University)","Sun, Lei (Xi'an Jiaotong University); Tian, Zhiqiang (Xi'an Jiaotong University); Chen, Zhang (Xi'an Jiaotong University); Luo, Wenrui (Xi'an Jiaotong University); Du, Shaoyi (Xi'an Jiaotong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153144850,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
6108,pub.1140775404,10.1007/s00259-021-05497-8,34463809,PMC8803714,"Analytical performance of aPROMISE: automated anatomic contextualization, detection, and quantification of [18F]DCFPyL (PSMA) imaging for standardized reporting","PurposeThe application of automated image analyses could improve and facilitate standardization and consistency of quantification in [18F]DCFPyL (PSMA) PET/CT scans. In the current study, we analytically validated aPROMISE, a software as a medical device that segments organs in low-dose CT images with deep learning, and subsequently detects and quantifies potential pathological lesions in PSMA PET/CT.MethodsTo evaluate the deep learning algorithm, the automated segmentations of the low-dose CT component of PSMA PET/CT scans from 20 patients were compared to manual segmentations. Dice scores were used to quantify the similarities between the automated and manual segmentations. Next, the automated quantification of tracer uptake in the reference organs and detection and pre-segmentation of potential lesions were evaluated in 339 patients with prostate cancer, who were all enrolled in the phase II/III OSPREY study. Three nuclear medicine physicians performed the retrospective independent reads of OSPREY images with aPROMISE. Quantitative consistency was assessed by the pairwise Pearson correlations and standard deviation between the readers and aPROMISE. The sensitivity of detection and pre-segmentation of potential lesions was evaluated by determining the percent of manually selected abnormal lesions that were automatically detected by aPROMISE.ResultsThe Dice scores for bone segmentations ranged from 0.88 to 0.95. The Dice scores of the PSMA PET/CT reference organs, thoracic aorta and liver, were 0.89 and 0.97, respectively. Dice scores of other visceral organs, including prostate, were observed to be above 0.79. The Pearson correlation for blood pool reference was higher between any manual reader and aPROMISE, than between any pair of manual readers. The standard deviations of reference organ uptake across all patients as determined by aPROMISE (SD = 0.21 blood pool and SD = 1.16 liver) were lower compared to those of the manual readers. Finally, the sensitivity of aPROMISE detection and pre-segmentation was 91.5% for regional lymph nodes, 90.6% for all lymph nodes, and 86.7% for bone in metastatic patients.ConclusionIn this analytical study, we demonstrated the segmentation accuracy of the deep learning algorithm, the consistency in quantitative assessment across multiple readers, and the high sensitivity in detecting potential lesions. The study provides a foundational framework for clinical evaluation of aPROMISE in standardized reporting of PSMA PET/CT.",,"Open access funding provided by Lund University. Supported by EXINI Diagnostics AB (a wholly owned subsidiary of Progenics Pharmaceuticals Inc, USA). Dr Nickols is a PCF Young Investigator.",European Journal of Nuclear Medicine and Molecular Imaging,,"Humans; Image Processing, Computer-Assisted; Male; Positron Emission Tomography Computed Tomography; Prostate; Prostatic Neoplasms; Retrospective Studies",2021-08-31,2021,2021-08-31,2022-02,49,3,1041-1051,All OA, Hybrid,Article,"Johnsson, Kerstin; Brynolfsson, Johan; Sahlstedt, Hannicka; Nickols, Nicholas G.; Rettig, Matthew; Probst, Stephan; Morris, Michael J.; Bjartell, Anders; Eiber, Mathias; Anand, Aseem","Johnsson, Kerstin (Department of Data Science and Machine Learning, EXINI Diagnostics AB, Lund, Sweden); Brynolfsson, Johan (Department of Data Science and Machine Learning, EXINI Diagnostics AB, Lund, Sweden); Sahlstedt, Hannicka (Department of Data Science and Machine Learning, EXINI Diagnostics AB, Lund, Sweden); Nickols, Nicholas G. (Radiation Oncology Service, VA Greater Los Angeles Healthcare System, Los Angeles, CA, USA; Department of Radiation Oncology, David Geffen School of Medicine, University of California Los Angeles, Los Angeles, CA, USA; Department of Urology, David Geffen School of Medicine, University of California Los Angeles, Los Angeles, CA, USA; Institute of Urologic Oncology, Jonsson Comprehensive Cancer Center, University of California Los Angeles, Los Angeles, CA, USA); Rettig, Matthew (Department of Urology, David Geffen School of Medicine, University of California Los Angeles, Los Angeles, CA, USA; Institute of Urologic Oncology, Jonsson Comprehensive Cancer Center, University of California Los Angeles, Los Angeles, CA, USA; Division of Hematology-Oncology, Greater Los Angeles Healthcare System, Los Angeles, CA, USA); Probst, Stephan (Nuclear Medicine, Medical Imaging, Jewish General Hospital, McGill University, Montreal, QC, Canada); Morris, Michael J. (Department of Medicine, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Weill Cornell Medical College, New York, NY, USA); Bjartell, Anders (Department of Translational Medicine, Division of Urological Cancers, Lund University, Lund, Sweden); Eiber, Mathias (Department of Nuclear Medicine, Klinikum Rechts Der Isar, Technical University of Munich, Munich, Germany); Anand, Aseem (Department of Data Science and Machine Learning, EXINI Diagnostics AB, Lund, Sweden; Department of Medicine, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Department of Translational Medicine, Division of Urological Cancers, Lund University, Lund, Sweden)","Anand, Aseem (; Memorial Sloan Kettering Cancer Center; Lund University)","Johnsson, Kerstin (); Brynolfsson, Johan (); Sahlstedt, Hannicka (); Nickols, Nicholas G. (VA Greater Los Angeles Healthcare System; University of California, Los Angeles; University of California, Los Angeles; Jonsson Comprehensive Cancer Center); Rettig, Matthew (University of California, Los Angeles; Jonsson Comprehensive Cancer Center); Probst, Stephan (Jewish General Hospital; McGill University); Morris, Michael J. (Memorial Sloan Kettering Cancer Center; Cornell University); Bjartell, Anders (Lund University); Eiber, Mathias (Rechts der Isar Hospital; Technical University of Munich); Anand, Aseem (Memorial Sloan Kettering Cancer Center; Lund University)",12,12,4.49,8.69,https://link.springer.com/content/pdf/10.1007/s00259-021-05497-8.pdf,https://app.dimensions.ai/details/publication/pub.1140775404,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
6108,pub.1136777875,10.1111/jon.12850,33783929,,MRI Diffusion‐Weighted Imaging to Measure Infarct Volume: Assessment of Manual Segmentation Variability,"BACKGROUND AND PURPOSE: Manual segmentation of infarct volume on follow-up MRI diffusion-weighted imaging (MRI-DWI) is considered the gold standard but is prone to rater variability. We assess the variability of manual segmentations of MRI-DWI infarct volume.
METHODS: Consecutive patients (May 2018 to May 2019) with the anterior circulation stroke and endovascularly treated were enrolled. All patients underwent 24- to 32-hour follow-up MRI. Three users manually segmented DWI infarct volumes slice by slice twice. The reference standard of DWI infarct volume was generated by the STAPLE algorithm. Intra- and interrater reliability was evaluated using the intraclass correlation coefficient (ICC) by comparing manual segmentations with the reference standard. Spatial measurements were evaluated using metrics of the Dice similarity coefficient (DSC). Volumetric measurements were compared using the lesion volume.
RESULTS: The dataset consisted of 44 patients, mean (SD) age was 70.1 years (±10.3), 43% were women, and median baseline NIHSS score was 16. Among three users, the mean DSC for MRI-DWI infarct volume segmentations ranged from 80.6% ± 11.7% to 88.6% ± 7.5%, and the mean absolute volume difference was 2.8 ± 6.8 to 13.0 ± 14.0 ml. Interrater ICC among the users for DSC and infarct volume was .86 (95% confidence interval [95% CI]: .78-.91) and .997 (95% CI: .995-.998). Intrarater ICC for the three users was .83 (95% CI: .69-.93), .84 (95% CI: .72-.91), and .80 (95% CI: .64-.89) for DSC, and .99 (95% CI: .987-.996), .991 (95% CI: .983-.995), and .996 (95% CI: .993-.998) for infarct volume.
CONCLUSIONS: Manual segmentation of infarct volume on follow-up MRI-DWI shows excellent agreement and good spatial overlap with the reference standard, suggesting its usefulness for measuring infarct volume on 24- to 32-hour MRI-DWI.",,,Journal of Neuroimaging,,Aged, Algorithms, Brain Infarction, Diffusion Magnetic Resonance Imaging, Endovascular Procedures, Female, Humans, Male, Middle Aged, Reproducibility of Results,2021-03-30,2021,2021-03-30,2021-05,31,3,541-550,Closed,Article,"Cimflova, Petra; Kral, Jiri; Volny, Ondrej; Horn, MacKenzie; Ojha, Piyush; Cabal, Martin; Kasickova, Linda; Havelka, Jaroslav; Jonszta, Tomas; Bar, Michal; Qiu, Wu","Cimflova, Petra (Departments of Clinical Neurosciences, Calgary Stroke Program, Cumming School of Medicine, University of Calgary, Calgary, Canada; Department of Medical Imaging, St. Anne´s University Hospital and Faculty of Medicine, Masaryk University, Brno, Czech Republic; International Clinical Research Center, Stroke Research Program, St. Anne´s University Hospital, Brno, Czech Republic; Faculty of Medicine in Hradec Kralove, Charles University, Hradec Kralove, Czech Republic); Kral, Jiri (Department of Neurology, University Hospital Ostrava, Ostrava, Czech Republic; Faculty of Medicine, Masaryk University, Brno, Czech Republic); Volny, Ondrej (Departments of Clinical Neurosciences, Calgary Stroke Program, Cumming School of Medicine, University of Calgary, Calgary, Canada; International Clinical Research Center, Stroke Research Program, St. Anne´s University Hospital, Brno, Czech Republic; Department of Neurology, University Hospital Ostrava, Ostrava, Czech Republic); Horn, MacKenzie (Departments of Clinical Neurosciences, Calgary Stroke Program, Cumming School of Medicine, University of Calgary, Calgary, Canada); Ojha, Piyush (Departments of Clinical Neurosciences, Calgary Stroke Program, Cumming School of Medicine, University of Calgary, Calgary, Canada); Cabal, Martin (Department of Neurology, University Hospital Ostrava, Ostrava, Czech Republic); Kasickova, Linda (Department of Neurology, University Hospital Ostrava, Ostrava, Czech Republic; Faculty of Medicine, Masaryk University, Brno, Czech Republic); Havelka, Jaroslav (Department of Radiology, University Hospital Ostrava, Ostrava, Czech Republic); Jonszta, Tomas (Department of Radiology, University Hospital Ostrava, Ostrava, Czech Republic); Bar, Michal (Department of Neurology, University Hospital Ostrava, Ostrava, Czech Republic; Faculty of Medicine, Ostrava University, Ostrava, Czech Republic); Qiu, Wu (Departments of Clinical Neurosciences, Calgary Stroke Program, Cumming School of Medicine, University of Calgary, Calgary, Canada; Department of Radiology, Cumming School of Medicine, University of Calgary, Calgary, Canada)","Qiu, Wu (University of Calgary; University of Calgary)","Cimflova, Petra (University of Calgary; Masaryk University; St. Anne´s University Hospital; Charles University); Kral, Jiri (University Hospital Ostrava; Masaryk University); Volny, Ondrej (University of Calgary; St. Anne´s University Hospital; University Hospital Ostrava); Horn, MacKenzie (University of Calgary); Ojha, Piyush (University of Calgary); Cabal, Martin (University Hospital Ostrava); Kasickova, Linda (University Hospital Ostrava; Masaryk University); Havelka, Jaroslav (University Hospital Ostrava); Jonszta, Tomas (University Hospital Ostrava); Bar, Michal (University Hospital Ostrava; University of Ostrava); Qiu, Wu (University of Calgary; University of Calgary)",2,2,0.49,1.75,,https://app.dimensions.ai/details/publication/pub.1136777875,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,
6091,pub.1124145678,10.1038/s41598-019-57046-x,31953425,PMC6969030,Measurement of Prostate Volume with MRI (A Guide for the Perplexed): Biproximate Method with Analysis of Precision and Accuracy,"To review the anatomic basis of prostate boundary selection on T2-weighted magnetic resonance imaging (MRI). To introduce an alternative 3D ellipsoid measuring technique that maximizes precision, report the intra- and inter-observer reliability, and to advocate it’s use for research involving multiple observers. We demonstrate prostate boundary anatomy using gross pathology and MRI examples. This provides background for selecting key boundary marks when measuring prostate volume. An alternative ellipsoid volume method is then proposed using these boundaries in an attempt to improve inter-observer precision. An IRB approved retrospective study of 140 patients with elevated serum prostate specific antigen levels and/or abnormal digital rectal examinations was done with T2-weighted MRI applying a new (Biproximate) technique. Measurements were made by 2 examiners, correlated with each other for inter-observer precision and correlated with an expert observer for accuracy. Correlation statistics, linear regression analysis, and tests of means were applied using p ≤ 0.05 as the threshold for significance. Inter-observer correlation (precision) was 0.95 between observers. Correlation between these observers and the expert (accuracy) was 0.94 and 0.97 respectively. Intra-observer correlation for the expert was 0.98. Means for inter-rater reliability and accuracy were all the same (p = 0.001). We conclude that using more precise reproducible landmarks with biproximate technique, precision and accuracy of total prostate volume is found to be demonstrated.",,,Scientific Reports,,"Adult; Aged; Digital Rectal Examination; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Observer Variation; Prostate; Prostatic Neoplasms; Radiographic Image Interpretation, Computer-Assisted; Retrospective Studies; Sensitivity and Specificity",2020-01-17,2020,2020-01-17,,10,1,575,All OA, Gold,Article,"Wasserman, Neil F.; Niendorf, Eric; Spilseth, Benjamin","Wasserman, Neil F. (Department of Radiology, University of Minnesota, Mayo Mail Code 292, 420 Delaware Street S.E, 55455 (612) 626-3343, Minneapolis, MN, USA); Niendorf, Eric (Department of Radiology, University of Minnesota, Mayo Mail Code 292, 420 Delaware Street S.E, 55455 (612) 626-3343, Minneapolis, MN, USA); Spilseth, Benjamin (Department of Radiology, University of Minnesota, Mayo Mail Code 292, 420 Delaware Street S.E, 55455 (612) 626-3343, Minneapolis, MN, USA)","Wasserman, Neil F. (University of Minnesota)","Wasserman, Neil F. (University of Minnesota); Niendorf, Eric (University of Minnesota); Spilseth, Benjamin (University of Minnesota)",16,10,1.81,6.82,https://www.nature.com/articles/s41598-019-57046-x.pdf,https://app.dimensions.ai/details/publication/pub.1124145678,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
6090,pub.1146499553,10.1016/j.cmpb.2022.106770,35640389,,Medical image diagnosis of prostate tumor based on PSP-Net+VGG16 deep learning network,"BACKGROUND AND OBJECTIVE: Prostate cancer is the most common cancer of the male reproductive system. With the development of medical imaging technology, magnetic resonance images (MRI) have been used in the diagnosis and treatment of prostate cancer because of its clarity and non-invasiveness. Prostate MRI segmentation and diagnosis experience problems such as low tissue boundary contrast. The traditional segmentation method of manually drawing the contour boundary of the tissue cannot meet the clinical real-time requirements. How to quickly and accurately segment the prostate tumor has become an important research topic.
METHODS: This paper proposes a prostate tumor diagnosis based on the deep learning network PSP-Net+VGG16. The deep convolutional neural network segmentation method based on the PSP-Net constructs a atrous convolution residual structure model extraction network. First, the three-dimensional prostate MRI is converted to two-dimensional image slices, and then the slice input of the two-dimensional image is trained based on the PSP-Net neural network; and the VGG16 network is used to analyze the region of interest and classify prostate cancer and normal prostate.
RESULTS: According to the experimental results, the segmentation method based on the deep learning network PSP-Net is used to identify the data set samples. The segmentation accuracy is close to the Dice similarity coefficient and Hausdorff distance, and even exceeds the traditional prostate image segmentation method. The Dice index reached 91.3%, and the technique is superior in speed of processing. The predicted tumor markers are very close to the actual markers manually by clinicians; the classification accuracy and recognition rates of prostate MRI based on VGG16 are as high as 87.95% and 87.33%, and the accuracy rate and recall rate of the network model are relatively balanced. The area under curve index is also higher than other models, with good generalization ability.
CONCLUSION: Experiments show that prostate cancer diagnosis based on the deep learning network PSP-Net+VGG16 is superior in accuracy and processing time compared to other algorithms, and can be well applied to clinical prostate tumor diagnosis.",,,Computer Methods and Programs in Biomedicine,,"Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate; Prostatic Neoplasms",2022-03-23,2022,2022-03-23,2022-06,221,,106770,Closed,Article,"Ye, Li-Yin; Miao, Xiao-Yan; Cai, Wan-Song; Xu, Wan-Jiang","Ye, Li-Yin (Department of Urology, The First People's Hospital of Fuyang, Hangzhou 311400, China.); Miao, Xiao-Yan (Department of Radiation Oncology, The First People's Hospital of Fuyang, Hangzhou 311400, China. Electronic address: fyyyflk@163.com.); Cai, Wan-Song (Department of Urology, The First People's Hospital of Fuyang, Hangzhou 311400, China.); Xu, Wan-Jiang (Department of Urology, The First People's Hospital of Fuyang, Hangzhou 311400, China.)","Miao, Xiao-Yan ","Ye, Li-Yin (); Miao, Xiao-Yan (); Cai, Wan-Song (); Xu, Wan-Jiang ()",2,2,,,,https://app.dimensions.ai/details/publication/pub.1146499553,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,
6088,pub.1100316721,10.1002/mp.12748,29322528,,MR and CT data with multiobserver delineations of organs in the pelvic area—Part of the Gold Atlas project,"PURPOSE: We describe a public dataset with MR and CT images of patients performed in the same position with both multiobserver and expert consensus delineations of relevant organs in the male pelvic region. The purpose was to provide means for training and validation of segmentation algorithms and methods to convert MR to CT like data, i.e., so called synthetic CT (sCT).
ACQUISITION AND VALIDATION METHODS: T1- and T2-weighted MR images as well as CT data were collected for 19 patients at three different departments. Five experts delineated nine organs for each patient based on the T2-weighted MR images. An automatic method was used to fuse the delineations. Starting from each fused delineation, a consensus delineation was agreed upon by the five experts for each organ and patient. Segmentation overlap between user delineations with respect to the consensus delineations was measured to describe the spread of the collected data. Finally, an open-source software was used to create deformation vector fields describing the relation between MR and CT images to further increase the usability of the dataset.
DATA FORMAT AND USAGE NOTES: The dataset has been made publically available to be used for academic purposes, and can be accessed from https://zenodo.org/record/583096.
POTENTIAL APPLICATIONS: The dataset provides a useful source for training and validation of segmentation algorithms as well as methods to convert MR to CT-like data (sCT). To give some examples: The T2-weighted MR images with their consensus delineations can directly be used as a template in an existing atlas-based segmentation engine; the expert delineations are useful to validate the performance of a segmentation algorithm as they provide a way to measure variability among users which can be compared with the result of an automatic segmentation; and the pairwise deformably registered MR and CT images can be a source for an atlas-based sCT algorithm or for validation of sCT algorithm.","The study was a part of the National Swedish Gentle radiotherapy project, partly funded by the National Swedish agency for innovation (Vinnova).",,Medical Physics,,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Pelvis; Tomography, X-Ray Computed",2018-01-24,2018,2018-01-24,2018-03,45,3,1295-1300,All OA, Hybrid,Article,"Nyholm, Tufve; Svensson, Stina; Andersson, Sebastian; Jonsson, Joakim; Sohlin, Maja; Gustafsson, Christian; Kjellén, Elisabeth; Söderström, Karin; Albertsson, Per; Blomqvist, Lennart; Zackrisson, Björn; Olsson, Lars E.; Gunnlaugsson, Adalsteinn","Nyholm, Tufve (Department of Radiation Sciences, Umeå University, Umeå, Sweden); Svensson, Stina (RaySearch Laboratories AB, Stockholm, Sweden); Andersson, Sebastian (RaySearch Laboratories AB, Stockholm, Sweden); Jonsson, Joakim (Department of Radiation Sciences, Umeå University, Umeå, Sweden); Sohlin, Maja (Department of Radiation Physics, Institute of Clinical Sciences, Sahlgrenska University Hospital, Göteborg, Sweden); Gustafsson, Christian (Department of Hematology, Oncology and Radiation Physics, Skåne University Hospital, Lund, Sweden; Department of Medical Physics, Lund University, Malmö, Sweden); Kjellén, Elisabeth (Department of Hematology, Oncology and Radiation Physics, Skåne University Hospital, Lund, Sweden); Söderström, Karin (Department of Radiation Sciences, Umeå University, Umeå, Sweden); Albertsson, Per (Department of Oncology, Institute of Clinical Sciences, Sahlgrenska Academy, University of Gothenburg, Gothenburg, Sweden); Blomqvist, Lennart (Department of Radiation Sciences, Umeå University, Umeå, Sweden; Department of Molecular Medicine and Surgery, Karolinska Institutet, Stockholm, Sweden); Zackrisson, Björn (Department of Radiation Sciences, Umeå University, Umeå, Sweden); Olsson, Lars E. (Department of Hematology, Oncology and Radiation Physics, Skåne University Hospital, Lund, Sweden); Gunnlaugsson, Adalsteinn (Department of Hematology, Oncology and Radiation Physics, Skåne University Hospital, Lund, Sweden)","Nyholm, Tufve (Umeå University)","Nyholm, Tufve (Umeå University); Svensson, Stina (RaySearch Laboratories (Sweden)); Andersson, Sebastian (RaySearch Laboratories (Sweden)); Jonsson, Joakim (Umeå University); Sohlin, Maja (Sahlgrenska University Hospital); Gustafsson, Christian (Skåne University Hospital; Lund University); Kjellén, Elisabeth (Skåne University Hospital); Söderström, Karin (Umeå University); Albertsson, Per (University of Gothenburg); Blomqvist, Lennart (Umeå University; Karolinska Institute); Zackrisson, Björn (Umeå University); Olsson, Lars E. (Skåne University Hospital); Gunnlaugsson, Adalsteinn (Skåne University Hospital)",26,15,1.86,15.83,https://aapm.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/mp.12748,https://app.dimensions.ai/details/publication/pub.1100316721,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
6084,pub.1086013993,10.18383/j.tom.2017.00005,30042974,PMC6024456,Mask-Adapted Background Field Removal for Artifact Reduction in Quantitative Susceptibility Mapping of the Prostate,"We propose an alternative processing method for quantitative susceptibility mapping of the prostate that reduces artifacts and enables better visibility and quantification of calcifications and other lesions. Three-dimensional gradient-echo magnetic resonance data were obtained from 26 patients at 3 T who previously received a planning computed tomography of the prostate. Phase images were unwrapped using Laplacian-based phase unwrapping. The background field was removed with the V-SHARP method using tissue masks for the entire abdomen (Method 1) and masks that excluded bone and the rectum (Method 2). Susceptibility maps were calculated with the iLSQR method. The quality of susceptibility maps was assessed by one radiologist and two physicists who rated the data for visibility of lesions and data quality on a scale from 1 (poor) to 4 (good). The readers rated susceptibility maps computed with Method 2 to be, on average, better for visibility of lesions with a score of 2.9 ± 1.1 and image quality with a score of 2.8 ± 0.8 compared with maps computed with Method 1 (2.4 ± 1.2/2.3 ± 1.0). Regarding strong artifacts, these could be removed using adapted masks, and the susceptibility values seemed less biased by the artifacts. Thus, using an adapted mask for background field removal when calculating susceptibility maps of the prostate from phase data reduces artifacts and improves visibility of lesions.",,,Tomography,,,2017-06-01,2017,2017-06-01,2017-06,3,2,96-100,All OA, Gold,Article,"Straub, Sina; Emmerich, Julian; Schlemmer, Heinz-Peter; Maier-Hein, Klaus H.; Ladd, Mark E.; Röthke, Matthias C.; Bonekamp, David; Laun, Frederik B.","Straub, Sina (Department of Medical Physics in Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany;); Emmerich, Julian (Department of Medical Physics in Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany;); Schlemmer, Heinz-Peter (Department of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany;); Maier-Hein, Klaus H. (Junior Group Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany; and); Ladd, Mark E. (Department of Medical Physics in Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany;); Röthke, Matthias C. (Department of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany;); Bonekamp, David (Department of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany;); Laun, Frederik B. (Department of Medical Physics in Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany;; Institute of Radiology, University Hospital Erlangen, Erlangen, Germany)","Straub, Sina (German Cancer Research Center)","Straub, Sina (German Cancer Research Center); Emmerich, Julian (German Cancer Research Center); Schlemmer, Heinz-Peter (German Cancer Research Center); Maier-Hein, Klaus H. (German Cancer Research Center); Ladd, Mark E. (German Cancer Research Center); Röthke, Matthias C. (German Cancer Research Center); Bonekamp, David (German Cancer Research Center); Laun, Frederik B. (German Cancer Research Center; Universitätsklinikum Erlangen)",8,6,0.41,2.42,https://doi.org/10.18383/j.tom.2017.00005,https://app.dimensions.ai/details/publication/pub.1086013993,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 51 Physical Sciences,,,,,,,,,
6043,pub.1146838111,10.1186/s13014-022-02035-0,35366918,PMC8976981,Explainable AI for CNN-based prostate tumor segmentation in multi-parametric MRI correlated to whole mount histopathology,"Automatic prostate tumor segmentation is often unable to identify the lesion even if multi-parametric MRI data is used as input, and the segmentation output is difficult to verify due to the lack of clinically established ground truth images. In this work we use an explainable deep learning model to interpret the predictions of a convolutional neural network (CNN) for prostate tumor segmentation. The CNN uses a U-Net architecture which was trained on multi-parametric MRI data from 122 patients to automatically segment the prostate gland and prostate tumor lesions. In addition, co-registered ground truth data from whole mount histopathology images were available in 15 patients that were used as a test set during CNN testing. To be able to interpret the segmentation results of the CNN, heat maps were generated using the Gradient Weighted Class Activation Map (Grad-CAM) method. The CNN achieved a mean Dice Sorensen Coefficient 0.62 and 0.31 for the prostate gland and the tumor lesions -with the radiologist drawn ground truth and 0.32 with whole-mount histology ground truth for tumor lesions. Dice Sorensen Coefficient between CNN predictions and manual segmentations from MRI and histology data were not significantly different. In the prostate the Grad-CAM heat maps could differentiate between tumor and healthy prostate tissue, which indicates that the image information in the tumor was essential for the CNN segmentation.","Grant support by the Klaus Tschira Stiftung GmbH, Heidelberg, Germany is gratefully acknowledged. The support from MathWorks and the contribution of Arnie Berlin (The MathWorks Inc., Novi, MI, United States) in creation of new software and data processing techniques used in major parts of this work is gratefully acknowledged.","Open Access funding enabled and organized by Projekt DEAL. This work was supported by a research grant from the Klaus Tschira Stiftung GmbH (Grant No. 00.014.2019), and by the German Science Foundation (DFG) under research grant BO 3025/14-1.",Radiation Oncology,,"Humans; Magnetic Resonance Imaging; Male; Multiparametric Magnetic Resonance Imaging; Neural Networks, Computer; Prostatic Neoplasms",2022-04-02,2022,2022-04-02,2022-12,17,1,65,All OA, Gold,Article,"Gunashekar, Deepa Darshini; Bielak, Lars; Hägele, Leonard; Oerther, Benedict; Benndorf, Matthias; Grosu, Anca-L.; Brox, Thomas; Zamboglou, Constantinos; Bock, Michael","Gunashekar, Deepa Darshini (Department of Radiology, Medical Physics, Medical Center University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany); Bielak, Lars (Department of Radiology, Medical Physics, Medical Center University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany; German Cancer Consortium (DKTK), Partner Site Freiburg, Freiburg, Germany); Hägele, Leonard (Department of Radiology, Medical Physics, Medical Center University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany); Oerther, Benedict (Department of Radiology, Medical Center University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany); Benndorf, Matthias (Department of Radiology, Medical Center University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany); Grosu, Anca-L. (German Cancer Consortium (DKTK), Partner Site Freiburg, Freiburg, Germany; Department of Radiology, Medical Center University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany); Brox, Thomas (Department of Computer Science, University of Freiburg, Freiburg, Germany); Zamboglou, Constantinos (German Cancer Consortium (DKTK), Partner Site Freiburg, Freiburg, Germany; Department of Radiology, Medical Center University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany); Bock, Michael (Department of Radiology, Medical Physics, Medical Center University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany; German Cancer Consortium (DKTK), Partner Site Freiburg, Freiburg, Germany)","Gunashekar, Deepa Darshini (University Medical Center Freiburg; University of Freiburg)","Gunashekar, Deepa Darshini (University Medical Center Freiburg; University of Freiburg); Bielak, Lars (University Medical Center Freiburg; University of Freiburg; German Cancer Research Center); Hägele, Leonard (University Medical Center Freiburg; University of Freiburg); Oerther, Benedict (University Medical Center Freiburg; University of Freiburg); Benndorf, Matthias (University Medical Center Freiburg; University of Freiburg); Grosu, Anca-L. (German Cancer Research Center; University Medical Center Freiburg; University of Freiburg); Brox, Thomas (University of Freiburg); Zamboglou, Constantinos (German Cancer Research Center; University Medical Center Freiburg; University of Freiburg); Bock, Michael (University Medical Center Freiburg; University of Freiburg; German Cancer Research Center)",6,6,,,https://ro-journal.biomedcentral.com/track/pdf/10.1186/s13014-022-02035-0,https://app.dimensions.ai/details/publication/pub.1146838111,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
5903,pub.1137493587,10.1002/mp.14901,33905566,,MR to ultrasound image registration with segmentation‐based learning for HDR prostate brachytherapy,"PURPOSE: Propagation of contours from high-quality magnetic resonance (MR) images to treatment planning ultrasound (US) images with severe needle artifacts is a challenging task, which can greatly aid the organ contouring in high dose rate (HDR) prostate brachytherapy. In this study, a deep learning approach was developed to automatize this registration procedure for HDR brachytherapy practice.
METHODS: Because of the lack of training labels and difficulty of accurate registration from inferior image quality, a new segmentation-based registration framework was proposed for this multi-modality image registration problem. The framework consisted of two segmentation networks and a deformable registration network, based on the weakly -supervised registration strategy. Specifically, two 3D V-Nets were trained for the prostate segmentation on the MR and US images separately, to generate the weak supervision labels for the registration network training. Besides the image pair, the corresponding prostate probability maps from the segmentation were further fed to the registration network to predict the deformation matrix, and an augmentation method was designed to randomly scale the input and label probability maps during the registration network training. The overlap between the deformed and fixed prostate contours was analyzed to evaluate the registration accuracy. Three datasets were collected from our institution for the MR and US image segmentation networks, and the registration network learning, which contained 121, 104, and 63 patient cases, respectively.
RESULTS: The mean Dice similarity coefficient (DSC) results of the two prostate segmentation networks are 0.86 ± 0.05 and 0.90 ± 0.03, for MR images and the US images after the needle insertion, respectively. The mean DSC, center-of-mass (COM) distance, Hausdorff distance (HD), and averaged symmetric surface distance (ASSD) results for the registration of manual prostate contours were 0.87 ± 0.05, 1.70 ± 0.89 mm, 7.21 ± 2.07 mm, 1.61 ± 0.64 mm, respectively. By providing the prostate probability map from the segmentation to the registration network, as well as applying the random map augmentation method, the evaluation results of the four metrics were all improved, such as an increase in DSC from 0.83 ± 0.08 to 0.86 ± 0.06 and from 0.86 ± 0.06 to 0.87 ± 0.05, respectively.
CONCLUSIONS: A novel segmentation-based registration framework was proposed to automatically register prostate MR images to the treatment planning US images with metal artifacts, which not only largely saved the labor work on the data preparation, but also improved the registration accuracy. The evaluation results showed the potential of this approach in HDR prostate brachytherapy practice.","This research was supported by a grant from Varian Medical Systems (Palo Alto, CA).",,Medical Physics,,"Brachytherapy; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostatic Neoplasms; Ultrasonography",2021-05-14,2021,2021-05-14,2021-06,48,6,3074-3083,Closed,Article,"Chen, Yizheng; Xing, Lei; Yu, Lequan; Liu, Wu; Fahimian, Benjamin Pooya; Niedermayr, Thomas; Bagshaw, Hilary P.; Buyyounouski, Mark; Han, Bin","Chen, Yizheng (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Xing, Lei (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Yu, Lequan (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Liu, Wu (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Fahimian, Benjamin Pooya (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Niedermayr, Thomas (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Bagshaw, Hilary P. (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Buyyounouski, Mark (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Han, Bin (Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA)","Han, Bin (Stanford University)","Chen, Yizheng (Stanford University); Xing, Lei (Stanford University); Yu, Lequan (Stanford University); Liu, Wu (Stanford University); Fahimian, Benjamin Pooya (Stanford University); Niedermayr, Thomas (Stanford University); Bagshaw, Hilary P. (Stanford University); Buyyounouski, Mark (Stanford University); Han, Bin (Stanford University)",7,7,1.72,8.76,,https://app.dimensions.ai/details/publication/pub.1137493587,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
5903,pub.1131976790,10.1155/2020/8861035,33144873,PMC7596462,Evaluation of Multimodal Algorithms for the Segmentation of Multiparametric MRI Prostate Images,"Prostate segmentation in multiparametric magnetic resonance imaging (mpMRI) can help to support prostate cancer diagnosis and therapy treatment. However, manual segmentation of the prostate is subjective and time-consuming. Many deep learning monomodal networks have been developed for automatic whole prostate segmentation from T2-weighted MR images. We aimed to investigate the added value of multimodal networks in segmenting the prostate into the peripheral zone (PZ) and central gland (CG). We optimized and evaluated monomodal DenseVNet, multimodal ScaleNet, and monomodal and multimodal HighRes3DNet, which yielded dice score coefficients (DSC) of 0.875, 0.848, 0.858, and 0.890 in WG, respectively. Multimodal HighRes3DNet and ScaleNet yielded higher DSC with statistical differences in PZ and CG only compared to monomodal DenseVNet, indicating that multimodal networks added value by generating better segmentation between PZ and CG regions but did not improve the WG segmentation. No significant difference was observed in the apex and base of WG segmentation between monomodal and multimodal networks, indicating that the segmentations at the apex and base were more affected by the general network architecture. The number of training data was also varied for DenseVNet and HighRes3DNet, from 20 to 120 in steps of 20. DenseVNet was able to yield DSC of higher than 0.65 even for special cases, such as TURP or abnormal prostate, whereas HighRes3DNet's performance fluctuated with no trend despite being the best network overall. Multimodal networks did not add value in segmenting special cases but generally reduced variations in segmentation compared to the same matched monomodal network.","This study was supported by the National University Health System (NUHS) Centre Grant Seed Funding, Singapore (NUHSCGSF/2019/07). We would also like to acknowledge Dr. Wynne Yuru Chua (M.D.) and Dr. Bertrand Wei Leng Ang (M.D.) from the National University Health System (NUHS), Singapore, for teaching prostate segmentation.",,Computational and Mathematical Methods in Medicine,,"Algorithms; Computational Biology; Databases, Factual; Deep Learning; Humans; Image Interpretation, Computer-Assisted; Machine Learning; Male; Mathematical Concepts; Multiparametric Magnetic Resonance Imaging; Neural Networks, Computer; Pattern Recognition, Automated; Prostatic Neoplasms",2020-10-20,2020,2020-10-20,2020-10-20,2020,,8861035,All OA, Gold,Article,"Nai, Ying-Hwey; Teo, Bernice W.; Tan, Nadya L.; Chua, Koby Yi Wei; Wong, Chun Kit; O'Doherty, Sophie; Stephenson, Mary C.; Schaefferkoetter, Josh; Thian, Yee Liang; Chiong, Edmund; Reilhac, Anthonin","Nai, Ying-Hwey (Clinical Imaging Research Centre, Yong Loo Lin School of Medicine, National University of Singapore, Singapore); Teo, Bernice W. (Nanyang Junior College, Singapore); Tan, Nadya L. (St. Joseph's Institution International, Singapore); Chua, Koby Yi Wei (Anglo-Chinese Independent, Singapore); Wong, Chun Kit (Clinical Imaging Research Centre, Yong Loo Lin School of Medicine, National University of Singapore, Singapore); O'Doherty, Sophie (Clinical Imaging Research Centre, Yong Loo Lin School of Medicine, National University of Singapore, Singapore); Stephenson, Mary C. (Clinical Imaging Research Centre, Yong Loo Lin School of Medicine, National University of Singapore, Singapore); Schaefferkoetter, Josh (Clinical Imaging Research Centre, Yong Loo Lin School of Medicine, National University of Singapore, Singapore; Joint Department of Medical Imaging, University Health Network, Toronto, Canada; Siemens Medical Solutions USA, Inc., Molecular Imaging, Knoxville, TN, USA); Thian, Yee Liang (Department of Diagnostic Imaging, National University Hospital, Singapore); Chiong, Edmund (Department of Surgery (Urology), Yong Loo Lin School of Medicine, National University of Singapore, Singapore; Department of Urology, National University Hospital, Singapore); Reilhac, Anthonin (Clinical Imaging Research Centre, Yong Loo Lin School of Medicine, National University of Singapore, Singapore)","Nai, Ying-Hwey (National University of Singapore)","Nai, Ying-Hwey (National University of Singapore); Teo, Bernice W. (); Tan, Nadya L. (); Chua, Koby Yi Wei (); Wong, Chun Kit (National University of Singapore); O'Doherty, Sophie (National University of Singapore); Stephenson, Mary C. (National University of Singapore); Schaefferkoetter, Josh (National University of Singapore; University Health Network); Thian, Yee Liang (National University Hospital); Chiong, Edmund (National University of Singapore; National University Hospital); Reilhac, Anthonin (National University of Singapore)",7,7,0.95,2.93,https://downloads.hindawi.com/journals/cmmm/2020/8861035.pdf,https://app.dimensions.ai/details/publication/pub.1131976790,40 Engineering, 4003 Biomedical Engineering, 49 Mathematical Sciences, 4901 Applied Mathematics,,,,,,,,
5894,pub.1130324716,10.3390/jimaging6090083,34460740,PMC8321056,Investigating the Performance of Generative Adversarial Networks for Prostate Tissue Detection and Segmentation,"The manual delineation of region of interest (RoI) in 3D magnetic resonance imaging (MRI) of the prostate is time-consuming and subjective. Correct identification of prostate tissue is helpful to define a precise RoI to be used in CAD systems in clinical practice during diagnostic imaging, radiotherapy and monitoring the progress of disease. Conditional GAN (cGAN), cycleGAN and U-Net models and their performances were studied for the detection and segmentation of prostate tissue in 3D multi-parametric MRI scans. These models were trained and evaluated on MRI data from 40 patients with biopsy-proven prostate cancer. Due to the limited amount of available training data, three augmentation schemes were proposed to artificially increase the training samples. These models were tested on a clinical dataset annotated for this study and on a public dataset (PROMISE12). The cGAN model outperformed the U-Net and cycleGAN predictions owing to the inclusion of paired image supervision. Based on our quantitative results, cGAN gained a Dice score of 0.78 and 0.75 on the private and the PROMISE12 public datasets, respectively.",The authors would like to acknowledge Alun Jones and Sandy Spence for their support and maintenance of the GPU and the computer systems used for this research.,This research received no external funding.,Journal of Imaging,,,2020-08-24,2020,2020-08-24,,6,9,83,All OA, Gold,Article,"Birbiri, Ufuk Cem; Hamidinekoo, Azam; Grall, Amélie; Malcolm, Paul; Zwiggelaar, Reyer","Birbiri, Ufuk Cem (Department of Computer Engineering, Middle East Technical University, Ankara 06800, Turkey;, cem.birbiri@metu.edu.tr); Hamidinekoo, Azam (Division of Molecular Pathology, Institute of Cancer Research (ICR), London SM2 5NG, UK;, azam.nekoo@icr.ac.uk); Grall, Amélie (Probayes, 38330 Montbonnot, France;, ameliegrall5@gmail.com); Malcolm, Paul (Department of Radiology, Norfolk & Norwich University Hospital, Norwich NR4 7UY, UK;, paul.malcolm@nnuh.nhs.uk); Zwiggelaar, Reyer (Department of Computer Science, Aberystwyth University, Aberystwyth SY23 3DB, UK)","Zwiggelaar, Reyer (Aberystwyth University)","Birbiri, Ufuk Cem (Middle East Technical University); Hamidinekoo, Azam (Institute of Cancer Research); Grall, Amélie (ProbaYes (France)); Malcolm, Paul (Norfolk and Norwich University Hospital); Zwiggelaar, Reyer (Aberystwyth University)",7,6,0.62,2.35,https://www.mdpi.com/2313-433X/6/9/83/pdf?version=1598258640,https://app.dimensions.ai/details/publication/pub.1130324716,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,
5878,pub.1128884667,10.1088/1361-6560/aba166,32604080,,Automated stroke lesion segmentation in non-contrast CT scans using dense multi-path contextual generative adversarial network,"Stroke lesion volume is a key radiologic measurement in assessing prognosis of acute ischemic stroke (AIS) patients. The aim of this paper is to develop an automated segmentation method for accurately segmenting follow-up ischemic and hemorrhagic lesion from multislice non-contrast CT (NCCT) volumes of AIS patients. This paper proposes a 2D dense multi-path contextual generative adversarial network (MPC-GAN) where a dense multi-path 2D U-Net is utilized as the generator and a discriminator network is applied to regularize the generator. Contextual information (i.e. bilateral intensity difference, distance map and lesion location probability) are input into the generator and discriminator. The proposed method is validated separately on follow-up NCCT volumes of 60 patients with ischemic infarcts and NCCT volumes of 70 patients with hemorrhages. Quantitative results demonstrated that the proposed MPC-GAN method obtained a Dice coefficient (DC) of 70.6% for ischemic infarct segmentation and a DC of 76.5% for hemorrhage segmentation compared with manual segmented lesions, outperforming several benchmark methods. Additional volumetric analyses demonstrated that the MPC-GAN segmented lesion volume correlated well with manual measurements (Pearson correlation coefficients were 0.926 and 0.927 for ischemic infarcts and hemorrhages, respectively). The proposed MPC-GAN method can accurately segment ischemic infarcts and hemorrhages from NCCT volumes of AIS patients.",This study was supported by fundings from the Canadian Institutes of Health Research and Alberta Innovate Health Solutions.,,Physics in Medicine and Biology,,"Female; Humans; Image Processing, Computer-Assisted; Male; Neural Networks, Computer; Stroke; Tomography, X-Ray Computed",2020-11-05,2020,2020-11-05,2020-11-07,65,21,215013,Closed,Article,"Kuang, Hulin; Menon, Bijoy K; Qiu, Wu","Kuang, Hulin (Department of Clinical Neurosciences, University of Calgary, Calgary, Alberta, T2N 2T9, Canada); Menon, Bijoy K (Department of Clinical Neurosciences, University of Calgary, Calgary, Alberta, T2N 2T9, Canada); Qiu, Wu (Department of Clinical Neurosciences, University of Calgary, Calgary, Alberta, T2N 2T9, Canada)","Kuang, Hulin (University of Calgary); Menon, Bijoy K (University of Calgary); Qiu, Wu (University of Calgary)","Kuang, Hulin (University of Calgary); Menon, Bijoy K (University of Calgary); Qiu, Wu (University of Calgary)",8,8,0.98,6.07,,https://app.dimensions.ai/details/publication/pub.1128884667,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
5878,pub.1144526662,10.1007/s00330-021-08408-5,35001157,,Combined model-based and deep learning-based automated 3D zonal segmentation of the prostate on T2-weighted MR images: clinical evaluation,"Objective
To train and to test for prostate zonal segmentation an existing algorithm already trained for whole-gland segmentation.MethodsThe algorithm, combining model-based and deep learning–based approaches, was trained for zonal segmentation using the NCI-ISBI-2013 dataset and 70 T2-weighted datasets acquired at an academic centre. Test datasets were randomly selected among examinations performed at this centre on one of two scanners (General Electric, 1.5 T; Philips, 3 T) not used for training. Automated segmentations were corrected by two independent radiologists. When segmentation was initiated outside the prostate, images were cropped and segmentation repeated. Factors influencing the algorithm’s mean Dice similarity coefficient (DSC) and its precision were assessed using beta regression.ResultsEighty-two test datasets were selected; one was excluded. In 13/81 datasets, segmentation started outside the prostate, but zonal segmentation was possible after image cropping. Depending on the radiologist chosen as reference, algorithm’s median DSCs were 96.4/97.4%, 91.8/93.0% and 79.9/89.6% for whole-gland, central gland and anterior fibromuscular stroma (AFMS) segmentations, respectively. DSCs comparing radiologists’ delineations were 95.8%, 93.6% and 81.7%, respectively. For all segmentation tasks, the scanner used for imaging significantly influenced the mean DSC and its precision, and the mean DSC was significantly lower in cases with initial segmentation outside the prostate. For central gland segmentation, the mean DSC was also significantly lower in larger prostates. The radiologist chosen as reference had no significant impact, except for AFMS segmentation.ConclusionsThe algorithm performance fell within the range of inter-reader variability but remained significantly impacted by the scanner used for imaging.Key Points• Median Dice similarity coefficients obtained by the algorithm fell within human inter-reader variability for the three segmentation tasks (whole gland, central gland, anterior fibromuscular stroma).• The scanner used for imaging significantly impacted the performance of the automated segmentation for the three segmentation tasks.• The performance of the automated segmentation of the anterior fibromuscular stroma was highly variable across patients and showed also high variability across the two radiologists.",,This research project was sponsored and funded by the Hospices Civils de Lyon and performed under the framework of the collaboration between the Hospices Civils de Lyon and Philips that is part of the GOPI public contract whose holder is Philips.,European Radiology,,"Algorithms; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Pelvis; Prostate",2022-01-10,2022,2022-01-10,2022-05,32,5,3248-3259,Closed,Article,"Rouvière, Olivier; Moldovan, Paul Cezar; Vlachomitrou, Anna; Gouttard, Sylvain; Riche, Benjamin; Groth, Alexandra; Rabotnikov, Mark; Ruffion, Alain; Colombel, Marc; Crouzet, Sébastien; Weese, Juergen; Rabilloud, Muriel","Rouvière, Olivier (Department of Urinary and Vascular Imaging, Hôpital Edouard Herriot, Hospices Civils de Lyon, Pavillon B, 5 place d’Arsonval, F-69437, Lyon, France; Université de Lyon, F-69003, Lyon, France; Faculté de Médecine Lyon Est, Université Lyon 1, F-69003, Lyon, France; INSERM, LabTau, U1032, Lyon, France); Moldovan, Paul Cezar (Department of Urinary and Vascular Imaging, Hôpital Edouard Herriot, Hospices Civils de Lyon, Pavillon B, 5 place d’Arsonval, F-69437, Lyon, France); Vlachomitrou, Anna (Philips France, 33 rue de Verdun, CS 60 055, 92156, Suresnes Cedex, France); Gouttard, Sylvain (Department of Urinary and Vascular Imaging, Hôpital Edouard Herriot, Hospices Civils de Lyon, Pavillon B, 5 place d’Arsonval, F-69437, Lyon, France); Riche, Benjamin (Service de Biostatistique Et Bioinformatique, Pôle Santé Publique, Hospices Civils de Lyon, F-69003, Lyon, France; Laboratoire de Biométrie Et Biologie Évolutive, Équipe Biostatistique-Santé, UMR 5558, CNRS, F-69100, Villeurbanne, France); Groth, Alexandra (Philips Research, Röntgenstrasse 24-26, 22335, Hamburg, Germany); Rabotnikov, Mark (Philips, MATAM Industrial Park, 3508409, Haifa, Israel); Ruffion, Alain (Department of Urology, Centre Hospitalier Lyon Sud, Hospices Civils de Lyon, F-69310, Pierre-Bénite, France); Colombel, Marc (Université de Lyon, F-69003, Lyon, France; Faculté de Médecine Lyon Est, Université Lyon 1, F-69003, Lyon, France; Department of Urology, Hôpital Edouard Herriot, Hospices Civils de Lyon, F-69437, Lyon, France); Crouzet, Sébastien (Department of Urology, Hôpital Edouard Herriot, Hospices Civils de Lyon, F-69437, Lyon, France); Weese, Juergen (Philips Research, Röntgenstrasse 24-26, 22335, Hamburg, Germany); Rabilloud, Muriel (Université de Lyon, F-69003, Lyon, France; Faculté de Médecine Lyon Est, Université Lyon 1, F-69003, Lyon, France; Service de Biostatistique Et Bioinformatique, Pôle Santé Publique, Hospices Civils de Lyon, F-69003, Lyon, France; Laboratoire de Biométrie Et Biologie Évolutive, Équipe Biostatistique-Santé, UMR 5558, CNRS, F-69100, Villeurbanne, France)","Rouvière, Olivier (Hôpital Édouard-Herriot; University of Lyon System; Claude Bernard University Lyon 1; Laboratory of Therapeutic Applications of Ultrasound)","Rouvière, Olivier (Hôpital Édouard-Herriot; University of Lyon System; Claude Bernard University Lyon 1; Laboratory of Therapeutic Applications of Ultrasound); Moldovan, Paul Cezar (Hôpital Édouard-Herriot); Vlachomitrou, Anna (Philips (France)); Gouttard, Sylvain (Hôpital Édouard-Herriot); Riche, Benjamin (Hospices Civils de Lyon; Biometry and Evolutionary Biology Laboratory); Groth, Alexandra (Philips (Germany)); Rabotnikov, Mark (Philips (Israel)); Ruffion, Alain (Centre Hospitalier Lyon Sud); Colombel, Marc (University of Lyon System; Claude Bernard University Lyon 1; Hôpital Édouard-Herriot); Crouzet, Sébastien (Hôpital Édouard-Herriot); Weese, Juergen (Philips (Germany)); Rabilloud, Muriel (University of Lyon System; Claude Bernard University Lyon 1; Hospices Civils de Lyon; Biometry and Evolutionary Biology Laboratory)",4,4,,,,https://app.dimensions.ai/details/publication/pub.1144526662,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
5865,pub.1122090655,10.1016/j.ejrad.2019.108716,31707168,,Variability of manual segmentation of the prostate in axial T2-weighted MRI: A multi-reader study,"PURPOSE: To evaluate the interreader variability in prostate and seminal vesicle (SV) segmentation on T2w MRI.
METHODS: Six readers segmented the peripheral zone (PZ), transitional zone (TZ) and SV slice-wise on axial T2w prostate MRI examinations of n = 80 patients. Twenty different similarity scores, including dice score (DS), Hausdorff distance (HD) and volumetric similarity coefficient (VS), were computed with the VISCERAL EvaluateSegmentation software for all structures combined and separately for the whole gland (WG = PZ + TZ), TZ and SV. Differences between base, midgland and apex were evaluated with DS slice-wise. Descriptive statistics for similarity scores were computed. Wilcoxon testing to evaluate differences of DS, HD and VS was performed.
RESULTS: Overall segmentation variability was good with a mean DS of 0.859 (±SD = 0.0542), HD of 36.6 (±34.9 voxels) and VS of 0.926 (±0.065). The WG showed a DS, HD and VS of 0.738 (±0.144), 36.2 (±35.6 vx) and 0.853 (±0.143), respectively. The TZ showed generally lower variability with a DS of 0.738 (±0.144), HD of 24.8 (±16 vx) and VS of 0.908 (±0.126). The lowest variability was found for the SV with DS of 0.884 (±0.0407), HD of 17 (±10.9 vx) and VS of 0.936 (±0.0509). We found a markedly lower DS of the segmentations in the apex (0.85 ± 0.12) compared to the base (0.87 ± 0.10, p < 0.01) and the midgland (0.89 ± 0.10, p < 0.001).
CONCLUSIONS: We report baseline values for interreader variability of prostate and SV segmentation on T2w MRI. Variability was highest in the apex, lower in the base, and lowest in the midgland.",None.,,European Journal of Radiology,,"Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Prostate; Prostatic Neoplasms; Reproducibility of Results",2019-10-25,2019,2019-10-25,2019-12,121,,108716,All OA, Green,Article,"Becker, Anton S; Chaitanya, Krishna; Schawkat, Khoschy; Muehlematter, Urs J; Hötker, Andreas M; Konukoglu, Ender; Donati, Olivio F","Becker, Anton S (Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Zurich, Switzerland; Department of Radiology, Memorial Sloan Kettering Cancer Center, New York City, USA. Electronic address: anton.becker@usz.ch.); Chaitanya, Krishna (Computer Vision Laboratory, Department of Information Technology and Electrical Engineering, ETH Zurich, Switzerland.); Schawkat, Khoschy (Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Zurich, Switzerland; Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, USA.); Muehlematter, Urs J (Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Zurich, Switzerland.); Hötker, Andreas M (Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Zurich, Switzerland.); Konukoglu, Ender (Computer Vision Laboratory, Department of Information Technology and Electrical Engineering, ETH Zurich, Switzerland.); Donati, Olivio F (Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Zurich, Switzerland.)","Becker, Anton S (University Hospital of Zurich; Memorial Sloan Kettering Cancer Center)","Becker, Anton S (University Hospital of Zurich; Memorial Sloan Kettering Cancer Center); Chaitanya, Krishna (ETH Zurich); Schawkat, Khoschy (University Hospital of Zurich; Beth Israel Deaconess Medical Center; Harvard University); Muehlematter, Urs J (University Hospital of Zurich); Hötker, Andreas M (University Hospital of Zurich); Konukoglu, Ender (ETH Zurich); Donati, Olivio F (University Hospital of Zurich)",27,20,1.45,9.54,https://www.zora.uzh.ch/id/eprint/176243/1/1-s2.0-S0720048X19303663-main.pdf,https://app.dimensions.ai/details/publication/pub.1122090655,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
5842,pub.1100463243,10.1117/1.jmi.5.2.021208,29376105,PMC5771127,PSNet: prostate segmentation on MRI based on a convolutional neural network,"Automatic segmentation of the prostate on magnetic resonance images (MRI) has many applications in prostate cancer diagnosis and therapy. We proposed a deep fully convolutional neural network (CNN) to segment the prostate automatically. Our deep CNN model is trained end-to-end in a single learning stage, which uses prostate MRI and the corresponding ground truths as inputs. The learned CNN model can be used to make an inference for pixel-wise segmentation. Experiments were performed on three data sets, which contain prostate MRI of 140 patients. The proposed CNN model of prostate segmentation (PSNet) obtained a mean Dice similarity coefficient of [Formula: see text] as compared to the manually labeled ground truth. Experimental results show that the proposed model could yield satisfactory segmentation of the prostate on MRI.","This research was supported in part by NIH Grant Nos. CA176684, CA156775, and CA204254.",,Journal of Medical Imaging,,,2018-01-17,2018,2018-01-17,2018-04,5,2,021208-021208,All OA, Green,Article,"Tian, Zhiqiang; Liu, Lizhi; Zhang, Zhenfeng; Fei, Baowei","Tian, Zhiqiang (Xi’an Jiaotong University, School of Software Engineering, Xi’an, China; Emory University School of Medicine, Department of Radiology and Imaging Sciences, Atlanta, Georgia, United States); Liu, Lizhi (Emory University School of Medicine, Department of Radiology and Imaging Sciences, Atlanta, Georgia, United States); Zhang, Zhenfeng (The Second Hospital of Guangzhou Medical University, Department of Radiology, Guangzhou, China); Fei, Baowei (Emory University School of Medicine, Department of Radiology and Imaging Sciences, Atlanta, Georgia, United States; Georgia Institute of Technology and Emory University, Wallace H. Coulter Department of Biomedical Engineering, Atlanta, Georgia, United States; Winship Cancer Institute of Emory University, Atlanta, Georgia, United States; Emory University, Department of Mathematics and Computer Science, Atlanta, Georgia, United States)","Fei, Baowei (Emory University; The Wallace H. Coulter Department of Biomedical Engineering; Winship Cancer Institute; Emory University)","Tian, Zhiqiang (Xi'an Jiaotong University; Emory University); Liu, Lizhi (Emory University); Zhang, Zhenfeng (Guangzhou Medical University); Fei, Baowei (Emory University; The Wallace H. Coulter Department of Biomedical Engineering; Winship Cancer Institute; Emory University)",75,36,3.58,21.89,https://europepmc.org/articles/pmc5771127?pdf=render,https://app.dimensions.ai/details/publication/pub.1100463243,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
5726,pub.1147570820,10.1002/mp.15698,35506596,,A novel registration‐based algorithm for prostate segmentation via the combination of SSM and CNN,"PURPOSE: Precise determination of target is an essential procedure in prostate interventions, such as prostate biopsy, lesion detection, and targeted therapy. However, the prostate delineation may be tough in some cases due to tissue ambiguity or lack of partial anatomical boundary. In this study, we propose a novel supervised registration-based algorithm for precise prostate segmentation, which combines the convolutional neural network (CNN) with a statistical shape model (SSM).
METHODS: The proposed network mainly consists of two branches. One called SSM-Net branch was exploited to predict the shape transform matrix, shape control parameters, and shape fine-tuning vector, for the generation of the prostate boundary. Furthermore, according to the inferred boundary, a normalized distance map was calculated as the output of SSM-Net. Another branch named ResU-Net was employed to predict a probability label map from the input images at the same time. Integrating the output of these two branches, the optimal weighted sum of the distance map and the probability map was regarded as the prostate segmentation.
RESULTS: Two public data sets PROMISE12 and NCI-ISBI 2013 were utilized to evaluate the performance of the proposed algorithm. The results demonstrated that the segmentation algorithm achieved the best performance with an SSM of 9500 nodes, which obtained a dice of 0.907 and an average surface distance of 1.85 mm. Compared with other methods, our algorithm delineates the prostate region more accurately and efficiently. In addition, we verified the impact of model elasticity augmentation and the fine-tuning item on the network segmentation capability. As a result, both factors have improved the delineation accuracy, with dice increased by 10% and 7%, respectively.
CONCLUSIONS: Our segmentation method has the potential to be an effective and robust approach for prostate segmentation.",None.,,Medical Physics,,"Algorithms; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Models, Statistical; Neural Networks, Computer; Prostate",2022-05-11,2022,2022-05-11,2022-08,49,8,5268-5282,All OA, Green,Article,"Qin, Chunxia; Tu, Puxun; Chen, Xiaojun; Troccaz, Jocelyne","Qin, Chunxia (School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China); Tu, Puxun (School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China); Chen, Xiaojun (School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China); Troccaz, Jocelyne (Université Grenoble Alpes, CNRS, Grenoble INP, TIMC, Grenoble, France)","Chen, Xiaojun (Shanghai Jiao Tong University)","Qin, Chunxia (Shanghai Jiao Tong University; Shanghai Jiao Tong University); Tu, Puxun (Shanghai Jiao Tong University); Chen, Xiaojun (Shanghai Jiao Tong University); Troccaz, Jocelyne (Grenoble Institute of Technology)",0,0,,,https://hal.archives-ouvertes.fr/hal-03654900/file/Accepted%20version-MedPhy-BW.pdf,https://app.dimensions.ai/details/publication/pub.1147570820,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
5718,pub.1140573020,10.1002/mp.15181,34418108,,Fully automated detection of prostate transition zone tumors on T2‐weighted and apparent diffusion coefficient (ADC) map MR images using U‐Net ensemble,"PURPOSE: Accurate detection of transition zone (TZ) prostate cancer (PCa) on magnetic resonance imaging (MRI) remains challenging using clinical subjective assessment due to overlap between PCa and benign prostatic hyperplasia (BPH). The objective of this paper is to describe a deep-learning-based framework for fully automated detection of PCa in the TZ using T2-weighted (T2W) and apparent diffusion coefficient (ADC) map MR images.
METHOD: This was a single-center IRB-approved cross-sectional study of men undergoing 3T MRI on two systems. The dataset consisted of 196 patients (103 with and 93 without clinically significant [Grade Group 2 or higher] TZ PCa) to train and test our proposed methodology, with an additional 168 patients with peripheral zone PCa used only for training. We proposed an ensemble of classifiers in which multiple U-Net-based models are designed for prediction of TZ PCa location on ADC map MR images, with initial automated segmentation of the prostate to guide detection. We compared accuracy of ADC alone to T2W and combined ADC+T2W MRI for input images, and investigated improvements using ensembles over their constituent models with different methods of diversity in individual models by hyperparameter configuration, loss function and model architecture.
RESULTS: Our developed algorithm reported sensitivity and precision of 0.829 and 0.617 in 56 test cases containing 31 instances of TZ PCa and in 25 patients without clinically significant TZ tumors. Patient-wise classification accuracy had an area under receiver operator characteristic curve (AUROC) of 0.974. Single U-Net models using ADC alone (sensitivity 0.829, precision 0.534) outperformed assessment using T2W (sensitivity 0.086, precision 0.081) and assessment using combined ADC+T2W (sensitivity 0.687, precision 0.489). While the ensemble of U-Nets with varying hyperparameters demonstrated the highest performance, all ensembles improved PCa detection compared to individual models, with sensitivities and precisions close to the collective best of constituent models.
CONCLUSION: We describe a deep-learning-based method for fully automated TZ PCa detection using ADC map MR images that outperformed assessment by T2W and ADC+T2W.","We acknowledge Drs. Steven Currin, Alayed Abdullahand, and Sabarish Narayanasamy for their contribution toward segmentation of prostate MR images and Dr. Trevor A Flood for his contribution in creating the prostate MRI‐Radical Prostatectomy database. We also acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). This research was enabled in part by support provided by Compute Ontario (www.computeontario.ca) and Compute Canada (www.computecanada.ca).",,Medical Physics,,Cross-Sectional Studies, Diffusion Magnetic Resonance Imaging, Humans, Magnetic Resonance Imaging, Male, Prostatic Neoplasms, Retrospective Studies,2021-08-30,2021,2021-08-30,2021-11,48,11,6889-6900,Closed,Article,"Wong, Timothy; Schieda, Nicola; Sathiadoss, Paul; Haroon, Mohammad; Abreu‐Gomez, Jorge; Ukwatta, Eranga","Wong, Timothy (School of Engineering, University of Guelph, Guelph, ON, Canada); Schieda, Nicola (Department of Radiology, University of Ottawa, Ottawa, ON, Canada); Sathiadoss, Paul (Department of Radiology, University of Ottawa, Ottawa, ON, Canada); Haroon, Mohammad (Department of Radiology, University of Ottawa, Ottawa, ON, Canada); Abreu‐Gomez, Jorge (Joint Department of Medical Imaging, University of Toronto, Toronto, ON, Canada); Ukwatta, Eranga (School of Engineering, University of Guelph, Guelph, ON, Canada)","Ukwatta, Eranga (University of Guelph)","Wong, Timothy (University of Guelph); Schieda, Nicola (University of Ottawa); Sathiadoss, Paul (University of Ottawa); Haroon, Mohammad (University of Ottawa); Abreu‐Gomez, Jorge (University of Toronto); Ukwatta, Eranga (University of Guelph)",3,3,1.0,2.05,,https://app.dimensions.ai/details/publication/pub.1140573020,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,
5718,pub.1132291083,10.1016/j.cmpb.2020.105821,33218704,,Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from Multi-Planar MRI,"BACKGROUND AND OBJECTIVE: Accurate and reliable segmentation of the prostate gland in MR images can support the clinical assessment of prostate cancer, as well as the planning and monitoring of focal and loco-regional therapeutic interventions. Despite the availability of multi-planar MR scans due to standardized protocols, the majority of segmentation approaches presented in the literature consider the axial scans only. In this work, we investigate whether a neural network processing anisotropic multi-planar images could work in the context of a semantic segmentation task, and if so, how this additional information would improve the segmentation quality.
METHODS: We propose an anisotropic 3D multi-stream CNN architecture, which processes additional scan directions to produce a high-resolution isotropic prostate segmentation. We investigate two variants of our architecture, which work on two (dual-plane) and three (triple-plane) image orientations, respectively. The influence of additional information used by these models is evaluated by comparing them with a single-plane baseline processing only axial images. To realize a fair comparison, we employ a hyperparameter optimization strategy to select optimal configurations for the individual approaches.
RESULTS: Training and evaluation on two datasets spanning multiple sites show statistical significant improvement over the plain axial segmentation (p<0.05 on the Dice similarity coefficient). The improvement can be observed especially at the base (0.898 single-plane vs. 0.906 triple-plane) and apex (0.888 single-plane vs. 0.901 dual-plane).
CONCLUSION: This study indicates that models employing two or three scan directions are superior to plain axial segmentation. The knowledge of precise boundaries of the prostate is crucial for the conservation of risk structures. Thus, the proposed models have the potential to improve the outcome of prostate cancer diagnosis and therapies.",,,Computer Methods and Programs in Biomedicine,,"Anisotropy; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate",2020-11-04,2020,2020-11-04,2021-03,200,,105821,All OA, Green,Article,"Meyer, Anneke; Chlebus, Grzegorz; Rak, Marko; Schindele, Daniel; Schostak, Martin; van Ginneken, Bram; Schenk, Andrea; Meine, Hans; Hahn, Horst K; Schreiber, Andreas; Hansen, Christian","Meyer, Anneke (Faculty of Computer Science and Research Campus STIMULATE, University of Magdeburg, Germany. Electronic address: anneke.meyer@ovgu.de.); Chlebus, Grzegorz (Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany; Radboud University Medical Center, Nijmegen, The Netherlands.); Rak, Marko (Faculty of Computer Science and Research Campus STIMULATE, University of Magdeburg, Germany.); Schindele, Daniel (Clinic of Urology and Pediatric Urology, University Hospital Magdeburg, Germany.); Schostak, Martin (Clinic of Urology and Pediatric Urology, University Hospital Magdeburg, Germany.); van Ginneken, Bram (Radboud University Medical Center, Nijmegen, The Netherlands; Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany.); Schenk, Andrea (Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany.); Meine, Hans (University of Bremen, Medical Image Computing Group, Bremen, Germany; Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany.); Hahn, Horst K (Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany.); Schreiber, Andreas (Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany.); Hansen, Christian (Faculty of Computer Science and Research Campus STIMULATE, University of Magdeburg, Germany.)","Meyer, Anneke (Otto-von-Guericke University Magdeburg)","Meyer, Anneke (Otto-von-Guericke University Magdeburg); Chlebus, Grzegorz (Fraunhofer Institute for Digital Medicine; Radboud University Nijmegen Medical Centre); Rak, Marko (Otto-von-Guericke University Magdeburg); Schindele, Daniel (University Hospital Magdeburg); Schostak, Martin (University Hospital Magdeburg); van Ginneken, Bram (Radboud University Nijmegen Medical Centre; Fraunhofer Institute for Digital Medicine); Schenk, Andrea (Fraunhofer Institute for Digital Medicine); Meine, Hans (University of Bremen; Fraunhofer Institute for Digital Medicine); Hahn, Horst K (Fraunhofer Institute for Digital Medicine); Schreiber, Andreas (Fraunhofer Institute for Digital Medicine); Hansen, Christian (Otto-von-Guericke University Magdeburg)",17,17,3.47,10.16,http://arxiv.org/pdf/2009.11120,https://app.dimensions.ai/details/publication/pub.1132291083,46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,,
5711,pub.1146268739,10.21037/qims-21-459,35502381,PMC9014147,Automated knee cartilage segmentation for heterogeneous clinical MRI using generative adversarial networks with transfer learning,"Background: This study aimed to build a deep learning model to automatically segment heterogeneous clinical MRI scans by optimizing a pre-trained model built from a homogeneous research dataset with transfer learning.
Methods: Conditional generative adversarial networks pretrained on the Osteoarthritis Initiative MR images was transferred to 30 sets of heterogenous MR images collected from clinical routines. Two trained radiologists manually segmented the 30 sets of clinical MR images for model training, validation and test. The model performance was compared to models trained from scratch with different datasets, as well as two radiologists. A 5-fold cross validation was performed.
Results: The transfer learning model obtained an overall averaged Dice coefficient of 0.819, an averaged 95 percentile Hausdorff distance of 1.463 mm, and an averaged average symmetric surface distance of 0.350 mm on the 5 random holdout test sets. A 5-fold cross validation had a mean Dice coefficient of 0.801, mean 95 percentile Hausdorff distance of 1.746 mm, and mean average symmetric surface distance of 0.364 mm. It outperformed other models and performed similarly as the radiologists.
Conclusions: A transfer learning model was able to automatically segment knee cartilage, with performance comparable to human, using heterogeneous clinical MR images with a small training data size. In addition, the model proved robust when tested through cross validation and on images from a different vendor. We found it feasible to perform fully automated cartilage segmentation of clinical knee MR images, which would facilitate the clinical application of quantitative MRI techniques and other prediction models for improved patient treatment planning.",,,Quantitative Imaging in Medicine and Surgery,,,2022-05,2022,2022-05,2022-05,0,0,0-0,All OA, Gold,Article,"Yang, Mingrui; Colak, Ceylan; Chundru, Kishore K; Gaj, Sibaji; Nanavati, Andreas; Jones, Morgan H; Winalski, Carl S; Subhas, Naveen; Li, Xiaojuan","Yang, Mingrui (Department of Biomedical Engineering, Lerner Research Institute, Cleveland Clinic, Cleveland, OH, USA.; Program of Advanced Musculoskeletal Imaging (PAMI), Cleveland Clinic, Cleveland, OH, USA.); Colak, Ceylan (Department of Diagnostic Radiology, Imaging Institute, Cleveland Clinic, Cleveland, OH, USA.); Chundru, Kishore K (Department of Diagnostic Radiology, Imaging Institute, Cleveland Clinic, Cleveland, OH, USA.); Gaj, Sibaji (Department of Biomedical Engineering, Lerner Research Institute, Cleveland Clinic, Cleveland, OH, USA.; Program of Advanced Musculoskeletal Imaging (PAMI), Cleveland Clinic, Cleveland, OH, USA.); Nanavati, Andreas (Department of Biomedical Engineering, Lerner Research Institute, Cleveland Clinic, Cleveland, OH, USA.; Program of Advanced Musculoskeletal Imaging (PAMI), Cleveland Clinic, Cleveland, OH, USA.); Jones, Morgan H (Department of Orthopaedic Surgery, Brigham and Women's Hospital, Boston, MA, USA.); Winalski, Carl S (Department of Biomedical Engineering, Lerner Research Institute, Cleveland Clinic, Cleveland, OH, USA.; Program of Advanced Musculoskeletal Imaging (PAMI), Cleveland Clinic, Cleveland, OH, USA.; Department of Diagnostic Radiology, Imaging Institute, Cleveland Clinic, Cleveland, OH, USA.); Subhas, Naveen (Program of Advanced Musculoskeletal Imaging (PAMI), Cleveland Clinic, Cleveland, OH, USA.; Department of Diagnostic Radiology, Imaging Institute, Cleveland Clinic, Cleveland, OH, USA.); Li, Xiaojuan (Department of Biomedical Engineering, Lerner Research Institute, Cleveland Clinic, Cleveland, OH, USA.; Program of Advanced Musculoskeletal Imaging (PAMI), Cleveland Clinic, Cleveland, OH, USA.; Department of Diagnostic Radiology, Imaging Institute, Cleveland Clinic, Cleveland, OH, USA.)",,"Yang, Mingrui (Cleveland Clinic; Cleveland Clinic); Colak, Ceylan (Cleveland Clinic); Chundru, Kishore K (Cleveland Clinic); Gaj, Sibaji (Cleveland Clinic; Cleveland Clinic); Nanavati, Andreas (Cleveland Clinic; Cleveland Clinic); Jones, Morgan H (Brigham and Women's Hospital); Winalski, Carl S (Cleveland Clinic; Cleveland Clinic; Cleveland Clinic); Subhas, Naveen (Cleveland Clinic; Cleveland Clinic); Li, Xiaojuan (Cleveland Clinic; Cleveland Clinic; Cleveland Clinic)",3,3,,,https://qims.amegroups.com/article/viewFile/89910/pdf,https://app.dimensions.ai/details/publication/pub.1146268739,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
5694,pub.1134697835,10.3390/app11020782,33680505,PMC7932306,Deep Learning-Based Methods for Prostate Segmentation in Magnetic Resonance Imaging,"Magnetic Resonance Imaging-based prostate segmentation is an essential task for adaptive radiotherapy and for radiomics studies whose purpose is to identify associations between imaging features and patient outcomes. Because manual delineation is a time-consuming task, we present three deep-learning (DL) approaches, namely UNet, efficient neural network (ENet), and efficient residual factorized convNet (ERFNet), whose aim is to tackle the fully-automated, real-time, and 3D delineation process of the prostate gland on T2-weighted MRI. While UNet is used in many biomedical image delineation applications, ENet and ERFNet are mainly applied in self-driving cars to compensate for limited hardware availability while still achieving accurate segmentation. We apply these models to a limited set of 85 manual prostate segmentations using the k-fold validation strategy and the Tversky loss function and we compare their results. We find that ENet and UNet are more accurate than ERFNet, with ENet much faster than UNet. Specifically, ENet obtains a dice similarity coefficient of 90.89% and a segmentation time of about 6 s using central processing unit (CPU) hardware to simulate real clinical conditions where graphics processing unit (GPU) is not always available. In conclusion, ENet could be efficiently applied for prostate delineation even in small image training datasets with potential benefit for patient management personalization.","This work was partially supported by grant W911NF-18-1-0281, funded by the USA Army Research Office (ARO): “Extending Accelerated Optimization into the PDE Framework”, and by grant R01-HL-143350 funded by the National Institute of Health (NIH) “Quantification of myocardial blood flow using Dynamic PET/CTA fused imagery to determine physiological significance of specific coronary lesions”.",This research received no external funding.,Applied Sciences,,,2021-01-02,2021,2021-01-15,2021-01-02,11,2,782,All OA, Gold,Article,"Comelli, Albert; Dahiya, Navdeep; Stefano, Alessandro; Vernuccio, Federica; Portoghese, Marzia; Cutaia, Giuseppe; Bruno, Alberto; Salvaggio, Giuseppe; Yezzi, Anthony","Comelli, Albert (Ri.MED Foundation, Via Bandiera, 11, 90133 Palermo, Italy;, acomelli@fondazionerimed.com; Institute of Molecular Bioimaging and Physiology, National Research Council (IBFM-CNR), 90015 Cefalù, Italy); Dahiya, Navdeep (Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA;, ndahiya3@gatech.edu, (N.D.);, anthony.yezzi@ece.gatech.edu, (A.Y.)); Stefano, Alessandro (Institute of Molecular Bioimaging and Physiology, National Research Council (IBFM-CNR), 90015 Cefalù, Italy); Vernuccio, Federica (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, 90127 Palermo, Italy;, federica.vernuccio@unipa.it, (F.V.);, marzia.portoghese@unipa.it, (M.P.);, giuseppe.cutaia@community.unipa.it, (G.C.);, bruno-alberto@hotmail.it, (A.B.);, p.salvaggio@libero.it, (G.S.)); Portoghese, Marzia (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, 90127 Palermo, Italy;, federica.vernuccio@unipa.it, (F.V.);, marzia.portoghese@unipa.it, (M.P.);, giuseppe.cutaia@community.unipa.it, (G.C.);, bruno-alberto@hotmail.it, (A.B.);, p.salvaggio@libero.it, (G.S.)); Cutaia, Giuseppe (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, 90127 Palermo, Italy;, federica.vernuccio@unipa.it, (F.V.);, marzia.portoghese@unipa.it, (M.P.);, giuseppe.cutaia@community.unipa.it, (G.C.);, bruno-alberto@hotmail.it, (A.B.);, p.salvaggio@libero.it, (G.S.)); Bruno, Alberto (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, 90127 Palermo, Italy;, federica.vernuccio@unipa.it, (F.V.);, marzia.portoghese@unipa.it, (M.P.);, giuseppe.cutaia@community.unipa.it, (G.C.);, bruno-alberto@hotmail.it, (A.B.);, p.salvaggio@libero.it, (G.S.)); Salvaggio, Giuseppe (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, 90127 Palermo, Italy;, federica.vernuccio@unipa.it, (F.V.);, marzia.portoghese@unipa.it, (M.P.);, giuseppe.cutaia@community.unipa.it, (G.C.);, bruno-alberto@hotmail.it, (A.B.);, p.salvaggio@libero.it, (G.S.)); Yezzi, Anthony (Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA;, ndahiya3@gatech.edu, (N.D.);, anthony.yezzi@ece.gatech.edu, (A.Y.))","Stefano, Alessandro ","Comelli, Albert (Ri.MED); Dahiya, Navdeep (Georgia Institute of Technology); Stefano, Alessandro (); Vernuccio, Federica (University of Palermo); Portoghese, Marzia (University of Palermo); Cutaia, Giuseppe (University of Palermo); Bruno, Alberto (University of Palermo); Salvaggio, Giuseppe (University of Palermo); Yezzi, Anthony (Georgia Institute of Technology)",37,37,5.48,31.65,https://www.mdpi.com/2076-3417/11/2/782/pdf?version=1610934718,https://app.dimensions.ai/details/publication/pub.1134697835,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,
5694,pub.1133371506,10.1002/mp.14651,33300190,,A new data augmentation method based on local image warping for medical image segmentation,"PURPOSE: The segmentation accuracy of medical images was improved by increasing the number of training samples using a local image warping technique. The performance of the proposed method was evaluated in the segmentation of breast masses, prostate and brain tumors, and lung nodules.
METHODS: We propose a simple data augmentation method which is called stochastic evolution (SE). Specifically, the idea of SE stems from our thinking about the deterioration of the diseased tissue and the healing process. In order to simulate this natural process, we implement it according to the local distortion algorithm in image warping. In other words, the irregular deterioration and healing processes of the diseased tissue is simulated according to the direction of the local distortion, thereby producing a natural sample that is indistinguishable by humans.
RESULTS: The proposed method is evaluated on four segmentation tasks of breast masses, prostate, brain tumors, and lung nodules. Comparing the experimental results of four segmentation methods based on the UNet segmentation architecture without adding any expanded data during training, the accuracy and the Hausdorff distance obtained in our approach remain almost the same as other methods. However, the dice similarity coefficient (DSC) and sensitivity (SEN) have both improved to some extent. Among them, DSC is increased by 5.2%, 2.8%, 1.0%, and 3.2%, respectively; SEN is increased by 6.9%, 4.3%, 1.2%, and 4.5%, respectively.
CONCLUSIONS: Experimental results show that the proposed SE data augmentation method could improve the segmentation accuracy of breast masses, prostate, brain tumors, and lung nodules. The method also shows the robustness with different image datasets and imaging modalities.",This work was supported by the National Key R&amp,D Program of China (Grant No. 2017YFC0112804) and the National Natural Science Foundation of China (Grant No. 81671768).,,Medical Physics,,"Algorithms; Breast; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostate",2021-03-09,2021,2021-03-09,2021-04,48,4,1685-1696,Closed,Article,"Liu, Hong; Cao, Haichao; Song, Enmin; Ma, Guangzhi; Xu, Xiangyang; Jin, Renchao; Liu, Tengying; Liu, Lei; Liu, Daiyang; Hung, Chih‐Cheng","Liu, Hong (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Cao, Haichao (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Song, Enmin (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Ma, Guangzhi (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Xu, Xiangyang (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Jin, Renchao (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Liu, Tengying (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Liu, Lei (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Liu, Daiyang (School of Computer Science & Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Hung, Chih‐Cheng (The Laboratory for Machine Vision and Security Research, Kennesaw State University, 1000 Chastain Rd., Kennesaw, GA, 30144, USA)","Song, Enmin (Huazhong University of Science and Technology)","Liu, Hong (Huazhong University of Science and Technology); Cao, Haichao (Huazhong University of Science and Technology); Song, Enmin (Huazhong University of Science and Technology); Ma, Guangzhi (Huazhong University of Science and Technology); Xu, Xiangyang (Huazhong University of Science and Technology); Jin, Renchao (Huazhong University of Science and Technology); Liu, Tengying (Huazhong University of Science and Technology); Liu, Lei (Huazhong University of Science and Technology); Liu, Daiyang (Huazhong University of Science and Technology); Hung, Chih‐Cheng (Kennesaw State University)",1,1,0.51,0.68,,https://app.dimensions.ai/details/publication/pub.1133371506,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
5694,pub.1127179658,10.1088/1361-6560/ab8cd6,32330922,PMC7771987,Label-driven magnetic resonance imaging (MRI)-transrectal ultrasound (TRUS) registration using weakly supervised learning for MRI-guided prostate radiotherapy,"Registration and fusion of magnetic resonance imaging (MRI) and transrectal ultrasound (TRUS) of the prostate can provide guidance for prostate brachytherapy. However, accurate registration remains a challenging task due to the lack of ground truth regarding voxel-level spatial correspondence, limited field of view, low contrast-to-noise ratio, and signal-to-noise ratio in TRUS. In this study, we proposed a fully automated deep learning approach based on a weakly supervised method to address these issues. We employed deep learning techniques to combine image segmentation and registration, including affine and nonrigid registration, to perform an automated deformable MRI-TRUS registration. To start with, we trained two separate fully convolutional neural networks (CNNs) to perform a pixel-wise prediction for MRI and TRUS prostate segmentation. Then, to provide the initialization of the registration, a 2D CNN was used to register MRI-TRUS prostate images using an affine registration. After that, a 3D UNET-like network was applied for nonrigid registration. For both the affine and nonrigid registration, pairs of MRI-TRUS labels were concatenated and fed into the neural networks for training. Due to the unavailability of ground-truth voxel-level correspondences and the lack of accurate intensity-based image similarity measures, we propose to use prostate label-derived volume overlaps and surface agreements as an optimization objective function for weakly supervised network training. Specifically, we proposed a hybrid loss function that integrated a Dice loss, a surface-based loss, and a bending energy regularization loss for the nonrigid registration. The Dice and surface-based losses were used to encourage the alignment of the prostate label between the MRI and the TRUS. The bending energy regularization loss was used to achieve a smooth deformation field. Thirty-six sets of patient data were used to test our registration method. The image registration results showed that the deformed MR image aligned well with the TRUS image, as judged by corresponding cysts and calcifications in the prostate. The quantitative results showed that our method produced a mean target registration error (TRE) of 2.53 ± 1.39 mm and a mean Dice loss of 0.91 ± 0.02. The mean surface distance (MSD) and Hausdorff distance (HD) between the registered MR prostate shape and TRUS prostate shape were 0.88 and 4.41 mm, respectively. This work presents a deep learning-based, weakly supervised network for accurate MRI-TRUS image registration. Our proposed method has achieved promising registration performance in terms of Dice loss, TRE, MSD, and HD.","This research is supported in part by the National Cancer Institute of the National Institutes of Health under Award Number R01CA215718 (XY), the Department of Defense (DoD) Prostate Cancer Research Program (PCRP) Award W81XWH-17-1-0438 (TL) and W81XWH-17-1-0439 (AJ) and the Dunwoody Golf Club Prostate Cancer Research Award (XY), a philanthropic award provided by the Winship Cancer Institute of Emory University.",,Physics in Medicine and Biology,,"Brachytherapy; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Prostatic Neoplasms; Radiotherapy, Image-Guided; Rectum; Supervised Machine Learning; Ultrasonography",2020-06-26,2020,2020-06-26,2020-07-07,65,13,135002,All OA, Green,Article,"Zeng, Qiulan; Fu, Yabo; Tian, Zhen; Lei, Yang; Zhang, Yupei; Wang, Tonghe; Mao, Hui; Liu, Tian; Curran, Walter J; Jani, Ashesh B; Patel, Pretesh; Yang, Xiaofeng","Zeng, Qiulan (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America); Fu, Yabo (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America); Tian, Zhen (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America; Winship Cancer Institute, Emory University, Atlanta, Georgia, United States of America); Lei, Yang (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America); Zhang, Yupei (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America); Wang, Tonghe (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America; Winship Cancer Institute, Emory University, Atlanta, Georgia, United States of America); Mao, Hui (Winship Cancer Institute, Emory University, Atlanta, Georgia, United States of America; Department of Radiology and Imaging Sciences, Emory University, Atlanta, Georgia, United States of America); Liu, Tian (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America; Winship Cancer Institute, Emory University, Atlanta, Georgia, United States of America); Curran, Walter J (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America; Winship Cancer Institute, Emory University, Atlanta, Georgia, United States of America); Jani, Ashesh B (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America; Winship Cancer Institute, Emory University, Atlanta, Georgia, United States of America); Patel, Pretesh (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America; Winship Cancer Institute, Emory University, Atlanta, Georgia, United States of America); Yang, Xiaofeng (Department of Radiation Oncology, Emory University, Atlanta, Georgia, United States of America; Winship Cancer Institute, Emory University, Atlanta, Georgia, United States of America)","Yang, Xiaofeng (Emory University; Winship Cancer Institute)","Zeng, Qiulan (Emory University); Fu, Yabo (Emory University); Tian, Zhen (Emory University; Winship Cancer Institute); Lei, Yang (Emory University); Zhang, Yupei (Emory University); Wang, Tonghe (Emory University; Winship Cancer Institute); Mao, Hui (Winship Cancer Institute; Emory University); Liu, Tian (Emory University; Winship Cancer Institute); Curran, Walter J (Emory University; Winship Cancer Institute); Jani, Ashesh B (Emory University; Winship Cancer Institute); Patel, Pretesh (Emory University; Winship Cancer Institute); Yang, Xiaofeng (Emory University; Winship Cancer Institute)",24,22,2.5,18.2,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7771987,https://app.dimensions.ai/details/publication/pub.1127179658,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
5692,pub.1140000339,10.1109/access.2021.3100585,34527506,PMC8438764,Training Convolutional Networks for Prostate Segmentation With Limited Data,"Multi-zonal segmentation is a critical component of computer-aided diagnostic systems for detecting and staging prostate cancer. Previously, convolutional neural networks such as the U-Net have been used to produce fully automatic multi-zonal prostate segmentation on magnetic resonance images (MRIs) with performance comparable to human experts, but these often require large amounts of manually segmented training data to produce acceptable results. For institutions that have limited amounts of labeled MRI exams, it is not clear how much data is needed to train a segmentation model, and which training strategy should be used to maximize the value of the available data. This work compares how the strategies of transfer learning and aggregated training using publicly available external data can improve segmentation performance on internal, site-specific prostate MR images, and evaluates how the performance varies with the amount of internal data used for training. Cross training experiments were performed to show that differences between internal and external data were impactful. Using a standard U-Net architecture, optimizations were performed to select between 2D and 3D variants, and to determine the depth of fine-tuning required for optimal transfer learning. With the optimized architecture, the performance of transfer learning and aggregated training were compared for a range of 5-40 internal datasets. The results show that both strategies consistently improve performance and produced segmentation results that are comparable to that of human experts with approximately 20 site-specific MRI datasets. These findings can help guide the development of site-specific prostate segmentation models for both clinical and research applications.","This work was supported in part by the National Institutes of Health under Grant P41 EB027061, in part by the U.S. Department of Defense under Grant W81XWH-15-1-0477, and in part by the University of Minnesota and NIH MN-REACH Program. The authors have no conflicts of interest.",,IEEE Access,,,2021-07-27,2021,2021-07-27,2021,9,,109214-109223,All OA, Gold,Article,"Saunders, Sara L.; Leng, Ethan; Spilseth, Benjamin; Wasserman, Neil; Metzger, Gregory J.; Bolan, Patrick J.","Saunders, Sara L. (Biomedical Engineering, University of Minnesota, Minneapolis, MN, 55455, USA); Leng, Ethan (Biomedical Engineering, University of Minnesota, Minneapolis, MN, 55455, USA); Spilseth, Benjamin (Department of Radiology, University of Minnesota, Minneapolis, MN, 55455, USA); Wasserman, Neil (Department of Radiology, University of Minnesota, Minneapolis, MN, 55455, USA); Metzger, Gregory J. (Department of Radiology, University of Minnesota, Minneapolis, MN, 55455, USA; Center for Magnetic Resonance Research, University of Minnesota, Minneapolis, MN, 55455, USA); Bolan, Patrick J. (Department of Radiology, University of Minnesota, Minneapolis, MN, 55455, USA; Center for Magnetic Resonance Research, University of Minnesota, Minneapolis, MN, 55455, USA)","Saunders, Sara L. (University of Minnesota)","Saunders, Sara L. (University of Minnesota); Leng, Ethan (University of Minnesota); Spilseth, Benjamin (University of Minnesota); Wasserman, Neil (University of Minnesota); Metzger, Gregory J. (University of Minnesota; University of Minnesota); Bolan, Patrick J. (University of Minnesota; University of Minnesota)",4,4,0.72,3.27,https://ieeexplore.ieee.org/ielx7/6287639/9312710/09499065.pdf,https://app.dimensions.ai/details/publication/pub.1140000339,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5692,pub.1128639481,10.1002/mp.14337,32564359,PMC7586964,Registration of presurgical MRI and histopathology images from radical prostatectomy via RAPSODI,"PURPOSE: Magnetic resonance imaging (MRI) has great potential to improve prostate cancer diagnosis; however, subtle differences between cancer and confounding conditions render prostate MRI interpretation challenging. The tissue collected from patients who undergo radical prostatectomy provides a unique opportunity to correlate histopathology images of the prostate with preoperative MRI to accurately map the extent of cancer from histopathology images onto MRI. We seek to develop an open-source, easy-to-use platform to align presurgical MRI and histopathology images of resected prostates in patients who underwent radical prostatectomy to create accurate cancer labels on MRI.
METHODS: Here, we introduce RAdiology Pathology Spatial Open-Source multi-Dimensional Integration (RAPSODI), the first open-source framework for the registration of radiology and pathology images. RAPSODI relies on three steps. First, it creates a three-dimensional (3D) reconstruction of the histopathology specimen as a digital representation of the tissue before gross sectioning. Second, RAPSODI registers corresponding histopathology and MRI slices. Third, the optimized transforms are applied to the cancer regions outlined on the histopathology images to project those labels onto the preoperative MRI.
RESULTS: We tested RAPSODI in a phantom study where we simulated various conditions, for example, tissue shrinkage during fixation. Our experiments showed that RAPSODI can reliably correct multiple artifacts. We also evaluated RAPSODI in 157 patients from three institutions that underwent radical prostatectomy and have very different pathology processing and scanning. RAPSODI was evaluated in 907 corresponding histpathology-MRI slices and achieved a Dice coefficient of 0.97 ± 0.01 for the prostate, a Hausdorff distance of 1.99 ± 0.70 mm for the prostate boundary, a urethra deviation of 3.09 ± 1.45 mm, and a landmark deviation of 2.80 ± 0.59 mm between registered histopathology images and MRI.
CONCLUSION: Our robust framework successfully mapped the extent of cancer from histopathology slices onto MRI providing labels from training machine learning methods to detect cancer on MRI.","We acknowledge the following funding sources: Department of Radiology, Stanford University, NIH 5T32 EB009653, Training in Biomedical Imaging Instrumentation at Stanford (to RRS), Mark and Mary Stevens Interdisciplinary Graduate Fellowship, Wu Tsai Neuroscience Institute (to JBW).",,Medical Physics,,Humans, Magnetic Resonance Imaging, Male, Prostate, Prostatectomy, Prostatic Neoplasms, Radiology, Seminal Vesicles,2020-07-18,2020,2020-07-18,2020-09,47,9,4177-4188,All OA, Hybrid,Article,"Rusu, Mirabela; Shao, Wei; Kunder, Christian A.; Wang, Jeffrey B.; Soerensen, Simon J. C.; Teslovich, Nikola C.; Sood, Rewa R.; Chen, Leo C.; Fan, Richard E.; Ghanouni, Pejman; Brooks, James D.; Sonn, Geoffrey A.","Rusu, Mirabela (Department of Radiology, School of Medicine, Stanford University, Stanford, CA, 94305, USA); Shao, Wei (Department of Radiology, School of Medicine, Stanford University, Stanford, CA, 94305, USA); Kunder, Christian A. (Department of Pathology, School of Medicine, Stanford University, Stanford, CA, 94305, USA); Wang, Jeffrey B. (School of Medicine, Stanford University, Stanford, CA, 94305, USA); Soerensen, Simon J. C. (Department of Urology, School of Medicine, Stanford University, Stanford, CA, 94305, USA; Department of Urology, Aarhus University Hospital, Aarhus, Denmark); Teslovich, Nikola C. (Department of Urology, School of Medicine, Stanford University, Stanford, CA, 94305, USA); Sood, Rewa R. (Department of Electrical Engineering, Stanford University, Stanford, CA, 94305, USA); Chen, Leo C. (Department of Urology, School of Medicine, Stanford University, Stanford, CA, 94305, USA); Fan, Richard E. (Department of Urology, School of Medicine, Stanford University, Stanford, CA, 94305, USA); Ghanouni, Pejman (Department of Radiology, School of Medicine, Stanford University, Stanford, CA, 94305, USA); Brooks, James D. (Department of Urology, School of Medicine, Stanford University, Stanford, CA, 94305, USA); Sonn, Geoffrey A. (Department of Radiology, School of Medicine, Stanford University, Stanford, CA, 94305, USA; Department of Urology, School of Medicine, Stanford University, Stanford, CA, 94305, USA)","Rusu, Mirabela (Stanford University)","Rusu, Mirabela (Stanford University); Shao, Wei (Stanford University); Kunder, Christian A. (Stanford University); Wang, Jeffrey B. (Stanford University); Soerensen, Simon J. C. (Stanford University; Aarhus University Hospital); Teslovich, Nikola C. (Stanford University); Sood, Rewa R. (Stanford University); Chen, Leo C. (Stanford University); Fan, Richard E. (Stanford University); Ghanouni, Pejman (Stanford University); Brooks, James D. (Stanford University); Sonn, Geoffrey A. (Stanford University; Stanford University)",22,18,1.69,9.02,https://aapm.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/mp.14337,https://app.dimensions.ai/details/publication/pub.1128639481,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,
5669,pub.1143528700,10.1016/j.compbiomed.2021.105107,34872011,,Segmentation of male pelvic organs on computed tomography with a deep neural network fine-tuned by a level-set method,"Computed Tomography (CT) imaging is used in Radiation Therapy planning, where the treatment is carefully tailored to each patient in order to maximize radiation dose to the target while decreasing adverse effects to nearby healthy tissues. A crucial step in this process is manual organ contouring, which if performed automatically could considerably decrease the time to starting treatment and improve outcomes. Computerized segmentation of male pelvic organs has been studied for decades and deep learning models have brought considerable advances to the field, but improvements are still demanded. A two-step framework for automatic segmentation of the prostate, bladder and rectum is presented: a convolutional neural network enhanced with attention gates performs an initial segmentation, followed by a region-based active contour model to fine-tune the segmentations to each patient's specific anatomy. The framework was evaluated on a large collection of planning CTs of patients who had Radiation Therapy for prostate cancer. The Surface Dice Coefficient improved from 79.41 to 81.00% on segmentation of the prostate, 94.03-95.36% on the bladder and 82.17-83.68% on the rectum, comparing the proposed framework with the baseline convolutional neural network. This study shows that traditional image segmentation algorithms can help improve the immense gains that deep learning models have brought to the medical imaging segmentation field.","The authors would like to acknowledge and thank Fundação para a Ciência e Tecnologia (FCT) for the PhD grant (reference SFRH/BD/146 887/2 019) awarded to the first author, which this work is a part of.",,Computers in Biology and Medicine,,,2021-12-02,2021,2021-12-02,2022-01,140,,105107,All OA, Green,Article,"Almeida, Gonçalo; Figueira, Ana Rita; Lencart, Joana; Tavares, João Manuel R S","Almeida, Gonçalo (Instituto de Ciência e Inovação em Engenharia Mecânica e Engenharia Industrial, Faculdade de Engenharia, Universidade do Porto, Porto, Portugal. Electronic address: galmeida@inegi.up.pt.); Figueira, Ana Rita (Serviço de Radioterapia, Centro Hospitalar Universitário de São João, Porto, Portugal. Electronic address: ana.figueira@chsj.min-saude.pt.); Lencart, Joana (Serviço de Física Médica e Grupo de Física Médica Radiobiologia e Protecção Radiológica do Centro de Investigação, Instituto Português de Oncologia do Porto (CI-IPOP), Porto, Portugal. Electronic address: joana.lencart@ipoporto.min-saude.pt.); Tavares, João Manuel R S (Instituto de Ciência e Inovação em Engenharia Mecânica e Engenharia Industrial, Departamento de Engenharia Mecânica, Faculdade de Engenharia, Universidade do Porto, Porto, Portugal. Electronic address: tavares@fe.up.pt.)","Tavares, João Manuel R S (University of Porto)","Almeida, Gonçalo (University of Porto); Figueira, Ana Rita (Hospital de São João); Lencart, Joana (IPO Porto); Tavares, João Manuel R S (University of Porto)",0,0,,0.0,https://repositorio-aberto.up.pt/bitstream/10216/138016/2/517240.1.pdf,https://app.dimensions.ai/details/publication/pub.1143528700,31 Biological Sciences, 3102 Bioinformatics and Computational Biology, 42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences, 4601 Applied Computing,,,,,,
5669,pub.1139159306,10.1371/journal.pone.0253829,34170972,PMC8232529,Harnessing clinical annotations to improve deep learning performance in prostate segmentation,"PURPOSE: Developing large-scale datasets with research-quality annotations is challenging due to the high cost of refining clinically generated markup into high precision annotations. We evaluated the direct use of a large dataset with only clinically generated annotations in development of high-performance segmentation models for small research-quality challenge datasets.
MATERIALS AND METHODS: We used a large retrospective dataset from our institution comprised of 1,620 clinically generated segmentations, and two challenge datasets (PROMISE12: 50 patients, ProstateX-2: 99 patients). We trained a 3D U-Net convolutional neural network (CNN) segmentation model using our entire dataset, and used that model as a template to train models on the challenge datasets. We also trained versions of the template model using ablated proportions of our dataset, and evaluated the relative benefit of those templates for the final models. Finally, we trained a version of the template model using an out-of-domain brain cancer dataset, and evaluated the relevant benefit of that template for the final models. We used five-fold cross-validation (CV) for all training and evaluation across our entire dataset.
RESULTS: Our model achieves state-of-the-art performance on our large dataset (mean overall Dice 0.916, average Hausdorff distance 0.135 across CV folds). Using this model as a pre-trained template for refining on two external datasets significantly enhanced performance (30% and 49% enhancement in Dice scores respectively). Mean overall Dice and mean average Hausdorff distance were 0.912 and 0.15 for the ProstateX-2 dataset, and 0.852 and 0.581 for the PROMISE12 dataset. Using even small quantities of data to train the template enhanced performance, with significant improvements using 5% or more of the data.
CONCLUSION: We trained a state-of-the-art model using unrefined clinical prostate annotations and found that its use as a template model significantly improved performance in other prostate segmentation tasks, even when trained with only 5% of the original dataset.","KVS acknowledges support from National Cancer Institute grant F30CA210329, National Institute of General Medical Studies grant GM08042, and the UCLA-Caltech Medical Scientist Training Program. CWA acknowledges funding from National Cancer Institute grants R21CA220352 P50CA092131. LSM acknowledges funding from National Cancer Institute grants R01CA195505 and R01CA158627. SH acknowledges that this project has been funded in whole or in part with federal funds from the National Cancer Institute, National Institutes of Health, under Contract No. HHSN261200800001E. TS, SM, and BT acknowledge that this project was supported in part by the Intramural Research Program of the NIH. The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the US Government. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. There was no additional external funding received for this study.","KVS acknowledges support from National Cancer Institute grant F30CA210329, National Institute of General Medical Studies grant GM08042, and the UCLA-Caltech Medical Scientist Training Program. CWA acknowledges funding from National Cancer Institute grants R21CA220352 P50CA092131. LSM acknowledges funding from National Cancer Institute grants R01CA195505 and R01CA158627. SH acknowledges that this project has been funded in whole or in part with federal funds from the National Cancer Institute, National Institutes of Health, under Contract No. HHSN261200800001E. TS, SM, and BT acknowledge that this project was supported in part by the Intramural Research Program of the NIH. The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the US Government. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. There was no additional external funding received for this study.",PLOS ONE,,"Data Curation; Databases, Factual; Deep Learning; Humans; Male; Prostate; Retrospective Studies; Tomography, X-Ray Computed",2021-06-25,2021,2021-06-25,,16,6,e0253829,All OA, Gold,Article,"Sarma, Karthik V.; Raman, Alex G.; Dhinagar, Nikhil J.; Priester, Alan M.; Harmon, Stephanie; Sanford, Thomas; Mehralivand, Sherif; Turkbey, Baris; Marks, Leonard S.; Raman, Steven S.; Speier, William; Arnold, Corey W.","Sarma, Karthik V. (University of California, Los Angeles, Los Angeles, CA, United States of America); Raman, Alex G. (University of California, Los Angeles, Los Angeles, CA, United States of America; Western University of Health Sciences, Pomona, CA, United States of America); Dhinagar, Nikhil J. (University of California, Los Angeles, Los Angeles, CA, United States of America; Keck School of Medicine, University of Southern California, Los Angeles, CA, United States of America); Priester, Alan M. (University of California, Los Angeles, Los Angeles, CA, United States of America); Harmon, Stephanie (National Cancer Institute, National Institutes of Health, Bethesda, MD, United States of America; Clinical Research Directorate, Frederick National Laboratory for Cancer Research, Frederick, MD, United States of America); Sanford, Thomas (National Cancer Institute, National Institutes of Health, Bethesda, MD, United States of America; SUNY Upstate Medical Center, Syracuse, NY, United States of America); Mehralivand, Sherif (National Cancer Institute, National Institutes of Health, Bethesda, MD, United States of America); Turkbey, Baris (National Cancer Institute, National Institutes of Health, Bethesda, MD, United States of America); Marks, Leonard S. (University of California, Los Angeles, Los Angeles, CA, United States of America); Raman, Steven S. (University of California, Los Angeles, Los Angeles, CA, United States of America); Speier, William (University of California, Los Angeles, Los Angeles, CA, United States of America); Arnold, Corey W. (University of California, Los Angeles, Los Angeles, CA, United States of America)","Arnold, Corey W. (University of California, Los Angeles)","Sarma, Karthik V. (University of California, Los Angeles); Raman, Alex G. (University of California, Los Angeles; Western University of Health Sciences); Dhinagar, Nikhil J. (University of California, Los Angeles; University of Southern California); Priester, Alan M. (University of California, Los Angeles); Harmon, Stephanie (National Cancer Institute; Frederick National Laboratory for Cancer Research); Sanford, Thomas (National Cancer Institute; SUNY Upstate Medical University); Mehralivand, Sherif (National Cancer Institute); Turkbey, Baris (National Cancer Institute); Marks, Leonard S. (University of California, Los Angeles); Raman, Steven S. (University of California, Los Angeles); Speier, William (University of California, Los Angeles); Arnold, Corey W. (University of California, Los Angeles)",3,3,0.41,1.82,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0253829&type=printable,https://app.dimensions.ai/details/publication/pub.1139159306,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,3 Good Health and Well Being,,,,,,,,,
5665,pub.1149407319,10.1016/j.compbiomed.2022.105817,35841780,,Prostate158 - An expert-annotated 3T MRI dataset and algorithm for prostate cancer detection,"BACKGROUND: The development of deep learning (DL) models for prostate segmentation on magnetic resonance imaging (MRI) depends on expert-annotated data and reliable baselines, which are often not publicly available. This limits both reproducibility and comparability.
METHODS: Prostate158 consists of 158 expert annotated biparametric 3T prostate MRIs comprising T2w sequences and diffusion-weighted sequences with apparent diffusion coefficient maps. Two U-ResNets trained for segmentation of anatomy (central gland, peripheral zone) and suspicious lesions for prostate cancer (PCa) with a PI-RADS score of ≥4 served as baseline algorithms. Segmentation performance was evaluated using the Dice similarity coefficient (DSC), the Hausdorff distance (HD), and the average surface distance (ASD). The Wilcoxon test with Bonferroni correction was used to evaluate differences in performance. The generalizability of the baseline model was assessed using the open datasets Medical Segmentation Decathlon and PROSTATEx.
RESULTS: Compared to Reader 1, the models achieved a DSC/HD/ASD of 0.88/18.3/2.2 for the central gland, 0.75/22.8/1.9 for the peripheral zone, and 0.45/36.7/17.4 for PCa. Compared with Reader 2, the DSC/HD/ASD were 0.88/17.5/2.6 for the central gland, 0.73/33.2/1.9 for the peripheral zone, and 0.4/39.5/19.1 for PCa. Interrater agreement measured in DSC/HD/ASD was 0.87/11.1/1.0 for the central gland, 0.75/15.8/0.74 for the peripheral zone, and 0.6/18.8/5.5 for PCa. Segmentation performances on the Medical Segmentation Decathlon and PROSTATEx were 0.82/22.5/3.4; 0.86/18.6/2.5 for the central gland, and 0.64/29.2/4.7; 0.71/26.3/2.2 for the peripheral zone.
CONCLUSIONS: We provide an openly accessible, expert-annotated 3T dataset of prostate MRI and a reproducible benchmark to foster the development of prostate segmentation algorithms.","LCA is grateful for her participation in the BIH Charité–Junior Clinician and Clinician Scientist Program and KKB is grateful for his participation in the BIH Charité Digital Clinician Scientist Program, all funded by the Charité–Universitätsmedizin Berlin and the Berlin Institute of Health.",,Computers in Biology and Medicine,,Algorithms, Humans, Magnetic Resonance Imaging, Male, Prostate, Prostatic Neoplasms, Reproducibility of Results, Retrospective Studies,2022-07-11,2022,2022-07-11,2022-09,148,,105817,Closed,Article,"Adams, Lisa C; Makowski, Marcus R; Engel, Günther; Rattunde, Maximilian; Busch, Felix; Asbach, Patrick; Niehues, Stefan M; Vinayahalingam, Shankeeth; van Ginneken, Bram; Litjens, Geert; Bressem, Keno K","Adams, Lisa C (Charité - Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Luisenstraße 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany; Berlin Institute of Health at Charité - Universitätsmedizin Berlin, Charitéplatz 1, 10117, Berlin, Germany. Electronic address: lisa.christine.adams@gmail.com.); Makowski, Marcus R (Technical University of Munich, Department of Diagnostic and Interventional Radiology, Faculty of Medicine, Ismaninger Str. 22, 81675, Munich, Germany.); Engel, Günther (Charité - Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Luisenstraße 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany; Institute for Diagnostic and Interventional Radiology, Georg-August University, Göttingen, Germany.); Rattunde, Maximilian (Charité - Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Luisenstraße 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany.); Busch, Felix (Charité - Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Luisenstraße 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany.); Asbach, Patrick (Charité - Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Luisenstraße 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany.); Niehues, Stefan M (Charité - Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Luisenstraße 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany.); Vinayahalingam, Shankeeth (Department of Oral and Maxillofacial Surgery, Radboud University Medical Center, Nijmegen, GA, the Netherlands.); van Ginneken, Bram (Radboud University Medical Center, Nijmegen, GA, the Netherlands.); Litjens, Geert (Radboud University Medical Center, Nijmegen, GA, the Netherlands.); Bressem, Keno K (Charité - Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Institute for Radiology, Luisenstraße 7, 10117, Hindenburgdamm 30, 12203, Berlin, Germany; Berlin Institute of Health at Charité - Universitätsmedizin Berlin, Charitéplatz 1, 10117, Berlin, Germany.)","Adams, Lisa C (Charité - University Medicine Berlin; Berlin Institute of Health at Charité - Universitätsmedizin Berlin)","Adams, Lisa C (Charité - University Medicine Berlin; Berlin Institute of Health at Charité - Universitätsmedizin Berlin); Makowski, Marcus R (Technical University of Munich); Engel, Günther (Charité - University Medicine Berlin; University of Göttingen); Rattunde, Maximilian (Charité - University Medicine Berlin); Busch, Felix (Charité - University Medicine Berlin); Asbach, Patrick (Charité - University Medicine Berlin); Niehues, Stefan M (Charité - University Medicine Berlin); Vinayahalingam, Shankeeth (Radboud University Nijmegen Medical Centre); van Ginneken, Bram (Radboud University Nijmegen Medical Centre); Litjens, Geert (Radboud University Nijmegen Medical Centre); Bressem, Keno K (Charité - University Medicine Berlin; Berlin Institute of Health at Charité - Universitätsmedizin Berlin)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1149407319,31 Biological Sciences, 3102 Bioinformatics and Computational Biology, 42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences, 4601 Applied Computing
5575,pub.1016056914,10.4172/2167-0870.1000e117,25143872,PMC4136383,Emerging Tools for Computer-Aided Diagnosis and Prognostication,,Funding This work was supported by the Myocardial Applied Genomics Network (MAGNet) National Institutes of Health grant R01HL105993.,,Journal of Clinical Trials,,,2014-02-24,2014,2014-02-24,2014,4,2,e117,All OA, Green,Article,"Ritter, Scott; Margulies, Kenneth B","Ritter, Scott (Department of Medicine, Division of Cardiovascular Medicine, Cardiovascular Institute, Perelman School of Medicine, University of Pennsylvania); Margulies, Kenneth B (Department of Medicine, Division of Cardiovascular Medicine, Cardiovascular Institute, Perelman School of Medicine, University of Pennsylvania)",,"Ritter, Scott (University of Pennsylvania); Margulies, Kenneth B (University of Pennsylvania)",1,0,0.07,,https://europepmc.org/articles/pmc4136383?pdf=render,https://app.dimensions.ai/details/publication/pub.1016056914,,,,,,,,,,,,
5388,pub.1128926104,10.1109/tmi.2020.3006437,32746129,PMC7704933,Confidence Calibration and Predictive Uncertainty Estimation for Deep Medical Image Segmentation,"Fully convolutional neural networks (FCNs), and in particular U-Nets, have achieved state-of-the-art results in semantic segmentation for numerous medical imaging applications. Moreover, batch normalization and Dice loss have been used successfully to stabilize and accelerate training. However, these networks are poorly calibrated i.e. they tend to produce overconfident predictions for both correct and erroneous classifications, making them unreliable and hard to interpret. In this paper, we study predictive uncertainty estimation in FCNs for medical image segmentation. We make the following contributions: 1) We systematically compare cross-entropy loss with Dice loss in terms of segmentation quality and uncertainty estimation of FCNs; 2) We propose model ensembling for confidence calibration of the FCNs trained with batch normalization and Dice loss; 3) We assess the ability of calibrated FCNs to predict segmentation quality of structures and detect out-of-distribution test examples. We conduct extensive experiments across three medical image segmentation applications of the brain, the heart, and the prostate to evaluate our contributions. The results of this study offer considerable insight into the predictive uncertainty estimation and out-of-distribution detection in medical image segmentation and provide practical recipes for confidence calibration. Moreover, we consistently demonstrate that model ensembling improves confidence calibration.","This work was supported in part by the U.S. National Institutes of Health under Grant P41EB015898, in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada, and in part by the Canadian Institutes of Health Research (CIHR).",,IEEE Transactions on Medical Imaging,,"Brain; Calibration; Humans; Image Processing, Computer-Assisted; Male; Neural Networks, Computer; Uncertainty",2020-11-30,2020,2020-11-30,2020-12,39,12,3868-3878,All OA, Green,Article,"Mehrtash, Alireza; Wells, William M.; Tempany, Clare M.; Abolmaesumi, Purang; Kapur, Tina","Mehrtash, Alireza (Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, V6T 1Z4, Canada; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, 02115, USA); Wells, William M. (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, 02115, USA); Tempany, Clare M. (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, 02115, USA); Abolmaesumi, Purang (Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, V6T 1Z4, Canada); Kapur, Tina (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, 02115, USA)","Kapur, Tina (Brigham and Women's Hospital; Harvard University)","Mehrtash, Alireza (University of British Columbia; Brigham and Women's Hospital; Harvard University); Wells, William M. (Brigham and Women's Hospital; Harvard University); Tempany, Clare M. (Brigham and Women's Hospital; Harvard University); Abolmaesumi, Purang (University of British Columbia); Kapur, Tina (Brigham and Women's Hospital; Harvard University)",120,113,3.5,61.84,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7704933,https://app.dimensions.ai/details/publication/pub.1128926104,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5382,pub.1149500948,10.1109/tmi.2022.3191535,35839185,,DLTTA: Dynamic Learning Rate for Test-Time Adaptation on Cross-Domain Medical Images,"Test-time adaptation (TTA) has increasingly been an important topic to efficiently tackle the cross-domain distribution shift at test time for medical images from different institutions. Previous TTA methods have a common limitation of using a fixed learning rate for all the test samples. Such a practice would be sub-optimal for TTA, because test data may arrive sequentially therefore the scale of distribution shift would change frequently. To address this problem, we propose a novel dynamic learning rate adjustment method for test-time adaptation, called DLTTA, which dynamically modulates the amount of weights update for each test image to account for the differences in their distribution shift. Specifically, our DLTTA is equipped with a memory bank based estimation scheme to effectively measure the discrepancy of a given test sample. Based on this estimated discrepancy, a dynamic learning rate adjustment strategy is then developed to achieve a suitable degree of adaptation for each test sample. The effectiveness and general applicability of our DLTTA is extensively demonstrated on three tasks including retinal optical coherence tomography (OCT) segmentation, histopathological image classification, and prostate 3D MRI segmentation. Our method achieves effective and fast test-time adaptation with consistent performance improvement over current state-of-the-art test-time adaptation methods. Code is available at https://github.com/med-air/DLTTA.",,"This work was supported in part by the Hong Kong Innovation and Technology Fund under Project ITS/238/21, Project ITS/170/20, and Project GHP/110/19SZ, in part by The Chinese University of Hong Kong Shun Hing Institute of Advanced Engineering under Project MMT-p5-20, and in part by the Shenzhen-Hong Kong Collaboration Development Zone.",IEEE Transactions on Medical Imaging,,"Male; Humans; Prostate; Tomography, Optical Coherence; Retina; Magnetic Resonance Imaging",2022-12-02,2022,2022-12-02,2022-12,41,12,3575-3586,All OA, Green,Article,"Yang, Hongzheng; Chen, Cheng; Jiang, Meirui; Liu, Quande; Cao, Jianfeng; Heng, Pheng Ann; Dou, Qi","Yang, Hongzheng (Department of Artificial Intelligence, Beihang University, Beijing, 100191, China); Chen, Cheng (Department of Computer Science and Engineering, Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, Shatin, Hong Kong, SAR); Jiang, Meirui (Department of Computer Science and Engineering, Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, Shatin, Hong Kong, SAR); Liu, Quande (Department of Computer Science and Engineering, Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, Shatin, Hong Kong, SAR); Cao, Jianfeng (Department of Computer Science and Engineering, Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, Shatin, Hong Kong, SAR); Heng, Pheng Ann (Department of Computer Science and Engineering, Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, Shatin, Hong Kong, SAR); Dou, Qi (Department of Computer Science and Engineering, Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, Shatin, Hong Kong, SAR)","Chen, Cheng (Chinese University of Hong Kong)","Yang, Hongzheng (Beihang University); Chen, Cheng (Chinese University of Hong Kong); Jiang, Meirui (Chinese University of Hong Kong); Liu, Quande (Chinese University of Hong Kong); Cao, Jianfeng (Chinese University of Hong Kong); Heng, Pheng Ann (Chinese University of Hong Kong); Dou, Qi (Chinese University of Hong Kong)",1,1,,,http://arxiv.org/pdf/2205.13723,https://app.dimensions.ai/details/publication/pub.1149500948,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5382,pub.1132961523,10.1016/j.artmed.2020.101998,33461691,,Interactive medical image segmentation via a point-based interaction,"Due to low tissue contrast, irregular shape, and large location variance, segmenting the objects from different medical imaging modalities (e.g., CT, MR) is considered as an important yet challenging task. In this paper, a novel method is presented for interactive medical image segmentation with the following merits. (1) Its design is fundamentally different from previous pure patch-based and image-based segmentation methods. It is observed that during delineation, the physician repeatedly check the intensity from area inside-object to outside-object to determine the boundary, which indicates that comparison in an inside-out manner is extremely important. Thus, the method innovatively models the segmentation task as learning the representation of bi-directional sequential patches, starting from (or ending in) the given central point of the object. This can be realized by the proposed ConvRNN network embedded with a gated memory propagation unit. (2) Unlike previous interactive methods (requiring bounding box or seed points), the proposed method only asks the physician to merely click on the rough central point of the object before segmentation, which could simultaneously enhance the performance and reduce the segmentation time. (3) The method is utilized in a multi-level framework for better performance. It has been systematically evaluated in three different segmentation tasks, including CT kidney tumor, MR prostate, and PROMISE12 challenge, showing promising results compared with state-of-the-art methods.",,,Artificial Intelligence in Medicine,,"Algorithms; Humans; Magnetic Resonance Imaging; Male; Tomography, X-Ray Computed",2020-11-28,2020,2020-11-28,2021-01,111,,101998,Closed,Article,"Zhang, Jian; Shi, Yinghuan; Sun, Jinquan; Wang, Lei; Zhou, Luping; Gao, Yang; Shen, Dinggang","Zhang, Jian (State Key Laboratory for Novel Software Technology, Nanjing University, China.); Shi, Yinghuan (State Key Laboratory for Novel Software Technology, Nanjing University, China; National Institute of Healthcare Data Science, Nanjing University, China.); Sun, Jinquan (State Key Laboratory for Novel Software Technology, Nanjing University, China.); Wang, Lei (School of Computing and Information Technology, University of Wollongong, Australia.); Zhou, Luping (School of Electrical and Information Engineering, University of Sydney, Australia.); Gao, Yang (State Key Laboratory for Novel Software Technology, Nanjing University, China; National Institute of Healthcare Data Science, Nanjing University, China.); Shen, Dinggang (School of Biomedical Engineering, ShanghaiTech University, China; Shanghai United Imaging Intelligence Co., Ltd., China; Department of Artificial Intelligence, Korea University, Republic of Korea.)","Gao, Yang (Nanjing University); Shen, Dinggang (ShanghaiTech University; ; Korea University)","Zhang, Jian (Nanjing University); Shi, Yinghuan (Nanjing University); Sun, Jinquan (Nanjing University); Wang, Lei (University of Wollongong); Zhou, Luping (The University of Sydney); Gao, Yang (Nanjing University); Shen, Dinggang (ShanghaiTech University; Korea University)",11,11,0.9,5.24,,https://app.dimensions.ai/details/publication/pub.1132961523,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
5382,pub.1128443296,10.1002/mp.14327,32533855,PMC8681870,Graph‐convolutional‐network‐based interactive prostate segmentation in MR images,"PURPOSE: Accurate and robust segmentation of the prostate from magnetic resonance (MR) images is extensively applied in many clinical applications in prostate cancer diagnosis and treatment. The purpose of this study is the development of a robust interactive segmentation method for accurate segmentation of the prostate from MR images.
METHODS: We propose an interactive segmentation method based on a graph convolutional network (GCN) to refine the automatically segmented results. An atrous multiscale convolutional neural network (CNN) encoder is proposed to learn representative features to obtain accurate segmentations. Based on the multiscale feature, a GCN block is presented to predict the prostate contour in both automatic and interactive manners. To preserve the prostate boundary details and effectively train the GCN, a contour matching loss is proposed. The performance of the proposed algorithm was evaluated on 41 in-house MR subjects and 30 PROMISE12 test subjects.
RESULT: The proposed method yields mean Dice similarity coefficients of 93.8 ± 1.2% and 94.4 ± 1.0% on our in-house and PROMISE12 datasets, respectively. The experimental results show that the proposed method outperforms several state-of-the-art segmentation methods.
CONCLUSION: The proposed interactive segmentation method based on the GCN can accurately segment the prostate from MR images. Our method has a variety of applications in prostate cancer imaging.","This work was supported in part by NSFC under grant No. 61876148 and No. 81602583. This work was also supported in part by the Fundamental Research Funds for the Central Universities No. XJJ2018254, and China Postdoctoral Science Foundation No. 2018M631164.",,Medical Physics,,"Algorithms; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate",2020-07-13,2020,2020-07-13,2020-09,47,9,4164-4176,All OA, Green,Article,"Tian, Zhiqiang; Li, Xiaojian; Zheng, Yaoyue; Chen, Zhang; Shi, Zhong; Liu, Lizhi; Fei, Baowei","Tian, Zhiqiang (School of Software Engineering, Xi’an Jiaotong University, Xi’an, 710049, China); Li, Xiaojian (School of Software Engineering, Xi’an Jiaotong University, Xi’an, 710049, China); Zheng, Yaoyue (School of Software Engineering, Xi’an Jiaotong University, Xi’an, 710049, China); Chen, Zhang (School of Software Engineering, Xi’an Jiaotong University, Xi’an, 710049, China); Shi, Zhong (Institute of Cancer and Basic Medicine, Chinese Academy of Sciences and Cancer Hospital of the University of Chinese Academy of Sciences, Hangzhou, 310022, China); Liu, Lizhi (Center of Medical Imaging and Image‐guided Therapy, Sun Yat‐Sen University Cancer Center, Guangzhou, 510060, China; State Key Laboratory of Oncology in South China, Guangzhou, 510060, China); Fei, Baowei (Department of Bioengineering, University of Texas at Dallas, Richardson, Texas, 75035, USA; Advanced Imaging Research Center, University of Texas Southwestern Medical Center, Dallas, Texas, 75080, USA; Department of Radiology, University of Texas Southwestern Medical Center, Dallas, Texas, 75080, USA)","Tian, Zhiqiang (Xi'an Jiaotong University)","Tian, Zhiqiang (Xi'an Jiaotong University); Li, Xiaojian (Xi'an Jiaotong University); Zheng, Yaoyue (Xi'an Jiaotong University); Chen, Zhang (Xi'an Jiaotong University); Shi, Zhong (University of Chinese Academy of Sciences); Liu, Lizhi (Sun Yat-sen University Cancer Center; Sun Yat-sen University); Fei, Baowei (The University of Texas at Dallas; The University of Texas Southwestern Medical Center; The University of Texas Southwestern Medical Center)",30,28,3.09,12.3,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8681870,https://app.dimensions.ai/details/publication/pub.1128443296,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
5376,pub.1129478814,10.1016/j.neunet.2020.07.011,32721843,,Discretely-constrained deep network for weakly supervised segmentation,"An efficient strategy for weakly-supervised segmentation is to impose constraints or regularization priors on target regions. Recent efforts have focused on incorporating such constraints in the training of convolutional neural networks (CNN), however this has so far been done within a continuous optimization framework. Yet, various segmentation constraints and regularization priors can be modeled and optimized more efficiently in a discrete formulation. This paper proposes a method, based on the alternating direction method of multipliers (ADMM) algorithm, to train a CNN with discrete constraints and regularization priors. This method is applied to the segmentation of medical images with weak annotations, where both size constraints and boundary length regularization are enforced. Experiments on two benchmark datasets for medical image segmentation show our method to provide significant improvements compared to existing approaches in terms of segmentation accuracy, constraint satisfaction and convergence speed.","We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), and thank NVIDIA corporation, USA for supporting this work through their GPU grant program.",,Neural Networks,,"Algorithms; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Pattern Recognition, Automated; Supervised Machine Learning",2020-07-18,2020,2020-07-18,2020-10,130,,297-308,All OA, Green,Article,"Peng, Jizong; Kervadec, Hoel; Dolz, Jose; Ben Ayed, Ismail; Pedersoli, Marco; Desrosiers, Christian","Peng, Jizong (Department of Software and IT Engineering, ETS Montreal, 1100 Notre-Dame W., Montreal, H3C 1K3, Canada. Electronic address: jizong.peng.1@etsmtl.ca.); Kervadec, Hoel (Department of Automated Production, ETS Montreal, 1100 Notre-Dame W., Montreal, H3C 1K3, Canada.); Dolz, Jose (Department of Software and IT Engineering, ETS Montreal, 1100 Notre-Dame W., Montreal, H3C 1K3, Canada.); Ben Ayed, Ismail (Department of Automated Production, ETS Montreal, 1100 Notre-Dame W., Montreal, H3C 1K3, Canada.); Pedersoli, Marco (Department of Automated Production, ETS Montreal, 1100 Notre-Dame W., Montreal, H3C 1K3, Canada.); Desrosiers, Christian (Department of Software and IT Engineering, ETS Montreal, 1100 Notre-Dame W., Montreal, H3C 1K3, Canada. Electronic address: christian.desrosiers@etsmtl.ca.)","Peng, Jizong ","Peng, Jizong (); Kervadec, Hoel (); Dolz, Jose (); Ben Ayed, Ismail (); Pedersoli, Marco (); Desrosiers, Christian ()",19,17,1.02,9.41,http://arxiv.org/pdf/1908.05770,https://app.dimensions.ai/details/publication/pub.1129478814,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
5376,pub.1128241573,10.3390/s20113183,32503330,PMC7309110,Evaluation of Deep Neural Networks for Semantic Segmentation of Prostate in T2W MRI,"In this paper, we present an evaluation of four encoder-decoder CNNs in the segmentation of the prostate gland in T2W magnetic resonance imaging (MRI) image. The four selected CNNs are FCN, SegNet, U-Net, and DeepLabV3+, which was originally proposed for the segmentation of road scene, biomedical, and natural images. Segmentation of prostate in T2W MRI images is an important step in the automatic diagnosis of prostate cancer to enable better lesion detection and staging of prostate cancer. Therefore, many research efforts have been conducted to improve the segmentation of the prostate gland in MRI images. The main challenges of prostate gland segmentation are blurry prostate boundary and variability in prostate anatomical structure. In this work, we investigated the performance of encoder-decoder CNNs for segmentation of prostate gland in T2W MRI. Image pre-processing techniques including image resizing, center-cropping and intensity normalization are applied to address the issues of inter-patient and inter-scanner variability as well as the issue of dominating background pixels over prostate pixels. In addition, to enrich the network with more data, to increase data variation, and to improve its accuracy, patch extraction and data augmentation are applied prior to training the networks. Furthermore, class weight balancing is used to avoid having biased networks since the number of background pixels is much higher than the prostate pixels. The class imbalance problem is solved by utilizing weighted cross-entropy loss function during the training of the CNN model. The performance of the CNNs is evaluated in terms of the Dice similarity coefficient (DSC) and our experimental results show that patch-wise DeepLabV3+ gives the best performance with DSC equal to 92 . 8 % . This value is the highest DSC score compared to the FCN, SegNet, and U-Net that also competed the recently published state-of-the-art method of prostate segmentation.",The authors would like to thank Hilwati Hashim from the UiTM Sungai Buloh Hospital and Syazarina Sharis Osman from the HUKM Medical Centre for helping with the manual prostate delineation.,"This research is supported by two research grants: (1) International Collaborative Research Fund between Universiti Teknologi PETRONAS (UTP) and Universitas Muhammadiyah Purwokerto (UMP) under grant number 015ME0-098, and (2) the Yayasan Universiti Teknologi PETRONAS under grant number YUTP-FRG 015LC0-292.",Sensors,,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate; Semantics",2020-06-03,2020,2020-06-03,,20,11,3183,All OA, Gold,Article,"Khan, Zia; Yahya, Norashikin; Alsaih, Khaled; Ali, Syed Saad Azhar; Meriaudeau, Fabrice","Khan, Zia (Centre for Intelligent Signal and Imaging Research (CISIR), Department of Electrical and Electronic Engineering, Universiti Teknologi PETRONAS, Seri Iskandar 32610, Malaysia;, zia_17004635@utp.edu.my, (Z.K.);, khaledalsaih@gmail.com, (K.A.);, saad.azhar@utp.edu.my, (S.S.A.A.)); Yahya, Norashikin (Centre for Intelligent Signal and Imaging Research (CISIR), Department of Electrical and Electronic Engineering, Universiti Teknologi PETRONAS, Seri Iskandar 32610, Malaysia;, zia_17004635@utp.edu.my, (Z.K.);, khaledalsaih@gmail.com, (K.A.);, saad.azhar@utp.edu.my, (S.S.A.A.)); Alsaih, Khaled (Centre for Intelligent Signal and Imaging Research (CISIR), Department of Electrical and Electronic Engineering, Universiti Teknologi PETRONAS, Seri Iskandar 32610, Malaysia;, zia_17004635@utp.edu.my, (Z.K.);, khaledalsaih@gmail.com, (K.A.);, saad.azhar@utp.edu.my, (S.S.A.A.)); Ali, Syed Saad Azhar (Centre for Intelligent Signal and Imaging Research (CISIR), Department of Electrical and Electronic Engineering, Universiti Teknologi PETRONAS, Seri Iskandar 32610, Malaysia;, zia_17004635@utp.edu.my, (Z.K.);, khaledalsaih@gmail.com, (K.A.);, saad.azhar@utp.edu.my, (S.S.A.A.)); Meriaudeau, Fabrice (ImViA/ITFIM, University of Burgundy, 21078 Dijon, France;, fabrice.meriaudeau@u-bourgogne.fr)","Yahya, Norashikin ","Khan, Zia (); Yahya, Norashikin (); Alsaih, Khaled (); Ali, Syed Saad Azhar (); Meriaudeau, Fabrice (University of Burgundy)",36,35,2.9,14.19,https://www.mdpi.com/1424-8220/20/11/3183/pdf?version=1591752152,https://app.dimensions.ai/details/publication/pub.1128241573,46 Information and Computing Sciences, 4606 Distributed Computing and Systems Software,,,,,,,,,,
5375,pub.1149779127,10.3390/bioengineering9080343,35892756,PMC9394419,A Fusion Biopsy Framework for Prostate Cancer Based on Deformable Superellipses and nnU-Net,"In prostate cancer, fusion biopsy, which couples magnetic resonance imaging (MRI) with transrectal ultrasound (TRUS), poses the basis for targeted biopsy by allowing the comparison of information coming from both imaging modalities at the same time. Compared with the standard clinical procedure, it provides a less invasive option for the patients and increases the likelihood of sampling cancerous tissue regions for the subsequent pathology analyses. As a prerequisite to image fusion, segmentation must be achieved from both MRI and TRUS domains. The automatic contour delineation of the prostate gland from TRUS images is a challenging task due to several factors including unclear boundaries, speckle noise, and the variety of prostate anatomical shapes. Automatic methodologies, such as those based on deep learning, require a huge quantity of training data to achieve satisfactory results. In this paper, the authors propose a novel optimization formulation to find the best superellipse, a deformable model that can accurately represent the prostate shape. The advantage of the proposed approach is that it does not require extensive annotations, and can be used independently of the specific transducer employed during prostate biopsies. Moreover, in order to show the clinical applicability of the method, this study also presents a module for the automatic segmentation of the prostate gland from MRI, exploiting the nnU-Net framework. Lastly, segmented contours from both imaging domains are fused with a customized registration algorithm in order to create a tool that can help the physician to perform a targeted prostate biopsy by interacting with the graphical user interface.",,This research received no external funding.,Bioengineering,,,2022-07-26,2022,2022-07-26,,9,8,343,All OA, Gold,Article,"Altini, Nicola; Brunetti, Antonio; Napoletano, Valeria Pia; Girardi, Francesca; Allegretti, Emanuela; Hussain, Sardar Mehboob; Brunetti, Gioacchino; Triggiani, Vito; Bevilacqua, Vitoantonio; Buongiorno, Domenico","Altini, Nicola (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Brunetti, Antonio (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.); Apulian Bioengineering s.r.l., Via delle Violette n.14, 70026 Modugno, BA, Italy); Napoletano, Valeria Pia (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Girardi, Francesca (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Allegretti, Emanuela (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Hussain, Sardar Mehboob (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.)); Brunetti, Gioacchino (Masmec Biomed SpA, Via delle Violette n.14, 70026 Modugno, BA, Italy;, gioacchino.brunetti@masmecbiomed.com, (G.B.);, vito.triggiani@masmecbiomed.com, (V.T.)); Triggiani, Vito (Masmec Biomed SpA, Via delle Violette n.14, 70026 Modugno, BA, Italy;, gioacchino.brunetti@masmecbiomed.com, (G.B.);, vito.triggiani@masmecbiomed.com, (V.T.)); Bevilacqua, Vitoantonio (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.); Apulian Bioengineering s.r.l., Via delle Violette n.14, 70026 Modugno, BA, Italy); Buongiorno, Domenico (Department of Electrical and Information Engineering (DEI), Polytechnic University of Bari, 70126 Bari, BA, Italy;, antonio.brunetti@poliba.it, (A.B.);, v.napoletano3@studenti.poliba.it, (V.P.N.);, f.girardi@studenti.poliba.it, (F.G.);, e.allegretti@studenti.poliba.it, (E.A.);, sardarmehboob.hussain@poliba.it, (S.M.H.);, vitoantonio.bevilacqua@poliba.it, (V.B.);, domenico.buongiorno@poliba.it, (D.B.); Apulian Bioengineering s.r.l., Via delle Violette n.14, 70026 Modugno, BA, Italy)","Altini, Nicola (Polytechnic University of Bari; )","Altini, Nicola (Polytechnic University of Bari); Brunetti, Antonio (Polytechnic University of Bari); Napoletano, Valeria Pia (Polytechnic University of Bari); Girardi, Francesca (Polytechnic University of Bari); Allegretti, Emanuela (Polytechnic University of Bari); Hussain, Sardar Mehboob (Polytechnic University of Bari); Brunetti, Gioacchino (); Triggiani, Vito (); Bevilacqua, Vitoantonio (Polytechnic University of Bari); Buongiorno, Domenico (Polytechnic University of Bari)",1,1,,,https://www.mdpi.com/2306-5354/9/8/343/pdf?version=1659416933,https://app.dimensions.ai/details/publication/pub.1149779127,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,
5375,pub.1145493242,10.1007/s10334-022-01003-9,35150363,PMC9363383,Pseudo-T2 mapping for normalization of T2-weighted prostate MRI,"ObjectiveSignal intensity normalization is necessary to reduce heterogeneity in T2-weighted (T2W) magnetic resonance imaging (MRI) for quantitative analysis of multicenter data. AutoRef is an automated dual-reference tissue normalization method that normalizes transversal prostate T2W MRI by creating a pseudo-T2 map. The aim of this study was to evaluate the accuracy of pseudo-T2s and multicenter standardization performance for AutoRef with three pairs of reference tissues: fat/muscle (AutoRefF), femoral head/muscle (AutoRefFH) and pelvic bone/muscle (AutoRefPB).Materials and methodsT2s measured by multi-echo spin echo (MESE) were compared to AutoRef pseudo-T2s in the whole prostate (WP) and zones (PZ and TZ/CZ/AFS) for seven asymptomatic volunteers with a paired Wilcoxon signed-rank test. AutoRef normalization was assessed on T2W images from a multicenter evaluation set of 1186 prostate cancer patients. Performance was measured by inter-patient histogram intersections of voxel intensities in the WP before and after normalization in a selected subset of 80 cases.ResultsAutoRefFH pseudo-T2s best approached MESE T2s in the volunteer study, with no significant difference shown (WP: p = 0.30, TZ/CZ/AFS: p = 0.22, PZ: p = 0.69). All three AutoRef versions increased inter-patient histogram intersections in the multicenter dataset, with median histogram intersections of 0.505 (original data), 0.738 (AutoRefFH), 0.739 (AutoRefF) and 0.726 (AutoRefPB).DiscussionAll AutoRef versions reduced variation in the multicenter data. AutoRefFH pseudo-T2s were closest to experimentally measured T2s.",,"Open access funding provided by NTNU Norwegian University of Science and Technology (incl St. Olavs Hospital - Trondheim University Hospital). Ministry of Science and Technology, Taiwan (Grant Number MOST 109–2628-B-182A-007); National Health Research Institutes, Taiwan (NHRI-109BCCO-MF-202009–02); Chang Gung Medical Foundation (Grant Number CLRPG3K0022, CPRPG3G0023); The Liaison Committee for Education, Research and Innovation in Central Norway (Grant Number 905481); Norwegian University of Science and Technology (NTNU); The Research Council of Norway (Grant Number 295013); The Liaison Committee between the Central Norway Regional Health Authority and the Norwegian University of Science and Technology (Grant Number 90265300).","Magnetic Resonance Materials in Physics, Biology and Medicine",,Humans, Magnetic Resonance Imaging, Magnetic Resonance Spectroscopy, Male, Pelvis, Prostate, Prostatic Neoplasms,2022-02-12,2022,2022-02-12,2022-08,35,4,573-585,All OA, Hybrid,Article,"Sørland, Kaia Ingerdatter; Sunoqrot, Mohammed R. S.; Sandsmark, Elise; Langørgen, Sverre; Bertilsson, Helena; Trimble, Christopher G.; Lin, Gigin; Selnæs, Kirsten M.; Goa, Pål E.; Bathen, Tone F.; Elschot, Mattijs","Sørland, Kaia Ingerdatter (Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Olav Kyrres Gate 9, 7030, Trondheim, Norway); Sunoqrot, Mohammed R. S. (Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Olav Kyrres Gate 9, 7030, Trondheim, Norway); Sandsmark, Elise (Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, Trondheim, Norway); Langørgen, Sverre (Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, Trondheim, Norway); Bertilsson, Helena (Department of Cancer Research and Molecular Medicine, Norwegian University of Science and Technology, Trondheim, Norway; Department of Urology, St. Olavs Hospital, Trondheim University Hospital, Trondheim, Norway); Trimble, Christopher G. (Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Olav Kyrres Gate 9, 7030, Trondheim, Norway); Lin, Gigin (Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital at Linkou and Chang Gung University, 5 Fuhsing St., Guishan, 33382, Taoyuan, Taiwan); Selnæs, Kirsten M. (Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Olav Kyrres Gate 9, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, Trondheim, Norway); Goa, Pål E. (Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, Trondheim, Norway; Department of Physics, Norwegian University of Science and Technology, Trondheim, Norway); Bathen, Tone F. (Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Olav Kyrres Gate 9, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, Trondheim, Norway); Elschot, Mattijs (Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Olav Kyrres Gate 9, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, Trondheim, Norway)","Sørland, Kaia Ingerdatter (Norwegian University of Science and Technology); Elschot, Mattijs (Norwegian University of Science and Technology; St Olav's University Hospital)","Sørland, Kaia Ingerdatter (Norwegian University of Science and Technology); Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology); Sandsmark, Elise (St Olav's University Hospital); Langørgen, Sverre (St Olav's University Hospital); Bertilsson, Helena (Norwegian University of Science and Technology; St Olav's University Hospital); Trimble, Christopher G. (Norwegian University of Science and Technology); Lin, Gigin (Chang Gung University); Selnæs, Kirsten M. (Norwegian University of Science and Technology; St Olav's University Hospital); Goa, Pål E. (St Olav's University Hospital; Norwegian University of Science and Technology); Bathen, Tone F. (Norwegian University of Science and Technology; St Olav's University Hospital); Elschot, Mattijs (Norwegian University of Science and Technology; St Olav's University Hospital)",1,1,,,https://link.springer.com/content/pdf/10.1007/s10334-022-01003-9.pdf,https://app.dimensions.ai/details/publication/pub.1145493242,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,
5375,pub.1128994453,10.1016/j.ijrobp.2020.06.076,32634543,,Machine Segmentation of Pelvic Anatomy in MRI-Assisted Radiosurgery (MARS) for Prostate Cancer Brachytherapy,"PURPOSE: To investigate machine segmentation of pelvic anatomy in magnetic resonance imaging (MRI)-assisted radiosurgery (MARS) for prostate cancer using prostate brachytherapy MRIs acquired with different pulse sequences and image contrasts.
METHODS AND MATERIALS: Two hundred 3-dimensional (3D) preimplant and postimplant prostate brachytherapy MRI scans were acquired with a T2-weighted sequence, a T2/T1-weighted sequence, or a T1-weighted sequence. One hundred twenty deep machine learning models were trained to segment the prostate, seminal vesicles, external urinary sphincter, rectum, and bladder using the MRI scans acquired with T2-weighted and T2/T1-weighted image contrast. The deep machine learning models consisted of 18 fully convolutional networks (FCNs) with different convolutional encoders. Both 2-dimensional and 3D U-Net FCNs were constructed for comparison. Six objective functions were investigated: cross-entropy, Jaccard distance, focal loss, and 3 variations of Tversky distance. The performance of the models was compared using similarity metrics, including pixel accuracy, Jaccard index, Dice similarity coefficient (DSC), 95% Hausdorff distance, relative volume difference, Matthews correlation coefficient, precision, recall, and average symmetrical surface distance. We selected the highest-performing architecture and investigated how the amount of training data, use of skip connections, and data augmentation affected segmentation performance. In addition, we investigated whether segmentation on T1-weighted MRI was possible with FCNs trained on only T2-weighted and T2/T1-weighted image contrast.
RESULTS: Overall, an FCN with a DenseNet201 encoder trained via cross-entropy minimization yielded the highest combined segmentation performance. For the 53 3D test MRI scans acquired with T2-weighted or T2/T1-weighted image contrast, the DSCs of the prostate, external urinary sphincter, seminal vesicles, rectum, and bladder were 0.90 ± 0.04, 0.70 ± 0.15, 0.80 ± 0.12, 0.91 ± 0.06, and 0.96 ± 0.04, respectively, after model fine-tuning. For the 5 T1-weighted images, the DSCs of these organs were 0.82 ± 0.07, 0.17 ± 0.15, 0.46 ± 0.21, 0.87 ± 0.06, and 0.88 ± 0.05, respectively.
CONCLUSIONS: Machine segmentation of the prostate and surrounding anatomy on 3D MRIs acquired with different pulse sequences for MARS low-dose-rate prostate brachytherapy is possible with a single FCN.","Jeremiah W. Sanders would like to acknowledge the donors of the Pauline Altman-Goldstein Foundation Discovery Fellowship. The authors would like to acknowledge Sarah Bronson from Scientific Publications, Research Medical Library at The University of Texas MD Anderson Cancer Center for professionally editing this manuscript.",,International Journal of Radiation Oncology • Biology • Physics,,"Brachytherapy; Cohort Studies; Deep Learning; Entropy; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging, Interventional; Male; Neural Networks, Computer; Pelvis; Prostate; Prostatic Neoplasms; Radiosurgery; Rectum; Retrospective Studies; Seminal Vesicles; Urethra; Urinary Bladder",2020-07-04,2020,2020-07-04,2020-12,108,5,1292-1303,Closed,Article,"Sanders, Jeremiah W; Lewis, Gary D; Thames, Howard D; Kudchadker, Rajat J; Venkatesan, Aradhana M; Bruno, Teresa L; Ma, Jingfei; Pagel, Mark D; Frank, Steven J","Sanders, Jeremiah W (Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, Texas; Medical Physics Graduate Program, MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences, Houston, Texas. Electronic address: jsanders1@mdanderson.org.); Lewis, Gary D (Department of Radiation Oncology, University of Arkansas for Medical Sciences, Little Rock, Arkansas.); Thames, Howard D (Department of Biostatistics, The University of Texas MD Anderson Cancer Center, Houston, Texas.); Kudchadker, Rajat J (Medical Physics Graduate Program, MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences, Houston, Texas; Department of Radiation Physics, The University of Texas MD Anderson Cancer Center, Texas.); Venkatesan, Aradhana M (Medical Physics Graduate Program, MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences, Houston, Texas; Department of Diagnostic Radiology, The University of Texas MD Anderson Cancer Center, Houston, Texas.); Bruno, Teresa L (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, Texas.); Ma, Jingfei (Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, Texas; Medical Physics Graduate Program, MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences, Houston, Texas.); Pagel, Mark D (Department of Cancer Systems Imaging, The University of Texas MD Anderson Cancer Center, Texas.); Frank, Steven J (Department of Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, Texas.)","Sanders, Jeremiah W (The University of Texas MD Anderson Cancer Center)","Sanders, Jeremiah W (The University of Texas MD Anderson Cancer Center); Lewis, Gary D (University of Arkansas for Medical Sciences); Thames, Howard D (The University of Texas MD Anderson Cancer Center); Kudchadker, Rajat J (The University of Texas MD Anderson Cancer Center); Venkatesan, Aradhana M (The University of Texas MD Anderson Cancer Center); Bruno, Teresa L (The University of Texas MD Anderson Cancer Center); Ma, Jingfei (The University of Texas MD Anderson Cancer Center); Pagel, Mark D (The University of Texas MD Anderson Cancer Center); Frank, Steven J (The University of Texas MD Anderson Cancer Center)",13,11,1.84,6.63,,https://app.dimensions.ai/details/publication/pub.1128994453,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
5375,pub.1124959757,10.1109/tmi.2020.2973595,32070947,PMC7393676,Generalizing Deep Learning for Medical Image Segmentation to Unseen Domains via Deep Stacked Transformation,"Recent advances in deep learning for medical image segmentation demonstrate expert-level accuracy. However, application of these models in clinically realistic environments can result in poor generalization and decreased accuracy, mainly due to the domain shift across different hospitals, scanner vendors, imaging protocols, and patient populations etc. Common transfer learning and domain adaptation techniques are proposed to address this bottleneck. However, these solutions require data (and annotations) from the target domain to retrain the model, and is therefore restrictive in practice for widespread model deployment. Ideally, we wish to have a trained (locked) model that can work uniformly well across unseen domains without further training. In this paper, we propose a deep stacked transformation approach for domain generalization. Specifically, a series of n stacked transformations are applied to each image during network training. The underlying assumption is that the ""expected"" domain shift for a specific medical imaging modality could be simulated by applying extensive data augmentation on a single source domain, and consequently, a deep model trained on the augmented ""big"" data (BigAug) could generalize well on unseen domains. We exploit four surprisingly effective, but previously understudied, image-based characteristics for data augmentation to overcome the domain generalization problem. We train and evaluate the BigAug model (with n=9 transformations) on three different 3D segmentation tasks (prostate gland, left atrial, left ventricle) covering two medical imaging modalities (MRI and ultrasound) involving eight publicly available challenge datasets. The results show that when training on relatively small dataset (n = 10~32 volumes, depending on the size of the available datasets) from a single source domain: (i) BigAug models degrade an average of 11%(Dice score change) from source to unseen domain, substantially better than conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation method (degrading 25%), (ii) BigAug is better than ""shallower"" stacked transforms (i.e. those with fewer transforms) on unseen domains and demonstrates modest improvement to conventional augmentation on the source domain, (iii) after training with BigAug on one source domain, performance on an unseen domain is similar to training a model from scratch on that domain when using the same number of training samples. When training on large datasets (n = 465 volumes) with BigAug, (iv) application to unseen domains reaches the performance of state-of-the-art fully supervised models that are trained and tested on their source domains. These findings establish a strong benchmark for the study of domain generalization in medical imaging, and can be generalized to the design of highly robust deep segmentation models for clinical deployment.","This work was supported in part by the NIH Center for Interventional Oncology and the Intramural Research Program of the NIH. NIH and NVIDIA have a cooperative research and development agreement. This project has been funded in whole or in part with federal funds from the National Cancer Institute, National Institutes of Health, under Contract No. HHSN261200800001E.",,IEEE Transactions on Medical Imaging,,"Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male",2020-02-12,2020,2020-02-12,2020-07,39,7,2531-2540,All OA, Green,Article,"Zhang, Ling; Wang, Xiaosong; Yang, Dong; Sanford, Thomas; Harmon, Stephanie; Turkbey, Baris; Wood, Bradford J.; Roth, Holger; Myronenko, Andriy; Xu, Daguang; Xu, Ziyue","Zhang, Ling (Nvidia Corporation, Bethesda, MD, 20814, USA; PAII Inc., Bethesda, MD, 20817, USA); Wang, Xiaosong (Nvidia Corporation, Bethesda, MD, 20814, USA); Yang, Dong (Nvidia Corporation, Bethesda, MD, 20814, USA); Sanford, Thomas (National Institutes of Health Clinical Center, Bethesda, MD, 20892, USA); Harmon, Stephanie (Clinical Research Directorate, Frederick National Laboratory for Cancer Research, National Cancer Institute, Bethesda, MD, 20892, USA); Turkbey, Baris (National Institutes of Health Clinical Center, Bethesda, MD, 20892, USA); Wood, Bradford J. (National Institutes of Health Clinical Center, Bethesda, MD, 20892, USA); Roth, Holger (Nvidia Corporation, Bethesda, MD, 20814, USA); Myronenko, Andriy (Nvidia Corporation, Bethesda, MD, 20814, USA); Xu, Daguang (Nvidia Corporation, Bethesda, MD, 20814, USA); Xu, Ziyue (Nvidia Corporation, Bethesda, MD, 20814, USA)","Zhang, Ling (Nvidia (United States); )","Zhang, Ling (Nvidia (United States)); Wang, Xiaosong (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Sanford, Thomas (National Institutes of Health Clinical Center); Harmon, Stephanie (National Cancer Institute); Turkbey, Baris (National Institutes of Health Clinical Center); Wood, Bradford J. (National Institutes of Health Clinical Center); Roth, Holger (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Xu, Daguang (Nvidia (United States)); Xu, Ziyue (Nvidia (United States))",148,139,7.7,69.19,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7393676,https://app.dimensions.ai/details/publication/pub.1124959757,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
5358,pub.1131076578,10.1109/tmi.2020.3025517,32956051,PMC8202780,Boundary Coding Representation for Organ Segmentation in Prostate Cancer Radiotherapy,"Accurate segmentation of the prostate and organs at risk (OARs, e.g., bladder and rectum) in male pelvic CT images is a critical step for prostate cancer radiotherapy. Unfortunately, the unclear organ boundary and large shape variation make the segmentation task very challenging. Previous studies usually used representations defined directly on unclear boundaries as context information to guide segmentation. Those boundary representations may not be so discriminative, resulting in limited performance improvement. To this end, we propose a novel boundary coding network (BCnet) to learn a discriminative representation for organ boundary and use it as the context information to guide the segmentation. Specifically, we design a two-stage learning strategy in the proposed BCnet: 1) Boundary coding representation learning. Two sub-networks under the supervision of the dilation and erosion masks transformed from the manually delineated organ mask are first separately trained to learn the spatial-semantic context near the organ boundary. Then we encode the organ boundary based on the predictions of these two sub-networks and design a multi-atlas based refinement strategy by transferring the knowledge from training data to inference. 2) Organ segmentation. The boundary coding representation as context information, in addition to the image patches, are used to train the final segmentation network. Experimental results on a large and diverse male pelvic CT dataset show that our method achieves superior performance compared with several state-of-the-art methods.",This work was supported in part by NIH under Grant 5R01CA206100.,,IEEE Transactions on Medical Imaging,,"Humans; Male; Pelvis; Prostatic Neoplasms; Rectum; Tomography, X-Ray Computed",2020-12-29,2020,2020-12-29,2021-01,40,1,310-320,All OA, Green,Article,"Wang, Shuai; Liu, Mingxia; Lian, Jun; Shen, Dinggang","Wang, Shuai (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Liu, Mingxia (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Lian, Jun (Department of Radiation Oncology, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Shen, Dinggang (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA; Department of Artificial Intelligence, Korea University, Seoul, 02841, South Korea)","Lian, Jun (University of North Carolina at Chapel Hill)","Wang, Shuai (University of North Carolina at Chapel Hill); Liu, Mingxia (University of North Carolina at Chapel Hill); Lian, Jun (University of North Carolina at Chapel Hill); Shen, Dinggang (University of North Carolina at Chapel Hill; Korea University)",6,6,1.57,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8202780,https://app.dimensions.ai/details/publication/pub.1131076578,46 Information and Computing Sciences,,,,,,,,,,,
5358,pub.1130984905,10.3390/diagnostics10090714,32961895,PMC7555425,A Quality Control System for Automated Prostate Segmentation on T2-Weighted MRI,"Computer-aided detection and diagnosis (CAD) systems have the potential to improve robustness and efficiency compared to traditional radiological reading of magnetic resonance imaging (MRI). Fully automated segmentation of the prostate is a crucial step of CAD for prostate cancer, but visual inspection is still required to detect poorly segmented cases. The aim of this work was therefore to establish a fully automated quality control (QC) system for prostate segmentation based on T2-weighted MRI. Four different deep learning-based segmentation methods were used to segment the prostate for 585 patients. First order, shape and textural radiomics features were extracted from the segmented prostate masks. A reference quality score (QS) was calculated for each automated segmentation in comparison to a manual segmentation. A least absolute shrinkage and selection operator (LASSO) was trained and optimized on a randomly assigned training dataset (N = 1756, 439 cases from each segmentation method) to build a generalizable linear regression model based on the radiomics features that best estimated the reference QS. Subsequently, the model was used to estimate the QSs for an independent testing dataset (N = 584, 146 cases from each segmentation method). The mean ± standard deviation absolute error between the estimated and reference QSs was 5.47 ± 6.33 on a scale from 0 to 100. In addition, we found a strong correlation between the estimated and reference QSs (rho = 0.70). In conclusion, we developed an automated QC system that may be helpful for evaluating the quality of automated prostate segmentations.","We would like to thank SPIE, the AAPM, the NCI and Radboud University Nijmegen (Nijmegen, Netherlands) in addition to the organizers of the PROSTATEx and PROMISE12 challenges for making their datasets available. In addition, we would like to thank Inom Mirzaev from Ohio State University (Ohio, USA), Fausto Milletari from the Technical University of Munich (Munich, Germany) and Fabian Isensee from the German Cancer Research Center (Heidelberg, Germany) for making their segmentation methods publicly available. We would like to thank Sverre Langørgen from St. Olavs Hospital, Trondheim University Hospital (Trondheim, Norway), for supervising the prostate delineation process for the in-house collected dataset.","This research was funded by the Norwegian University of Science and Technology (NTNU) Biotechnology (grant number 81770928; M.R.S.S.), the Research Council of Norway (grant number 295013; T.F.B.) and the liaison committee between the Central Norway Regional Health Authority and the Norwegian University of Science and Technology (grant numbers 90368401; G.A.N. and 90265300; M.E.).",Diagnostics,,,2020-09-18,2020,2020-09-18,,10,9,714,All OA, Gold,Article,"Sunoqrot, Mohammed R. S.; Selnæs, Kirsten M.; Sandsmark, Elise; Nketiah, Gabriel A.; Zavala-Romero, Olmo; Stoyanova, Radka; Bathen, Tone F.; Elschot, Mattijs","Sunoqrot, Mohammed R. S. (Department of Circulation and Medical Imaging, NTNU—Norwegian University of Science and Technology, 7030 Trondheim, Norway;, Kirsten.Margrete.Selnes@stolav.no, (K.M.S.);, gabriel.a.nketiah@ntnu.no, (G.A.N.);, tone.f.bathen@ntnu.no, (T.F.B.);, mattijs.elschot@ntnu.no, (M.E.)); Selnæs, Kirsten M. (Department of Circulation and Medical Imaging, NTNU—Norwegian University of Science and Technology, 7030 Trondheim, Norway;, Kirsten.Margrete.Selnes@stolav.no, (K.M.S.);, gabriel.a.nketiah@ntnu.no, (G.A.N.);, tone.f.bathen@ntnu.no, (T.F.B.);, mattijs.elschot@ntnu.no, (M.E.); Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030 Trondheim, Norway;, elise.sandsmark@stolav.no); Sandsmark, Elise (Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030 Trondheim, Norway;, elise.sandsmark@stolav.no); Nketiah, Gabriel A. (Department of Circulation and Medical Imaging, NTNU—Norwegian University of Science and Technology, 7030 Trondheim, Norway;, Kirsten.Margrete.Selnes@stolav.no, (K.M.S.);, gabriel.a.nketiah@ntnu.no, (G.A.N.);, tone.f.bathen@ntnu.no, (T.F.B.);, mattijs.elschot@ntnu.no, (M.E.); Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030 Trondheim, Norway;, elise.sandsmark@stolav.no); Zavala-Romero, Olmo (Department of Radiation Oncology, University of Miami Miller School of Medicine, Miami, FL 33136, USA;, ozavala@coaps.fsu.edu, (O.Z.-R.);, rstoyanova@med.miami.edu, (R.S.); Center for Ocean-Atmospheric Prediction Studies, Florida State University, Tallahassee, FL 32306, USA); Stoyanova, Radka (Department of Radiation Oncology, University of Miami Miller School of Medicine, Miami, FL 33136, USA;, ozavala@coaps.fsu.edu, (O.Z.-R.);, rstoyanova@med.miami.edu, (R.S.)); Bathen, Tone F. (Department of Circulation and Medical Imaging, NTNU—Norwegian University of Science and Technology, 7030 Trondheim, Norway;, Kirsten.Margrete.Selnes@stolav.no, (K.M.S.);, gabriel.a.nketiah@ntnu.no, (G.A.N.);, tone.f.bathen@ntnu.no, (T.F.B.);, mattijs.elschot@ntnu.no, (M.E.); Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030 Trondheim, Norway;, elise.sandsmark@stolav.no); Elschot, Mattijs (Department of Circulation and Medical Imaging, NTNU—Norwegian University of Science and Technology, 7030 Trondheim, Norway;, Kirsten.Margrete.Selnes@stolav.no, (K.M.S.);, gabriel.a.nketiah@ntnu.no, (G.A.N.);, tone.f.bathen@ntnu.no, (T.F.B.);, mattijs.elschot@ntnu.no, (M.E.); Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030 Trondheim, Norway;, elise.sandsmark@stolav.no)","Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology; )","Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology); Selnæs, Kirsten M. (Norwegian University of Science and Technology; St Olav's University Hospital); Sandsmark, Elise (St Olav's University Hospital); Nketiah, Gabriel A. (Norwegian University of Science and Technology; St Olav's University Hospital); Zavala-Romero, Olmo (University of Miami; Florida State University); Stoyanova, Radka (University of Miami); Bathen, Tone F. (Norwegian University of Science and Technology; St Olav's University Hospital); Elschot, Mattijs (Norwegian University of Science and Technology; St Olav's University Hospital)",12,12,1.52,5.11,https://www.mdpi.com/2075-4418/10/9/714/pdf?version=1600432487,https://app.dimensions.ai/details/publication/pub.1130984905,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
5355,pub.1153877138,10.1186/s13244-022-01340-2,36543901,PMC9772373,Automatic segmentation of prostate zonal anatomy on MRI: a systematic review of the literature,"ObjectivesAccurate zonal segmentation of prostate boundaries on MRI is a critical prerequisite for automated prostate cancer detection based on PI-RADS. Many articles have been published describing deep learning methods offering great promise for fast and accurate segmentation of prostate zonal anatomy. The objective of this review was to provide a detailed analysis and comparison of applicability and efficiency of the published methods for automatic segmentation of prostate zonal anatomy by systematically reviewing the current literature.MethodsA Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) was conducted until June 30, 2021, using PubMed, ScienceDirect, Web of Science and EMBase databases. Risk of bias and applicability based on Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) criteria adjusted with Checklist for Artificial Intelligence in Medical Imaging (CLAIM) were assessed.
ResultsA total of 458 articles were identified, and 33 were included and reviewed. Only 2 articles had a low risk of bias for all four QUADAS-2 domains. In the remaining, insufficient details about database constitution and segmentation protocol provided sources of bias (inclusion criteria, MRI acquisition, ground truth). Eighteen different types of terminology for prostate zone segmentation were found, while 4 anatomic zones are described on MRI. Only 2 authors used a blinded reading, and 4 assessed inter-observer variability.
ConclusionsOur review identified numerous methodological flaws and underlined biases precluding us from performing quantitative analysis for this review. This implies low robustness and low applicability in clinical practice of the evaluated methods. Actually, there is not yet consensus on quality criteria for database constitution and zonal segmentation methodology.",The authors thank Thomas Desmousseaux for his help with prostate drawings.,The authors state that this work has not received any funding.,Insights into Imaging,,,2022-12-21,2022,2022-12-21,,13,1,202,All OA, Gold,Article,"Wu, Carine; Montagne, Sarah; Hamzaoui, Dimitri; Ayache, Nicholas; Delingette, Hervé; Renard-Penna, Raphaële","Wu, Carine (Sorbonne Université, Paris, France; Academic Department of Radiology, Hôpital Tenon, Assistance Publique des Hôpitaux de Paris, 4 Rue de La Chine, 75020, Paris, France); Montagne, Sarah (Sorbonne Université, Paris, France; Academic Department of Radiology, Hôpital Tenon, Assistance Publique des Hôpitaux de Paris, 4 Rue de La Chine, 75020, Paris, France; Academic Department of Radiology, Hôpital Pitié-Salpétrière, Assistance Publique des Hôpitaux de Paris, Paris, France; GRC N° 5, Oncotype-Uro, Sorbonne Université, Paris, France); Hamzaoui, Dimitri (Inria, Epione Team, Sophia Antipolis, Université Côte d’Azur, Nice, France); Ayache, Nicholas (Inria, Epione Team, Sophia Antipolis, Université Côte d’Azur, Nice, France); Delingette, Hervé (Inria, Epione Team, Sophia Antipolis, Université Côte d’Azur, Nice, France); Renard-Penna, Raphaële (Sorbonne Université, Paris, France; Academic Department of Radiology, Hôpital Tenon, Assistance Publique des Hôpitaux de Paris, 4 Rue de La Chine, 75020, Paris, France; Academic Department of Radiology, Hôpital Pitié-Salpétrière, Assistance Publique des Hôpitaux de Paris, Paris, France; GRC N° 5, Oncotype-Uro, Sorbonne Université, Paris, France)","Wu, Carine (Sorbonne University; )","Wu, Carine (Sorbonne University); Montagne, Sarah (Sorbonne University; Pitié-Salpêtrière Hospital; Sorbonne University); Hamzaoui, Dimitri (Université Côte d'Azur); Ayache, Nicholas (Université Côte d'Azur); Delingette, Hervé (Université Côte d'Azur); Renard-Penna, Raphaële (Sorbonne University; Pitié-Salpêtrière Hospital; Sorbonne University)",0,0,,,https://insightsimaging.springeropen.com/counter/pdf/10.1186/s13244-022-01340-2,https://app.dimensions.ai/details/publication/pub.1153877138,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
5355,pub.1149884999,10.1186/s41747-022-00288-8,35909214,PMC9339427,"Artificial intelligence for prostate MRI: open datasets, available applications, and grand challenges","Artificial intelligence (AI) for prostate magnetic resonance imaging (MRI) is starting to play a clinical role for prostate cancer (PCa) patients. AI-assisted reading is feasible, allowing workflow reduction. A total of 3,369 multi-vendor prostate MRI cases are available in open datasets, acquired from 2003 to 2021 in Europe or USA at 3 T (n = 3,018; 89.6%) or 1.5 T (n = 296; 8.8%), 346 cases scanned with endorectal coil (10.3%), 3,023 (89.7%) with phased-array surface coils; 412 collected for anatomical segmentation tasks, 3,096 for PCa detection/classification; for 2,240 cases lesions delineation is available and 56 cases have matching histopathologic images; for 2,620 cases the PSA level is provided; the total size of all open datasets amounts to approximately 253 GB. Of note, quality of annotations provided per dataset highly differ and attention must be paid when using these datasets (e.g., data overlap). Seven grand challenges and commercial applications from eleven vendors are here considered. Few small studies provided prospective validation. More work is needed, in particular validation on large-scale multi-institutional, well-curated public datasets to test general applicability. Moreover, AI needs to be explored for clinical stages other than detection/characterization (e.g., follow-up, prognosis, interventions, and focal treatment).",,"This work was financed by the Research Council of Norway (Grant Number 295013), the Norwegian Cancer Society and Prostatakreftforeningen (Grant Number 215951), the Liaison Committee between the Central Norway Regional Health Authority and the Norwegian University of Science and Technology (Grant Numbers 90265300 and 90793700), EU H2020 ProCAncer-I (Grant Number 952159), EU H2020 PANCAIM (Grant Number 101016851), and EU IMI2 PIONEER (Grant Number 777492). Open access funding provided by Norwegian University of Science and Technology.",European Radiology Experimental,,Artificial Intelligence, Humans, Magnetic Resonance Imaging, Male, Prostate, Prostatic Neoplasms, Sensitivity and Specificity,2022-08-01,2022,2022-08-01,,6,1,35,All OA, Gold,Article,"Sunoqrot, Mohammed R. S.; Saha, Anindo; Hosseinzadeh, Matin; Elschot, Mattijs; Huisman, Henkjan","Sunoqrot, Mohammed R. S. (Department of Circulation and Medical Imaging, NTNU–Norwegian University of Science and Technology, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030, Trondheim, Norway); Saha, Anindo (Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6525 GA, Nijmegen, The Netherlands); Hosseinzadeh, Matin (Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6525 GA, Nijmegen, The Netherlands); Elschot, Mattijs (Department of Circulation and Medical Imaging, NTNU–Norwegian University of Science and Technology, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030, Trondheim, Norway); Huisman, Henkjan (Department of Circulation and Medical Imaging, NTNU–Norwegian University of Science and Technology, 7030, Trondheim, Norway; Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud University Medical Center, 6525 GA, Nijmegen, The Netherlands)","Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology; St Olav's University Hospital)","Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology; St Olav's University Hospital); Saha, Anindo (Radboud University Nijmegen Medical Centre); Hosseinzadeh, Matin (Radboud University Nijmegen Medical Centre); Elschot, Mattijs (Norwegian University of Science and Technology; St Olav's University Hospital); Huisman, Henkjan (Norwegian University of Science and Technology; Radboud University Nijmegen Medical Centre)",6,6,,,https://eurradiolexp.springeropen.com/counter/pdf/10.1186/s41747-022-00288-8,https://app.dimensions.ai/details/publication/pub.1149884999,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,
5355,pub.1141610429,10.1002/mp.15268,34609756,PMC8597653,Fully automated multiorgan segmentation of female pelvic magnetic resonance images with coarse‐to‐fine convolutional neural network,"PURPOSE: Brachytherapy combined with external beam radiotherapy (EBRT) is the standard treatment for cervical cancer and has been shown to improve overall survival rates compared to EBRT only. Magnetic resonance (MR) imaging is used for radiotherapy (RT) planning and image guidance due to its excellent soft tissue image contrast. Rapid and accurate segmentation of organs at risk (OAR) is a crucial step in MR image-guided RT. In this paper, we propose a fully automated two-step convolutional neural network (CNN) approach to delineate multiple OARs from T2-weighted (T2W) MR images.
METHODS: We employ a coarse-to-fine segmentation strategy. The coarse segmentation step first identifies the approximate boundary of each organ of interest and crops the MR volume around the centroid of organ-specific region of interest (ROI). The cropped ROI volumes are then fed to organ-specific fine segmentation networks to produce detailed segmentation of each organ. A three-dimensional (3-D) U-Net is trained to perform the coarse segmentation. For the fine segmentation, a 3-D Dense U-Net is employed in which a modified 3-D dense block is incorporated into the 3-D U-Net-like network to acquire inter and intra-slice features and improve information flow while reducing computational complexity. Two sets of T2W MR images (221 cases for MR1 and 62 for MR2) were taken with slightly different imaging parameters and used for our network training and test. The network was first trained on MR1 which was a larger sample set. The trained model was then transferred to the MR2 domain via a fine-tuning approach. Active learning strategy was utilized for selecting the most valuable data from MR2 to be included in the adaptation via transfer learning.
RESULTS: The proposed method was tested on 20 MR1 and 32 MR2 test sets. Mean ± SD dice similarity coefficients are 0.93 ± 0.04, 0.87 ± 0.03, and 0.80 ± 0.10 on MR1 and 0.94 ± 0.05, 0.88 ± 0.04, and 0.80 ± 0.05 on MR2 for bladder, rectum, and sigmoid, respectively. Hausdorff distances (95th percentile) are 4.18 ± 0.52, 2.54 ± 0.41, and 5.03 ± 1.31 mm on MR1 and 2.89 ± 0.33, 2.24 ± 0.40, and 3.28 ± 1.08 mm on MR2, respectively. The performance of our method is superior to other state-of-the-art segmentation methods.
CONCLUSIONS: We proposed a two-step CNN approach for fully automated segmentation of female pelvic MR bladder, rectum, and sigmoid from T2W MR volume. Our experimental results demonstrate that the developed method is accurate, fast, and reproducible, and outperforms alternative state-of-the-art methods for OAR segmentation significantly (p < 0.05).",This work was supported by the National Institutes of Health under Grant No. R01CA237005.,,Medical Physics,,"Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Organs at Risk; Pelvis; Tomography, X-Ray Computed",2021-10-21,2021,2021-10-21,2021-11,48,11,7028-7042,Closed,Article,"Zabihollahy, Fatemeh; Viswanathan, Akila N; Schmidt, Ehud J; Morcos, Marc; Lee, Junghoon","Zabihollahy, Fatemeh (Department of Radiation Oncology and Molecular Radiation Sciences, Johns Hopkins University, Baltimore, Maryland, USA); Viswanathan, Akila N (Department of Radiation Oncology and Molecular Radiation Sciences, Johns Hopkins University, Baltimore, Maryland, USA); Schmidt, Ehud J (Division of Cardiology, Department of Medicine, Johns Hopkins University, Baltimore, Maryland, USA); Morcos, Marc (Department of Radiation Oncology and Molecular Radiation Sciences, Johns Hopkins University, Baltimore, Maryland, USA); Lee, Junghoon (Department of Radiation Oncology and Molecular Radiation Sciences, Johns Hopkins University, Baltimore, Maryland, USA)","Lee, Junghoon (Johns Hopkins University)","Zabihollahy, Fatemeh (Johns Hopkins University); Viswanathan, Akila N (Johns Hopkins University); Schmidt, Ehud J (Johns Hopkins University); Morcos, Marc (Johns Hopkins University); Lee, Junghoon (Johns Hopkins University)",13,13,2.26,16.28,,https://app.dimensions.ai/details/publication/pub.1141610429,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
5355,pub.1124921558,10.1109/tmi.2020.2974574,32078543,,MS-Net: Multi-Site Network for Improving Prostate Segmentation With Heterogeneous MRI Data,"Automated prostate segmentation in MRI is highly demanded for computer-assisted diagnosis. Recently, a variety of deep learning methods have achieved remarkable progress in this task, usually relying on large amounts of training data. Due to the nature of scarcity for medical images, it is important to effectively aggregate data from multiple sites for robust model training, to alleviate the insufficiency of single-site samples. However, the prostate MRIs from different sites present heterogeneity due to the differences in scanners and imaging protocols, raising challenges for effective ways of aggregating multi-site data for network training. In this paper, we propose a novel multi-site network (MS-Net) for improving prostate segmentation by learning robust representations, leveraging multiple sources of data. To compensate for the inter-site heterogeneity of different MRI datasets, we develop Domain-Specific Batch Normalization layers in the network backbone, enabling the network to estimate statistics and perform feature normalization for each site separately. Considering the difficulty of capturing the shared knowledge from multiple datasets, a novel learning paradigm, i.e., Multi-site-guided Knowledge Transfer, is proposed to enhance the kernels to extract more generic representations from multi-site data. Extensive experiments on three heterogeneous prostate MRI datasets demonstrate that our MS-Net improves the performance across all datasets consistently, and outperforms state-of-the-art methods for multi-site learning.",This work was supported in part by the HongKong Research GrantsCouncil under ProjectCUHK14225616 and in part by the National Natural Science Foundation of China underProject U1813204.,,IEEE Transactions on Medical Imaging,,"Deep Learning; Diagnosis, Computer-Assisted; Humans; Magnetic Resonance Imaging; Male; Prostate",2020-02-17,2020,2020-02-17,2020-09,39,9,2713-2724,All OA, Green,Article,"Liu, Quande; Dou, Qi; Yu, Lequan; Heng, Pheng Ann","Liu, Quande (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Dou, Qi (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Yu, Lequan (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Radiation Oncology, Stanford University, Stanford, CA, 94305, USA); Heng, Pheng Ann (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Beijing, 100864, China)","Dou, Qi (Chinese University of Hong Kong)","Liu, Quande (Chinese University of Hong Kong); Dou, Qi (Chinese University of Hong Kong); Yu, Lequan (Chinese University of Hong Kong; Stanford University); Heng, Pheng Ann (Chinese University of Hong Kong; Chinese Academy of Sciences)",99,89,6.4,51.02,http://arxiv.org/pdf/2002.03366,https://app.dimensions.ai/details/publication/pub.1124921558,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5342,pub.1048259117,10.1007/s13246-014-0278-5,24859771,,Opportunities for image analysis in radiation oncology,,,,Physical and Engineering Sciences in Medicine,,"Diagnostic Imaging; Humans; Image Interpretation, Computer-Assisted; Image Processing, Computer-Assisted; Radiation Oncology",2014-05-24,2014,2014-05-24,2014-06,37,2,275-277,All OA, Hybrid,Article,"Dowling, Jason A.","Dowling, Jason A. (Commonwealth Scientific and Industrial Research Organisation, Australian e-Health Research Centre, UQ Health Sciences Building Royal Brisbane and Women’s Hospital, 4029, Herston, QLD, Australia)","Dowling, Jason A. (Australian e-Health Research Centre)","Dowling, Jason A. (Australian e-Health Research Centre)",2,0,0.17,0.55,https://link.springer.com/content/pdf/10.1007/s13246-014-0278-5.pdf,https://app.dimensions.ai/details/publication/pub.1048259117,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
5336,pub.1153962286,10.1038/s41598-022-26482-7,36566313,PMC9790020,Leveraging image complexity in macro-level neural network design for medical image segmentation,"Recent progress in encoder–decoder neural network architecture design has led to significant performance improvements in a wide range of medical image segmentation tasks. However, state-of-the-art networks for a given task may be too computationally demanding to run on affordable hardware, and thus users often resort to practical workarounds by modifying various macro-level design aspects. Two common examples are downsampling of the input images and reducing the network depth or size to meet computer memory constraints. In this paper, we investigate the effects of these changes on segmentation performance and show that image complexity can be used as a guideline in choosing what is best for a given dataset. We consider four statistical measures to quantify image complexity and evaluate their suitability on ten different public datasets. For the purpose of our illustrative experiments, we use DeepLabV3+ (deep large-size), M2U-Net (deep lightweight), U-Net (shallow large-size), and U-Net Lite (shallow lightweight). Our results suggest that median frequency is the best complexity measure when deciding on an acceptable input downsampling factor and using a deep versus shallow, large-size versus lightweight network. For high-complexity datasets, a lightweight network running on the original images may yield better segmentation results than a large-size network running on downsampled images, whereas the opposite may be the case for low-complexity images.",,,Scientific Reports,,"Image Processing, Computer-Assisted; Neural Networks, Computer; Costs and Cost Analysis",2022-12-24,2022,2022-12-24,,12,1,22286,All OA, Gold,Article,"Khan, Tariq M.; Naqvi, Syed S.; Meijering, Erik","Khan, Tariq M. (School of Computer Science and Engineering, University of New South Wales, Sydney, NSW, Australia); Naqvi, Syed S. (Department of Electrical and Computer Engineering, COMSATS University, Islamabad, Pakistan); Meijering, Erik (School of Computer Science and Engineering, University of New South Wales, Sydney, NSW, Australia)","Khan, Tariq M. (UNSW Sydney)","Khan, Tariq M. (UNSW Sydney); Naqvi, Syed S. (COMSATS University Islamabad); Meijering, Erik (UNSW Sydney)",0,0,,,https://www.nature.com/articles/s41598-022-26482-7.pdf,https://app.dimensions.ai/details/publication/pub.1153962286,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5336,pub.1151335092,10.1109/tmi.2022.3210133,36155434,,Domain and Content Adaptive Convolution Based Multi-Source Domain Generalization for Medical Image Segmentation,"The domain gap caused mainly by variable medical image quality renders a major obstacle on the path between training a segmentation model in the lab and applying the trained model to unseen clinical data. To address this issue, domain generalization methods have been proposed, which however usually use static convolutions and are less flexible. In this paper, we propose a multi-source domain generalization model based on the domain and content adaptive convolution (DCAC) for the segmentation of medical images across different modalities. Specifically, we design the domain adaptive convolution (DAC) module and content adaptive convolution (CAC) module and incorporate both into an encoder-decoder backbone. In the DAC module, a dynamic convolutional head is conditioned on the predicted domain code of the input to make our model adapt to the unseen target domain. In the CAC module, a dynamic convolutional head is conditioned on the global image features to make our model adapt to the test image. We evaluated the DCAC model against the baseline and four state-of-the-art domain generalization methods on the prostate segmentation, COVID-19 lesion segmentation, and optic cup/optic disc segmentation tasks. Our results not only indicate that the proposed DCAC model outperforms all competing methods on each segmentation task but also demonstrate the effectiveness of the DAC and CAC modules. Code is available at https://git.io/DCAC.","The authors acknowledge the RSNA and Society of Thoracic Radiology (STR), the European Society of Medical Imaging Informatics, the American College of Radiology, and the American Association of Physicists in Medicine, and their critical role in the creation of the free publicly available RICORD dataset used for this study. They also appreciate the efforts devoted by the authors of [10] and [24] to collect and share the prostate MR and fundus imaging data for comparing generalizable medical image segmentation algorithms.","This work was supported in part by the National Natural Science Foundation of China under Grant 62171377, in part by the National Key Research and Development Program of China under Grant 2022YFC2009903/2022YFC2009900, and in part by the Key Research and Development Program of Shaanxi Province, China, under Grant 2022GY-084.",IEEE Transactions on Medical Imaging,,,2022-09-26,2022,2022-09-26,2022-09-26,42,1,233-244,All OA, Green,Article,"Hu, Shishuai; Liao, Zehui; Zhang, Jianpeng; Xia, Yong","Hu, Shishuai (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, China); Liao, Zehui (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, China); Zhang, Jianpeng (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, China); Xia, Yong (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, China)","Xia, Yong (Northwestern Polytechnical University)","Hu, Shishuai (Northwestern Polytechnical University); Liao, Zehui (Northwestern Polytechnical University); Zhang, Jianpeng (Northwestern Polytechnical University); Xia, Yong (Northwestern Polytechnical University)",1,1,,,http://arxiv.org/pdf/2109.05676,https://app.dimensions.ai/details/publication/pub.1151335092,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
5336,pub.1149553433,10.1002/mp.15861,35851482,,Multi‐scale discriminative network for prostate cancer lesion segmentation in multiparametric MR images,"PURPOSE: The accurate and reliable segmentation of prostate cancer (PCa) lesions using multiparametric magnetic resonance imaging (mpMRI) sequences, is crucial to the image-guided intervention and treatment of prostate disease. For PCa lesion segmentation, it is essential to reliably combine local and global information to retain the features of small targets at multiple scales. Therefore, this study proposes a multi-scale segmentation network with a cascading pyramid convolution module (CPCM) and a double-input channel attention module (DCAM) for the automated and accurate segmentation of PCa lesions using mpMRI.
METHODS: First, the region of interest was extracted from the data by clipping to enlarge the target region and reduce the background noise interference. Next, four CPCMs with large convolution kernels in their skip connection paths were designed to improve the feature extraction capability of the network for small targets. At the same time, a convolution decomposition was applied to reduce the computational complexity. Finally, the DCAM was adopted in the decoder to provide bottom-up semantic discriminative guidance; it can use the semantic information of the network's deep features to guide the shallow output of features with a higher discriminant ability. A residual refinement module (RRM) was also designed to strengthen the recognition ability of each stage. The feature maps of the skip connection and the decoder all go through the RRM.
RESULTS: For the Initiative for Collaborative Computer Vision Benchmarking (I2CVB) dataset, our proposed model achieved a Dice similarity coefficient (DSC) of 79.31% and an average boundary distance (ABD) of 4.15 mm. For the Prostate Multiparametric MRI (PROMM) dataset, our method greatly improved the DSC to 82.11% and obtained an ABD of 3.64 mm.
CONCLUSIONS: The experimental results of two different mpMRI prostate datasets demonstrate that our model is more accurate and reliable on small targets. In addition, it outperforms other state-of-the-art methods.","This work was supported by the Science and Technology Commission of Shanghai Municipality (Nos. 17411952300, 20DZ2254400, 21DZ2200600, and 20DZ2261200). We greatly appreciate the Initiative for Collaborative Computer Vision Benchmarking (I2CVB) for sharing the prostate mpMRI dataset analyzed in this study.",,Medical Physics,,Humans, Male, Prostatic Neoplasms, Magnetic Resonance Imaging,2022-07-30,2022,2022-07-30,2022-11,49,11,7001-7015,Closed,Article,"Liu, Yatong; Zhu, Yu; Wang, Wei; Zheng, Bingbing; Qin, Xiangxiang; Wang, Peijun","Liu, Yatong (School of Information Science and Technology, East China University of Science and Technology, Shanghai, P. R. China); Zhu, Yu (School of Information Science and Technology, East China University of Science and Technology, Shanghai, P. R. China; Shanghai Engineering Research Center of Internet of Things for Respiratory Medicine, Shanghai, P. R. China); Wang, Wei (Department of Radiology, Tongji Hospital, Tongji University School of Medicine, Shanghai, P. R. China); Zheng, Bingbing (School of Information Science and Technology, East China University of Science and Technology, Shanghai, P. R. China); Qin, Xiangxiang (School of Information Science and Technology, East China University of Science and Technology, Shanghai, P. R. China); Wang, Peijun (Department of Radiology, Tongji Hospital, Tongji University School of Medicine, Shanghai, P. R. China)","Zhu, Yu (East China University of Science and Technology; ); Wang, Wei (Tongji University)","Liu, Yatong (East China University of Science and Technology); Zhu, Yu (East China University of Science and Technology); Wang, Wei (Tongji University); Zheng, Bingbing (East China University of Science and Technology); Qin, Xiangxiang (East China University of Science and Technology); Wang, Peijun (Tongji University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1149553433,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,
5336,pub.1143925197,10.3389/fninf.2021.782262,34975444,PMC8717777,Co-optimization Learning Network for MRI Segmentation of Ischemic Penumbra Tissues,"Convolutional neural networks (CNNs) have brought hope for the medical image auxiliary diagnosis. However, the shortfall of labeled medical image data is the bottleneck that limits the performance improvement of supervised CNN methods. In addition, annotating a large number of labeled medical image data is often expensive and time-consuming. In this study, we propose a co-optimization learning network (COL-Net) for Magnetic Resonance Imaging (MRI) segmentation of ischemic penumbra tissues. COL-Net base on the limited labeled samples and consists of an unsupervised reconstruction network (R), a supervised segmentation network (S), and a transfer block (T). The reconstruction network extracts the robust features from reconstructing pseudo unlabeled samples, which is the auxiliary branch of the segmentation network. The segmentation network is used to segment the target lesions under the limited labeled samples and the auxiliary of the reconstruction network. The transfer block is used to co-optimization the feature maps between the bottlenecks of the reconstruction network and segmentation network. We propose a mix loss function to optimize COL-Net. COL-Net is verified on the public ischemic penumbra segmentation challenge (SPES) with two dozen labeled samples. Results demonstrate that COL-Net has high predictive accuracy and generalization with the Dice coefficient of 0.79. The extended experiment also shows COL-Net outperforms most supervised segmentation methods. COL-Net is a meaningful attempt to alleviate the limited labeled sample problem in medical image segmentation.",,,Frontiers in Neuroinformatics,,,2021-12-16,2021,2021-12-16,,15,,782262,All OA, Gold,Article,"Liu, Liangliang; Zhang, Jing; Wang, Jin-xiang; Xiong, Shufeng; Zhang, Hui","Liu, Liangliang (College of Information and Management Science, Henan Agricultural University, Zhengzhou, China); Zhang, Jing (Department of Computer Science, Henan Quality Engineering Vocational College, Pingdingshan, China); Wang, Jin-xiang (Department of Computer Science, University of Melbourne, Parkville, VIC, Australia); Xiong, Shufeng (College of Information and Management Science, Henan Agricultural University, Zhengzhou, China); Zhang, Hui (College of Information and Management Science, Henan Agricultural University, Zhengzhou, China)","Zhang, Hui (Henan Agricultural University)","Liu, Liangliang (Henan Agricultural University); Zhang, Jing (); Wang, Jin-xiang (University of Melbourne); Xiong, Shufeng (Henan Agricultural University); Zhang, Hui (Henan Agricultural University)",1,1,0.54,0.82,https://www.frontiersin.org/articles/10.3389/fninf.2021.782262/pdf,https://app.dimensions.ai/details/publication/pub.1143925197,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5332,pub.1149024700,10.1109/tmi.2022.3186731,35759584,,Attention-Assisted Adversarial Model for Cerebrovascular Segmentation in 3D TOF-MRA Volumes,"Cerebrovascular segmentation in time-of-flight magnetic resonance angiography (TOF-MRA) volumes is essential for a variety of diagnostic and analytical applications. However, accurate cerebrovascular segmentation in 3D TOF-MRA is faced with multiple issues, including vast variations in cerebrovascular morphology and intensity, noisy background, and severe class imbalance between foreground cerebral vessels and background. In this work, a 3D adversarial network model called A-SegAN is proposed to segment cerebral vessels in TOF-MRA volumes. The proposed model is composed of a segmentation network A-SegS to predict segmentation maps, and a critic network A-SegC to discriminate predictions from ground truth. Based on this model, the aforementioned issues are addressed by the prevailing visual attention mechanism. First, A-SegS is incorporated with feature-attention blocks to filter out discriminative feature maps, though the cerebrovascular has varied appearances. Second, a hard-example-attention loss is exploited to boost the training of A-SegS on hard samples. Further, A-SegC is combined with an input-attention layer to attach importance to foreground cerebrovascular class. The proposed methods were evaluated on a self-constructed voxel-wise annotated cerebrovascular TOF-MRA segmentation dataset, and experimental results indicate that A-SegAN achieves competitive or better cerebrovascular segmentation results compared to other deep learning methods, effectively alleviating the above issues.",,This work was supported in part by the National Key Research and Development Program of China under Grant 2019YFB1311301 and in part by the Beijing Natural Science Foundation under Grant 4222007.,IEEE Transactions on Medical Imaging,,Magnetic Resonance Angiography, Algorithms,2022-12-02,2022,2022-12-02,2022-12,41,12,3520-3532,Closed,Article,"Chen, Ying; Jin, Darui; Guo, Bin; Bai, Xiangzhi","Chen, Ying (Image Processing Center, Beihang University, Beijing, 102206, China); Jin, Darui (Image Processing Center, Beihang University, Beijing, 102206, China; ShenYuan Honors College, Beihang University, Beijing, 100191, China); Guo, Bin (Image Processing Center, Beihang University, Beijing, 102206, China); Bai, Xiangzhi (Image Processing Center, Beihang University, Beijing, 102206, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100083, China)","Bai, Xiangzhi (Beihang University; Beihang University; Beihang University)","Chen, Ying (Beihang University); Jin, Darui (Beihang University; Beihang University); Guo, Bin (Beihang University); Bai, Xiangzhi (Beihang University; Beihang University; Beihang University)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1149024700,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
5220,pub.1119913669,10.1016/j.diii.2019.05.008,31358460,,Annotated normal CT data of the abdomen for deep learning: Challenges and strategies for implementation,"PURPOSE: The purpose of this study was to report procedures developed to annotate abdominal computed tomography (CT) images from subjects without pancreatic disease that will be used as the input for deep convolutional neural networks (DNN) for development of deep learning algorithms for automatic recognition of a normal pancreas.
MATERIALS AND METHODS: Dual-phase contrast-enhanced volumetric CT acquired from 2005 to 2009 from potential kidney donors were retrospectively assessed. Four trained human annotators manually and sequentially annotated 22 structures in each datasets, then expert radiologists confirmed the annotation. For efficient annotation and data management, a commercial software package that supports three-dimensional segmentation was used.
RESULTS: A total of 1150 dual-phase CT datasets from 575 subjects were annotated. There were 229 men and 346 women (mean age: 45±12years; range: 18-79years). The mean intra-observer intra-subject dual-phase CT volume difference of all annotated structures was 4.27mL (7.65%). The deep network prediction for multi-organ segmentation showed high fidelity with 89.4% and 1.29mm in terms of mean Dice similarity coefficients and mean surface distances, respectively.
CONCLUSIONS: A reliable data collection/annotation process for abdominal structures was developed. This process can be used to generate large datasets appropriate for deep learning.",,"This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research. Drs. Park, Chu, Fishman, Yuille, Fouladi, Shayesteh, Graves, and Kawamoto have grant support by the Lustgarten Foundation for Pancreatic Cancer Research.",Diagnostic and Interventional Imaging,,"Abdomen; Adolescent; Adult; Aged; Deep Learning; Female; Humans; Male; Middle Aged; Retrospective Studies; Tomography, X-Ray Computed; Young Adult",2019-07-26,2019,2019-07-26,2020-01,101,1,35-44,All OA, Bronze,Article,"Park, S.; Chu, L.C.; Fishman, E.K.; Yuille, A.L.; Vogelstein, B.; Kinzler, K.W.; Horton, K.M.; Hruban, R.H.; Zinreich, E.S.; Fouladi, D. Fadaei; Shayesteh, S.; Graves, J.; Kawamoto, S.","Park, S. (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA); Chu, L.C. (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA); Fishman, E.K. (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA); Yuille, A.L. (Department of Computer Science, Johns Hopkins University, School of Arts and Sciences, Baltimore, MD 21218, USA); Vogelstein, B. (Sidney Kimmel Comprehensive Cancer Center, Johns Hopkins University, School of Medicine, Baltimore, MD 21287, USA; Johns Hopkins University, School of Medicine, Ludwig Center for Cancer Genetics and Therapeutics, Baltimore, MD 21205, USA); Kinzler, K.W. (Sidney Kimmel Comprehensive Cancer Center, Johns Hopkins University, School of Medicine, Baltimore, MD 21287, USA; Johns Hopkins University, School of Medicine, Ludwig Center for Cancer Genetics and Therapeutics, Baltimore, MD 21205, USA); Horton, K.M. (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA); Hruban, R.H. (Department of Pathology, The Sol Goldman Pancreatic Cancer Research Center, Johns Hopkins University, School of Medicine, Baltimore, MD 21205, USA); Zinreich, E.S. (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA); Fouladi, D. Fadaei (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA); Shayesteh, S. (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA); Graves, J. (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA); Kawamoto, S. (The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University, School of Medicine, 601N. Caroline Street, Baltimore, MD 21287, USA)","Kawamoto, S. (Johns Hopkins University)","Park, S. (Johns Hopkins University); Chu, L.C. (Johns Hopkins University); Fishman, E.K. (Johns Hopkins University); Yuille, A.L. (Johns Hopkins University); Vogelstein, B. (Sidney Kimmel Comprehensive Cancer Center; Johns Hopkins University); Kinzler, K.W. (Sidney Kimmel Comprehensive Cancer Center; Johns Hopkins University); Horton, K.M. (Johns Hopkins University); Hruban, R.H. (Johns Hopkins University); Zinreich, E.S. (Johns Hopkins University); Fouladi, D. Fadaei (Johns Hopkins University); Shayesteh, S. (Johns Hopkins University); Graves, J. (Johns Hopkins University); Kawamoto, S. (Johns Hopkins University)",43,30,4.35,30.03,https://doi.org/10.1016/j.diii.2019.05.008,https://app.dimensions.ai/details/publication/pub.1119913669,32 Biomedical and Clinical Sciences, 46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
5083,pub.1150157554,10.1109/tcbb.2022.3198284,35951571,,DFTNet: Dual-Path Feature Transfer Network for Weakly Supervised Medical Image Segmentation,"Medical image segmentation has long suffered from the problem of expensive labels. Acquiring pixel-level annotations is time-consuming, labor-intensive, and relies on extensive expert knowledge. Bounding box annotations, in contrast, are relatively easy to acquire. Thus, in this paper, we explore to segment images through a novel Dual-path Feature Transfer design with only bounding box annotations. Specifically, a Target-aware Reconstructor is proposed to extract target-related features by reconstructing the pixels within the bounding box through the channel and spatial attention module. Then, a sliding Feature Fusion and Transfer Module (FFTM) fuses the extracted features from Reconstructor and transfers them to guide the Segmentor for segmentation. Finally, we present the Confidence Ranking Loss (CRLoss) which dynamically assigns weights to the loss of each pixel based on the network's confidence. CRLoss mitigates the impact of inaccurate pseudo-labels on performance. Extensive experiments demonstrate that our proposed model achieves state-of-the-art performance on the Medical Segmentation Decathlon (MSD) Brain Tumour and PROMISE12 datasets.",,,IEEE/ACM Transactions on Computational Biology and Bioinformatics,,,2022-08-11,2022,2022-08-11,2022-08-11,PP,99,1-12,Closed,Article,"Cai, Wentian; Xie, Linsen; Yang, Weixian; Li, Yijiang; Gao, Ying; Wang, Tingting","Cai, Wentian (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Xie, Linsen (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Yang, Weixian (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Li, Yijiang (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Gao, Ying (School of Computer Science and Engineering, South China University of Technology, Guangzhou, P. R. China); Wang, Tingting (Faculty of Information Technology, Macau University of Science and Technology, Taipa, Macao)",,"Cai, Wentian (South China University of Technology); Xie, Linsen (South China University of Technology); Yang, Weixian (South China University of Technology); Li, Yijiang (South China University of Technology); Gao, Ying (South China University of Technology); Wang, Tingting (Macau University of Science and Technology)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1150157554,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,,
5083,pub.1149601356,10.1109/tmi.2022.3192483,35853071,,Personalized Retrogress-Resilient Federated Learning Toward Imbalanced Medical Data,"Clinically oriented deep learning algorithms, combined with large-scale medical datasets, have significantly promoted computer-aided diagnosis. To address increasing ethical and privacy issues, Federated Learning (FL) adopts a distributed paradigm to collaboratively train models, rather than collecting samples from multiple institutions for centralized training. Despite intensive research on FL, two major challenges are still existing when applying FL in the real-world medical scenarios, including the performance degradation (i.e., retrogress) after each communication and the intractable class imbalance. Thus, in this paper, we propose a novel personalized FL framework to tackle these two problems. For the retrogress problem, we first devise a Progressive Fourier Aggregation (PFA) at the server side to gradually integrate parameters of client models in the frequency domain. Then, at the client side, we design a Deputy-Enhanced Transfer (DET) to smoothly transfer global knowledge to the personalized local model. For the class imbalance problem, we propose the Conjoint Prototype-Aligned (CPA) loss to facilitate the balanced optimization of the FL framework. Considering the inaccessibility of private local data to other participants in FL, the CPA loss calculates the global conjoint objective based on global imbalance, and then adjusts the client-side local training through the prototype-aligned refinement to eliminate the imbalance gap with such a balanced goal. Extensive experiments are performed on real-world dermoscopic and prostate MRI FL datasets. The experimental results demonstrate the advantages of our FL framework in real-world medical scenarios, by outperforming state-of-the-art FL methods with a large margin. The source code is available at https://github.com/CityU-AIM-Group/PRR-Imbalancehttps://github.com/CityU-AIM-Group/PRR-Imbalance.",,This work was supported in part by the Shenzhen-Hong Kong Innovation Circle Category D Project under Grant SGDX2019081623300177 (CityU 9240008) and in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2020A1515111070.,IEEE Transactions on Medical Imaging,,"Male; Humans; Algorithms; Diagnosis, Computer-Assisted; Pelvis; Prostate; Software",2022-12-02,2022,2022-12-02,2022-12,41,12,3663-3674,Closed,Article,"Chen, Zhen; Yang, Chen; Zhu, Meilu; Peng, Zhe; Yuan, Yixuan","Chen, Zhen (Department of Electrical Engineering, City University of Hong Kong, Hong Kong, SAR, China); Yang, Chen (Department of Electrical Engineering, City University of Hong Kong, Hong Kong, SAR, China); Zhu, Meilu (Department of Electrical Engineering, City University of Hong Kong, Hong Kong, SAR, China); Peng, Zhe (Department of Computer Science, Hong Kong Baptist University, Hong Kong, SAR, China); Yuan, Yixuan (Department of Electrical Engineering, City University of Hong Kong, Hong Kong, SAR, China)","Yuan, Yixuan (City University of Hong Kong)","Chen, Zhen (City University of Hong Kong); Yang, Chen (City University of Hong Kong); Zhu, Meilu (City University of Hong Kong); Peng, Zhe (Hong Kong Baptist University); Yuan, Yixuan (City University of Hong Kong)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1149601356,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,,
5059,pub.1147022509,10.1016/j.media.2022.102457,35461016,,Source free domain adaptation for medical image segmentation with fourier style mining,"Unsupervised domain adaptation (UDA) aims to exploit the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled target domain. Existing UDA techniques typically assume that samples from source and target domains are freely accessible during the training. However, it may be impractical to access source images due to privacy concerns, especially in medical imaging scenarios with the patient information. To tackle this issue, we devise a novel source free domain adaptation framework with fourier style mining, where only a well-trained source segmentation model is available for the adaptation to the target domain. Our framework is composed of two stages: a generation stage and an adaptation stage. In the generation stage, we design a Fourier Style Mining (FSM) generator to inverse source-like images through statistic information of the pretrained source model and mutual Fourier Transform. These generated source-like images can provide source data distribution and benefit the domain alignment. In the adaptation stage, we design a Contrastive Domain Distillation (CDD) module to achieve feature-level adaptation, including a domain distillation loss to transfer relation knowledge and a domain contrastive loss to narrow down the domain gap by a self-supervised paradigm. Besides, a Compact-Aware Domain Consistency (CADC) module is proposed to enhance consistency learning by filtering out noisy pseudo labels with shape compactness metric, thus achieving output-level adaptation. Extensive experiments on cross-device and cross-centre datasets are conducted for polyp and prostate segmentation, and our method delivers impressive performance compared with state-of-the-art domain adaptation methods. The source code is available at https://github.com/CityU-AIM-Group/SFDA-FSM.","This work was supported by Shenzhen-Hong Kong Innovation Circle Category D Project SGDX2019081623300177 (CityU 9240008) and Hong Kong Research Grants Council (RGC) General Research Fund 11211221 (CityU 9043152). We would like to thank Dr. Hu Jiancong from Department of Endoscopic Surgery, the Sixth Affiliated Hospital, Sun Yat-sen University for providing clinical knowledge for endoscopy disease diagnosis.",,Medical Image Analysis,,"Humans; Image Processing, Computer-Assisted; Software",2022-04-12,2022,2022-04-12,2022-07,79,,102457,Closed,Article,"Yang, Chen; Guo, Xiaoqing; Chen, Zhen; Yuan, Yixuan","Yang, Chen (Department of Electrical Engineering, City University of Hong Kong, Hong Kong SAR, China.); Guo, Xiaoqing (Department of Electrical Engineering, City University of Hong Kong, Hong Kong SAR, China.); Chen, Zhen (Department of Electrical Engineering, City University of Hong Kong, Hong Kong SAR, China.); Yuan, Yixuan (Department of Electrical Engineering, City University of Hong Kong, Hong Kong SAR, China. Electronic address: yxyuan.ee@cityu.edu.hk.)","Yuan, Yixuan (City University of Hong Kong)","Yang, Chen (City University of Hong Kong); Guo, Xiaoqing (City University of Hong Kong); Chen, Zhen (City University of Hong Kong); Yuan, Yixuan (City University of Hong Kong)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1147022509,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,,
5055,pub.1149144086,10.1142/s0129065722500435,35912583,,An Efficient Semi-Supervised Framework with Multi-Task and Curriculum Learning for Medical Image Segmentation,"A practical problem in supervised deep learning for medical image segmentation is the lack of labeled data which is expensive and time-consuming to acquire. In contrast, there is a considerable amount of unlabeled data available in the clinic. To make better use of the unlabeled data and improve the generalization on limited labeled data, in this paper, a novel semi-supervised segmentation method via multi-task curriculum learning is presented. Here, curriculum learning means that when training the network, simpler knowledge is preferentially learned to assist the learning of more difficult knowledge. Concretely, our framework consists of a main segmentation task and two auxiliary tasks, i.e. the feature regression task and target detection task. The two auxiliary tasks predict some relatively simpler image-level attributes and bounding boxes as the pseudo labels for the main segmentation task, enforcing the pixel-level segmentation result to match the distribution of these pseudo labels. In addition, to solve the problem of class imbalance in the images, a bounding-box-based attention (BBA) module is embedded, enabling the segmentation network to concern more about the target region rather than the background. Furthermore, to alleviate the adverse effects caused by the possible deviation of pseudo labels, error tolerance mechanisms are also adopted in the auxiliary tasks, including inequality constraint and bounding-box amplification. Our method is validated on ACDC2017 and PROMISE12 datasets. Experimental results demonstrate that compared with the full supervision method and state-of-the-art semi-supervised methods, our method yields a much better segmentation performance on a small labeled dataset. Code is available at https://github.com/DeepMedLab/MTCL.",,,International Journal of Neural Systems,,"Curriculum; Data Curation; Datasets as Topic; Image Processing, Computer-Assisted; Supervised Machine Learning",2022-07-30,2022,2022-07-30,2022-09,32,9,2250043,Closed,Article,"Wang, Kaiping; Wang, Yan; Zhan, Bo; Yang, Yujie; Zu, Chen; Wu, Xi; Zhou, Jiliu; Nie, Dong; Zhou, Luping","Wang, Kaiping (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.); Wang, Yan (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.); Zhan, Bo (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.); Yang, Yujie (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.); Zu, Chen (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.); Wu, Xi (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.); Zhou, Jiliu (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.); Nie, Dong (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.); Zhou, Luping (College of Computer Science, Sichuan University, Section 1, Southern 1st Ring Rd, Chengdu, Sichuan 610065, P. R. China.)","Wang, Yan (Sichuan University)","Wang, Kaiping (Sichuan University); Wang, Yan (Sichuan University); Zhan, Bo (Sichuan University); Yang, Yujie (Sichuan University); Zu, Chen (Sichuan University); Wu, Xi (Sichuan University); Zhou, Jiliu (Sichuan University); Nie, Dong (Sichuan University); Zhou, Luping (Sichuan University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1149144086,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
5055,pub.1112313855,10.1117/1.jmi.6.1.014501,30820440,PMC6384414,Prostate zonal segmentation in 1.5T and 3T T2W MRI using a convolutional neural network,"Zonal segmentation of the prostate gland using magnetic resonance imaging (MRI) is clinically important for prostate cancer (PCa) diagnosis and image-guided treatments. A two-dimensional convolutional neural network (CNN) based on the U-net architecture was evaluated for segmentation of the central gland (CG) and peripheral zone (PZ) using a dataset of 40 patients (34 PCa positive and 6 PCa negative) scanned on two different MRI scanners (1.5T GE and 3T Siemens). Images were cropped around the prostate gland to exclude surrounding tissues, resampled to 0.5 × 0.5 × 0.5    mm   voxels  and z  -score normalized before being propagated through the CNN. Performance was evaluated using the Dice similarity coefficient (DSC) and mean absolute distance (MAD) in a fivefold cross-validation setup. Overall performance showed DSC of 0.794 and 0.692, and MADs of 3.349 and 2.993 for CG and PZ, respectively. Dividing the gland into apex, mid, and base showed higher DSC for the midgland compared to apex and base for both CG and PZ. We found no significant difference in DSC between the two scanners. A larger dataset, preferably with multivendor scanners, is necessary for validation of the proposed algorithm; however, our results are promising and have clinical potential.",The authors of this paper would like to acknowledge Lemaître et al. for making the dataset publicly available.,,Journal of Medical Imaging,,,2019-01,2019,2019-02-22,2019-01,6,1,014501-014501,All OA, Green,Article,"Jensen, Carina; Sørensen, Kristine Storm; Jørgensen, Cecilia Klitgaard; Nielsen, Camilla Winther; Høy, Pia Christine; Langkilde, Niels Christian; Østergaard, Lasse Riis","Jensen, Carina (Aalborg University Hospital, Department of Medical Physics, Department of Oncology, Aalborg, Denmark); Sørensen, Kristine Storm (Aalborg University, Department of Health Science and Technology, Aalborg, Denmark); Jørgensen, Cecilia Klitgaard (Aalborg University, Department of Health Science and Technology, Aalborg, Denmark); Nielsen, Camilla Winther (Aalborg University, Department of Health Science and Technology, Aalborg, Denmark); Høy, Pia Christine (Aalborg University, Department of Health Science and Technology, Aalborg, Denmark); Langkilde, Niels Christian (Aalborg University Hospital, Department of Urology, Aalborg, Denmark); Østergaard, Lasse Riis (Aalborg University, Department of Health Science and Technology, Aalborg, Denmark)","Jensen, Carina (Aalborg University Hospital)","Jensen, Carina (Aalborg University Hospital); Sørensen, Kristine Storm (Aalborg University); Jørgensen, Cecilia Klitgaard (Aalborg University); Nielsen, Camilla Winther (Aalborg University); Høy, Pia Christine (Aalborg University); Langkilde, Niels Christian (Aalborg University Hospital); Østergaard, Lasse Riis (Aalborg University)",15,12,0.53,5.3,https://europepmc.org/articles/pmc6384414?pdf=render,https://app.dimensions.ai/details/publication/pub.1112313855,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
5048,pub.1115994798,10.1007/s10278-018-0169-5,31144146,PMC6737137,Image Annotation by Eye Tracking: Accuracy and Precision of Centerlines of Obstructed Small-Bowel Segments Placed Using Eye Trackers,"Small-bowel obstruction (SBO) is a common and important disease, for which machine learning tools have yet to be developed. Image annotation is a critical first step for development of such tools. This study assesses whether image annotation by eye tracking is sufficiently accurate and precise to serve as a first step in the development of machine learning tools for detection of SBO on CT. Seven subjects diagnosed with SBO by CT were included in the study. For each subject, an obstructed segment of bowel was chosen. Three observers annotated the centerline of the segment by manual fiducial placement and by visual fiducial placement using a Tobii 4c eye tracker. Each annotation was repeated three times. The distance between centerlines was calculated after alignment using dynamic time warping (DTW) and statistically compared to clinical thresholds for diagnosis of SBO. Intra-observer DTW distance between manual and visual centerlines was calculated as a measure of accuracy. These distances were 1.1 ± 0.2, 1.3 ± 0.4, and 1.8 ± 0.2 cm for the three observers and were less than 1.5 cm for two of three observers (P < 0.01). Intra- and inter-observer DTW distances between centerlines placed with each method were calculated as measures of precision. These distances were 0.6 ± 0.1 and 0.8 ± 0.2 cm for manual centerlines, 1.1 ± 0.4 and 1.9 ± 0.6 cm for visual centerlines, and were less than 3.0 cm in all cases (P < 0.01). Results suggest that eye tracking–based annotation is sufficiently accurate and precise for small-bowel centerline annotation for use in machine learning–based applications.",,Dr. Kang Wang was supported in part by the NIH grant T32EB005970.,Journal of Digital Imaging,,"Adult; Aged; Female; Fixation, Ocular; Humans; Intestinal Obstruction; Intestine, Small; Male; Middle Aged; Pattern Recognition, Automated; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Retrospective Studies; Tomography, X-Ray Computed",2019-05-29,2019,2019-05-29,2019-10,32,5,855-864,All OA, Green,Article,"Lucas, Alfredo; Wang, Kang; Santillan, Cynthia; Hsiao, Albert; Sirlin, Claude B.; Murphy, Paul M.","Lucas, Alfredo (Department of Bioengineering, University of California, San Diego, CA, USA); Wang, Kang (Department of Radiology, University of California, San Diego, CA, USA); Santillan, Cynthia (Department of Radiology, University of California, San Diego, CA, USA); Hsiao, Albert (Department of Radiology, University of California, San Diego, CA, USA); Sirlin, Claude B. (Department of Radiology, University of California, San Diego, CA, USA); Murphy, Paul M. (Department of Radiology, University of California, San Diego, CA, USA)","Murphy, Paul M. (University of California, San Diego)","Lucas, Alfredo (University of California, San Diego); Wang, Kang (University of California, San Diego); Santillan, Cynthia (University of California, San Diego); Hsiao, Albert (University of California, San Diego); Sirlin, Claude B. (University of California, San Diego); Murphy, Paul M. (University of California, San Diego)",3,2,0.29,1.26,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6737137,https://app.dimensions.ai/details/publication/pub.1115994798,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
5040,pub.1146261887,10.1117/1.jmi.9.2.024001,35300345,PMC8920492,Automatic zonal segmentation of the prostate from 2D and 3D T2-weighted MRI and evaluation for clinical use,"Purpose: An accurate zonal segmentation of the prostate is required for prostate cancer (PCa) management with MRI. Approach: The aim of this work is to present UFNet, a deep learning-based method for automatic zonal segmentation of the prostate from T2-weighted (T2w) MRI. It takes into account the image anisotropy, includes both spatial and channelwise attention mechanisms and uses loss functions to enforce prostate partition. The method was applied on a private multicentric three-dimensional T2w MRI dataset and on the public two-dimensional T2w MRI dataset ProstateX. To assess the model performance, the structures segmented by the algorithm on the private dataset were compared with those obtained by seven radiologists of various experience levels. Results: On the private dataset, we obtained a Dice score (DSC) of 93.90 ± 2.85  for the whole gland (WG), 91.00 ± 4.34  for the transition zone (TZ), and 79.08 ± 7.08  for the peripheral zone (PZ). Results were significantly better than other compared networks' ( p - value < 0.05  ). On ProstateX, we obtained a DSC of 90.90 ± 2.94  for WG, 86.84 ± 4.33  for TZ, and 78.40 ± 7.31  for PZ. These results are similar to state-of-the art results and, on the private dataset, are coherent with those obtained by radiologists. Zonal locations and sectorial positions of lesions annotated by radiologists were also preserved. Conclusions: Deep learning-based methods can provide an accurate zonal segmentation of the prostate leading to a consistent zonal location and sectorial position of lesions, and therefore can be used as a helping tool for PCa diagnosis.","We thank Julien Castelneau, software engineer at Inria, for his help in the development of MedInria Software (MedInria—Medical image visualization and processing software by Inria44). The authors are grateful to the OPAL infrastructure from the Université Côte d’Azur for providing resources and support. We also thank Alexandre Allera, Malek Ezziane, Anna Luzurier, Raphaelle Quint, and Mehdi Kalai for providing prostate segmentations. This work has been supported by the French government, through the 3IA Côte d’Azur and UCA DS4H Investments in the Future project managed by the National Research Agency (ANR) with the reference numbers ANR-19-P3IA-0002 and ANR-17-EURE-0004, and by the Health Data Center of the AP-HP (Assistance Publique-Hôpitaux de Paris). Private data were extracted from the Clinical Data Warehouse of the Greater Paris University Hospitals (Assistance Publique-Hôpitaux de Paris).",,Journal of Medical Imaging,,,2022-03-14,2022,2022-03-14,2022-03,9,2,024001-024001,All OA, Green,Article,"Hamzaoui, Dimitri; Montagne, Sarah; Renard-Penna, Raphaële; Ayache, Nicholas; Delingette, Hervé","Hamzaoui, Dimitri (Université Côte d’Azur, Inria, Epione Project-Team, Sophia Antipolis, Valbonne, France); Montagne, Sarah (Sorbonne Université, Radiology Department, CHU La Pitié Salpétrière/Tenon, Paris, France); Renard-Penna, Raphaële (Sorbonne Université, Radiology Department, CHU La Pitié Salpétrière/Tenon, Paris, France); Ayache, Nicholas (Université Côte d’Azur, Inria, Epione Project-Team, Sophia Antipolis, Valbonne, France); Delingette, Hervé (Université Côte d’Azur, Inria, Epione Project-Team, Sophia Antipolis, Valbonne, France)","Hamzaoui, Dimitri (Université Côte d'Azur)","Hamzaoui, Dimitri (Université Côte d'Azur); Montagne, Sarah (Sorbonne University); Renard-Penna, Raphaële (Sorbonne University); Ayache, Nicholas (Université Côte d'Azur); Delingette, Hervé (Université Côte d'Azur)",1,1,,,https://hal.archives-ouvertes.fr/hal-03587074v2/file/JMI-21261R_online.pdf,https://app.dimensions.ai/details/publication/pub.1146261887,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
5039,pub.1101785821,10.1007/s11548-018-1742-6,29589259,,Prostate segmentation in transrectal ultrasound using magnetic resonance imaging priors,"PurposeIn the current standard of care, real-time transrectal ultrasound (TRUS) is commonly used for prostate brachytherapy guidance. As TRUS provides limited soft tissue contrast, segmenting the prostate gland in TRUS images is often challenging and subject to inter-observer and intra-observer variability, especially at the base and apex where the gland boundary is hard to define. Magnetic resonance imaging (MRI) has higher soft tissue contrast allowing the prostate to be contoured easily. In this paper, we aim to show that prostate segmentation in TRUS images informed by MRI priors can improve on prostate segmentation that relies only on TRUS images.MethodsFirst, we compare the TRUS-based prostate segmentation used in the treatment of 598 patients with a high-quality MRI prostate atlas and observe inconsistencies at the apex and base. Second, motivated by this finding, we propose an alternative TRUS segmentation technique that is fully automatic and uses MRI priors. The algorithm uses a convolutional neural network to segment the prostate in TRUS images at mid-gland, where the gland boundary can be clearly seen. It then reconstructs the gland boundary at the apex and base with the aid of a statistical shape model built from an MRI atlas of 78 patients.ResultsCompared to the clinical TRUS segmentation, our method achieves similar mid-gland segmentation results in the 598-patient database. For the seven patients who had both TRUS and MRI, our method achieved more accurate segmentation of the base and apex with the MRI segmentation used as ground truth.ConclusionOur results suggest that utilizing MRI priors in TRUS prostate segmentation could potentially improve the performance at base and apex.","This work was funded by Natural Sciences and Engineering Research Council of Canada (NSERC), the Canadian Institutes of Health Research (CIHR), and the Prostate Cancer Canada (PCC). We would like to thank the support from the Charles Laszlo Chair in Biomedical Engineering held by Professor Salcudean. The authors also gratefully acknowledge the help from physicians and staff at the Vancouver Cancer Centre who have contributed to this project.",,International Journal of Computer Assisted Radiology and Surgery,,"Algorithms; Endosonography; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Models, Statistical; Prostate; Prostatic Neoplasms; ROC Curve; Rectum",2018-03-27,2018,2018-03-27,2018-06,13,6,749-757,Closed,Article,"Zeng, Qi; Samei, Golnoosh; Karimi, Davood; Kesch, Claudia; Mahdavi, Sara S.; Abolmaesumi, Purang; Salcudean, Septimiu E.","Zeng, Qi (Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada); Samei, Golnoosh (Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada); Karimi, Davood (Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada); Kesch, Claudia (Vancouver Prostate Centre, University of British Columbia, Vancouver, BC, Canada); Mahdavi, Sara S. (British Columbia Cancer Agency, Vancouver, BC, Canada); Abolmaesumi, Purang (Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada); Salcudean, Septimiu E. (Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada)","Zeng, Qi (University of British Columbia)","Zeng, Qi (University of British Columbia); Samei, Golnoosh (University of British Columbia); Karimi, Davood (University of British Columbia); Kesch, Claudia (University of British Columbia); Mahdavi, Sara S. (BC Cancer Agency); Abolmaesumi, Purang (University of British Columbia); Salcudean, Septimiu E. (University of British Columbia)",18,10,0.73,5.25,,https://app.dimensions.ai/details/publication/pub.1101785821,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
5035,pub.1150632330,10.1109/tcyb.2022.3195447,36044511,,Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation,"Automatic tumor or lesion segmentation is a crucial step in medical image analysis for computer-aided diagnosis. Although the existing methods based on convolutional neural networks (CNNs) have achieved the state-of-the-art performance, many challenges still remain in medical tumor segmentation. This is because, although the human visual system can detect symmetries in 2-D images effectively, regular CNNs can only exploit translation invariance, overlooking further inherent symmetries existing in medical images, such as rotations and reflections. To solve this problem, we propose a novel group equivariant segmentation framework by encoding those inherent symmetries for learning more precise representations. First, kernel-based equivariant operations are devised on each orientation, which allows it to effectively address the gaps of learning symmetries in existing approaches. Then, to keep segmentation networks globally equivariant, we design distinctive group layers with layer-wise symmetry constraints. Finally, based on our novel framework, extensive experiments conducted on real-world clinical data demonstrate that a group equivariant Res-UNet (called GER-UNet) outperforms its regular CNN-based counterpart and the state-of-the-art segmentation methods in the tasks of hepatic tumor segmentation, COVID-19 lung infection segmentation, and retinal vessel detection. More importantly, the newly built GER-UNet also shows potential in reducing the sample complexity and the redundancy of filters, upgrading current segmentation CNNs, and delineating organs on other medical imaging modalities.",,,IEEE Transactions on Cybernetics,,,2022-08-31,2022,2022-08-31,2022-08-31,PP,99,1-12,All OA, Green,Article,"Pang, Shuchao; Du, Anan; Orgun, Mehmet A.; Wang, Yan; Sheng, Quan Z.; Wang, Shoujin; Huang, Xiaoshui; Yu, Zhenmei","Pang, Shuchao (School of Cyber Science and Engineering, Nanjing University of Science and Technology, Nanjing, China); Du, Anan (School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia); Orgun, Mehmet A. (School of Computing, Macquarie University, Sydney, NSW, Australia); Wang, Yan (School of Computing, Macquarie University, Sydney, NSW, Australia); Sheng, Quan Z. (School of Computing, Macquarie University, Sydney, NSW, Australia); Wang, Shoujin (Data Science Institute, University of Technology Sydney, Ultimo, NSW, Australia); Huang, Xiaoshui (Shanghai AI Laboratory, Shanghai, China); Yu, Zhenmei (School of Data and Computer Science, Shandong Women&#x2019;s University, Jinan, China)",,"Pang, Shuchao (Nanjing University of Science and Technology); Du, Anan (University of Technology Sydney); Orgun, Mehmet A. (Macquarie University); Wang, Yan (Macquarie University); Sheng, Quan Z. (Macquarie University); Wang, Shoujin (University of Technology Sydney); Huang, Xiaoshui (Shanghai Artificial Intelligence Laboratory); Yu, Zhenmei ()",2,2,,,http://arxiv.org/pdf/2207.14472,https://app.dimensions.ai/details/publication/pub.1150632330,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
5035,pub.1150454946,10.1016/j.media.2022.102596,36084564,PMC9400372,Distance-based detection of out-of-distribution silent failures for Covid-19 lung lesion segmentation,"Automatic segmentation of ground glass opacities and consolidations in chest computer tomography (CT) scans can potentially ease the burden of radiologists during times of high resource utilisation. However, deep learning models are not trusted in the clinical routine due to failing silently on out-of-distribution (OOD) data. We propose a lightweight OOD detection method that leverages the Mahalanobis distance in the feature space and seamlessly integrates into state-of-the-art segmentation pipelines. The simple approach can even augment pre-trained models with clinically relevant uncertainty quantification. We validate our method across four chest CT distribution shifts and two magnetic resonance imaging applications, namely segmentation of the hippocampus and the prostate. Our results show that the proposed method effectively detects far- and near-OOD samples across all explored scenarios.","This work was supported by the RACOON network under BMBF, Germany grant number [01KX2021]; and the Bundesministerium für Gesundheit (BMG), Germany with grant [ZMVI1-2520DAT03A].",,Medical Image Analysis,,"Humans; Male; COVID-19; Tomography, X-Ray Computed; Magnetic Resonance Imaging; Lung Diseases; Lung",2022-08-24,2022,2022-08-24,2022-11,82,,102596,All OA, Bronze,Article,"González, Camila; Gotkowski, Karol; Fuchs, Moritz; Bucher, Andreas; Dadras, Armin; Fischbach, Ricarda; Kaltenborn, Isabel Jasmin; Mukhopadhyay, Anirban","González, Camila (Darmstadt University of Technology, Karolinenplatz 5, 64289 Darmstadt, Germany. Electronic address: camila.gonzalez@gris.tu-darmstadt.de.); Gotkowski, Karol (Darmstadt University of Technology, Karolinenplatz 5, 64289 Darmstadt, Germany.); Fuchs, Moritz (Darmstadt University of Technology, Karolinenplatz 5, 64289 Darmstadt, Germany.); Bucher, Andreas (Uniklinik Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.); Dadras, Armin (Uniklinik Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.); Fischbach, Ricarda (Uniklinik Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.); Kaltenborn, Isabel Jasmin (Uniklinik Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.); Mukhopadhyay, Anirban (Darmstadt University of Technology, Karolinenplatz 5, 64289 Darmstadt, Germany.)","González, Camila (TU Darmstadt)","González, Camila (TU Darmstadt); Gotkowski, Karol (TU Darmstadt); Fuchs, Moritz (TU Darmstadt); Bucher, Andreas (); Dadras, Armin (); Fischbach, Ricarda (); Kaltenborn, Isabel Jasmin (); Mukhopadhyay, Anirban (TU Darmstadt)",1,1,,,https://doi.org/10.1016/j.media.2022.102596,https://app.dimensions.ai/details/publication/pub.1150454946,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
5035,pub.1149990125,10.1002/mp.15895,35923153,,Automatic segmentation of prostate MRI based on 3D pyramid pooling Unet,"PURPOSE: Automatic segmentation of prostate magnetic resonance (MR) images is crucial for the diagnosis, evaluation, and prognosis of prostate diseases (including prostate cancer). In recent years, the mainstream segmentation method for the prostate has been converted to convolutional neural networks. However, owing to the complexity of the tissue structure in MR images and the limitations of existing methods in spatial context modeling, the segmentation performance should be improved further.
METHODS: In this study, we proposed a novel 3D pyramid pool Unet that benefits from the pyramid pooling structure embedded in the skip connection (SC) and the deep supervision (DS) in the up-sampling of the 3D Unet. The parallel SC of the conventional 3D Unet network causes low-resolution information to be sent to the feature map repeatedly, resulting in blurred image features. To overcome the shortcomings of the conventional 3D Unet, we merge each decoder layer with the feature map of the same scale as the encoder and the smaller scale feature map of the pyramid pooling encoder. This SC combines the low-level details and high-level semantics at two different levels of feature maps. In addition, pyramid pooling performs multifaceted feature extraction on each image behind the convolutional layer, and DS learns hierarchical representations from comprehensive aggregated feature maps, which can improve the accuracy of the task.
RESULTS: Experiments on 3D prostate MR images of 78 patients demonstrated that our results were highly correlated with expert manual segmentation. The average relative volume difference and Dice similarity coefficient of the prostate volume area were 2.32% and 91.03%, respectively.
CONCLUSION: Quantitative experiments demonstrate that, compared with other methods, the results of our method are highly consistent with the expert manual segmentation.","This work was supported in part by the National Natural Science Foundation of China (Grant no. 82260362), in part by the National Key R&amp;D Program of China (Grant no. 2021ZD0111000), in part by the Hainan Provincial Natural Science Foundation of China (Grant no. 621MS019), in part by the Key R&amp;D Project of Hainan Province (Grant no. ZDYF2021SHFZ243), in part by the Major Science and Technology Project of Haikou (Grant no. 2020‐009), in part by the Innovative Research Project of Postgraduates in Hainan Province (Grant no. Qhyb2021‐09), in part by the Innovative Research Project of Postgraduates in Hainan Province (Grant no. Qhyb2021‐10).",,Medical Physics,,"Male; Humans; Prostate; Prostatic Neoplasms; Learning; Magnetic Resonance Imaging; Neural Networks, Computer; Image Processing, Computer-Assisted",2022-12-31,2022,2022-12-31,2023-02,50,2,906-921,Closed,Article,"Li, Yuchun; Lin, Cong; Zhang, Yu; Feng, Siling; Huang, Mengxing; Bai, Zhiming","Li, Yuchun (State Key Laboratory of Marine Resource Utilization in South China Sea, School of information and Communication Engineering, Hainan University, Haikou, China); Lin, Cong (State Key Laboratory of Marine Resource Utilization in South China Sea, School of information and Communication Engineering, Hainan University, Haikou, China; College of Electronics and Information Engineering, Guangdong Ocean University, Zhanjiang, China); Zhang, Yu (College of Computer science and Technology, Hainan University, Haikou, China); Feng, Siling (State Key Laboratory of Marine Resource Utilization in South China Sea, School of information and Communication Engineering, Hainan University, Haikou, China); Huang, Mengxing (State Key Laboratory of Marine Resource Utilization in South China Sea, School of information and Communication Engineering, Hainan University, Haikou, China); Bai, Zhiming (Haikou Municipal People's Hospital and Central South University Xiangya Medical College Affiliated Hospital, Haikou, China)","Feng, Siling (Hainan University); Huang, Mengxing (Hainan University)","Li, Yuchun (Hainan University); Lin, Cong (Hainan University; Guangdong Ocean University); Zhang, Yu (Hainan University); Feng, Siling (Hainan University); Huang, Mengxing (Hainan University); Bai, Zhiming ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149990125,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
5035,pub.1125614982,10.1002/mp.14134,32166768,,Automatic prostate segmentation using deep learning on clinically diverse 3D transrectal ultrasound images,"PURPOSE: Needle-based procedures for diagnosing and treating prostate cancer, such as biopsy and brachytherapy, have incorporated three-dimensional (3D) transrectal ultrasound (TRUS) imaging to improve needle guidance. Using these images effectively typically requires the physician to manually segment the prostate to define the margins used for accurate registration, targeting, and other guidance techniques. However, manual prostate segmentation is a time-consuming and difficult intraoperative process, often occurring while the patient is under sedation (biopsy) or anesthetic (brachytherapy). Minimizing procedure time with a 3D TRUS prostate segmentation method could provide physicians with a quick and accurate prostate segmentation, and allow for an efficient workflow with improved patient throughput to enable faster patient access to care. The purpose of this study was to develop a supervised deep learning-based method to segment the prostate in 3D TRUS images from different facilities, generated using multiple acquisition methods and commercial ultrasound machine models to create a generalizable algorithm for needle-based prostate cancer procedures.
METHODS: Our proposed method for 3D segmentation involved prediction on two-dimensional (2D) slices sampled radially around the approximate central axis of the prostate, followed by reconstruction into a 3D surface. A 2D U-Net was modified, trained, and validated using images from 84 end-fire and 122 side-fire 3D TRUS images acquired during clinical biopsies and brachytherapy procedures. Modifications to the expansion section of the standard U-Net included the addition of 50% dropouts and the use of transpose convolutions instead of standard upsampling followed by convolution to reduce overfitting and improve performance, respectively. Manual contours provided the annotations needed for the training, validation, and testing datasets, with the testing dataset consisting of 20 end-fire and 20 side-fire unseen 3D TRUS images. Since predicting with 2D images has the potential to lose spatial and structural information, comparisons to 3D reconstruction and optimized 3D networks including 3D V-Net, Dense V-Net, and High-resolution 3D-Net were performed following an investigation into different loss functions. An extended selection of absolute and signed error metrics were computed, including pixel map comparisons [dice similarity coefficient (DSC), recall, and precision], volume percent differences (VPD), mean surface distance (MSD), and Hausdorff distance (HD), to assess 3D segmentation accuracy.
RESULTS: Overall, our proposed reconstructed modified U-Net performed with a median [first quartile, third quartile] absolute DSC, recall, precision, VPD, MSD, and HD of 94.1 [92.6, 94.9]%, 96.0 [93.1, 98.5]%, 93.2 [88.8, 95.4]%, 5.78 [2.49, 11.50]%, 0.89 [0.73, 1.09] mm, and 2.89 [2.37, 4.35] mm, respectively. When compared to the best-performing optimized 3D network (i.e., 3D V-Net with a Dice plus cross-entropy loss function), our proposed method performed with a significant improvement across nearly all metrics. A computation time <0.7 s per prostate was observed, which is a sufficiently short segmentation time for intraoperative implementation.
CONCLUSIONS: Our proposed algorithm was able to provide a fast and accurate 3D segmentation across variable 3D TRUS prostate images, enabling a generalizable intraoperative solution for needle-based prostate cancer procedures. This method has the potential to decrease procedure times, supporting the increasing interest in needle-based 3D TRUS approaches.","The authors are grateful for the funding support from the Ontario Institute of Cancer Research (OICR), the Canadian Institutes of Health Research (CIHR), and Natural Sciences and Engineering Research Council of Canada (NSERC). N. Orlando was supported in part by the Translational Breast Cancer Research Unit (TBCRU). The authors would also like to thank Dr. Derek Cool and Dr. Ashley Mercado for their assistance in collecting images during prostate biopsy procedures, and Dr. David Tessier for his help in organizing the image dataset.",,Medical Physics,,"Brachytherapy; Deep Learning; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Male; Prostate; Prostatic Neoplasms; Ultrasonography",2020-04-08,2020,2020-04-08,2020-06,47,6,2413-2426,All OA, Bronze,Article,"Orlando, Nathan; Gillies, Derek J.; Gyacskov, Igor; Romagnoli, Cesare; D’Souza, David; Fenster, Aaron","Orlando, Nathan (Department of Medical Biophysics, Western University, London, ON, N6A 3K7, Canada; Robarts Research Institute, Western University, London, ON, N6A 3K7, Canada); Gillies, Derek J. (Department of Medical Biophysics, Western University, London, ON, N6A 3K7, Canada; Robarts Research Institute, Western University, London, ON, N6A 3K7, Canada); Gyacskov, Igor (Robarts Research Institute, Western University, London, ON, N6A 3K7, Canada); Romagnoli, Cesare (Department of Medical Imaging, Western University, London, ON, N6A 3K7, Canada; London Health Sciences Centre, London, ON, N6A 5W9, Canada); D’Souza, David (Department of Oncology, Western University, London, ON, N6A 3K7, Canada; London Health Sciences Centre, London, ON, N6A 5W9, Canada); Fenster, Aaron (Department of Medical Biophysics, Western University, London, ON, N6A 3K7, Canada; Robarts Research Institute, Western University, London, ON, N6A 3K7, Canada; Department of Medical Imaging, Western University, London, ON, N6A 3K7, Canada; Department of Oncology, Western University, London, ON, N6A 3K7, Canada)","Orlando, Nathan (Western University; Western University); Gillies, Derek J. (Western University; Western University)","Orlando, Nathan (Western University; Western University); Gillies, Derek J. (Western University; Western University); Gyacskov, Igor (Western University); Romagnoli, Cesare (Western University; London Health Sciences Centre); D’Souza, David (Western University; London Health Sciences Centre); Fenster, Aaron (Western University; Western University; Western University; Western University)",38,32,3.39,28.81,https://aapm.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/mp.14134,https://app.dimensions.ai/details/publication/pub.1125614982,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
5029,pub.1106322233,10.1007/s00066-018-1348-5,30140944,PMC6358462,Towards a universal MRI atlas of the prostate and prostate zones,"Background and purposeThe aim of this study was to evaluate an automatic multi-atlas-based segmentation method for generating prostate, peripheral (PZ), and transition zone (TZ) contours on MRIs with and without fat saturation (±FS), and compare MRIs from different vendor MRI systems.MethodsT2-weighted (T2) and fat-saturated (T2FS) MRIs were acquired on 3T GE (GE, Waukesha, WI, USA) and Siemens (Erlangen, Germany) systems. Manual prostate and PZ contours were used to create atlas libraries. As a test MRI is entered, the procedure for atlas segmentation automatically identifies the atlas subjects that best match the test subject, followed by a normalized intensity-based free-form deformable registration. The contours are transformed to the test subject, and Dice similarity coefficients (DSC) and Hausdorff distances between atlas-generated and manual contours were used to assess performance.ResultsThree atlases were generated based on GE_T2 (n = 30), GE_T2FS (n = 30), and Siem_T2FS (n = 31). When test images matched the contrast and vendor of the atlas, DSCs of 0.81 and 0.83 for T2 ± FS were obtained (baseline performance). Atlases performed with higher accuracy when segmenting (i) T2FS vs. T2 images, likely due to a superior contrast between prostate vs. surrounding tissue; (ii) prostate vs. zonal anatomy; (iii) in the mid-gland vs. base and apex. Atlases performance declined when tested with images with differing contrast and MRI vendor. Conversely, combined atlases showed similar performance to baseline.ConclusionThe MRI atlas-based segmentation method achieved good results for prostate, PZ, and TZ compared to expert contoured volumes. Combined atlases performed similarly to matching atlas and scan type. The technique is fast, fully automatic, and implemented on commercially available clinical platform.",,,Strahlentherapie und Onkologie,,"Anatomy, Artistic; Atlases as Topic; Commerce; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostate; Sensitivity and Specificity",2018-08-23,2018,2018-08-23,2019-02,195,2,121-130,All OA, Green,Article,"Padgett, Kyle R.; Swallen, Amy; Pirozzi, Sara; Piper, Jon; Chinea, Felix M.; Abramowitz, Matthew C.; Nelson, Aaron; Pollack, Alan; Stoyanova, Radka","Padgett, Kyle R. (Department of Radiology, Miller School of Medicine University of Miami, Miami, FL, USA); Swallen, Amy (Research and Development, MIM software Inc., Cleveland, OH, USA); Pirozzi, Sara (Research and Development, MIM software Inc., Cleveland, OH, USA); Piper, Jon (Research and Development, MIM software Inc., Cleveland, OH, USA); Chinea, Felix M. (Department of Radiology, Miller School of Medicine University of Miami, Miami, FL, USA); Abramowitz, Matthew C. (Department of Radiology, Miller School of Medicine University of Miami, Miami, FL, USA); Nelson, Aaron (Research and Development, MIM software Inc., Cleveland, OH, USA); Pollack, Alan (Department of Radiology, Miller School of Medicine University of Miami, Miami, FL, USA); Stoyanova, Radka (Department of Radiation Oncology, Miller School of Medicine University of Miami, 1475 NW 12th Avenue, Suite 1515J, 33136, Miami, FL, USA)","Stoyanova, Radka (University of Miami)","Padgett, Kyle R. (University of Miami); Swallen, Amy (); Pirozzi, Sara (); Piper, Jon (); Chinea, Felix M. (University of Miami); Abramowitz, Matthew C. (University of Miami); Nelson, Aaron (); Pollack, Alan (University of Miami); Stoyanova, Radka (University of Miami)",7,4,0.81,2.04,https://europepmc.org/articles/pmc6358462?pdf=render,https://app.dimensions.ai/details/publication/pub.1106322233,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
5003,pub.1125949116,10.1007/s00066-020-01607-x,32221622,PMC8418872,Segmentation of prostate and prostate zones using deep learning,"PurposeDevelop a deep-learning-based segmentation algorithm for prostate and its peripheral zone (PZ) that is reliable across multiple MRI vendors.MethodsThis is a retrospective study. The dataset consisted of 550 MRIs (Siemens-330, General Electric[GE]-220). A multistream 3D convolutional neural network is used for automatic segmentation of the prostate and its PZ using T2-weighted (T2-w) MRI. Prostate and PZ were manually contoured on axial T2‑w. The network uses axial, coronal, and sagittal T2‑w series as input. The preprocessing of the input data includes bias correction, resampling, and image normalization. A dataset from two MRI vendors (Siemens and GE) is used to test the proposed network. Six different models were trained, three for the prostate and three for the PZ. Of the three, two were trained on data from each vendor separately, and a third (Combined) on the aggregate of the datasets. The Dice coefficient (DSC) is used to compare the manual and predicted segmentation.ResultsFor prostate segmentation, the Combined model obtained DSCs of 0.893 ± 0.036 and 0.825 ± 0.112 (mean ± standard deviation) on Siemens and GE, respectively. For PZ, the best DSCs were from the Combined model: 0.811 ± 0.079 and 0.788 ± 0.093. While the Siemens model underperformed on the GE dataset and vice versa, the Combined model achieved robust performance on both datasets.ConclusionThe proposed network has a performance comparable to the interexpert variability for segmenting the prostate and its PZ. Combining images from different MRI vendors on the training of the network is of paramount importance for building a universal model for prostate and PZ segmentation.",,,Strahlentherapie und Onkologie,,"Algorithms; Datasets as Topic; Deep Learning; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Prostate; Stochastic Processes",2020-03-27,2020,2020-03-27,2020-10,196,10,932-942,Closed,Article,"Zavala-Romero, Olmo; Breto, Adrian L.; Xu, Isaac R.; Chang, Yu-Cherng C.; Gautney, Nicole; Dal Pra, Alan; Abramowitz, Matthew C.; Pollack, Alan; Stoyanova, Radka","Zavala-Romero, Olmo (Department of Radiation Oncology, Sylvester Comprehensive Cancer Center, University of Miami Miller School of Medicine, Miami, FL, USA); Breto, Adrian L. (Department of Radiation Oncology, Sylvester Comprehensive Cancer Center, University of Miami Miller School of Medicine, Miami, FL, USA); Xu, Isaac R. (Department of Radiation Oncology, Sylvester Comprehensive Cancer Center, University of Miami Miller School of Medicine, Miami, FL, USA); Chang, Yu-Cherng C. (University of Miami Miller School of Medicine, Miami, FL, USA); Gautney, Nicole (Department of Radiation Oncology, Sylvester Comprehensive Cancer Center, University of Miami Miller School of Medicine, Miami, FL, USA); Dal Pra, Alan (Department of Radiation Oncology, Sylvester Comprehensive Cancer Center, University of Miami Miller School of Medicine, Miami, FL, USA); Abramowitz, Matthew C. (Department of Radiation Oncology, Sylvester Comprehensive Cancer Center, University of Miami Miller School of Medicine, Miami, FL, USA); Pollack, Alan (Department of Radiation Oncology, Sylvester Comprehensive Cancer Center, University of Miami Miller School of Medicine, Miami, FL, USA); Stoyanova, Radka (Department of Radiation Oncology, Sylvester Comprehensive Cancer Center, University of Miami Miller School of Medicine, Miami, FL, USA)","Stoyanova, Radka (Sylvester Comprehensive Cancer Center)","Zavala-Romero, Olmo (Sylvester Comprehensive Cancer Center); Breto, Adrian L. (Sylvester Comprehensive Cancer Center); Xu, Isaac R. (Sylvester Comprehensive Cancer Center); Chang, Yu-Cherng C. (University of Miami); Gautney, Nicole (Sylvester Comprehensive Cancer Center); Dal Pra, Alan (Sylvester Comprehensive Cancer Center); Abramowitz, Matthew C. (Sylvester Comprehensive Cancer Center); Pollack, Alan (Sylvester Comprehensive Cancer Center); Stoyanova, Radka (Sylvester Comprehensive Cancer Center)",26,21,2.8,12.75,,https://app.dimensions.ai/details/publication/pub.1125949116,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,,
5003,pub.1107293237,10.1016/j.ejmp.2018.09.005,30337011,,Rotationally resliced 3D prostate segmentation of MR images using Bhattacharyya similarity and active band theory,"PURPOSE: In this article, we propose a novel, semi-automatic segmentation method to process 3D MR images of the prostate using the Bhattacharyya coefficient and active band theory with the goal of providing technical support for computer-aided diagnosis and surgery of the prostate.
METHODS: Our method consecutively segments a stack of rotationally resectioned 2D slices of a prostate MR image by assessing the similarity of the shape and intensity distribution in neighboring slices. 2D segmentation is first performed on an initial slice by manually selecting several points on the prostate boundary, after which the segmentation results are propagated consecutively to neighboring slices. A framework of iterative graph cuts is used to optimize the energy function, which contains a global term for the Bhattacharyya coefficient with the help of an auxiliary function. Our method does not require previously segmented data for training or for building statistical models, and manual intervention can be applied flexibly and intuitively, indicating the potential utility of this method in the clinic.
RESULTS: We tested our method on 3D T2-weighted MR images from the ISBI dataset and PROMISE12 dataset of 129 patients, and the Dice similarity coefficients were 90.34 ± 2.21% and 89.32 ± 3.08%, respectively. The comparison was performed with several state-of-the-art methods, and the results demonstrate that the proposed method is robust and accurate, achieving similar or higher accuracy than other methods without requiring training.
CONCLUSION: The proposed algorithm for segmenting 3D MR images of the prostate is accurate, robust, and readily applicable to a clinical environment for computer-aided surgery or diagnosis.","This research was supported by the National Natural Science Foundation of China (grants 81471758, 81701795 and 60972102) and the National Key Research and Development Program of China (2017YFC0110700). And this research was also partially supported by the Program of Shanghai Academic/Technology Research Leaders (16XD1424900).",,Physica Medica,,"Algorithms; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Prostate",2018-09-28,2018,2018-09-28,2018-10,54,,56-65,Closed,Article,"Tang, Zhixian; Wang, Manning; Song, Zhijian","Tang, Zhixian (Digital Medical Research Center, School of Basic Medical Sciences, Fudan University, Shanghai, China; Shanghai Key Laboratory of Medical Imaging Computing and Computer Assisted Intervention, Shanghai, China); Wang, Manning (Digital Medical Research Center, School of Basic Medical Sciences, Fudan University, Shanghai, China; Shanghai Key Laboratory of Medical Imaging Computing and Computer Assisted Intervention, Shanghai, China); Song, Zhijian (Digital Medical Research Center, School of Basic Medical Sciences, Fudan University, Shanghai, China; Shanghai Key Laboratory of Medical Imaging Computing and Computer Assisted Intervention, Shanghai, China)","Wang, Manning (Fudan University; ); Song, Zhijian (Fudan University; )","Tang, Zhixian (Fudan University); Wang, Manning (Fudan University); Song, Zhijian (Fudan University)",6,3,0.15,2.2,,https://app.dimensions.ai/details/publication/pub.1107293237,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
5003,pub.1103995269,10.1007/s11548-018-1785-8,29766373,,Prostate segmentation in MRI using a convolutional neural network architecture and training strategy based on statistical shape models,"PurposeMost of the existing convolutional neural network (CNN)-based medical image segmentation methods are based on methods that have originally been developed for segmentation of natural images. Therefore, they largely ignore the differences between the two domains, such as the smaller degree of variability in the shape and appearance of the target volume and the smaller amounts of training data in medical applications. We propose a CNN-based method for prostate segmentation in MRI that employs statistical shape models to address these issues.MethodsOur CNN predicts the location of the prostate center and the parameters of the shape model, which determine the position of prostate surface keypoints. To train such a large model for segmentation of 3D images using small data (1) we adopt a stage-wise training strategy by first training the network to predict the prostate center and subsequently adding modules for predicting the parameters of the shape model and prostate rotation, (2) we propose a data augmentation method whereby the training images and their prostate surface keypoints are deformed according to the displacements computed based on the shape model, and (3) we employ various regularization techniques.ResultsOur proposed method achieves a Dice score of 0.88, which is obtained by using both elastic-net and spectral dropout for regularization. Compared with a standard CNN-based method, our method shows significantly better segmentation performance on the prostate base and apex. Our experiments also show that data augmentation using the shape model significantly improves the segmentation results.ConclusionsPrior knowledge about the shape of the target organ can improve the performance of CNN-based segmentation methods, especially where image features are not sufficient for a precise segmentation. Statistical shape models can also be employed to synthesize additional training data that can ease the training of large CNNs.","This project is funded by the Canadian Institutes of Health Research (CIHR) and the Prostate Cancer Canada (PCC). We would like to thank the support from the Charles Laszlo Chair in Biomedical Engineering held by Professor Salcudean. The authors also gratefully acknowledge the help from physicians and staff at the Vancouver Cancer Centre who have contributed to this project (Grant Nos. MOP-142439, D2016-1352).",,International Journal of Computer Assisted Radiology and Surgery,,"Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Machine Learning; Magnetic Resonance Imaging; Male; Models, Statistical; Neural Networks, Computer; Prostate",2018-05-15,2018,2018-05-15,2018-08,13,8,1211-1219,Closed,Article,"Karimi, Davood; Samei, Golnoosh; Kesch, Claudia; Nir, Guy; Salcudean, Septimiu E.","Karimi, Davood (Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada); Samei, Golnoosh (Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada); Kesch, Claudia (British Columbia Cancer Agency, Vancouver, BC, Canada); Nir, Guy (Department of Urologic Sciences, University of British Columbia, Vancouver, BC, Canada); Salcudean, Septimiu E. (Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada)","Karimi, Davood (University of British Columbia)","Karimi, Davood (University of British Columbia); Samei, Golnoosh (University of British Columbia); Kesch, Claudia (BC Cancer Agency); Nir, Guy (University of British Columbia); Salcudean, Septimiu E. (University of British Columbia)",60,31,2.03,20.66,,https://app.dimensions.ai/details/publication/pub.1103995269,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,,
5001,pub.1112830204,10.1016/j.ijrobp.2019.03.017,30890447,,Automatic Segmentation of the Prostate on CT Images Using Deep Neural Networks (DNN),"PURPOSE: Recent advances in deep neural networks (DNNs) have unlocked opportunities for their application for automatic image segmentation. We have evaluated a DNN-based algorithm for automatic segmentation of the prostate gland on a large cohort of patient images.
METHODS AND MATERIALS: Planning-CT data sets for 1114 patients with prostate cancer were retrospectively selected and divided into 2 groups. Group A contained 1104 data sets, with 1 physician-generated prostate gland contour for each data set. Among these image sets, 771 were used for training, 193 for validation, and 140 for testing. Group B contained 10 data sets, each including prostate contours delineated by 5 independent physicians and a consensus contour generated using the STAPLE method in the CERR software package. All images were resampled to a spatial resolution of 1 × 1 × 1.5 mm. A region (128 × 128 × 64 voxels) containing the prostate was selected to train a DNN. The best-performing model on the validation data sets was used to segment the prostate on all testing images. Results were compared between DNN and physician-generated contours using the Dice similarity coefficient, Hausdorff distances, regional contour distances, and center-of-mass distances.
RESULTS: The mean Dice similarity coefficients between DNN-based prostate segmentation and physician-generated contours for test data in Group A, Group B, and Group B-consensus were 0.85 ± 0.06 (range, 0.65-0.93), 0.85 ± 0.04 (range, 0.80-0.91), and 0.88 ± 0.03 (range, 0.82-0.92), respectively. The Hausdorff distance was 7.0 ± 3.5 mm, 7.3 ± 2.0 mm, and 6.3 ± 2.0 mm for Group A, Group B, and Group B-consensus, respectively. The mean center-of-mass distances for all 3 data set groups were within 5 mm.
CONCLUSIONS: A DNN-based algorithm was used to automatically segment the prostate for a large cohort of patients with prostate cancer. DNN-based prostate segmentations were compared to the consensus contour for a smaller group of patients; the agreement between DNN segmentations and consensus contour was similar to the agreement reported in a previous study. Clinical use of DNNs is promising, but further investigation is warranted.",,,International Journal of Radiation Oncology • Biology • Physics,,"Algorithms; Humans; Machine Learning; Male; Neural Networks, Computer; Prostate; Prostatic Neoplasms; Radiotherapy Planning, Computer-Assisted; Retrospective Studies",2019-03-16,2019,2019-03-16,2019-07,104,4,924-932,Closed,Article,"Liu, Chang; Gardner, Stephen J; Wen, Ning; Elshaikh, Mohamed A; Siddiqui, Farzan; Movsas, Benjamin; Chetty, Indrin J","Liu, Chang (Department of Radiation Oncology, Josephine Ford Cancer Institute, Henry Ford Health System, Detroit, Michigan. Electronic address: cliu1@hfhs.org.); Gardner, Stephen J (Department of Radiation Oncology, Josephine Ford Cancer Institute, Henry Ford Health System, Detroit, Michigan.); Wen, Ning (Department of Radiation Oncology, Josephine Ford Cancer Institute, Henry Ford Health System, Detroit, Michigan.); Elshaikh, Mohamed A (Department of Radiation Oncology, Josephine Ford Cancer Institute, Henry Ford Health System, Detroit, Michigan.); Siddiqui, Farzan (Department of Radiation Oncology, Josephine Ford Cancer Institute, Henry Ford Health System, Detroit, Michigan.); Movsas, Benjamin (Department of Radiation Oncology, Josephine Ford Cancer Institute, Henry Ford Health System, Detroit, Michigan.); Chetty, Indrin J (Department of Radiation Oncology, Josephine Ford Cancer Institute, Henry Ford Health System, Detroit, Michigan.)","Liu, Chang (Henry Ford Health System)","Liu, Chang (Henry Ford Health System); Gardner, Stephen J (Henry Ford Health System); Wen, Ning (Henry Ford Health System); Elshaikh, Mohamed A (Henry Ford Health System); Siddiqui, Farzan (Henry Ford Health System); Movsas, Benjamin (Henry Ford Health System); Chetty, Indrin J (Henry Ford Health System)",59,38,3.54,25.74,,https://app.dimensions.ai/details/publication/pub.1112830204,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
4880,pub.1126787871,10.1007/s00330-020-06786-w,32306078,,Manual prostate cancer segmentation in MRI: interreader agreement and volumetric correlation with transperineal template core needle biopsy,"ObjectivesTo assess interreader agreement of manual prostate cancer lesion segmentation on multiparametric MR images (mpMRI). The secondary aim was to compare tumor volume estimates between MRI segmentation and transperineal template saturation core needle biopsy (TTSB).MethodsWe retrospectively reviewed patients who had undergone mpMRI of the prostate at our institution and who had received TTSB within 190 days of the examination. Seventy-eight cancer lesions with Gleason score of at least 3 + 4 = 7 were manually segmented in T2-weighted images by 3 radiologists and 1 medical student. Twenty lesions were also segmented in apparent diffusion coefficient (ADC) and dynamic contrast enhanced (DCE) series. First, 20 volumetric similarity scores were computed to quantify interreader agreement. Second, manually segmented cancer lesion volumes were compared with TTSB-derived estimates by Bland-Altman analysis and Wilcoxon testing.ResultsInterreader agreement across all readers was only moderate with mean T2 Dice score of 0.57 (95%CI 0.39–0.70), volumetric similarity coefficient of 0.74 (0.48–0.89), and Hausdorff distance of 5.23 mm (3.17–9.32 mm). Discrepancy of volume estimate between MRI and TTSB was increasing with tumor size. Discrepancy was significantly different between tumors with a Gleason score 3 + 4 vs. higher grade tumors (0.66 ml vs. 0.78 ml; p = 0.007). There were no significant differences between T2, ADC, and DCE segmentations.ConclusionsWe found at best moderate interreader agreement of manual prostate cancer segmentation in mpMRI. Additionally, our study suggests a systematic discrepancy between the tumor volume estimate by MRI segmentation and TTSB core length, especially for large and high-grade tumors.Key Points• Manual prostate cancer segmentation in mpMRI shows moderate interreader agreement.• There are no significant differences between T2, ADC, and DCE segmentation agreements.• There is a systematic difference between volume estimates derived from biopsy and MRI.",,The authors state that this work has not received any funding.,European Radiology,,"Aged; Biopsy; Biopsy, Large-Core Needle; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Neoplasm Grading; Perineum; Prostatic Neoplasms; Retrospective Studies",2020-04-19,2020,2020-04-19,2020-09,30,9,4806-4815,Closed,Article,"Liechti, Marc R.; Muehlematter, Urs J.; Schneider, Aurelia F.; Eberli, Daniel; Rupp, Niels J.; Hötker, Andreas M.; Donati, Olivio F.; Becker, Anton S.","Liechti, Marc R. (Institute of Diagnostic and Interventional Radiology, University Hospital of Zurich, Zurich, Switzerland); Muehlematter, Urs J. (Institute of Diagnostic and Interventional Radiology, University Hospital of Zurich, Zurich, Switzerland); Schneider, Aurelia F. (Institute of Diagnostic and Interventional Radiology, University Hospital of Zurich, Zurich, Switzerland); Eberli, Daniel (Department of Urology, University Hospital of Zurich, Zurich, Switzerland); Rupp, Niels J. (Department of Pathology and Molecular Pathology, University Hospital of Zurich, Zurich, Switzerland); Hötker, Andreas M. (Institute of Diagnostic and Interventional Radiology, University Hospital of Zurich, Zurich, Switzerland); Donati, Olivio F. (Institute of Diagnostic and Interventional Radiology, University Hospital of Zurich, Zurich, Switzerland); Becker, Anton S. (Institute of Diagnostic and Interventional Radiology, University Hospital of Zurich, Zurich, Switzerland; Department of Radiology, Memorial Sloan Kettering Cancer Center, New York, NY, USA)","Becker, Anton S. (University Hospital of Zurich; Memorial Sloan Kettering Cancer Center)","Liechti, Marc R. (University Hospital of Zurich); Muehlematter, Urs J. (University Hospital of Zurich); Schneider, Aurelia F. (University Hospital of Zurich); Eberli, Daniel (University Hospital of Zurich); Rupp, Niels J. (University Hospital of Zurich); Hötker, Andreas M. (University Hospital of Zurich); Donati, Olivio F. (University Hospital of Zurich); Becker, Anton S. (University Hospital of Zurich; Memorial Sloan Kettering Cancer Center)",13,11,1.39,5.54,,https://app.dimensions.ai/details/publication/pub.1126787871,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
4872,pub.1121505576,10.1117/1.jmi.6.4.044001,31592439,PMC6777650,Active learning strategy and hybrid training for infarct segmentation on diffusion MRI with a U-shaped network,"Automatic and reliable stroke lesion segmentation from diffusion magnetic resonance imaging (MRI) is critical for patient care. Methods using neural networks have been developed, but the rate of false positives limits their use in clinical practice. A training strategy applied to three-dimensional deconvolutional neural networks for stroke lesion segmentation on diffusion MRI was proposed. Infarcts were segmented by experts on diffusion MRI for 929 patients. We divided each database as follows: 60% for a training set, 20% for validation, and 20% for testing. Our hypothesis was a two-phase hybrid learning scheme, in which the network was first trained with whole MRI (regular phase) and then, in a second phase (hybrid phase), alternately with whole MRI and patches. Patches were actively selected from the discrepancy between expert and model segmentation at the beginning of each batch. On the test population, the performances after the regular and hybrid phases were compared. A statistically significant Dice improvement with hybrid training compared with regular training was demonstrated ( p < 0.01  ). The mean Dice reached 0.711 ± 0.199  . False positives were reduced by almost 30% with hybrid training ( p < 0.01  ). Our hybrid training strategy empowered deep neural networks for more accurate infarct segmentations on diffusion MRI.","This work was supported by public grants from the French “Agence Nationale de la Recherche” within the context of the “Investments for the Future” program, referenced ANR-10-LABX-57 and named “TRAIL” (Translational Research and Advanced Imaging Laboratory, project DEEP-STROKE). “Region Nouvelle Aquitaine” also supported this work. The databases were funded by public grants from the French government (PHRC protocole hospitalier de recherche clinique). T.T. also received financial support from the ARSEP Foundation and from the French “Agence Nationale de la Recherche” within the context of the “Investments for the Future” program, referenced ANR-10-LABX-43 and named BRAIN. The authors would like to thank Enago (www.enago.com) for the English language review.",,Journal of Medical Imaging,,,2019-10-04,2019,2019-10-04,2019-10,6,4,044001-044001,All OA, Green,Article,"Olivier, Aurélien; Moal, Olivier; Moal, Bertrand; Munsch, Fanny; Okubo, Gosuke; Sibon, Igor; Dousset, Vincent; Tourdias, Thomas","Olivier, Aurélien (DESKi, Bordeaux, France); Moal, Olivier (DESKi, Bordeaux, France); Moal, Bertrand (DESKi, Bordeaux, France); Munsch, Fanny (Université de Bordeaux, Neurocentre Magendie, Inserm U1215, Bordeaux, France); Okubo, Gosuke (Université de Bordeaux, Neurocentre Magendie, Inserm U1215, Bordeaux, France); Sibon, Igor (Université Bordeaux Segalen, CHU de Bordeaux, Unité Neuro-Vasculaire, Bordeaux, France; Université de Bordeaux, UMR 5287 CNRS, Bordeaux, France); Dousset, Vincent (Université de Bordeaux, Neurocentre Magendie, Inserm U1215, Bordeaux, France; CHU Bordeaux, Neuroimagerie Diagnostique et Thérapeutique, Bordeaux, France); Tourdias, Thomas (Université de Bordeaux, Neurocentre Magendie, Inserm U1215, Bordeaux, France; CHU Bordeaux, Neuroimagerie Diagnostique et Thérapeutique, Bordeaux, France)","Moal, Olivier ","Olivier, Aurélien (); Moal, Olivier (); Moal, Bertrand (); Munsch, Fanny (University of Bordeaux); Okubo, Gosuke (University of Bordeaux); Sibon, Igor (University of Bordeaux); Dousset, Vincent (University of Bordeaux; Centre Hospitalier Universitaire de Bordeaux); Tourdias, Thomas (University of Bordeaux; Centre Hospitalier Universitaire de Bordeaux)",2,1,0.3,0.56,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6777650,https://app.dimensions.ai/details/publication/pub.1121505576,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 40 Engineering, 4003 Biomedical Engineering,,,,,,,,
4860,pub.1112645127,10.1117/12.2512282,32528212,PMC7289512,A semiautomatic approach for prostate segmentation in MR images using local texture classification and statistical shape modeling,"Segmentation of the prostate in magnetic resonance (MR) images has many applications in image-guided treatment planning and procedures such as biopsy and focal therapy. However, manual delineation of the prostate boundary is a time-consuming task with high inter-observer variation. In this study, we proposed a semiautomated, three-dimensional (3D) prostate segmentation technique for T2-weighted MR images based on shape and texture analysis. The prostate gland shape is usually globular with a smoothly curved surface that could be accurately modeled and reconstructed if the locations of a limited number of well-distributed surface points are known. For a training image set, we used an inter-subject correspondence between the prostate surface points to model the prostate shape variation based on a statistical point distribution modeling. We also studied the local texture difference between prostate and non-prostate tissues close to the prostate surface. To segment a new image, we used the learned prostate shape and texture characteristics to search for the prostate border close to an initially estimated prostate surface. We used 23 MR images for training, and 14 images for testing the algorithm performance. We compared the results to two sets of experts' manual reference segmentations. The measured mean ± standard deviation of error values for the whole gland were 1.4 ± 0.4 mm, 8.5 ± 2.0 mm, and 86 ± 3% in terms of mean absolute distance (MAD), Hausdorff distance (HDist), and Dice similarity coefficient (DSC). The average measured differences between the two experts on the same datasets were 1.5 mm (MAD), 9.0 mm (HDist), and 83% (DSC). The proposed algorithm illustrated a fast, accurate, and robust performance for 3D prostate segmentation. The accuracy of the algorithm is within the inter-expert variability observed in manual segmentation and comparable to the best performance results reported in the literature.",,,Proceedings of SPIE,"Medical Imaging 2019: Image-Guided Procedures, Robotic Interventions, and Modeling",,2019-02,2019,2019-03-08,2019-02,10951,,109512i-109512i-12,All OA, Green,Proceeding,"Shahedi, Maysam; Halicek, Martin; Li, Qinmei; Liu, Lizhi; Zhang, Zhenfeng; Verma, Sadhna; Schuster, David M; Fei, Baowei","Shahedi, Maysam (Department of Bioengineering, The University of Texas at Dallas, Richardson, TX.); Halicek, Martin (Department of Bioengineering, The University of Texas at Dallas, Richardson, TX.; Department of Biomedical Engineering, Emory University and Georgia Institute of Technology, Atlanta, GA.); Li, Qinmei (Department of Bioengineering, The University of Texas at Dallas, Richardson, TX.; Department of Radiology, The Second Affiliated Hospital of Guangzhou, Medical University, Guangzhou, China.); Liu, Lizhi (State Key Laboratory of Oncology Collaborative Innovation Center for Cancer Medicine, Sun Yat-Sen University Cancer Center, Guangzhou, China.); Zhang, Zhenfeng (Department of Radiology, The Second Affiliated Hospital of Guangzhou, Medical University, Guangzhou, China.); Verma, Sadhna (Department of Radiology, University of Cincinnati Medical Center and The Veterans Administration Hospital, Cincinnati, OH.); Schuster, David M (Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA.); Fei, Baowei (Department of Bioengineering, The University of Texas at Dallas, Richardson, TX.; Department of Radiology, University of Texas Southwestern Medical Center, Dallas, TX.)",,"Shahedi, Maysam (The University of Texas at Dallas); Halicek, Martin (The University of Texas at Dallas; Georgia Institute of Technology); Li, Qinmei (The University of Texas at Dallas; Second Affiliated Hospital of Guangzhou Medical University); Liu, Lizhi (Sun Yat-sen University Cancer Center); Zhang, Zhenfeng (Second Affiliated Hospital of Guangzhou Medical University); Verma, Sadhna (University of Cincinnati Medical Center); Schuster, David M (Emory University); Fei, Baowei (The University of Texas at Dallas; The University of Texas Southwestern Medical Center)",3,3,0.17,2.1,https://europepmc.org/articles/pmc7289512?pdf=render,https://app.dimensions.ai/details/publication/pub.1112645127,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
4860,pub.1106021832,10.1007/s11548-018-1841-4,30088208,PMC6177294,Deep dense multi-path neural network for prostate segmentation in magnetic resonance imaging,"PurposeWe propose an approach of 3D convolutional neural network to segment the prostate in MR images.MethodsA 3D deep dense multi-path convolutional neural network that follows the framework of the encoder–decoder design is proposed. The encoder is built based upon densely connected layers that learn the high-level feature representation of the prostate. The decoder interprets the features and predicts the whole prostate volume by utilizing a residual layout and grouped convolution. A set of sub-volumes of MR images, centered at the prostate, is generated and fed into the proposed network for training purpose. The performance of the proposed network is compared to previously reported approaches.ResultsTwo independent datasets were employed to assess the proposed network. In quantitative evaluations, the proposed network achieved 95.11 and 89.01 Dice coefficients for the two datasets. The segmentation results were robust to variations in MR images. In comparison experiments, the segmentation performance of the proposed network was comparable to the previously reported approaches. In qualitative evaluations, the segmentation results by the proposed network were well matched to the ground truth provided by human experts.ConclusionsThe proposed network is capable of segmenting the prostate in an accurate and robust manner. This approach can be applied to other types of medical images.",This was supported by the National Research Foundation of Korea through the Korea Government (MSIP) under Grant 2016R1C1B2012433.,,International Journal of Computer Assisted Radiology and Surgery,,"Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate",2018-08-07,2018,2018-08-07,2018-11,13,11,1687-1696,All OA, Green,Article,"To, Minh Nguyen Nhat; Vu, Dang Quoc; Turkbey, Baris; Choyke, Peter L.; Kwak, Jin Tae","To, Minh Nguyen Nhat (Department of Computer Science and Engineering, Sejong University, 05006, Seoul, South Korea); Vu, Dang Quoc (Department of Computer Science and Engineering, Sejong University, 05006, Seoul, South Korea); Turkbey, Baris (Molecular Imaging Program, National Cancer Institute, National Institutes of Health, 20892, Bethesda, MD, USA); Choyke, Peter L. (Molecular Imaging Program, National Cancer Institute, National Institutes of Health, 20892, Bethesda, MD, USA); Kwak, Jin Tae (Department of Computer Science and Engineering, Sejong University, 05006, Seoul, South Korea)","Kwak, Jin Tae (Sejong University)","To, Minh Nguyen Nhat (Sejong University); Vu, Dang Quoc (Sejong University); Turkbey, Baris (National Cancer Institute); Choyke, Peter L. (National Cancer Institute); Kwak, Jin Tae (Sejong University)",43,20,2.11,12.54,https://europepmc.org/articles/pmc6177294?pdf=render,https://app.dimensions.ai/details/publication/pub.1106021832,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
4837,pub.1131293254,10.1109/access.2020.3027738,34812359,PMC8545278,MSD-Net: Multi-Scale Discriminative Network for COVID-19 Lung Infection Segmentation on CT,"Since the first patient reported in December 2019, 2019 novel coronavirus disease (COVID-19) has become global pandemic with more than 10 million total confirmed cases and 500 thousand related deaths. Using deep learning methods to quickly identify COVID-19 and accurately segment the infected area can help control the outbreak and assist in treatment. Computed tomography (CT) as a fast and easy clinical method, it is suitable for assisting in diagnosis and treatment of COVID-19. According to clinical manifestations, COVID-19 lung infection areas can be divided into three categories: ground-glass opacities, interstitial infiltrates and consolidation. We proposed a multi-scale discriminative network (MSD-Net) for multi-class segmentation of COVID-19 lung infection on CT. In the MSD-Net, we proposed pyramid convolution block (PCB), channel attention block (CAB) and residual refinement block (RRB). The PCB can increase the receptive field by using different numbers and different sizes of kernels, which strengthened the ability to segment the infected areas of different sizes. The CAB was used to fusion the input of the two stages and focus features on the area to be segmented. The role of RRB was to refine the feature maps. Experimental results showed that the dice similarity coefficient (DSC) of the three infection categories were 0.7422,0.7384,0.8769 respectively. For sensitivity and specificity, the results of three infection categories were (0.8593, 0.9742), (0.8268,0.9869) and (0.8645,0.9889) respectively. The experimental results demonstrated that the network proposed in this paper can effectively segment the COVID-19 infection on CT images. It can be adopted for assisting in diagnosis and treatment of COVID-19.","This work was supported in part by the Qingdao City Science and Technology under Grant 20-4-1-5-nsh, and in part by the Qingdao West Coast New District Science and Technology Project under Grant 2019-59. (Bingbing Zheng and Yaoqi Liu co-first authors.)",,IEEE Access,,,2020-01-01,2020,2020-09-29,2020-01-01,8,,185786-185795,All OA, Gold,Article,"Zheng, Bingbing; Liu, Yaoqi; Zhu, Yu; Yu, Fuli; Jiang, Tianjiao; Yang, Dawei; Xu, Tao","Zheng, Bingbing (School of Information Science and Engineering, East China University of Science and Technology, Shanghai, 200237, China); Liu, Yaoqi (The Affiliated Hospital of Qingdao University, Qingdao, 266000, China); Zhu, Yu (School of Information Science and Engineering, East China University of Science and Technology, Shanghai, 200237, China); Yu, Fuli (School of Information Science and Engineering, East China University of Science and Technology, Shanghai, 200237, China); Jiang, Tianjiao (The Affiliated Hospital of Qingdao University, Qingdao, 266000, China); Yang, Dawei (Department of Pulmonary Medicine, Zhongshan Hospital, Fudan University, Shanghai, 200032, China); Xu, Tao (The Affiliated Hospital of Qingdao University, Qingdao, 266000, China)","Zhu, Yu (East China University of Science and Technology)","Zheng, Bingbing (East China University of Science and Technology); Liu, Yaoqi (Affiliated Hospital of Qingdao University); Zhu, Yu (East China University of Science and Technology); Yu, Fuli (East China University of Science and Technology); Jiang, Tianjiao (Affiliated Hospital of Qingdao University); Yang, Dawei (Fudan University; Zhongshan Hospital); Xu, Tao (Affiliated Hospital of Qingdao University)",32,32,2.09,,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09208691.pdf,https://app.dimensions.ai/details/publication/pub.1131293254,40 Engineering, 46 Information and Computing Sciences,3 Good Health and Well Being,,,,,,,,,
4837,pub.1105006047,10.1007/s00261-018-1660-7,29926137,,Radiomics and radiogenomics of prostate cancer,"Radiomics and radiogenomics are attractive research topics in prostate cancer. Radiomics mainly focuses on extraction of quantitative information from medical imaging, whereas radiogenomics aims to correlate these imaging features to genomic data. The purpose of this review is to provide a brief overview summarizing recent progress in the application of radiomics-based approaches in prostate cancer and to discuss the potential role of radiogenomics in prostate cancer.","This project has been funded in whole or in part with federal funds from the National Cancer Institute, National Institutes of Health, under Contract No. HHSN261200800001E. The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government. This research was also made possible through the NIH Medical Research Scholars Program, a public-private partnership supported jointly by the NIH and generous contributions to the Foundation for the NIH from the Doris Duke Charitable Foundation, the American Association for Dental Research, the Colgate-Palmolive Company, Genentech, Elsevier, and other private donors.",,Abdominal Radiology,,Genomics, Humans, Male, Prostatic Neoplasms,2018-06-20,2018,2018-06-20,2019-06,44,6,2021-2029,Closed,Article,"Smith, Clayton P.; Czarniecki, Marcin; Mehralivand, Sherif; Stoyanova, Radka; Choyke, Peter L.; Harmon, Stephanie; Turkbey, Baris","Smith, Clayton P. (Molecular Imaging Program, National Cancer Institute, NIH, 10 Center drive, room B3B85, 20892, Bethesda, MD, USA; Georgetown University School of Medicine, Washington, DC, USA); Czarniecki, Marcin (Molecular Imaging Program, National Cancer Institute, NIH, 10 Center drive, room B3B85, 20892, Bethesda, MD, USA); Mehralivand, Sherif (Molecular Imaging Program, National Cancer Institute, NIH, 10 Center drive, room B3B85, 20892, Bethesda, MD, USA; Urologic Oncology Branch, National Cancer Institute, NIH, Bethesda, MD, USA; Department of Urology and Pediatric Urology, University Medical Center Mainz, Mainz, Germany); Stoyanova, Radka (Department of Radiation Oncology, University of Miami Miller School of Medicine, Miami, FL, USA); Choyke, Peter L. (Molecular Imaging Program, National Cancer Institute, NIH, 10 Center drive, room B3B85, 20892, Bethesda, MD, USA); Harmon, Stephanie (Clinical Research Directorate/Clinical Monitoring Research Program, Leidos Biomedical Research, Inc, NCI Campus at Frederick, Frederick, MD, USA); Turkbey, Baris (Molecular Imaging Program, National Cancer Institute, NIH, 10 Center drive, room B3B85, 20892, Bethesda, MD, USA)","Turkbey, Baris (National Cancer Institute)","Smith, Clayton P. (National Cancer Institute; Georgetown University); Czarniecki, Marcin (National Cancer Institute); Mehralivand, Sherif (National Cancer Institute; National Cancer Institute; Johannes Gutenberg University of Mainz; University Medical Center of the Johannes Gutenberg University Mainz); Stoyanova, Radka (University of Miami); Choyke, Peter L. (National Cancer Institute); Harmon, Stephanie (Leidos (United States)); Turkbey, Baris (National Cancer Institute)",36,20,2.85,8.96,,https://app.dimensions.ai/details/publication/pub.1105006047,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,3 Good Health and Well Being,,,,,,,
4837,pub.1011991385,10.1117/1.jmi.2.2.025002,26158111,PMC4479588,Three-dimensional nonrigid landmark-based magnetic resonance to transrectal ultrasound registration for image-guided prostate biopsy,"Registration of three-dimensional (3-D) magnetic resonance (MR) to 3-D transrectal ultrasound (TRUS) prostate images is an important step in the planning and guidance of 3-D TRUS guided prostate biopsy. In order to accurately and efficiently perform the registration, a nonrigid landmark-based registration method is required to account for the different deformations of the prostate when using these two modalities. We describe a nonrigid landmark-based method for registration of 3-D TRUS to MR prostate images. The landmark-based registration method first makes use of an initial rigid registration of 3-D MR to 3-D TRUS images using six manually placed approximately corresponding landmarks in each image. Following manual initialization, the two prostate surfaces are segmented from 3-D MR and TRUS images and then nonrigidly registered using the following steps: (1) rotationally reslicing corresponding segmented prostate surfaces from both 3-D MR and TRUS images around a specified axis, (2) an approach to find point correspondences on the surfaces of the segmented surfaces, and (3) deformation of the surface of the prostate in the MR image to match the surface of the prostate in the 3-D TRUS image and the interior using a thin-plate spline algorithm. The registration accuracy was evaluated using 17 patient prostate MR and 3-D TRUS images by measuring the target registration error (TRE). Experimental results showed that the proposed method yielded an overall mean TRE of [Formula: see text] for the rigid registration and [Formula: see text] for the nonrigid registration, which is favorably comparable to a clinical requirement for an error of less than 2.5 mm. A landmark-based nonrigid 3-D MR-TRUS registration approach is proposed, which takes into account the correspondences on the prostate surface, inside the prostate, as well as the centroid of the prostate. Experimental results indicate that the proposed method yields clinically sufficient accuracy.",,,Journal of Medical Imaging,,,2015-06-24,2015,2015-06-24,2015-06-24,2,2,025002-025002,All OA, Green,Article,"Sun, Yue; Qiu, Wu; Yuan, Jing; Romagnoli, Cesare; Fenster, Aaron","Sun, Yue (University of Western Ontario, Imaging Research Laboratories, Robarts Research Institute, London, Ontario N6A 5K8, Canada); Qiu, Wu (University of Western Ontario, Imaging Research Laboratories, Robarts Research Institute, London, Ontario N6A 5K8, Canada); Yuan, Jing (University of Western Ontario, Imaging Research Laboratories, Robarts Research Institute, London, Ontario N6A 5K8, Canada); Romagnoli, Cesare (University of Western Ontario, Department of Medical Imaging, London, Ontario N6A 5K8, Canada); Fenster, Aaron (University of Western Ontario, Imaging Research Laboratories, Robarts Research Institute, London, Ontario N6A 5K8, Canada; University of Western Ontario, Department of Medical Imaging, London, Ontario N6A 5K8, Canada; University of Western Ontario, Department of Medical Biophysics, London, Ontario N6A 5K8, Canada)","Sun, Yue (Western University)","Sun, Yue (Western University); Qiu, Wu (Western University); Yuan, Jing (Western University); Romagnoli, Cesare (Western University); Fenster, Aaron (Western University; Western University; Western University)",6,1,0.18,1.24,https://europepmc.org/articles/pmc4479588?pdf=render,https://app.dimensions.ai/details/publication/pub.1011991385,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4834,pub.1110426892,10.1109/jbhi.2018.2885214,30530377,,Cloud Deployment of High-Resolution Medical Image Analysis With TOMAAT,"BACKGROUND: Deep learning has been recently applied to a multitude of computer vision and medical image analysis problems. Although recent research efforts have improved the state of the art, most of the methods cannot be easily accessed, compared or used by other researchers or clinicians. Even if developers publish their code and pre-trained models on the internet, integration in stand-alone applications and existing workflows is often not straightforward, especially for clinical research partners. In this paper, we propose an open-source framework to provide AI-enabled medical image analysis through the network.
METHODS: TOMAAT provides a cloud environment for general medical image analysis, composed of three basic components: (i) an announcement service, maintaining a public registry of (ii) multiple distributed server nodes offering various medical image analysis solutions, and (iii) client software offering simple interfaces for users. Deployment is realized through HTTP-based communication, along with an API and wrappers for common image manipulations during pre- and post-processing.
RESULTS: We demonstrate the utility and versatility of TOMAAT on several hallmark medical image analysis tasks: segmentation, diffeomorphic deformable atlas registration, landmark localization, and workflow integration. Through TOMAAT, the high hardware demands, setup and model complexity of demonstrated approaches are transparent to users, who are provided with simple client interfaces. We present example clients in three-dimensional Slicer, in the web browser, on iOS devices and in a commercially available, certified medical image analysis suite.
CONCLUSION: TOMAAT enables deployment of state-of-the-art image segmentation in the cloud, fostering interaction among deep learning researchers and medical collaborators in the clinic. Currently, a public announcement service is hosted by the authors, and several ready-to-use services are registered and enlisted at http://tomaat.cloud.","This work was supported by the German Federal Ministry of Education and Health (BMBF), in connection with the foundation of the German Center for Vertigo and Balance Disorders (DSGZ) under Grant 01 EO 0901. (Fausto Milletari and Johann Frei contributed equally to this work.) The authors would like to thank the staff of ImFusion for providing us with information on their TOMAAT client implementation.",,IEEE Journal of Biomedical and Health Informatics,,"Algorithms; Cloud Computing; Deep Learning; Diagnostic Imaging; Humans; Image Interpretation, Computer-Assisted",2018-12-05,2018,2018-12-05,2019-05,23,3,969-977,Closed,Article,"Milletari, Fausto; Frei, Johann; Aboulatta, Moustafa; Vivar, Gerome; Ahmadi, Seyed-Ahmad","Milletari, Fausto (Nvidia Inc., Santa Clara, CA 95051, USA); Frei, Johann (German Center for Vertigo and Balance Disorders (DSGZ), Ludwig-Maximilians University, Munich, 81377, Germany; Technical University of Munich Garching, Garching bei M, nchen, 85748, Germany); Aboulatta, Moustafa (German Center for Vertigo and Balance Disorders (DSGZ), Ludwig-Maximilians University, Munich, 81377, Germany; Technical University of Munich Garching, Garching bei M, nchen, 85748, Germany); Vivar, Gerome (German Center for Vertigo and Balance Disorders (DSGZ), Ludwig-Maximilians University, Munich, 81377, Germany; Technical University of Munich Garching, Garching bei M, nchen, 85748, Germany); Ahmadi, Seyed-Ahmad (German Center for Vertigo and Balance Disorders (DSGZ), Ludwig-Maximilians University, Munich, 81377, Germany; Technical University of Munich Garching, Garching bei M, nchen, 85748, Germany)",,"Milletari, Fausto (Nvidia (United States)); Frei, Johann (Ludwig-Maximilians-Universität München; Technical University of Munich); Aboulatta, Moustafa (Ludwig-Maximilians-Universität München; Technical University of Munich); Vivar, Gerome (Ludwig-Maximilians-Universität München; Technical University of Munich); Ahmadi, Seyed-Ahmad (Ludwig-Maximilians-Universität München; Technical University of Munich)",11,8,0.95,3.0,,https://app.dimensions.ai/details/publication/pub.1110426892,46 Information and Computing Sciences, 4606 Distributed Computing and Systems Software,,,,,,,,,,,
4821,pub.1150005020,10.21037/qims-22-332,36185062,PMC9511434,Automated volumetric and statistical shape assessment of cam-type morphology of the femoral head-neck region from clinical 3D magnetic resonance images,"Background: Femoroacetabular impingement (FAI) cam morphology is routinely assessed using manual measurements of two-dimensional (2D) alpha angles which are prone to high rater variability and do not provide direct three-dimensional (3D) data on these osseous formations. We present CamMorph, a fully automated 3D pipeline for segmentation, statistical shape assessment and measurement of cam volume, surface area and height from clinical magnetic resonance (MR) images of the hip in FAI patients.
Methods: The novel CamMorph pipeline involves two components: (I) accurate proximal femur segmentation generated by combining the 3D U-net to identify both global (region) and local (edge) features in clinical MR images and focused shape modelling to generate a 3D anatomical model for creating patient-specific proximal femur models; (II) patient-specific anatomical information from 3D focused shape modelling to simulate 'healthy' femoral bone models with cam-affected region constraints applied to the anterosuperior femoral head-neck region to quantify cam morphology in FAI patients. The CamMorph pipeline, which generates patient-specific data within 5 min, was used to analyse multi-site clinical MR images of the hip to measure and assess cam morphology in male (n=56) and female (n=41) FAI patients.
Results: There was excellent agreement between manual and CamMorph segmentations of the proximal femur as demonstrated by the mean Dice similarity index (DSI; 0.964±0.006), 95% Hausdorff distance (HD; 2.123±0.876 mm) and average surface distance (ASD; 0.539±0.189 mm) values. Compared to female FAI patients, male patients had a significantly larger median cam volume (969.22 vs. 272.97 mm3, U=240.0, P<0.001), mean surface area [657.36 vs. 306.93 mm2, t(95)=8.79, P<0.001], median maximum-height (3.66 vs. 2.15 mm, U=407.0, P<0.001) and median average-height (1.70 vs. 0.86 mm, U=380.0, P<0.001).
Conclusions: The fully automated 3D CamMorph pipeline developed in the present study successfully segmented and measured cam morphology from clinical MR images of the hip in male and female patients with differing FAI severity and pathoanatomical characteristics.",We would like to thank Professor Kim Bennell who was a Principal Investigator for the FASHIoN trial and was involved in the data collection for the MR images used in this study. Funding: This work was supported by the University of Queensland and the Australian e-Health Research Centre under the Multi-Institutional Agreement for the National Health and Medical Research Council Development Grant APP1139868 entitled “MR Hip Intervention and Planning System to Enhance Clinical and Surgical Outcomes”.,,Quantitative Imaging in Medicine and Surgery,,,2022-10,2022,2022-10,2022-10,0,0,0-0,All OA, Gold,Article,"Bugeja, Jessica M.; Xia, Ying; Chandra, Shekhar S.; Murphy, Nicholas J.; Eyles, Jillian; Spiers, Libby; Crozier, Stuart; Hunter, David J.; Fripp, Jurgen; Engstrom, Craig","Bugeja, Jessica M. (School of Information Technology and Electrical Engineering, The University of Queensland, Brisbane, Australia, ;; Australian e-Health Research Centre, Commonwealth Scientific and Industrial Research Organisation, Health and Biosecurity, Herston, Australia, ;; ORCID: 0000-0003-3631-969X.); Xia, Ying (Australian e-Health Research Centre, Commonwealth Scientific and Industrial Research Organisation, Health and Biosecurity, Herston, Australia, ;); Chandra, Shekhar S. (School of Information Technology and Electrical Engineering, The University of Queensland, Brisbane, Australia, ;); Murphy, Nicholas J. (Kolling Institute of Medical Research, Institute of Bone and Joint Research, University of Sydney, Sydney, Australia, ;; Department of Orthopaedic Surgery, John Hunter Hospital, Newcastle, Australia, ;); Eyles, Jillian (Kolling Institute of Medical Research, Institute of Bone and Joint Research, University of Sydney, Sydney, Australia, ;; Department of Rheumatology, Royal North Shore Hospital, St Leonards, Australia, ;); Spiers, Libby (Centre for Health, Exercise and Sports Medicine, Department of Physiotherapy, University of Melbourne, Melbourne, Australia, ;); Crozier, Stuart (School of Information Technology and Electrical Engineering, The University of Queensland, Brisbane, Australia, ;); Hunter, David J. (Kolling Institute of Medical Research, Institute of Bone and Joint Research, University of Sydney, Sydney, Australia, ;; Department of Rheumatology, Royal North Shore Hospital, St Leonards, Australia, ;); Fripp, Jurgen (Australian e-Health Research Centre, Commonwealth Scientific and Industrial Research Organisation, Health and Biosecurity, Herston, Australia, ;); Engstrom, Craig (School of Human Movement Studies, The University of Queensland, Brisbane, Australia)","Bugeja, Jessica M. (University of Queensland; Australian e-Health Research Centre; )","Bugeja, Jessica M. (University of Queensland; Australian e-Health Research Centre); Xia, Ying (Australian e-Health Research Centre); Chandra, Shekhar S. (University of Queensland); Murphy, Nicholas J. (The University of Sydney; John Hunter Hospital); Eyles, Jillian (The University of Sydney; Royal North Shore Hospital); Spiers, Libby (University of Melbourne); Crozier, Stuart (University of Queensland); Hunter, David J. (The University of Sydney; Royal North Shore Hospital); Fripp, Jurgen (Australian e-Health Research Centre); Engstrom, Craig (University of Queensland)",1,1,,,https://qims.amegroups.com/article/viewFile/99115/pdf,https://app.dimensions.ai/details/publication/pub.1150005020,"40 Engineering; 4003 Biomedical Engineering; 51 Physical Sciences; 5102 Atomic, Molecular and Optical Physics",,,,,,,,,,,
4815,pub.1147842497,10.3390/jimaging8050133,35621897,PMC9146644,Integration of Deep Learning and Active Shape Models for More Accurate Prostate Segmentation in 3D MR Images,"Magnetic resonance imaging (MRI) has a growing role in the clinical workup of prostate cancer. However, manual three-dimensional (3D) segmentation of the prostate is a laborious and time-consuming task. In this scenario, the use of automated algorithms for prostate segmentation allows us to bypass the huge workload of physicians. In this work, we propose a fully automated hybrid approach for prostate gland segmentation in MR images using an initial segmentation of prostate volumes using a custom-made 3D deep network (VNet-T2), followed by refinement using an Active Shape Model (ASM). While the deep network focuses on three-dimensional spatial coherence of the shape, the ASM relies on local image information and this joint effort allows for improved segmentation of the organ contours. Our method is developed and tested on a dataset composed of T2-weighted (T2w) MRI prostatic volumes of 60 male patients. In the test set, the proposed method shows excellent segmentation performance, achieving a mean dice score and Hausdorff distance of 0.851 and 7.55 mm, respectively. In the future, this algorithm could serve as an enabling technology for the development of computer-aided systems for prostate cancer characterization in MR imaging.",,"This work was partially supported by the Cassa di Risparmio di Cuneo (CRC, Italy), Grant No. CRC_2016-0707.",Journal of Imaging,,,2022-05-11,2022,2022-05-11,,8,5,133,All OA, Gold,Article,"Salvi, Massimo; De Santi, Bruno; Pop, Bianca; Bosco, Martino; Giannini, Valentina; Regge, Daniele; Molinari, Filippo; Meiburger, Kristen M.","Salvi, Massimo (Biolab, PolitoBIOMed Lab, Department of Electronics and Telecommunications, Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129 Turin, Italy;, massimo.salvi@polito.it, (M.S.);, bianca.pop51@gmail.com, (B.P.);, filippo.molinari@polito.it, (F.M.)); De Santi, Bruno (Multi-Modality Medical Imaging (M3I), Technical Medical Centre, University of Twente, PB217, 7500 AE Enschede, The Netherlands;, b.desanti@utwente.nl); Pop, Bianca (Biolab, PolitoBIOMed Lab, Department of Electronics and Telecommunications, Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129 Turin, Italy;, massimo.salvi@polito.it, (M.S.);, bianca.pop51@gmail.com, (B.P.);, filippo.molinari@polito.it, (F.M.)); Bosco, Martino (Department of Pathology, Ospedale Michele e Pietro Ferrero, 12060 Verduno, Italy;, mbosco@aslcn2.it); Giannini, Valentina (Department of Surgical Sciences, University of Turin, 10126 Turin, Italy;, valentina.giannini@unito.it, (V.G.);, daniele.regge@unito.it, (D.R.); Department of Radiology, Candiolo Cancer Institute, FPO-IRCCS, 10060 Candiolo, Italy); Regge, Daniele (Department of Surgical Sciences, University of Turin, 10126 Turin, Italy;, valentina.giannini@unito.it, (V.G.);, daniele.regge@unito.it, (D.R.); Department of Radiology, Candiolo Cancer Institute, FPO-IRCCS, 10060 Candiolo, Italy); Molinari, Filippo (Biolab, PolitoBIOMed Lab, Department of Electronics and Telecommunications, Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129 Turin, Italy;, massimo.salvi@polito.it, (M.S.);, bianca.pop51@gmail.com, (B.P.);, filippo.molinari@polito.it, (F.M.)); Meiburger, Kristen M. (Biolab, PolitoBIOMed Lab, Department of Electronics and Telecommunications, Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129 Turin, Italy;, massimo.salvi@polito.it, (M.S.);, bianca.pop51@gmail.com, (B.P.);, filippo.molinari@polito.it, (F.M.))","Meiburger, Kristen M. (Polytechnic University of Turin)","Salvi, Massimo (Polytechnic University of Turin); De Santi, Bruno (University of Twente); Pop, Bianca (Polytechnic University of Turin); Bosco, Martino (); Giannini, Valentina (University of Turin; Candiolo Cancer Institute); Regge, Daniele (University of Turin; Candiolo Cancer Institute); Molinari, Filippo (Polytechnic University of Turin); Meiburger, Kristen M. (Polytechnic University of Turin)",1,1,,,https://www.mdpi.com/2313-433X/8/5/133/pdf?version=1652754867,https://app.dimensions.ai/details/publication/pub.1147842497,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,
4815,pub.1139173944,10.1016/j.media.2021.102146,34274692,,Self-paced and self-consistent co-training for semi-supervised image segmentation,"Deep co-training has recently been proposed as an effective approach for image segmentation when annotated data is scarce. In this paper, we improve existing approaches for semi-supervised segmentation with a self-paced and self-consistent co-training method. To help distillate information from unlabeled images, we first design a self-paced learning strategy for co-training that lets jointly-trained neural networks focus on easier-to-segment regions first, and then gradually consider harder ones. This is achieved via an end-to-end differentiable loss in the form of a generalized Jensen Shannon Divergence (JSD). Moreover, to encourage predictions from different networks to be both consistent and confident, we enhance this generalized JSD loss with an uncertainty regularizer based on entropy. The robustness of individual models is further improved using a self-ensembling loss that enforces their prediction to be consistent across different training iterations. We demonstrate the potential of our method on three challenging image segmentation problems with different image modalities, using a small fraction of labeled data. Results show clear advantages in terms of performance compared to the standard co-training baselines and recently proposed state-of-the-art approaches for semi-supervised segmentation.","This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grants Program under grant RGPIN-2018-05715;by the NSFC-Zhejiang Joint Fund of the Integration of Informatization and Industrialization under Grant No.U1909210, the National Natural Science Foundation of China under Grant (No.61772312).",,Medical Image Analysis,,"Entropy; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Supervised Machine Learning; Uncertainty",2021-06-26,2021,2021-06-26,2021-10,73,,102146,All OA, Green,Article,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (Department of Software and IT Engineering, Ecole de technologie supérieure, Montreal, H3C1K3, Canada. Electronic address: ping.wang.1@ens.etsmtl.ca.); Peng, Jizong (Department of Software and IT Engineering, Ecole de technologie supérieure, Montreal, H3C1K3, Canada. Electronic address: jizong.peng.1@etsmtl.net.); Pedersoli, Marco (Department of Software and IT Engineering, Ecole de technologie supérieure, Montreal, H3C1K3, Canada. Electronic address: marco.pedersoli@etsmtl.ca.); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, 250101, China. Electronic address: yfzhou@sdu.edu.cn.); Zhang, Caiming (School of Software, Shandong University, Jinan, 250101, China. Electronic address: czhang@sdu.edu.cn.); Desrosiers, Christian (Department of Software and IT Engineering, Ecole de technologie supérieure, Montreal, H3C1K3, Canada. Electronic address: christian.desrosiers@etsmtl.ca.)","Wang, Ping (École de Technologie Supérieure)","Wang, Ping (École de Technologie Supérieure); Peng, Jizong (École de Technologie Supérieure); Pedersoli, Marco (École de Technologie Supérieure); Zhou, Yuanfeng (Shandong University); Zhang, Caiming (Shandong University); Desrosiers, Christian (École de Technologie Supérieure)",19,19,1.94,,http://arxiv.org/pdf/2011.00325,https://app.dimensions.ai/details/publication/pub.1139173944,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
4810,pub.1153565997,10.1016/j.cmpb.2022.107307,36571889,,CASF-Net: Cross-attention and cross-scale fusion network for medical image segmentation,"BACKGROUND: Automatic segmentation of medical images has progressed greatly owing to the development of convolutional neural networks (CNNs). However, there are two uncertainties with current approaches based on convolutional operations: (1) how to eliminate the general limitations that CNNs lack the ability of modeling long-range dependencies and global contextual interactions, and (2) how to efficiently discover and integrate global and local features that are implied in the image. Notably, these two problems are interconnected, yet previous approaches mainly focus on the first problem and ignore the importance of information integration.
METHODS: In this paper, we propose a novel cross-attention and cross-scale fusion network (CASF-Net), which aims to explicitly tap the potential of dual-branch networks and fully integrate the coarse and fine-grained feature representations. Specifically, the well-designed dual-branch encoder hammers at modeling non-local dependencies and multi-scale contexts, significantly improving the quality of semantic segmentation. Moreover, the proposed cross-attention and cross-scale module efficiently perform multi-scale information fusion, being capable of further exploring the long-range contextual information.
RESULTS: Extensive experiments conducted on three different types of medical image segmentation tasks demonstrate the state-of-the-art performance of our proposed method both visually and numerically.
CONCLUSIONS: This paper assembles the feature representation capabilities of CNN and transformer and proposes cross-attention and cross-scale fusion algorithms. The promising results show new possibilities of using cross-fusion mechanisms in more downstream medical image tasks.",,"This work was supported in part by the National Key RD Program of China under Grant 2018YFE0126100, the National Natural Science Foundation of China under Grant 62276232, Zhejiang Provincial Natural Science Foundation of China under Grant LY19F030016, and the Scientific Research Fund of the National Health Commission of China under Grant WKJ-ZJ-2102.",Computer Methods and Programs in Biomedicine,,"Algorithms; Electric Power Supplies; Health Status; Neural Networks, Computer; Semantics; Image Processing, Computer-Assisted",2022-12-12,2022,2022-12-12,2023-02,229,,107307,Closed,Article,"Zheng, Jianwei; Liu, Hao; Feng, Yuchao; Xu, Jinshan; Zhao, Liang","Zheng, Jianwei (College of Computer Science and Engineering, Zhejiang University of Technology, Hangzhou 310014, China. Electronic address: zjw@zjut.edu.cn.); Liu, Hao (College of Computer Science and Engineering, Zhejiang University of Technology, Hangzhou 310014, China.); Feng, Yuchao (College of Computer Science and Engineering, Zhejiang University of Technology, Hangzhou 310014, China.); Xu, Jinshan (College of Computer Science and Engineering, Zhejiang University of Technology, Hangzhou 310014, China.); Zhao, Liang (Stomatological Hospital of Xiamen Medical College and the Xiamen Key Laboratory of Stomatological Disease Diagnosis and Treatment, Xiamen 361000, China. Electronic address: Zhao8752@yeah.net.)","Zhao, Liang ","Zheng, Jianwei (Zhejiang University of Technology); Liu, Hao (Zhejiang University of Technology); Feng, Yuchao (Zhejiang University of Technology); Xu, Jinshan (Zhejiang University of Technology); Zhao, Liang ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1153565997,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
4810,pub.1148810551,10.1016/j.cmpb.2022.106918,35779461,,Automatic prostate and peri-prostatic fat segmentation based on pyramid mechanism fusion network for T2-weighted MRI,"BACKGROUND AND OBJECTIVE: Automatic and accurate segmentation of prostate and peri-prostatic fat in male pelvic MRI images is a critical step in the diagnosis and prognosis of prostate cancer. The boundary of prostate tissue is not clear, which makes the task of automatic segmentation very challenging. The main issues, especially for the peri-prostatic fat, which is being offered for the first time, are hazy boundaries and a large form variation.
METHODS: We propose a pyramid mechanism fusion network (PMF-Net) to learn global features and more comprehensive context information. In the proposed PMF-Net, we devised two pyramid techniques in particular. A pyramid mechanism module made of dilated convolutions of varying rates is inserted before each down sample of the fundamental network architecture encoder. The module is intended to address the issue of information loss during the feature coding process, particularly in the case of segmentation object boundary information. In the transition stage from encoder to decoder, pyramid fusion module is designed to extract global features. The features of the decoder not only integrate the features of the previous stage after up sampling and the output features of pyramid mechanism, but also include the features of skipping connection transmission under the same scale of the encoder.
RESULTS: The segmentation results of prostate and peri-prostatic fat on numerous diverse male pelvic MRI datasets show that our proposed PMF-Net has higher performance than existing methods. The average surface distance (ASD) and Dice similarity coefficient (DSC) of prostate segmentation results reached 10.06 and 90.21%, respectively. The ASD and DSC of the peri-prostatic fat segmentation results reached 50.96 and 82.41%.
CONCLUSIONS: The results of our segmentation are substantially connected and consistent with those of expert manual segmentation. Furthermore, peri-prostatic fat segmentation is a new issue, and good automatic segmentation has substantial therapeutic implications.","This work was supported in part by the Key RD Project of Hainan province (Grant #: ZDYF2021SHFZ243), in part by the Major Science and Technology Project of Haikou (Grant #: 2020-009), in part by the Innovative Research Project of Postgraduates in Hainan Province (Qhyb2021-09), in part by the Hainan Provincial Natural Science Foundation of China (Grant #: 2019CXTD400), in part by the National Key Research and Development Program of China (Grant #: 2018YFB1404400).",,Computer Methods and Programs in Biomedicine,,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Pelvis; Prostate; Prostatic Neoplasms",2022-06-20,2022,2022-06-20,2022-08,223,,106918,Closed,Article,"Li, Yuchun; Wu, Yuanyuan; Huang, Mengxing; Zhang, Yu; Bai, Zhiming","Li, Yuchun (State Key Laboratory of Marine Resource Utilization in South China Sea, College of Information Science and Technology, Hainan University, Haikou 570288, China.); Wu, Yuanyuan (State Key Laboratory of Marine Resource Utilization in South China Sea, College of Information Science and Technology, Hainan University, Haikou 570288, China.); Huang, Mengxing (State Key Laboratory of Marine Resource Utilization in South China Sea, College of Information Science and Technology, Hainan University, Haikou 570288, China. Electronic address: huangmx09@163.com.); Zhang, Yu (School of Computer science and Technology, Hainan University, Haikou 570288, China.); Bai, Zhiming (Haikou Municipal People's Hospital and Central South University Xiangya Medical College Affiliated Hospital, Haikou 570288, China.)","Huang, Mengxing (Hainan University)","Li, Yuchun (Hainan University); Wu, Yuanyuan (Hainan University); Huang, Mengxing (Hainan University); Zhang, Yu (Hainan University); Bai, Zhiming ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1148810551,46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
4792,pub.1152613714,10.1109/tmi.2022.3220750,36350867,,A New Framework of Swarm Learning Consolidating Knowledge from Multi-Center Non-IID Data for Medical Image Segmentation,"Large training datasets are important for deep learning-based methods. For medical image segmentation, it could be however difficult to obtain large number of labeled training images solely from one center. Distributed learning, such as swarm learning, has the potential to use multi-center data without breaching data privacy. However, data distributions across centers can vary a lot due to the diverse imaging protocols and vendors (known as feature skew). Also, the regions of interest to be segmented could be different, leading to inhomogeneous label distributions (referred to as label skew). With such non-independently and identically distributed (Non-IID) data, the distributed learning could result in degraded models. In this work, we propose a novel swarm learning approach, which assembles local knowledge from each center while at the same time overcomes forgetting of global knowledge during local training. Specifically, the approach first leverages a label skew-awared loss to preserve the global label knowledge, and then aligns local feature distributions to consolidate global knowledge against local feature skew. We validated our method in three Non-IID scenarios using four public datasets, including the Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation (M&Ms) dataset, the Federated Tumor Segmentation (FeTS) dataset, the Multi-Modality Whole Heart Segmentation (MMWHS) dataset and the Multi-Site Prostate T2-weighted MRI segmentation (MSProsMRI) dataset. Results show that our method could achieve superior performance over existing methods. Code will be released via https://zmiclab.github.io/projects.html once the paper gets accepted.",,,IEEE Transactions on Medical Imaging,,,2022-11-09,2022,2022-11-09,2022-11-09,PP,99,1-1,Closed,Article,"Gao, Zheyao; Wu, Fuping; Gao, Weiguo; Zhuang, Xiahai","Gao, Zheyao (School of Data Science, Fudan University, Shanghai, China); Wu, Fuping (School of Data Science, Fudan University, Shanghai, China); Gao, Weiguo (School of Data Science, Fudan University, Shanghai, China); Zhuang, Xiahai (School of Data Science, Fudan University, Shanghai, China)",,"Gao, Zheyao (Fudan University); Wu, Fuping (Fudan University); Gao, Weiguo (Fudan University); Zhuang, Xiahai (Fudan University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152613714,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,,
4775,pub.1139216171,10.1016/j.media.2021.102153,34246848,,Computer-aided diagnosis of prostate cancer using multiparametric MRI and clinical features: A patient-level classification framework,"Computer-aided diagnosis (CAD) of prostate cancer (PCa) using multiparametric magnetic resonance imaging (mpMRI) is actively being investigated as a means to provide clinical decision support to radiologists. Typically, these systems are trained using lesion annotations. However, lesion annotations are expensive to obtain and inadequate for characterizing certain tumor types e.g. diffuse tumors and MRI invisible tumors. In this work, we introduce a novel patient-level classification framework, denoted PCF, that is trained using patient-level labels only. In PCF, features are extracted from three-dimensional mpMRI and derived parameter maps using convolutional neural networks and subsequently, combined with clinical features by a multi-classifier support vector machine scheme. The output of PCF is a probability value that indicates whether a patient is harboring clinically significant PCa (Gleason score ≥3+4) or not. PCF achieved mean area under the receiver operating characteristic curves of 0.79 and 0.86 on the PICTURE and PROSTATEx datasets respectively, using five-fold cross-validation. Clinical evaluation over a temporally separated PICTURE dataset cohort demonstrated comparable sensitivity and specificity to an experienced radiologist. We envision PCF finding most utility as a second reader during routine diagnosis or as a triage tool to identify low-risk patients who do not require a clinical read.","PM’s research is supported by the UCL EPSRC Centre for Doctoral Training in Intelligent, Integrated Imaging In Healthcare (i4health) (EP/L016478/1). MA’s research is supported by the Wellcome/EPSRC Centre for Medical Engineering King’s College London and by the London Medical Imaging and AI Centre for Value-Based Healthcare. HUA’s research is supported by core funding from the UK’s National Institute of Health Research (NIHR) Imperial Biomedical Research Centre. HUA currently also receives funding from the Wellcome Trust, Medical Research Council (UK), Cancer Research UK, Prostate Cancer UK, The Urology Foundation, BMA Foundation, Imperial Health Charity, Sonacare Inc., Trod Medical, and Sophiris Biocorp for trials in prostate cancer. ME and SP receive research support from the University College London/University College London Hospital (UCL/UCLH) Biomedical Research Centre",,Medical Image Analysis,,"Computers; Diagnosis, Computer-Assisted; Humans; Magnetic Resonance Imaging; Male; Multiparametric Magnetic Resonance Imaging; Neoplasm Grading; Prostatic Neoplasms",2021-06-29,2021,2021-06-29,2021-10,73,,102153,All OA, Green,Article,"Mehta, Pritesh; Antonelli, Michela; Ahmed, Hashim U; Emberton, Mark; Punwani, Shonit; Ourselin, Sébastien","Mehta, Pritesh (Department of Medical Physics and Biomedical Engineering, University College London, UK. Electronic address: pritesh.mehta.17@ucl.ac.uk.); Antonelli, Michela (Biomedical Engineering & Imaging Sciences School, King's College London, UK.); Ahmed, Hashim U (Imperial Prostate, Department of Surgery and Cancer, Faculty of Medicine, Imperial College London, UK.); Emberton, Mark (Division of Surgery and Interventional Science, University College London, UK.); Punwani, Shonit (Centre for Medical Imaging, University College London, UK.); Ourselin, Sébastien (Biomedical Engineering & Imaging Sciences School, King's College London, UK.)","Mehta, Pritesh (University College London)","Mehta, Pritesh (University College London); Antonelli, Michela (King's College London); Ahmed, Hashim U (Imperial College London); Emberton, Mark (University College London); Punwani, Shonit (University College London); Ourselin, Sébastien (King's College London)",9,9,1.49,6.51,https://discovery.ucl.ac.uk/10133892/8/Mehta_1-s2.0-S1361841521001997-main.pdf,https://app.dimensions.ai/details/publication/pub.1139216171,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4770,pub.1133288818,10.1038/s41592-020-01008-z,33288961,,nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation,"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.","This work was co-funded by the National Center for Tumor Diseases (NCT) in Heidelberg and the Helmholtz Imaging Platform (HIP). We thank our colleagues at DKFZ who were involved in the various challenge contributions, especially A. Klein, D. Zimmerer, J. Wasserthal, G. Koehler, T. Norajitra and S. Wirkert, who contributed to the Decathlon submission. We also thank the MITK team, which supported us in producing all medical dataset visualizations. We are also thankful to all the challenge organizers, who provided an important basis for our work. We want to especially mention N. Heller, who enabled the collection of all the details from the KiTS challenge through excellent challenge design, E. Kavur from the CHAOS team, who generated comprehensive leaderboard information for us, C. Petitjean, who provided detailed leaderboard information of the SegTHOR entries from ISBI 2019 and M. Maška, who patiently supported us during our Cell Tracking Challenge submission. We thank M. Wiesenfarth for his helpful advice concerning the ranking of methods and the visualization of rankings. We further thank C. Pape and T. Wollman for their crucial introductions to the CREMI and Cell Tracking Challenges, respectively. Last but not least, we thank O. Ronneberger and L. Maier-Hein for their important feedback on this manuscript.",,Nature Methods,,"Algorithms; Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer",2020-12-07,2020,2020-12-07,2021-02,18,2,203-211,All OA, Green,Article,"Isensee, Fabian; Jaeger, Paul F.; Kohl, Simon A. A.; Petersen, Jens; Maier-Hein, Klaus H.","Isensee, Fabian (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Faculty of Biosciences, University of Heidelberg, Heidelberg, Germany); Jaeger, Paul F. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany); Kohl, Simon A. A. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; DeepMind, London, UK); Petersen, Jens (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Faculty of Physics & Astronomy, University of Heidelberg, Heidelberg, Germany); Maier-Hein, Klaus H. (Division of Medical Image Computing, German Cancer Research Center, Heidelberg, Germany; Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany)","Maier-Hein, Klaus H. (German Cancer Research Center; University Hospital Heidelberg)","Isensee, Fabian (German Cancer Research Center; Heidelberg University); Jaeger, Paul F. (German Cancer Research Center); Kohl, Simon A. A. (German Cancer Research Center; DeepMind (United Kingdom)); Petersen, Jens (German Cancer Research Center; Heidelberg University); Maier-Hein, Klaus H. (German Cancer Research Center; University Hospital Heidelberg)",1224,1222,142.07,,http://arxiv.org/pdf/1904.08128,https://app.dimensions.ai/details/publication/pub.1133288818,31 Biological Sciences,,,,,,,,,,,
4728,pub.1059031673,10.1088/0031-9155/61/22/8070,27779139,,Fast automated segmentation of multiple objects via spatially weighted shape learning,"Active shape models (ASMs) have proved successful in automatic segmentation by using shape and appearance priors in a number of areas such as prostate segmentation, where accurate contouring is important in treatment planning for prostate cancer. The ASM approach however, is heavily reliant on a good initialisation for achieving high segmentation quality. This initialisation often requires algorithms with high computational complexity, such as three dimensional (3D) image registration. In this work, we present a fast, self-initialised ASM approach that simultaneously fits multiple objects hierarchically controlled by spatially weighted shape learning. Prominent objects are targeted initially and spatial weights are progressively adjusted so that the next (more difficult, less visible) object is simultaneously initialised using a series of weighted shape models. The scheme was validated and compared to a multi-atlas approach on 3D magnetic resonance (MR) images of 38 cancer patients and had the same (mean, median, inter-rater) Dice's similarity coefficients of (0.79, 0.81, 0.85), while having no registration error and a computational time of 12-15 min, nearly an order of magnitude faster than the multi-atlas approach.","This work was supported by Cancer Council New South Wales research grant RG11-05, the Prostate Cancer Foundation of Australia (Movember Young Investigator Grant YI2011) and Cure Cancer Australia.",,Physics in Medicine and Biology,,"Aged; Algorithms; Automation; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Middle Aged; Pelvis; Prostatic Neoplasms; Urinary Bladder",2016-10-25,2016,2016-10-25,2016-11-21,61,22,8070-8084,All OA, Green,Article,"Chandra, Shekhar S; Dowling, Jason A; Greer, Peter B; Martin, Jarad; Wratten, Chris; Pichler, Peter; Fripp, Jurgen; Crozier, Stuart","Chandra, Shekhar S (School of Information Technology and Electrical Engineering, The University of Queensland, Australia); Dowling, Jason A (Australian e-Health Research Centre, CSIRO, Australia); Greer, Peter B (Calvary Mater Newcastle Hospital, Australia; University of Newcastle, Australia); Martin, Jarad (Calvary Mater Newcastle Hospital, Australia; University of Newcastle, Australia); Wratten, Chris (Calvary Mater Newcastle Hospital, Australia; University of Newcastle, Australia); Pichler, Peter (University of Newcastle, Australia); Fripp, Jurgen (Australian e-Health Research Centre, CSIRO, Australia); Crozier, Stuart (School of Information Technology and Electrical Engineering, The University of Queensland, Australia)",,"Chandra, Shekhar S (University of Queensland); Dowling, Jason A (Australian e-Health Research Centre); Greer, Peter B (Calvary Mater Newcastle Hospital; University of Newcastle Australia); Martin, Jarad (Calvary Mater Newcastle Hospital; University of Newcastle Australia); Wratten, Chris (Calvary Mater Newcastle Hospital; University of Newcastle Australia); Pichler, Peter (University of Newcastle Australia); Fripp, Jurgen (Australian e-Health Research Centre); Crozier, Stuart (University of Queensland)",12,2,0.61,9.39,https://ogma.newcastle.edu.au:443/vital/access/services/Download/uon:29622/ATTACHMENT02,https://app.dimensions.ai/details/publication/pub.1059031673,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
4715,pub.1083400894,10.1038/srep41261,28145532,PMC5286513,Computational imaging reveals shape differences between normal and malignant prostates on MRI,"We seek to characterize differences in the shape of the prostate and the central gland (combined central and transitional zones) between men with biopsy confirmed prostate cancer and men who were identified as not having prostate cancer either on account of a negative biopsy or had pelvic imaging done for a non-prostate malignancy. T2w MRI from 70 men were acquired at three institutions. The cancer positive group (PCa+) comprised 35 biopsy positive (Bx+) subjects from three institutions (Gleason scores: 6–9, Stage: T1–T3). The negative group (PCa−) combined 24 biopsy negative (Bx−) from two institutions and 11 subjects diagnosed with rectal cancer but with no clinical or MRI indications of prostate cancer (Cl−). The boundaries of the prostate and central gland were delineated on T2w MRI by two expert raters and were used to construct statistical shape atlases for the PCa+, Bx− and Cl− prostates. An atlas comparison was performed via per-voxel statistical tests to localize shape differences (significance assessed at p < 0.05). The atlas comparison revealed central gland hypertrophy in the Bx− subpopulation, resulting in significant volume and posterior side shape differences relative to PCa+ group. Significant differences in the corresponding prostate shapes were noted at the apex when comparing the Cl− and PCa+ prostates.","Research reported in this publication was supported by the Department of Defense (W81XWH-13-1-0487), National Institutes of Health (R21CA167811-01, R21CA179327-01, R21CA195152-01,R01CA202752-01, U24CA199374-01), the National Institute of Diabetes and Digestive and Kidney Diseases (R01DK098503-02), the DOD Prostate Cancer Synergistic Idea Development Award (PC120857); the DOD Prostate Cancer Idea Development Award; the Ohio Third Frontier Technology development Grant, the Case Comprehensive Cancer Center Pilot Grant, the VelaSano Grant from the Cleveland Clinic, the Wallace H. Coulter Foundation Program in the Department of Biomedical Engineering at Case Western Reserve University, Australian Prostate Cancer Research Centre, St Vincent’s Prostate Cancer Centre. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.",,Scientific Reports,,"Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Organ Size; Prostate; Prostatic Neoplasms",2017-02-01,2017,2017-02-01,,7,1,41261,All OA, Gold,Article,"Rusu, Mirabela; Purysko, Andrei S.; Verma, Sadhna; Kiechle, Jonathan; Gollamudi, Jay; Ghose, Soumya; Herrmann, Karin; Gulani, Vikas; Paspulati, Raj; Ponsky, Lee; Böhm, Maret; Haynes, Anne-Maree; Moses, Daniel; Shnier, Ron; Delprado, Warick; Thompson, James; Stricker, Phillip; Madabhushi, Anant","Rusu, Mirabela (Department of Biomedical Engineering, Case Western Reserve University, 10900 Euclid Avenue, 44106, Cleveland, Ohio, USA; Present Address: GE Global Research Center, 1 Research Circle, Niskayuna, 12309, New York, USA.); Purysko, Andrei S. (Section of Abdominal Imaging and Nuclear Radiology Department, Imaging Institute, Cleveland Clinic, 9500 Euclid Avenue, 44195, Cleveland, OH, USA); Verma, Sadhna (University of Cincinnati Medical Center, 234 Goodman Street, 45267, Cincinnati, Ohio, USA); Kiechle, Jonathan (University Hospitals Cleveland Medical Center and Case Western Reserve University, 11100 Euclid Avenue, 44106, Cleveland, Ohio, USA); Gollamudi, Jay (University of Cincinnati Medical Center, 234 Goodman Street, 45267, Cincinnati, Ohio, USA; Present Address: Department of Radiology, University Hospitals Cleveland Medical Center and Case Western Reserve University, 11100 Euclid Avenue, Cleveland, Ohio, 44106, USA.); Ghose, Soumya (Department of Biomedical Engineering, Case Western Reserve University, 10900 Euclid Avenue, 44106, Cleveland, Ohio, USA); Herrmann, Karin (University Hospitals Cleveland Medical Center and Case Western Reserve University, 11100 Euclid Avenue, 44106, Cleveland, Ohio, USA); Gulani, Vikas (University Hospitals Cleveland Medical Center and Case Western Reserve University, 11100 Euclid Avenue, 44106, Cleveland, Ohio, USA); Paspulati, Raj (University Hospitals Cleveland Medical Center and Case Western Reserve University, 11100 Euclid Avenue, 44106, Cleveland, Ohio, USA); Ponsky, Lee (University Hospitals Cleveland Medical Center and Case Western Reserve University, 11100 Euclid Avenue, 44106, Cleveland, Ohio, USA); Böhm, Maret (Garvan Institute of Medical Research, The Kinghorn Cancer Centre, 384 Victoria St, 2010, Darlinghurst, New South Wales, Australia); Haynes, Anne-Maree (Garvan Institute of Medical Research, The Kinghorn Cancer Centre, 384 Victoria St, 2010, Darlinghurst, New South Wales, Australia); Moses, Daniel (Radiology Department, Spectrum Medical Imaging, 13-15 Silver St, 2031, Randwick, New South Wales, Australia); Shnier, Ron (Southern Radiology, Neuroscience Research Australia, Barker St & Easy St, 2031, Randwick, New South Wales, Australia); Delprado, Warick (Douglass Hanly Moir Pathology, 14 Giffnock Ave, 2113, Macquarie Park, New South Wales, Australia); Thompson, James (Garvan Institute of Medical Research, The Kinghorn Cancer Centre, 384 Victoria St, 2010, Darlinghurst, New South Wales, Australia; St. Vincent’s Prostate Cancer Center, 390 Victoria St, 2010, Darlinghurst, New South Wales, Australia; School of Medicine, University of New South Wales, 18 High St, 2052, Kensington, New South Wales, Australia); Stricker, Phillip (St. Vincent’s Prostate Cancer Center, 390 Victoria St, 2010, Darlinghurst, New South Wales, Australia); Madabhushi, Anant (Department of Biomedical Engineering, Case Western Reserve University, 10900 Euclid Avenue, 44106, Cleveland, Ohio, USA)","Rusu, Mirabela (Case Western Reserve University; ); Madabhushi, Anant (Case Western Reserve University)","Rusu, Mirabela (Case Western Reserve University); Purysko, Andrei S. (Cleveland Clinic); Verma, Sadhna (University of Cincinnati Medical Center); Kiechle, Jonathan (University Hospitals Cleveland Medical Center); Gollamudi, Jay (University of Cincinnati Medical Center; University Hospitals Cleveland Medical Center; Case Western Reserve University); Ghose, Soumya (Case Western Reserve University); Herrmann, Karin (University Hospitals Cleveland Medical Center); Gulani, Vikas (University Hospitals Cleveland Medical Center); Paspulati, Raj (University Hospitals Cleveland Medical Center); Ponsky, Lee (University Hospitals Cleveland Medical Center); Böhm, Maret (Garvan Institute of Medical Research); Haynes, Anne-Maree (Garvan Institute of Medical Research); Moses, Daniel (); Shnier, Ron (Neuroscience Research Australia); Delprado, Warick (Douglass Hanly Moir Pathology); Thompson, James (Garvan Institute of Medical Research; UNSW Sydney); Stricker, Phillip (); Madabhushi, Anant (Case Western Reserve University)",10,3,0.55,2.51,https://www.nature.com/articles/srep41261.pdf,https://app.dimensions.ai/details/publication/pub.1083400894,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4681,pub.1129788837,10.1007/s10334-020-00871-3,32737628,PMC8018925,Automated reference tissue normalization of T2-weighted MR images of the prostate using object recognition,"ObjectivesTo develop and evaluate an automated method for prostate T2-weighted (T2W) image normalization using dual-reference (fat and muscle) tissue.Materials and methodsTransverse T2W images from the publicly available PROMISE12 (N = 80) and PROSTATEx (N = 202) challenge datasets, and an in-house collected dataset (N = 60) were used. Aggregate channel features object detectors were trained to detect reference fat and muscle tissue regions, which were processed and utilized to normalize the 3D images by linear scaling. Mean prostate pseudo T2 values after normalization were compared to literature values. Inter-patient histogram intersections of voxel intensities in the prostate were compared between our approach, the original images, and other commonly used normalization methods. Healthy vs. malignant tissue classification performance was compared before and after normalization.ResultsThe prostate pseudo T2 values of the three tested datasets (mean ± standard deviation = 78.49 ± 9.42, 79.69 ± 6.34 and 79.29 ± 6.30 ms) corresponded well to T2 values from literature (80 ± 34 ms). Our normalization approach resulted in significantly higher (p < 0.001) inter-patient histogram intersections (median = 0.746) than the original images (median = 0.417) and most other normalization methods. Healthy vs. malignant classification also improved significantly (p < 0.001) in peripheral (AUC 0.826 vs. 0.769) and transition (AUC 0.743 vs. 0.678) zones.ConclusionAn automated dual-reference tissue normalization of T2W images could help improve the quantitative assessment of prostate cancer.","Open Access funding provided by NTNU Norwegian University of Science and Technology (incl St. Olavs Hospital - Trondheim University Hospital). We would like to thank Radboud University Nijmegen in addition to the organizers of the PROSTATEx and PROMISE12 challenges for making their datasets available. We would specifically like to thank Prof. Radka Stoyanova and her team at Miller School of Medicine (Miami, FL, USA) for providing us with the prostate delineations for the PROSTATEx dataset. We would like to thank Dr. Elise Sandsmark from St. Olavs Hospital, Trondheim University Hospital (Trondheim, Norway), for providing us with prostate delineations for the in-house collected dataset.","The Research Council of Norway (Grant Number 295013), Norwegian University of Science and Technology Biotechnology (Grant Number 81770928), and the liaison Committee between the Central Norway Regional Health Authority and the Norwegian University of Science and Technology (Grant Numbers 90368401 and 90265300).","Magnetic Resonance Materials in Physics, Biology and Medicine",,Humans, Magnetic Resonance Imaging, Male, Prostate, Prostatic Neoplasms,2020-07-31,2020,2020-07-31,2021-04,34,2,309-321,All OA, Hybrid,Article,"Sunoqrot, Mohammed R. S.; Nketiah, Gabriel A.; Selnæs, Kirsten M.; Bathen, Tone F.; Elschot, Mattijs","Sunoqrot, Mohammed R. S. (Department of Circulation and Medical Imaging, NTNU, Norwegian University of Science and Technology, 7030, Trondheim, Norway); Nketiah, Gabriel A. (Department of Circulation and Medical Imaging, NTNU, Norwegian University of Science and Technology, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030, Trondheim, Norway); Selnæs, Kirsten M. (Department of Circulation and Medical Imaging, NTNU, Norwegian University of Science and Technology, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030, Trondheim, Norway); Bathen, Tone F. (Department of Circulation and Medical Imaging, NTNU, Norwegian University of Science and Technology, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030, Trondheim, Norway); Elschot, Mattijs (Department of Circulation and Medical Imaging, NTNU, Norwegian University of Science and Technology, 7030, Trondheim, Norway; Department of Radiology and Nuclear Medicine, St. Olavs Hospital, Trondheim University Hospital, 7030, Trondheim, Norway)","Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology)","Sunoqrot, Mohammed R. S. (Norwegian University of Science and Technology); Nketiah, Gabriel A. (Norwegian University of Science and Technology; St Olav's University Hospital); Selnæs, Kirsten M. (Norwegian University of Science and Technology; St Olav's University Hospital); Bathen, Tone F. (Norwegian University of Science and Technology; St Olav's University Hospital); Elschot, Mattijs (Norwegian University of Science and Technology; St Olav's University Hospital)",11,10,2.01,5.39,https://link.springer.com/content/pdf/10.1007/s10334-020-00871-3.pdf,https://app.dimensions.ai/details/publication/pub.1129788837,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,
4681,pub.1092070808,10.1088/1361-6560/aa9104,28976369,,Regression and statistical shape model based substitute CT generation for MRI alone external beam radiation therapy from standard clinical MRI sequences,"In MR only radiation therapy planning, generation of the tissue specific HU map directly from the MRI would eliminate the need of CT image acquisition and may improve radiation therapy planning. The aim of this work is to generate and validate substitute CT (sCT) scans generated from standard T2 weighted MR pelvic scans in prostate radiation therapy dose planning. A Siemens Skyra 3T MRI scanner with laser bridge, flat couch and pelvic coil mounts was used to scan 39 patients scheduled for external beam radiation therapy for localized prostate cancer. For sCT generation a whole pelvis MRI (1.6 mm 3D isotropic T2w SPACE sequence) was acquired. Patients received a routine planning CT scan. Co-registered whole pelvis CT and T2w MRI pairs were used as training images. Advanced tissue specific non-linear regression models to predict HU for the fat, muscle, bladder and air were created from co-registered CT-MRI image pairs. On a test case T2w MRI, the bones and bladder were automatically segmented using a novel statistical shape and appearance model, while other soft tissues were separated using an Expectation-Maximization based clustering model. The CT bone in the training database that was most 'similar' to the segmented bone was then transformed with deformable registration to create the sCT component of the test case T2w MRI bone tissue. Predictions for the bone, air and soft tissue from the separate regression models were successively combined to generate a whole pelvis sCT. The change in monitor units between the sCT-based plans relative to the gold standard CT plan for the same IMRT dose plan was found to be [Formula: see text] (mean  ±  standard deviation) for 39 patients. The 3D Gamma pass rate was [Formula: see text] (2 mm/2%). The novel hybrid model is computationally efficient, generating an sCT in 20 min from standard T2w images for prostate cancer radiation therapy dose planning and DRR generation.",,,Physics in Medicine and Biology,,"Aged; Bone and Bones; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Models, Statistical; Multimodal Imaging; Organs at Risk; Pelvis; Prostatic Neoplasms; Radiotherapy Planning, Computer-Assisted; Radiotherapy, Image-Guided; Radiotherapy, Intensity-Modulated; Tomography, X-Ray Computed; Urinary Bladder",2017-10-27,2017,2017-10-27,2017-11-21,62,22,8566-8580,Closed,Article,"Ghose, Soumya; Greer, Peter B; Sun, Jidi; Pichler, Peter; Rivest-Henault, David; Mitra, Jhimli; Richardson, Haylea; Wratten, Chris; Martin, Jarad; Arm, Jameen; Best, Leah; Dowling, Jason A","Ghose, Soumya (Department of Biomedical Engineering, Case Western Reserve University, Cleveland, OH, United States of America); Greer, Peter B (University of Newcastle, Callaghan, New South Wales, Australia and Calvary Mater Newcastle Hospital, Waratah, New South Wales, Australia); Sun, Jidi (University of Newcastle, Callaghan, New South Wales, Australia); Pichler, Peter (Calvary Mater Newcastle Hospital, Waratah, New South Wales, Australia); Rivest-Henault, David (Commonwealth Scientific and Industrial Research Organization, Health and Biosecurity, The Australian e-Health Research Centre, Herston, QLD, Australia); Mitra, Jhimli (Department of Biomedical Engineering, Case Western Reserve University, Cleveland, OH, United States of America); Richardson, Haylea (Calvary Mater Newcastle Hospital, Waratah, New South Wales, Australia); Wratten, Chris (University of Newcastle, Callaghan, New South Wales, Australia and Calvary Mater Newcastle Hospital, Waratah, New South Wales, Australia); Martin, Jarad (University of Newcastle, Callaghan, New South Wales, Australia and Calvary Mater Newcastle Hospital, Waratah, New South Wales, Australia); Arm, Jameen (Calvary Mater Newcastle Hospital, Waratah, New South Wales, Australia); Best, Leah (Department of Radiology, Hunter New England Health, New Lambton, New South Wales, Australia); Dowling, Jason A (Commonwealth Scientific and Industrial Research Organization, Health and Biosecurity, The Australian e-Health Research Centre, Herston, QLD, Australia)",,"Ghose, Soumya (Case Western Reserve University); Greer, Peter B (University of Newcastle Australia; Calvary Mater Newcastle Hospital); Sun, Jidi (University of Newcastle Australia); Pichler, Peter (Calvary Mater Newcastle Hospital); Rivest-Henault, David (Australian e-Health Research Centre); Mitra, Jhimli (Case Western Reserve University); Richardson, Haylea (Calvary Mater Newcastle Hospital); Wratten, Chris (University of Newcastle Australia; Calvary Mater Newcastle Hospital); Martin, Jarad (University of Newcastle Australia; Calvary Mater Newcastle Hospital); Arm, Jameen (Calvary Mater Newcastle Hospital); Best, Leah (Hunter New England Local Health District); Dowling, Jason A (Australian e-Health Research Centre)",8,0,0.51,3.74,,https://app.dimensions.ai/details/publication/pub.1092070808,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
4678,pub.1092263827,10.1117/1.jmi.4.4.041307,29057288,PMC5644511,Fully automated segmentation of prostate whole gland and transition zone in diffusion-weighted MRI using convolutional neural networks,"Prostate cancer is a leading cause of cancer-related death among men. Multiparametric magnetic resonance imaging has become an essential part of the diagnostic evaluation of prostate cancer. The internationally accepted interpretation scheme (Pi-Rads v2) has different algorithms for scoring of the transition zone (TZ) and peripheral zone (PZ) of the prostate as tumors can appear different in these zones. Computer-aided detection tools have shown different performances in TZ and PZ and separating these zones for training and detection is essential. The TZ-PZ segmentation which requires the segmentation of prostate whole gland and TZ is typically done manually. We present a fully automatic algorithm for delineation of the prostate gland and TZ in diffusion-weighted imaging (DWI) via a stack of fully convolutional neural networks. The proposed algorithm first detects the slices that contain a portion of prostate gland within the three-dimensional DWI volume and then it segments the prostate gland and TZ automatically. The segmentation stage of the algorithm was applied to DWI images of 104 patients and median Dice similarity coefficients of 0.93 and 0.88 were achieved for the prostate gland and TZ, respectively. The detection of image slices with and without prostate gland had an average accuracy of 0.97.",,,Journal of Medical Imaging,,,2017-10-17,2017,2017-10-17,2017-10,4,4,041307-041307,All OA, Green,Article,"Clark, Tyler; Zhang, Junjie; Baig, Sameer; Wong, Alexander; Haider, Masoom A.; Khalvati, Farzad","Clark, Tyler (University of Toronto, Department of Medical Imaging, Sunnybrook Research Institute, Toronto, Canada); Zhang, Junjie (University of Toronto, Department of Medical Imaging, Sunnybrook Research Institute, Toronto, Canada); Baig, Sameer (University of Toronto, Department of Medical Imaging, Sunnybrook Research Institute, Toronto, Canada); Wong, Alexander (University of Waterloo, Department of Systems Design Engineering, Waterloo, Canada); Haider, Masoom A. (University of Toronto, Department of Medical Imaging, Sunnybrook Research Institute, Toronto, Canada); Khalvati, Farzad (University of Toronto, Department of Medical Imaging, Sunnybrook Research Institute, Toronto, Canada)","Khalvati, Farzad (University of Toronto)","Clark, Tyler (University of Toronto); Zhang, Junjie (University of Toronto); Baig, Sameer (University of Toronto); Wong, Alexander (University of Waterloo); Haider, Masoom A. (University of Toronto); Khalvati, Farzad (University of Toronto)",56,24,2.1,14.08,https://europepmc.org/articles/pmc5644511?pdf=render,https://app.dimensions.ai/details/publication/pub.1092263827,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4580,pub.1113713194,10.1109/tmi.2019.2913184,31021793,,Deep Attentive Features for Prostate Segmentation in 3D Transrectal Ultrasound,"Automatic prostate segmentation in transrectal ultrasound (TRUS) images is of essential importance for image-guided prostate interventions and treatment planning. However, developing such automatic solutions remains very challenging due to the missing/ambiguous boundary and inhomogeneous intensity distribution of the prostate in TRUS, as well as the large variability in prostate shapes. This paper develops a novel 3D deep neural network equipped with attention modules for better prostate segmentation in TRUS by fully exploiting the complementary information encoded in different layers of the convolutional neural network (CNN). Our attention module utilizes the attention mechanism to selectively leverage the multi-level features integrated from different layers to refine the features at each individual layer, suppressing the non-prostate noise at shallow layers of the CNN and increasing more prostate details into features at deep layers. Experimental results on challenging 3D TRUS volumes show that our method attains satisfactory segmentation performance. The proposed attention mechanism is a general strategy to aggregate multi-level deep features and has the potential to be used for other medical image segmentation tasks. The code is publicly available at https://github.com/wulalago/DAF3D.","This work was supported in part by the National Natural Science Foundation of China under Grant 61701312 and Grant 61571304, the Natural Science Foundation of Shenzhen University under Grant 2018010, the Shenzhen Peacock Plan under Grant KQTD2016053112051497, and the grant from the Research Grants Council of Hong Kong Special Administrative Region under Grant 14225616. The authors would like to thank the Associate Editor and the anonymous reviewers for their constructive comments.",,IEEE Transactions on Medical Imaging,,"Deep Learning; Humans; Imaging, Three-Dimensional; Male; Neural Networks, Computer; Prostate; Prostatic Neoplasms; Ultrasonography",2019-04-25,2019,2019-04-25,2019-12,38,12,2768-2778,All OA, Green,Article,"Wang, Yi; Dou, Haoran; Hu, Xiaowei; Zhu, Lei; Yang, Xin; Xu, Ming; Qin, Jing; Heng, Pheng-Ann; Wang, Tianfu; Ni, Dong","Wang, Yi (National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Health Science Center, School of Biomedical Engineering, Shenzhen University, Shenzhen, 518060, China; Medical UltraSound Image Computing Lab, Shenzhen, 518060, China); Dou, Haoran (National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Health Science Center, School of Biomedical Engineering, Shenzhen University, Shenzhen, 518060, China; Medical UltraSound Image Computing Lab, Shenzhen, 518060, China); Hu, Xiaowei (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Zhu, Lei (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong); Yang, Xin (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Xu, Ming (Department of Medical Ultrasonics, First Affiliated Hospital, Institute of Diagnostic and Interventional Ultrasound, Sun Yat-sen University, Guangzhou, 510275, China); Qin, Jing (Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong); Heng, Pheng-Ann (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Wang, Tianfu (National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Health Science Center, School of Biomedical Engineering, Shenzhen University, Shenzhen, 518060, China; Medical UltraSound Image Computing Lab, Shenzhen, 518060, China); Ni, Dong (National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Health Science Center, School of Biomedical Engineering, Shenzhen University, Shenzhen, 518060, China; Medical UltraSound Image Computing Lab, Shenzhen, 518060, China)","Wang, Yi (; )","Wang, Yi (); Dou, Haoran (); Hu, Xiaowei (Chinese University of Hong Kong); Zhu, Lei (Chinese University of Hong Kong; Hong Kong Polytechnic University); Yang, Xin (Chinese University of Hong Kong); Xu, Ming (Sun Yat-sen University); Qin, Jing (Hong Kong Polytechnic University); Heng, Pheng-Ann (Chinese University of Hong Kong); Wang, Tianfu (); Ni, Dong ()",95,75,6.11,,http://arxiv.org/pdf/1907.01743,https://app.dimensions.ai/details/publication/pub.1113713194,46 Information and Computing Sciences,,,,,,,,,,,
4574,pub.1134961628,10.1109/tmi.2021.3055428,33507867,PMC8246057,Multi-Site Infant Brain Segmentation Algorithms: The iSeg-2019 Challenge,"To better understand early brain development in health and disorder, it is critical to accurately segment infant brain magnetic resonance (MR) images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Deep learning-based methods have achieved state-of-the-art performance; h owever, one of the major limitations is that the learning-based methods may suffer from the multi-site issue, that is, the models trained on a dataset from one site may not be applicable to the datasets acquired from other sites with different imaging protocols/scanners. To promote methodological development in the community, the iSeg-2019 challenge (http://iseg2019.web.unc.edu) provides a set of 6-month infant subjects from multiple sites with different protocols/scanners for the participating methods. T raining/validation subjects are from UNC (MAP) and testing subjects are from UNC/UMN (BCP), Stanford University, and Emory University. By the time of writing, there are 30 automatic segmentation methods participated in the iSeg-2019. In this article, 8 top-ranked methods were reviewed by detailing their pipelines/implementations, presenting experimental results, and evaluating performance across different sites in terms of whole brain, regions of interest, and gyral landmark curves. We further pointed out their limitations and possible directions for addressing the multi-site issue. We find that multi-site consistency is still an open issue. We hope that the multi-site dataset in the iSeg-2019 and this review article will attract more researchers to address the challenging and critical multi-site issue in practice.","The work of Yue Sun, Kun Gao, and Li Wang were supported by the National Institutes of Health (NIH) under Grant MH109773 and Grant MH117943. The work of Zhengwang Wu, Toan Duc Bui, and Gang Li were supported in part by NIH under Grant MH117943. The work of Ian H. Gotlib was supported by NIH under Grant R21HD090493 and Grant R21MH111978. The work of Sarah Shultz and Longchuan Li were supported by NIH under Grant K01MH108741, Grant P50MH10029, Grant R01EB027147, Grant R01MH119251, and Grant R01MH118534.",,IEEE Transactions on Medical Imaging,,Algorithms, Brain, Brain Mapping, Gray Matter, Humans, Infant, Magnetic Resonance Imaging,2021-04-30,2021,2021-04-30,2021-05,40,5,1363-1376,All OA, Green,Article,"Sun, Yue; Gao, Kun; Wu, Zhengwang; Li, Guannan; Zong, Xiaopeng; Lei, Zhihao; Wei, Ying; Ma, Jun; Yang, Xiaoping; Feng, Xue; Zhao, Li; Le Phan, Trung; Shin, Jitae; Zhong, Tao; Zhang, Yu; Yu, Lequan; Li, Caizi; Basnet, Ramesh; Ahmad, M. Omair; Swamy, M. N. S.; Ma, Wenao; Dou, Qi; Bui, Toan Duc; Noguera, Camilo Bermudez; Landman, Bennett; Gotlib, Ian H.; Humphreys, Kathryn L.; Shultz, Sarah; Li, Longchuan; Niu, Sijie; Lin, Weili; Jewells, Valerie; Shen, Dinggang; Li, Gang; Wang, Li","Sun, Yue (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Gao, Kun (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Wu, Zhengwang (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Li, Guannan (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Zong, Xiaopeng (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Lei, Zhihao (College of Information Science and Engineering, Northeastern University, Shenyang, 110819, China); Wei, Ying (College of Information Science and Engineering, Northeastern University, Shenyang, 110819, China); Ma, Jun (Department of Mathematics, Nanjing University of Science and Technology, Nanjing, 210044, China); Yang, Xiaoping (Department of Mathematics, Nanjing University, Nanjing, 210093, China); Feng, Xue (Department of Biomedical Engineering, University of Virginia, Charlottesville, VA, 22904, USA); Zhao, Li (Diagnostic Imaging and Radiology Department, Children’s National Medical Center, Washington, DC, 20310, USA); Le Phan, Trung (Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, 16419, South Korea); Shin, Jitae (Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, 16419, South Korea); Zhong, Tao (School of Biomedical Engineering, Southern Medical University, Guangzhou, 510515, China); Zhang, Yu (School of Biomedical Engineering, Southern Medical University, Guangzhou, 510515, China); Yu, Lequan (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Li, Caizi (Shenzhen Institutes of Advanced Technology, Chinese Academy of Science, Shenzhen, 518055, China); Basnet, Ramesh (Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, H3G 1M8, Canada); Ahmad, M. Omair (Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, H3G 1M8, Canada); Swamy, M. N. S. (Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, H3G 1M8, Canada); Ma, Wenao (School of Informatics, Xiamen University, Xiamen, 361005, China); Dou, Qi (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong); Bui, Toan Duc (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Noguera, Camilo Bermudez (Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, 37204, USA); Landman, Bennett (Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, 37204, USA); Gotlib, Ian H. (Department of Psychology, Stanford University, Stanford, CA, 94305, USA); Humphreys, Kathryn L. (Department of Psychology and Human Development, Vanderbilt University, Nashville, TN, 37204, USA); Shultz, Sarah (Department of Pediatrics, Emory University, Atlanta, GA, 30322, USA); Li, Longchuan (Department of Pediatrics, Emory University, Atlanta, GA, 30322, USA); Niu, Sijie (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Lin, Weili (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Jewells, Valerie (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Shen, Dinggang (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Li, Gang (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Wang, Li (Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA)","Niu, Sijie (University of North Carolina at Chapel Hill)","Sun, Yue (University of North Carolina at Chapel Hill); Gao, Kun (University of North Carolina at Chapel Hill); Wu, Zhengwang (University of North Carolina at Chapel Hill); Li, Guannan (University of North Carolina at Chapel Hill); Zong, Xiaopeng (University of North Carolina at Chapel Hill); Lei, Zhihao (Northeastern University); Wei, Ying (Northeastern University); Ma, Jun (Nanjing University of Science and Technology); Yang, Xiaoping (Nanjing University); Feng, Xue (University of Virginia); Zhao, Li (Children’s National Health System); Le Phan, Trung (Sungkyunkwan University); Shin, Jitae (Sungkyunkwan University); Zhong, Tao (Southern Medical University); Zhang, Yu (Southern Medical University); Yu, Lequan (Chinese University of Hong Kong); Li, Caizi (Shenzhen Institutes of Advanced Technology); Basnet, Ramesh (Concordia University); Ahmad, M. Omair (Concordia University); Swamy, M. N. S. (Concordia University); Ma, Wenao (Xiamen University); Dou, Qi (Chinese University of Hong Kong); Bui, Toan Duc (University of North Carolina at Chapel Hill); Noguera, Camilo Bermudez (Vanderbilt University); Landman, Bennett (Vanderbilt University); Gotlib, Ian H. (Stanford University); Humphreys, Kathryn L. (Vanderbilt University); Shultz, Sarah (Emory University); Li, Longchuan (Emory University); Niu, Sijie (University of North Carolina at Chapel Hill); Lin, Weili (University of North Carolina at Chapel Hill); Jewells, Valerie (University of North Carolina at Chapel Hill); Shen, Dinggang (University of North Carolina at Chapel Hill); Li, Gang (University of North Carolina at Chapel Hill); Wang, Li (University of North Carolina at Chapel Hill)",30,28,4.55,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8246057,https://app.dimensions.ai/details/publication/pub.1134961628,46 Information and Computing Sciences,,,,,
4572,pub.1143679309,10.3390/cancers13236138,34885246,PMC8656605,AutoProstate: Towards Automated Reporting of Prostate MRI for Prostate Cancer Assessment Using Deep Learning,"Multiparametric magnetic resonance imaging (mpMRI) of the prostate is used by radiologists to identify, score, and stage abnormalities that may correspond to clinically significant prostate cancer (CSPCa). Automatic assessment of prostate mpMRI using artificial intelligence algorithms may facilitate a reduction in missed cancers and unnecessary biopsies, an increase in inter-observer agreement between radiologists, and an improvement in reporting quality. In this work, we introduce AutoProstate, a deep learning-powered framework for automatic MRI-based prostate cancer assessment. AutoProstate comprises of three modules: Zone-Segmenter, CSPCa-Segmenter, and Report-Generator. Zone-Segmenter segments the prostatic zones on T2-weighted imaging, CSPCa-Segmenter detects and segments CSPCa lesions using biparametric MRI, and Report-Generator generates an automatic web-based report containing four sections: Patient Details, Prostate Size and PSA Density, Clinically Significant Lesion Candidates, and Findings Summary. In our experiment, AutoProstate was trained using the publicly available PROSTATEx dataset, and externally validated using the PICTURE dataset. Moreover, the performance of AutoProstate was compared to the performance of an experienced radiologist who prospectively read PICTURE dataset cases. In comparison to the radiologist, AutoProstate showed statistically significant improvements in prostate volume and prostate-specific antigen density estimation. Furthermore, AutoProstate matched the CSPCa lesion detection sensitivity of the radiologist, which is paramount, but produced more false positive detections.","P.M.’s research is supported by the Engineering and Physical Sciences Research Council (EPSRC) [EP/R512400/1]. P.M.’s work was additionally supported by the EPSRC-funded UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare (i4health) [EP/S021930/1]. M.A.’s research is supported by the Wellcome/EPSRC Centre for Medical Engineering King’s College London and by the London Medical Imaging and AI Centre for Value-Based Healthcare. H.U.A.’s research is supported by core funding from the UK’s National Institute of Health Research (NIHR) Imperial Biomedical Research Centre. HUA currently also receives funding from the Wellcome Trust, Medical Research Council (UK), Cancer Research UK, Prostate Cancer UK, The Urology Foundation, BMA Foundation, Imperial Health Charity, Sonacare Inc., Trod Medical and Sophiris Biocorp for trials in prostate cancer. M.E. and S.P. receive research support from the University College London/University College London Hospital (UCL/UCLH) Biomedical Research Centre.",,Cancers,,,2021-12-06,2021,2021-12-06,,13,23,6138,All OA, Gold,Article,"Mehta, Pritesh; Antonelli, Michela; Singh, Saurabh; Grondecka, Natalia; Johnston, Edward W.; Ahmed, Hashim U.; Emberton, Mark; Punwani, Shonit; Ourselin, Sébastien","Mehta, Pritesh (Department of Medical Physics and Biomedical Engineering, University College London, London WC1E 6BT, UK; School of Biomedical Engineering Imaging Sciences, King’s College London, London SE1 7EH, UK;, michela.antonelli@kcl.ac.uk, (M.A.);, sebastien.ourselin@kcl.ac.uk, (S.O.)); Antonelli, Michela (School of Biomedical Engineering Imaging Sciences, King’s College London, London SE1 7EH, UK;, michela.antonelli@kcl.ac.uk, (M.A.);, sebastien.ourselin@kcl.ac.uk, (S.O.)); Singh, Saurabh (Centre for Medical Imaging, University College London, London WC1E 6BT, UK;, saurabh.singh@ucl.ac.uk, (S.S.);, s.punwani@ucl.ac.uk, (S.P.)); Grondecka, Natalia (Department of Medical Radiology, Medical University of Lublin, 20-059 Lublin, Poland;, ngrondecka@wp.pl); Johnston, Edward W. (Interventional Radiology, Royal Marsden Hospital, London SW3 6JJ, UK;, edward.johnston@ucl.ac.uk); Ahmed, Hashim U. (Imperial Prostate, Department of Surgery and Cancer, Faculty of Medicine, Imperial College London, London SW7 2AZ, UK;, hashim.ahmed@imperial.ac.uk); Emberton, Mark (Division of Surgery and Interventional Science, Faculty of Medical Sciences, University College London, London WC1E 6BT, UK;, m.emberton@ucl.ac.uk); Punwani, Shonit (Centre for Medical Imaging, University College London, London WC1E 6BT, UK;, saurabh.singh@ucl.ac.uk, (S.S.);, s.punwani@ucl.ac.uk, (S.P.)); Ourselin, Sébastien (School of Biomedical Engineering Imaging Sciences, King’s College London, London SE1 7EH, UK;, michela.antonelli@kcl.ac.uk, (M.A.);, sebastien.ourselin@kcl.ac.uk, (S.O.))","Mehta, Pritesh (University College London; King's College London; )","Mehta, Pritesh (University College London; King's College London); Antonelli, Michela (King's College London); Singh, Saurabh (University College London); Grondecka, Natalia (Medical University of Lublin); Johnston, Edward W. (Royal Marsden Hospital); Ahmed, Hashim U. (Imperial College London); Emberton, Mark (University College London); Punwani, Shonit (University College London); Ourselin, Sébastien (King's College London)",6,6,1.28,4.34,https://www.mdpi.com/2072-6694/13/23/6138/pdf?version=1639045170,https://app.dimensions.ai/details/publication/pub.1143679309,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4572,pub.1135658908,10.1016/j.media.2021.101984,33676101,,EIS-Net: Segmenting early infarct and scoring ASPECTS simultaneously on non-contrast CT of patients with acute ischemic stroke,"Detecting early infarct (EI) plays an essential role in patient selection for reperfusion therapy in the management of acute ischemic stroke (AIS). EI volume at acute or hyper-acute stage can be measured using advanced pre-treatment imaging, such as MRI and CT perfusion. In this study, a novel multi-task learning approach, EIS-Net, is proposed to segment EI and score Alberta Stroke Program Early CT Score (ASPECTS) simultaneously on baseline non-contrast CT (NCCT) scans of AIS patients. The EIS-Net comprises of a 3D triplet convolutional neural network (T-CNN) for EI segmentation and a multi-region classification network for ASPECTS scoring. T-CNN has triple encoders with original NCCT, mirrored NCCT, and atlas as inputs, as well as one decoder. A comparison disparity block (CDB) is designed to extract and enhance image contexts. In the decoder, a multi-level attention gate module (MAGM) is developed to recalibrate the features of the decoder for both segmentation and classification tasks. Evaluations using a high-quality dataset comprising of baseline NCCT and concomitant diffusion weighted MRI (DWI) as reference standard of 260 patients with AIS show that the proposed EIS-Net can accurately segment EI. The EIS-Net segmented EI volume strongly correlates with EI volume on DWI (r=0.919), and the mean difference between the two volumes is 8.5 mL. For ASPECTS scoring, the proposed EIS-Net achieves an intraclass correlation coefficient of 0.78 for total 10-point ASPECTS and a kappa of 0.75 for dichotomized ASPECTS (≤ 4 vs. >4). Both EI segmentation and ASPECTS scoring tasks achieve state-of-the-art performances.","This work was supported by Hunan Sci-Tech Innovation Programme (No. 2020GK2019), Hunan Provincial Science and Technology Program (No. 2018WK4001), Hunan Provincial Science and Technology Program (No. 2019CB1007), and the Canadian Institutes of Health Research and Alberta Innovates Health Solutions.",,Medical Image Analysis,,"Alberta; Brain Ischemia; Humans; Infarction; Ischemic Stroke; Stroke; Tomography, X-Ray Computed",2021-02-23,2021,2021-02-23,2021-05,70,,101984,Closed,Article,"Kuang, Hulin; Menon, Bijoy K; Sohn, Sung Il; Qiu, Wu","Kuang, Hulin (Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha, China; Department of Clinical Neuroscience, the University of Calgary, Calgary, Alberta, Canada.); Menon, Bijoy K (Department of Clinical Neuroscience, the University of Calgary, Calgary, Alberta, Canada; Department of Radiology, the University of Calgary, Calgary, Alberta, Canada.); Sohn, Sung Il (Department of Neurology, Keimyung University Dongsan Medical Center, Daegu, South Korea.); Qiu, Wu (Department of Clinical Neuroscience, the University of Calgary, Calgary, Alberta, Canada; Department of Radiology, the University of Calgary, Calgary, Alberta, Canada. Electronic address: wu.qiu@ucalgary.ca.)","Qiu, Wu (University of Calgary)","Kuang, Hulin (Central South University; University of Calgary); Menon, Bijoy K (University of Calgary); Sohn, Sung Il (Keimyung University Dongsan Medical Center); Qiu, Wu (University of Calgary)",15,15,2.24,,,https://app.dimensions.ai/details/publication/pub.1135658908,32 Biomedical and Clinical Sciences,,,,,,,,,,,,
4568,pub.1083746923,10.1007/s11548-017-1530-8,28188486,,Validation of a method for retroperitoneal tumor segmentation,"PurposeIn 2005, an application for surgical planning called AYRA®\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\textregistered }$$\end{document} was designed and validated by different surgeons and engineers at the Virgen del Rocío University Hospital, Seville (Spain). However, the segmentation methods included in AYRA and in other surgical planning applications are not able to segment accurately tumors that appear in soft tissue. The aims of this paper are to offer an exhaustive validation of an accurate semiautomatic segmentation tool to delimitate retroperitoneal tumors from CT images and to aid physicians in planning both radiotherapy doses and surgery.MethodsA panel of 6 experts manually segmented 11 cases of tumors, and the segmentation results were compared exhaustively with: the results provided by a surgical planning tool (AYRA), the segmentations obtained using a radiotherapy treatment planning system (Pinnacle®\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\textregistered }$$\end{document}), the segmentation results obtained by a group of experts in the delimitation of retroperitoneal tumors and the segmentation results using the algorithm under validation.Results11 cases of retroperitoneal tumors were tested. The proposed algorithm provided accurate results regarding the segmentation of the tumor. Moreover, the algorithm requires minimal computational time—an average of 90.5% less than that required when manually contouring the same tumor.ConclusionA method developed for the semiautomatic selection of retroperitoneal tumor has been validated in depth. AYRA, as well as other surgical and radiotherapy planning tools, could be greatly improved by including this algorithm.","This research has been cofinanced by P11-TIC-7727 (Government of Andalusia, Spain) and PT13/0006/0036RETIC (FEDER Funds and Department of Health, Regional Government of Andalusia). We would like to thank Jose Manuel Conde and María José Ortíz for their clinical contribution. VirSSPA is a software funded by the Andalusian Government, Spain and FEDER Funds.",,International Journal of Computer Assisted Radiology and Surgery,,"Adolescent; Adult; Algorithms; Humans; Imaging, Three-Dimensional; Male; Radiotherapy Planning, Computer-Assisted; Retroperitoneal Neoplasms; Tomography, X-Ray Computed; Young Adult",2017-02-10,2017,2017-02-10,2017-12,12,12,2055-2067,Closed,Article,"Suárez-Mejías, Cristina; Pérez-Carrasco, José A.; Serrano, Carmen; López-Guerra, José L.; Gómez-Cía, Tomás; Parra-Calderón, Carlos L.; Acha, Begoña","Suárez-Mejías, Cristina (Technological Innovation Group, Virgen del Rocío University Hospital, Avda Manuel Siurot, s/n, 41013, Sevilla, Spain); Pérez-Carrasco, José A. (Signal Theory and Communications Department, University of Seville, Camino de los Descubrimientos, s/n, 41092, Sevilla, Spain); Serrano, Carmen (Signal Theory and Communications Department, University of Seville, Camino de los Descubrimientos, s/n, 41092, Sevilla, Spain); López-Guerra, José L. (Oncology Unit, Virgen del Rocío University Hospital, Avda Manuel Siurot, s/n, 41013, Sevilla, Spain); Gómez-Cía, Tomás (Surgery Unit, Virgen del Rocío University Hospital, Avda Manuel Siurot, s/n, 41013, Sevilla, Spain); Parra-Calderón, Carlos L. (Technological Innovation Group, Virgen del Rocío University Hospital, Avda Manuel Siurot, s/n, 41013, Sevilla, Spain); Acha, Begoña (Signal Theory and Communications Department, University of Seville, Camino de los Descubrimientos, s/n, 41092, Sevilla, Spain)","Pérez-Carrasco, José A. (University of Seville)","Suárez-Mejías, Cristina (Hospital Universitario Virgen del Rocío); Pérez-Carrasco, José A. (University of Seville); Serrano, Carmen (University of Seville); López-Guerra, José L. (Hospital Universitario Virgen del Rocío); Gómez-Cía, Tomás (Hospital Universitario Virgen del Rocío); Parra-Calderón, Carlos L. (Hospital Universitario Virgen del Rocío); Acha, Begoña (University of Seville)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1083746923,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,,
4554,pub.1061696625,10.1109/tmi.2015.2496296,26540678,PMC4831070,Superpixel-Based Segmentation for 3D Prostate MR Images,"This paper proposes a method for segmenting the prostate on magnetic resonance (MR) images. A superpixel-based 3D graph cut algorithm is proposed to obtain the prostate surface. Instead of pixels, superpixels are considered as the basic processing units to construct a 3D superpixel-based graph. The superpixels are labeled as the prostate or background by minimizing an energy function using graph cut based on the 3D superpixel-based graph. To construct the energy function, we proposed a superpixel-based shape data term, an appearance data term, and two superpixel-based smoothness terms. The proposed superpixel-based terms provide the effectiveness and robustness for the segmentation of the prostate. The segmentation result of graph cuts is used as an initialization of a 3D active contour model to overcome the drawback of the graph cut. The result of 3D active contour model is then used to update the shape model and appearance model of the graph cut. Iterations of the 3D graph cut and 3D active contour model have the ability to jump out of local minima and obtain a smooth prostate surface. On our 43 MR volumes, the proposed method yields a mean Dice ratio of 89.3 ±1.9%. On PROMISE12 test data set, our method was ranked at the second place; the mean Dice ratio and standard deviation is 87.0±3.2%. The experimental results show that the proposed method outperforms several state-of-the-art prostate MRI segmentation methods.","This research is supported in part by NIH Grants CA156775 and CA176684, Georgia Cancer Coalition Distinguished Clinicians and Scientists Award, the Emory Molecular and Translational Imaging Center (CA128301). Z. Zhang was supported by the National Natural Science Foundation of China (81372274) and Natural Science Foundation of Guangdong Province (2014A030313033).",,IEEE Transactions on Medical Imaging,,"Algorithms; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Prostate",2015-10-30,2015,2015-10-30,2016-03,35,3,791-801,All OA, Green,Article,"Tian, Zhiqiang; Liu, Lizhi; Zhang, Zhenfeng; Fei, Baowei","Tian, Zhiqiang (Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, 30329, USA); Liu, Lizhi (Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, 30329, USA; Center for Medical Imaging & Image-guided Therapy, Sun Yat-Sen University Cancer Center, Guangzhou, China); Zhang, Zhenfeng (Center for Medical Imaging & Image-guided Therapy, Sun Yat-Sen University Cancer Center, Guangzhou, China); Fei, Baowei (Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, 30329, USA; Department of Biomedical Engineering, Emory University, Georgia Institute of Technology, Atlanta, GA, 30329, USA)","Fei, Baowei (Emory University; )","Tian, Zhiqiang (Emory University); Liu, Lizhi (Emory University; Sun Yat-sen University Cancer Center); Zhang, Zhenfeng (Sun Yat-sen University Cancer Center); Fei, Baowei (Emory University)",118,31,2.59,32.41,https://europepmc.org/articles/pmc4831070?pdf=render,https://app.dimensions.ai/details/publication/pub.1061696625,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
4547,pub.1153013167,10.1109/tmi.2022.3224067,36417741,,Causality-inspired Single-source Domain Generalization for Medical Image Segmentation,"Deep learning models usually suffer from the domain shift issue, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data are only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. In this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation scenarios: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-site prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains.",,,IEEE Transactions on Medical Imaging,,,2022-11-23,2022,2022-11-23,2022-11-23,PP,99,1-1,All OA, Green,Article,"Ouyang, Cheng; Chen, Chen; Li, Surui; Li, Zeju; Qin, Chen; Bai, Wenjia; Rueckert, Daniel","Ouyang, Cheng (Department of Computing, Imperial College London, UK); Chen, Chen (Department of Computing, Imperial College London, UK); Li, Surui (Department of Computing, Imperial College London, UK); Li, Zeju (Department of Computing, Imperial College London, UK); Qin, Chen (Department of Electrical and Electronic Engineering, Imperial College London, UK); Bai, Wenjia (Department of Computing, Imperial College London, UK); Rueckert, Daniel (Department of Computing, Imperial College London, UK)",,"Ouyang, Cheng (Imperial College London); Chen, Chen (Imperial College London); Li, Surui (Imperial College London); Li, Zeju (Imperial College London); Qin, Chen (Imperial College London); Bai, Wenjia (Imperial College London); Rueckert, Daniel (Imperial College London)",5,5,,,http://arxiv.org/pdf/2111.12525,https://app.dimensions.ai/details/publication/pub.1153013167,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
4537,pub.1141721607,10.1038/s41467-021-26216-9,34625565,PMC8501087,Annotation-efficient deep learning for automatic medical image segmentation,"Automatic medical image segmentation plays a critical role in scientific research and medical care. Existing high-performance deep learning methods typically rely on large training datasets with high-quality manual annotations, which are difficult to obtain in many clinical applications. Here, we introduce Annotation-effIcient Deep lEarning (AIDE), an open-source framework to handle imperfect training datasets. Methodological analyses and empirical evaluations are conducted, and we demonstrate that AIDE surpasses conventional fully-supervised models by presenting better performance on open datasets possessing scarce or noisy annotations. We further test AIDE in a real-life case study for breast tumor segmentation. Three datasets containing 11,852 breast images from three medical centers are employed, and AIDE, utilizing 10% training annotations, consistently produces segmentation maps comparable to those generated by fully-supervised counterparts or provided by independent radiologists. The 10-fold enhanced efficiency in utilizing expert labels has the potential to promote a wide range of biomedical applications.","This work was partly supported by Scientific and Technical Innovation 2030-“New Generation Artificial Intelligence” Project (2020AAA0104100, 2020AAA0104105), the National Natural Science Foundation of China (61871371, 81830056), Key-Area Research and Development Program of Guangdong Province (2018B010109009), the Basic Research Program of Shenzhen (JCYJ20180507182400762), Youth Innovation Promotion Association Program of the Chinese Academy of Sciences (2019351), National Key R&amp;D Program of China (2017YFE0103600), National Natural Science Foundation of China (81720108021), Zhongyuan Thousand Talents Plan Project (ZYQR201810117), Zhengzhou Collaborative Innovation Major Project (20XTZX05015). We thank all the participants and staff of the four challenges, CHAOS, NCI-ISBI 2013, PROMISE12, and QUBIQ. We thank Professor H. Peng for discussions.",,Nature Communications,,"Breast Neoplasms; Datasets as Topic; Deep Learning; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Retrospective Studies",2021-10-08,2021,2021-10-08,,12,1,5915,All OA, Gold,Article,"Wang, Shanshan; Li, Cheng; Wang, Rongpin; Liu, Zaiyi; Wang, Meiyun; Tan, Hongna; Wu, Yaping; Liu, Xinfeng; Sun, Hui; Yang, Rui; Liu, Xin; Chen, Jie; Zhou, Huihui; Ben Ayed, Ismail; Zheng, Hairong","Wang, Shanshan (Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Pazhou Laboratory, Guangzhou, Guangdong, China); Li, Cheng (Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China); Wang, Rongpin (Department of Medical Imaging, Guizhou Provincial People’s Hospital, Guiyang, Guizhou, China); Liu, Zaiyi (Department of Medical Imaging, Guangdong General Hospital, Guangdong Academy of Medical Sciences, Guangzhou, Guangdong, China); Wang, Meiyun (Department of Medical Imaging, Henan Provincial People’s Hospital & the People’s Hospital of Zhengzhou University, Zhengzhou, Henan, China); Tan, Hongna (Department of Medical Imaging, Henan Provincial People’s Hospital & the People’s Hospital of Zhengzhou University, Zhengzhou, Henan, China); Wu, Yaping (Department of Medical Imaging, Henan Provincial People’s Hospital & the People’s Hospital of Zhengzhou University, Zhengzhou, Henan, China); Liu, Xinfeng (Department of Medical Imaging, Guizhou Provincial People’s Hospital, Guiyang, Guizhou, China); Sun, Hui (Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China); Yang, Rui (Department of Urology, Renmin Hospital of Wuhan University, Wuhan, Hubei, China); Liu, Xin (Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China); Chen, Jie (Peng Cheng Laboratory, Shenzhen, Guangdong, China; School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University, Shenzhen, Guangdong, China); Zhou, Huihui (Brain Cognition and Brain Disease Institute, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China); Ben Ayed, Ismail (ETS Montreal, Montreal, Canada); Zheng, Hairong (Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China)","Wang, Shanshan (Shenzhen Institutes of Advanced Technology; Peng Cheng Laboratory; Guangdong Laboratory of Artificial Intelligence and Digital Economy); Li, Cheng (Shenzhen Institutes of Advanced Technology); Zheng, Hairong (Shenzhen Institutes of Advanced Technology)","Wang, Shanshan (Shenzhen Institutes of Advanced Technology; Peng Cheng Laboratory; Guangdong Laboratory of Artificial Intelligence and Digital Economy); Li, Cheng (Shenzhen Institutes of Advanced Technology); Wang, Rongpin (Guizhou Provincial People's Hospital); Liu, Zaiyi (); Wang, Meiyun (Henan Provincial People's Hospital); Tan, Hongna (Henan Provincial People's Hospital); Wu, Yaping (Henan Provincial People's Hospital); Liu, Xinfeng (Guizhou Provincial People's Hospital); Sun, Hui (Shenzhen Institutes of Advanced Technology); Yang, Rui (Renmin Hospital of Wuhan University); Liu, Xin (Shenzhen Institutes of Advanced Technology); Chen, Jie (Peng Cheng Laboratory; Peking University); Zhou, Huihui (Shenzhen Institutes of Advanced Technology); Ben Ayed, Ismail (); Zheng, Hairong (Shenzhen Institutes of Advanced Technology)",25,25,1.36,20.46,https://www.nature.com/articles/s41467-021-26216-9.pdf,https://app.dimensions.ai/details/publication/pub.1141721607,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
4537,pub.1127532907,10.3390/cancers12051204,32403240,PMC7281682,Applications of Artificial Intelligence to Prostate Multiparametric MRI (mpMRI): Current and Emerging Trends,"Prostate carcinoma is one of the most prevalent cancers worldwide. Multiparametric magnetic resonance imaging (mpMRI) is a non-invasive tool that can improve prostate lesion detection, classification, and volume quantification. Machine learning (ML), a branch of artificial intelligence, can rapidly and accurately analyze mpMRI images. ML could provide better standardization and consistency in identifying prostate lesions and enhance prostate carcinoma management. This review summarizes ML applications to prostate mpMRI and focuses on prostate organ segmentation, lesion detection and segmentation, and lesion characterization. A literature search was conducted to find studies that have applied ML methods to prostate mpMRI. To date, prostate organ segmentation and volume approximation have been well executed using various ML techniques. Prostate lesion detection and segmentation are much more challenging tasks for ML and were attempted in several studies. They largely remain unsolved problems due to data scarcity and the limitations of current ML algorithms. By contrast, prostate lesion characterization has been successfully completed in several studies because of better data availability. Overall, ML is well situated to become a tool that enhances radiologists' accuracy and speed.",,This review was partially funded by the Radiological Society of North America Medical Student Research Grant RMS1902 and the Alpha Omega Alpha Carolyn L. Kuckein Student Research Fellowship.,Cancers,,,2020-05-11,2020,2020-05-11,,12,5,1204,All OA, Gold,Article,"Bardis, Michelle D.; Houshyar, Roozbeh; Chang, Peter D.; Ushinsky, Alexander; Glavis-Bloom, Justin; Chahine, Chantal; Bui, Thanh-Lan; Rupasinghe, Mark; Filippi, Christopher G.; Chow, Daniel S.","Bardis, Michelle D. (Department of Radiology, University of California, Irvine, Orange, CA 92868-3201, USA;, rhoushya@hs.uci.edu, (R.H.);, changp6@hs.uci.edu, (P.D.C.);, jglavisb@hs.uci.edu, (J.G.-B.);, cchahin1@hs.uci.edu, (C.C.);, thanhltb@hs.uci.edu, (T.-L.B.);, mrupasin@hs.uci.edu, (M.R.);, chowd3@hs.uci.edu, (D.S.C.)); Houshyar, Roozbeh (Department of Radiology, University of California, Irvine, Orange, CA 92868-3201, USA;, rhoushya@hs.uci.edu, (R.H.);, changp6@hs.uci.edu, (P.D.C.);, jglavisb@hs.uci.edu, (J.G.-B.);, cchahin1@hs.uci.edu, (C.C.);, thanhltb@hs.uci.edu, (T.-L.B.);, mrupasin@hs.uci.edu, (M.R.);, chowd3@hs.uci.edu, (D.S.C.)); Chang, Peter D. (Department of Radiology, University of California, Irvine, Orange, CA 92868-3201, USA;, rhoushya@hs.uci.edu, (R.H.);, changp6@hs.uci.edu, (P.D.C.);, jglavisb@hs.uci.edu, (J.G.-B.);, cchahin1@hs.uci.edu, (C.C.);, thanhltb@hs.uci.edu, (T.-L.B.);, mrupasin@hs.uci.edu, (M.R.);, chowd3@hs.uci.edu, (D.S.C.)); Ushinsky, Alexander (Mallinckrodt Institute of Radiology, Washington University Saint Louis, St. Louis, MO 63110, USA;, aushinsky@wustl.edu); Glavis-Bloom, Justin (Department of Radiology, University of California, Irvine, Orange, CA 92868-3201, USA;, rhoushya@hs.uci.edu, (R.H.);, changp6@hs.uci.edu, (P.D.C.);, jglavisb@hs.uci.edu, (J.G.-B.);, cchahin1@hs.uci.edu, (C.C.);, thanhltb@hs.uci.edu, (T.-L.B.);, mrupasin@hs.uci.edu, (M.R.);, chowd3@hs.uci.edu, (D.S.C.)); Chahine, Chantal (Department of Radiology, University of California, Irvine, Orange, CA 92868-3201, USA;, rhoushya@hs.uci.edu, (R.H.);, changp6@hs.uci.edu, (P.D.C.);, jglavisb@hs.uci.edu, (J.G.-B.);, cchahin1@hs.uci.edu, (C.C.);, thanhltb@hs.uci.edu, (T.-L.B.);, mrupasin@hs.uci.edu, (M.R.);, chowd3@hs.uci.edu, (D.S.C.)); Bui, Thanh-Lan (Department of Radiology, University of California, Irvine, Orange, CA 92868-3201, USA;, rhoushya@hs.uci.edu, (R.H.);, changp6@hs.uci.edu, (P.D.C.);, jglavisb@hs.uci.edu, (J.G.-B.);, cchahin1@hs.uci.edu, (C.C.);, thanhltb@hs.uci.edu, (T.-L.B.);, mrupasin@hs.uci.edu, (M.R.);, chowd3@hs.uci.edu, (D.S.C.)); Rupasinghe, Mark (Department of Radiology, University of California, Irvine, Orange, CA 92868-3201, USA;, rhoushya@hs.uci.edu, (R.H.);, changp6@hs.uci.edu, (P.D.C.);, jglavisb@hs.uci.edu, (J.G.-B.);, cchahin1@hs.uci.edu, (C.C.);, thanhltb@hs.uci.edu, (T.-L.B.);, mrupasin@hs.uci.edu, (M.R.);, chowd3@hs.uci.edu, (D.S.C.)); Filippi, Christopher G. (Department of Radiology, North Shore University Hospital, Manhasset, NY 11030, USA;, sairaallapeikko@gmail.com); Chow, Daniel S. (Department of Radiology, University of California, Irvine, Orange, CA 92868-3201, USA;, rhoushya@hs.uci.edu, (R.H.);, changp6@hs.uci.edu, (P.D.C.);, jglavisb@hs.uci.edu, (J.G.-B.);, cchahin1@hs.uci.edu, (C.C.);, thanhltb@hs.uci.edu, (T.-L.B.);, mrupasin@hs.uci.edu, (M.R.);, chowd3@hs.uci.edu, (D.S.C.))","Bardis, Michelle D. (University of California, Irvine; )","Bardis, Michelle D. (University of California, Irvine); Houshyar, Roozbeh (University of California, Irvine); Chang, Peter D. (University of California, Irvine); Ushinsky, Alexander (Washington University in St. Louis); Glavis-Bloom, Justin (University of California, Irvine); Chahine, Chantal (University of California, Irvine); Bui, Thanh-Lan (University of California, Irvine); Rupasinghe, Mark (University of California, Irvine); Filippi, Christopher G. (North Shore University Hospital); Chow, Daniel S. (University of California, Irvine)",28,25,2.67,11.93,https://www.mdpi.com/2072-6694/12/5/1204/pdf?version=1590647789,https://app.dimensions.ai/details/publication/pub.1127532907,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4535,pub.1042421624,10.1002/mp.12048,27991675,PMC5540668,A supervoxel‐based segmentation method for prostate MR images,"PURPOSE: Segmentation of the prostate on MR images has many applications in prostate cancer management. In this work, we propose a supervoxel-based segmentation method for prostate MR images.
METHODS: A supervoxel is a set of pixels that have similar intensities, locations, and textures in a 3D image volume. The prostate segmentation problem is considered as assigning a binary label to each supervoxel, which is either the prostate or background. A supervoxel-based energy function with data and smoothness terms is used to model the label. The data term estimates the likelihood of a supervoxel belonging to the prostate by using a supervoxel-based shape feature. The geometric relationship between two neighboring supervoxels is used to build the smoothness term. The 3D graph cut is used to minimize the energy function to get the labels of the supervoxels, which yields the prostate segmentation. A 3D active contour model is then used to get a smooth surface by using the output of the graph cut as an initialization. The performance of the proposed algorithm was evaluated on 30 in-house MR image data and PROMISE12 dataset.
RESULTS: The mean Dice similarity coefficients are 87.2 ± 2.3% and 88.2 ± 2.8% for our 30 in-house MR volumes and the PROMISE12 dataset, respectively. The proposed segmentation method yields a satisfactory result for prostate MR images.
CONCLUSION: The proposed supervoxel-based method can accurately segment prostate MR images and can have a variety of application in prostate cancer diagnosis and therapy.","This research is supported in part by NIH grants R01CA156775 and R21CA176684, NSFC Project 61273252, Georgia Cancer Coalition Distinguished Clinicians and Scientists Award, and the Emory Molecular and Translational Imaging Center (NIH P50CA128301).",,Medical Physics,,"Algorithms; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Prostate; Prostatic Neoplasms",2017-02-16,2017,2017-02-16,2017-02,44,2,558-569,All OA, Green,Article,"Tian, Zhiqiang; Liu, Lizhi; Zhang, Zhenfeng; Xue, Jianru; Fei, Baowei","Tian, Zhiqiang (Department of Radiology and Imaging Sciences, School of Medicine, Emory University, 1841 Clifton Road NE, Atlanta, GA, 30329, USA); Liu, Lizhi (Department of Radiology and Imaging Sciences, School of Medicine, Emory University, 1841 Clifton Road NE, Atlanta, GA, 30329, USA; Center for Medical Imaging and Image‐guided Therapy, Sun Yat‐Sen University Cancer Center, 651 Dongfeng East Road, Guangzhou, 510060, P. R., China); Zhang, Zhenfeng (Center for Medical Imaging and Image‐guided Therapy, Sun Yat‐Sen University Cancer Center, 651 Dongfeng East Road, Guangzhou, 510060, P. R., China); Xue, Jianru (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, No.28 Xianning West Road, Xi'an, 710049, P. R., China); Fei, Baowei (Department of Radiology and Imaging Sciences, School of Medicine, Emory University, 1841 Clifton Road NE, Atlanta, GA, 30329, USA; Department of Biomedical Engineering, Emory University and Georgia Institute of Technology, 1841 Clifton Road NE, Atlanta, GA, 30329, USA)","Fei, Baowei (Emory University; Emory University)","Tian, Zhiqiang (Emory University); Liu, Lizhi (Emory University; Sun Yat-sen University Cancer Center); Zhang, Zhenfeng (Sun Yat-sen University Cancer Center); Xue, Jianru (Xi'an Jiaotong University); Fei, Baowei (Emory University; Emory University)",23,6,0.49,5.89,https://europepmc.org/articles/pmc5540668?pdf=render,https://app.dimensions.ai/details/publication/pub.1042421624,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
4532,pub.1112023902,10.1002/mrm.27677,30737827,,Development and clinical implementation of SeedNet: A sliding‐window convolutional neural network for radioactive seed identification in MRI‐assisted radiosurgery (MARS),"PURPOSE: To develop and evaluate a sliding-window convolutional neural network (CNN) for radioactive seed identification in MRI of the prostate after permanent implant brachytherapy.
METHODS: Sixty-eight patients underwent prostate cancer low-dose-rate (LDR) brachytherapy using radioactive seeds stranded with positive contrast MR-signal seed markers and were scanned using a balanced steady-state free precession pulse sequence with and without an endorectal coil (ERC). A sliding-window CNN algorithm (SeedNet) was developed to scan the prostate images using 3D sub-windows and to identify the implanted radioactive seeds. The algorithm was trained on sub-windows extracted from 18 patient images. Seed detection performance was evaluated by computing precision, recall, F1 -score, false discovery rate, and false-negative rate. Seed localization performance was evaluated by computing the RMS error (RMSE) between the manually identified and algorithm-inferred seed locations. SeedNet was implemented into a clinical software package and evaluated on sub-windows extracted from 40 test patients.
RESULTS: SeedNet achieved 97.6 ± 2.2% recall and 97.2 ± 1.9% precision for radioactive seed detection and 0.19 ± 0.04 mm RMSE for seed localization in the images acquired with an ERC. Without the ERC, the recall remained high, but the false-positive rate increased; the RMSE of the seed locations increased marginally. The clinical integration of SeedNet slightly increased the run-time, but the overall run-time was still low.
CONCLUSION: SeedNet can be used to perform automated radioactive seed identification in prostate MRI after LDR brachytherapy. Image quality improvement through pulse sequence optimization is expected to improve SeedNet's performance when imaging without an ERC.",The authors would like to thank The University of Texas MD Anderson Cancer Center’s Department of Scientific Publications for their help editing this paper.,,Magnetic Resonance in Medicine,,"Algorithms; Brachytherapy; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging, Interventional; Male; Neural Networks, Computer; Prostate; Prostatic Neoplasms; Radiosurgery; Retrospective Studies",2019-02-08,2019,2019-02-08,2019-06,81,6,3888-3900,Closed,Article,"Sanders, Jeremiah W.; Frank, Steven J.; Kudchadker, Rajat J.; Bruno, Teresa L.; Ma, Jingfei","Sanders, Jeremiah W. (Department of Imaging Physics, University of Texas MD Anderson Cancer Center, Houston, Texas; Medical Physics Graduate Program, University of Texas MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences, Houston, Texas); Frank, Steven J. (Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, Texas); Kudchadker, Rajat J. (Medical Physics Graduate Program, University of Texas MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences, Houston, Texas; Department of Radiation Physics, University of Texas MD Anderson Cancer Center, Houston, Texas); Bruno, Teresa L. (Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, Texas); Ma, Jingfei (Department of Imaging Physics, University of Texas MD Anderson Cancer Center, Houston, Texas; Medical Physics Graduate Program, University of Texas MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences, Houston, Texas)","Sanders, Jeremiah W. (The University of Texas MD Anderson Cancer Center; The University of Texas MD Anderson Cancer Center)","Sanders, Jeremiah W. (The University of Texas MD Anderson Cancer Center; The University of Texas MD Anderson Cancer Center); Frank, Steven J. (The University of Texas MD Anderson Cancer Center); Kudchadker, Rajat J. (The University of Texas MD Anderson Cancer Center; The University of Texas MD Anderson Cancer Center); Bruno, Teresa L. (The University of Texas MD Anderson Cancer Center); Ma, Jingfei (The University of Texas MD Anderson Cancer Center; The University of Texas MD Anderson Cancer Center)",12,3,1.15,2.33,,https://app.dimensions.ai/details/publication/pub.1112023902,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,,
4532,pub.1036690968,10.1016/j.pscychresns.2014.11.011,25665840,,Intracranial volume normalization methods: Considerations when investigating gender differences in regional brain volume,"Intracranial volume (ICV) normalization of regional brain volumes (v) is common practice in volumetric studies of the aging brain. Multiple normalization methods exist and this study aimed to investigate when each method is appropriate to use in gender dimorphism studies and how differences in v are affected by the choice of method. A new method based on weighted ICV matching is also presented. Theoretical reasoning and simulated experiments were followed by an evaluation using real data comprising 400 subjects, all 75 years old, whose ICV was segmented with a gold standard method. The presented method allows good visualization of volume relation between gender groups. A different gender dimorphism in volume was found depending on the normalization method used for both simulated and real data. Method performance was also seen to depend on the slope (B) and intercept (m) from the linear relation between v and ICV (v=B·ICV+m) as well as gender distribution in the cohort. A suggested work-flow for selecting ICV normalization method when investigating gender related differences in regional brain volume is presented.",This study was financially supported by the Swedish Research Council (VR 2012-2330).,,Psychiatry Research,,Adult, Aging, Brain, Cohort Studies, Female, Humans, Magnetic Resonance Imaging, Male, Organ Size, Sex Characteristics,2014-12-05,2014,2014-12-05,2015-03,231,3,227-235,Closed,Article,"Nordenskjöld, Richard; Malmberg, Filip; Larsson, Elna-Marie; Simmons, Andrew; Ahlström, Håkan; Johansson, Lars; Kullberg, Joel","Nordenskjöld, Richard (Department of Radiology, Uppsala University, MRT, Entrance 24, Uppsala University Hospital, SE-751 85 Uppsala, Sweden); Malmberg, Filip (Centre for Image Analysis, Uppsala University, Uppsala, Sweden); Larsson, Elna-Marie (Department of Radiology, Uppsala University, MRT, Entrance 24, Uppsala University Hospital, SE-751 85 Uppsala, Sweden); Simmons, Andrew (King׳s College London, Institute of Psychiatry, London, UK; NIHR Biomedical Research Centre for Mental Health and NIHR Biomedical Research Unit for Dementia, London, UK); Ahlström, Håkan (Department of Radiology, Uppsala University, MRT, Entrance 24, Uppsala University Hospital, SE-751 85 Uppsala, Sweden); Johansson, Lars (Department of Radiology, Uppsala University, MRT, Entrance 24, Uppsala University Hospital, SE-751 85 Uppsala, Sweden; AstraZeneca, Mölndal, Sweden); Kullberg, Joel (Department of Radiology, Uppsala University, MRT, Entrance 24, Uppsala University Hospital, SE-751 85 Uppsala, Sweden)","Nordenskjöld, Richard (Uppsala University)","Nordenskjöld, Richard (Uppsala University); Malmberg, Filip (Uppsala University); Larsson, Elna-Marie (Uppsala University); Simmons, Andrew (King's College London; NIHR Maudsley Dementia Biomedical Research Unit); Ahlström, Håkan (Uppsala University); Johansson, Lars (Uppsala University; AstraZeneca (Sweden)); Kullberg, Joel (Uppsala University)",38,11,1.7,7.89,,https://app.dimensions.ai/details/publication/pub.1036690968,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 52 Psychology, 5203 Clinical and Health Psychology
4532,pub.1036278134,10.1117/1.jmi.3.1.014003,26958579,PMC4760358,Visual saliency-based active learning for prostate magnetic resonance imaging segmentation,"We propose an active learning (AL) approach for prostate segmentation from magnetic resonance images. Our label query strategy is inspired from the principles of visual saliency that have similar considerations for choosing the most salient region. These similarities are encoded in a graph using classification maps and low-level features. Random walks are used to identify the most informative node, which is equivalent to the label query sample in AL. To reduce computation time, a volume of interest (VOI) is identified and all subsequent analysis, such as probability map generation using semisupervised random forest classifiers and label query, is restricted to this VOI. The negative log-likelihood of the probability maps serves as the penalty cost in a second-order Markov random field cost function, which is optimized using graph cuts for prostate segmentation. Experimental results on the Medical Image Computing and Computer Assisted Intervention (MICCAI) 2012 prostate segmentation challenge show the superior performance of our approach to conventional methods using fully supervised learning.",,,Journal of Medical Imaging,,,2016-02-19,2016,2016-02-19,2016-02-19,3,1,014003-014003,All OA, Green,Article,"Mahapatra, Dwarikanath; Buhmann, Joachim M.","Mahapatra, Dwarikanath (ETH Zurich, Department of Computer Science, CAB E65.1, Universitaetstrasse 6, Zurich 8092, Switzerland); Buhmann, Joachim M. (ETH Zurich, Department of Computer Science, CAB E65.1, Universitaetstrasse 6, Zurich 8092, Switzerland)","Mahapatra, Dwarikanath (ETH Zurich)","Mahapatra, Dwarikanath (ETH Zurich); Buhmann, Joachim M. (ETH Zurich)",11,5,0.22,2.37,https://europepmc.org/articles/pmc4760358?pdf=render,https://app.dimensions.ai/details/publication/pub.1036278134,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 40 Engineering, 4003 Biomedical Engineering,,,,,,,,
4532,pub.1003914161,10.1016/j.compbiomed.2016.12.010,28063376,,A comprehensive non-invasive framework for diagnosing prostate cancer,"Early detection of prostate cancer increases chances of patients' survival. Our automated non-invasive system for computer-aided diagnosis (CAD) of prostate cancer segments the prostate on diffusion-weighted magnetic resonance images (DW-MRI) acquired at different b-values, estimates its apparent diffusion coefficients (ADC), and classifies their descriptors - empirical cumulative distribution functions (CDF) - with a trained deep learning network. To segment the prostate, an evolving geometric (level-set-based) deformable model is guided by a speed function depending on intensity attributes extracted from the DW-MRI with nonnegative matrix factorization (NMF). For a more robust evolution, the attributes are fused with a probabilistic shape prior and estimated spatial dependencies between prostate voxels. To preserve continuity, the ADCs of the segmented prostate volume at different b-values are normalized and refined using a generalized Gauss-Markov random field image model. The CDFs of the refined ADCs at different b-values are considered global water diffusion features and used to distinguish between benign and malignant prostates. A deep learning network of stacked non-negativity-constrained auto-encoders (SNCAE) is trained to classify the benign or malignant prostates on the basis of the constructed CDFs. Our experiments on 53 clinical DW-MRI data sets resulted in 92.3% accuracy, 83.3% sensitivity, and 100% specificity, indicating that the proposed CAD system could be used as a reliable non-invasive diagnostic tool.",,,Computers in Biology and Medicine,,"Algorithms; Diffusion Magnetic Resonance Imaging; Early Detection of Cancer; Humans; Image Interpretation, Computer-Assisted; Machine Learning; Male; Pattern Recognition, Automated; Prostatic Neoplasms; ROC Curve; Reproducibility of Results; Sensitivity and Specificity",2016-12-23,2016,2016-12-23,2017-02,81,,148-158,Closed,Article,"Reda, Islam; Shalaby, Ahmed; Elmogy, Mohammed; Elfotouh, Ahmed Abou; Khalifa, Fahmi; El-Ghar, Mohamed Abou; Hosseini-Asl, Ehsan; Gimel'farb, Georgy; Werghi, Naoufel; El-Baz, Ayman","Reda, Islam (Faculty of Computers and Information, Mansoura University, Mansoura 35516, Egypt; Bioengineering Department, University of Louisville, Louisville KY 40292, USA); Shalaby, Ahmed (Bioengineering Department, University of Louisville, Louisville KY 40292, USA); Elmogy, Mohammed (Faculty of Computers and Information, Mansoura University, Mansoura 35516, Egypt; Bioengineering Department, University of Louisville, Louisville KY 40292, USA); Elfotouh, Ahmed Abou (Faculty of Computers and Information, Mansoura University, Mansoura 35516, Egypt); Khalifa, Fahmi (Bioengineering Department, University of Louisville, Louisville KY 40292, USA; Electronics and Communication Engineering Department, Mansoura University, Mansoura, Egypt); El-Ghar, Mohamed Abou (Radiology Department, Urology and Nephrology Center, University of Mansoura, Egypt); Hosseini-Asl, Ehsan (Electrical and Computer Engineering, University of Louisville, Louisville KY 40292, USA); Gimel'farb, Georgy (Department of Computer Science, University of Auckland, Auckland, New Zealand); Werghi, Naoufel (Khalifa University of Science Technology and Research, Abu Dhabi, UAE); El-Baz, Ayman (Bioengineering Department, University of Louisville, Louisville KY 40292, USA)","El-Baz, Ayman (University of Louisville)","Reda, Islam (Mansoura University; University of Louisville); Shalaby, Ahmed (University of Louisville); Elmogy, Mohammed (Mansoura University; University of Louisville); Elfotouh, Ahmed Abou (Mansoura University); Khalifa, Fahmi (University of Louisville; Mansoura University); El-Ghar, Mohamed Abou (Mansoura University); Hosseini-Asl, Ehsan (University of Louisville); Gimel'farb, Georgy (University of Auckland); Werghi, Naoufel (Khalifa University of Science and Technology); El-Baz, Ayman (University of Louisville)",32,16,0.44,7.56,,https://app.dimensions.ai/details/publication/pub.1003914161,31 Biological Sciences, 3102 Bioinformatics and Computational Biology, 42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences, 4601 Applied Computing,,,,,,,
4355,pub.1135644238,10.3390/diagnostics11020354,33672608,PMC7924061,Artificial Intelligence and Machine Learning in Prostate Cancer Patient Management—Current Trends and Future Perspectives,"Artificial intelligence (AI) is the field of computer science that aims to build smart devices performing tasks that currently require human intelligence. Through machine learning (ML), the deep learning (DL) model is teaching computers to learn by example, something that human beings are doing naturally. AI is revolutionizing healthcare. Digital pathology is becoming highly assisted by AI to help researchers in analyzing larger data sets and providing faster and more accurate diagnoses of prostate cancer lesions. When applied to diagnostic imaging, AI has shown excellent accuracy in the detection of prostate lesions as well as in the prediction of patient outcomes in terms of survival and treatment response. The enormous quantity of data coming from the prostate tumor genome requires fast, reliable and accurate computing power provided by machine learning algorithms. Radiotherapy is an essential part of the treatment of prostate cancer and it is often difficult to predict its toxicity for the patients. Artificial intelligence could have a future potential role in predicting how a patient will react to the therapy side effects. These technologies could provide doctors with better insights on how to plan radiotherapy treatment. The extension of the capabilities of surgical robots for more autonomous tasks will allow them to use information from the surgical field, recognize issues and implement the proper actions without the need for human intervention.",,"This work was supported by a grant of the Romanian Ministry of Education and Research, CNCS-UEFISCDI, project number PN-III-P1-1.1-PD-2019-0085, within PNCDI III.",Diagnostics,,,2021-02-20,2021,2021-02-20,,11,2,354,All OA, Gold,Article,"Tătaru, Octavian Sabin; Vartolomei, Mihai Dorin; Rassweiler, Jens J.; Virgil, Oșan; Lucarelli, Giuseppe; Porpiglia, Francesco; Amparore, Daniele; Manfredi, Matteo; Carrieri, Giuseppe; Falagario, Ugo; Terracciano, Daniela; de Cobelli, Ottavio; Busetto, Gian Maria; Del Giudice, Francesco; Ferro, Matteo","Tătaru, Octavian Sabin (The Institution Organizing University Doctoral Studies (I.O.S.U.D.), George Emil Palade University of Medicine, Pharmacy, Sciences and Technology from Târgu Mureș, 540142 Târgu Mureș, Romania;, sabin.tataru@gmail.com, (O.S.T.);, osan.virgil@gmail.com, (O.V.)); Vartolomei, Mihai Dorin (Department of Cell and Molecular Biology, George Emil Palade University of Medicine, Pharmacy, Sciences and Technology from Târgu Mureș, 540142 Târgu Mureș, Romania; Department of Urology, Medical University of Vienna, 1090 Vienna, Austria); Rassweiler, Jens J. (Department of Urology, SLK Kliniken Heilbronn, University of Heidelberg, 74074 Heilbronn, Germany;, jens.rassweiler@slk-kliniken.de); Virgil, Oșan (The Institution Organizing University Doctoral Studies (I.O.S.U.D.), George Emil Palade University of Medicine, Pharmacy, Sciences and Technology from Târgu Mureș, 540142 Târgu Mureș, Romania;, sabin.tataru@gmail.com, (O.S.T.);, osan.virgil@gmail.com, (O.V.)); Lucarelli, Giuseppe (Department of Emergency and Organ Transplantation-Urology, Andrology and Kidney Transplantation Unit, University of Bari, 70124 Bari, Italy;, giuseppe.lucarelli@inwind.it); Porpiglia, Francesco (Department of Urology, San Luigi Gonzaga Hospital, University of Turin, Orbassano, 10143 Turin, Italy;, porpiglia@libero.it, (F.P.);, daniele.amparore@unito.it, (D.A.);, matteo.manfredi85@gmail.com, (M.M.)); Amparore, Daniele (Department of Urology, San Luigi Gonzaga Hospital, University of Turin, Orbassano, 10143 Turin, Italy;, porpiglia@libero.it, (F.P.);, daniele.amparore@unito.it, (D.A.);, matteo.manfredi85@gmail.com, (M.M.)); Manfredi, Matteo (Department of Urology, San Luigi Gonzaga Hospital, University of Turin, Orbassano, 10143 Turin, Italy;, porpiglia@libero.it, (F.P.);, daniele.amparore@unito.it, (D.A.);, matteo.manfredi85@gmail.com, (M.M.)); Carrieri, Giuseppe (Department of Urology and Organ Transplantation, University of Foggia, 71122 Foggia, Italy;, giuseppe.caririeri@unifg.it, (G.C.);, ugofalagario@gmail.com, (U.F.)); Falagario, Ugo (Department of Urology and Organ Transplantation, University of Foggia, 71122 Foggia, Italy;, giuseppe.caririeri@unifg.it, (G.C.);, ugofalagario@gmail.com, (U.F.)); Terracciano, Daniela (Department of Translational Medical Sciences, University of Naples Federico II, 80131 Naples, Italy;, daniela.terracciano@unina.it); de Cobelli, Ottavio (Division of Urology, European Institute of Oncology (IEO)-IRCCS, 20141 Milan, Italy;, ottavio.decobelli@ieo.it, (O.d.C.);, matteo.ferro@ieo.it, (M.F.); Department of Oncology and Haematology-Oncology, Università degli Studi di Milano, 20122 Milan, Italy); Busetto, Gian Maria (Department of Urology and Renal Transplantation, University of Foggia, Policlinico Riuniti of Foggia, 71122 Foggia, Italy;, gianmaria.busetto@unifg.it); Del Giudice, Francesco (Department of Urology, Policlinico Umberto I, Sapienza University of Rome, 00185 Rome, Italy;, francesco.delgiudice@uniroma1.it); Ferro, Matteo (Division of Urology, European Institute of Oncology (IEO)-IRCCS, 20141 Milan, Italy;, ottavio.decobelli@ieo.it, (O.d.C.);, matteo.ferro@ieo.it, (M.F.))","Vartolomei, Mihai Dorin (; Medical University of Vienna)","Tătaru, Octavian Sabin (); Vartolomei, Mihai Dorin (Medical University of Vienna); Rassweiler, Jens J. (SLK-Kliniken Heilbronn); Virgil, Oșan (); Lucarelli, Giuseppe (University of Bari Aldo Moro); Porpiglia, Francesco (Ospedale San Luigi Gonzaga); Amparore, Daniele (Ospedale San Luigi Gonzaga); Manfredi, Matteo (Ospedale San Luigi Gonzaga); Carrieri, Giuseppe (University of Foggia); Falagario, Ugo (University of Foggia); Terracciano, Daniela (University of Naples Federico II); de Cobelli, Ottavio (European Institute of Oncology; University of Milan); Busetto, Gian Maria (University of Foggia); Del Giudice, Francesco (Sapienza University of Rome; Policlinico Umberto I); Ferro, Matteo (European Institute of Oncology)",49,49,8.8,35.47,https://www.mdpi.com/2075-4418/11/2/354/pdf,https://app.dimensions.ai/details/publication/pub.1135644238,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4334,pub.1146338404,10.1016/j.cmpb.2022.106752,35338887,,H-ProSeg: Hybrid ultrasound prostate segmentation based on explainability-guided mathematical model,"BACKGROUND AND OBJECTIVE: Accurate and robust prostate segmentation in transrectal ultrasound (TRUS) images is of great interest for image-guided prostate interventions and prostate cancer diagnosis. However, it remains a challenging task for various reasons, including a missing or ambiguous boundary between the prostate and surrounding tissues, the presence of shadow artifacts, intra-prostate intensity heterogeneity, and anatomical variations.
METHODS: Here, we present a hybrid method for prostate segmentation (H-ProSeg) in TRUS images, using a small number of radiologist-defined seed points as the prior points. This method consists of three subnetworks. The first subnetwork uses an improved principal curve-based model to obtain data sequences consisting of seed points and their corresponding projection index. The second subnetwork uses an improved differential evolution-based artificial neural network for training to decrease the model error. The third subnetwork uses the parameters of the artificial neural network to explain the smooth mathematical description of the prostate contour. The performance of the H-ProSeg method was assessed in 55 brachytherapy patients using Dice similarity coefficient (DSC), Jaccard similarity coefficient (Ω), and accuracy (ACC) values.
RESULTS: The H-ProSeg method achieved excellent segmentation accuracy, with DSC, Ω, and ACC values of 95.8%, 94.3%, and 95.4%, respectively. Meanwhile, the DSC, Ω, and ACC values of the proposed method were as high as 93.3%, 91.9%, and 93%, respectively, due to the influence of Gaussian noise (standard deviation of Gaussian function, σ = 50). Although the σ increased from 10 to 50, the DSC, Ω, and ACC values fluctuated by a maximum of approximately 2.5%, demonstrating the excellent robustness of our method.
CONCLUSIONS: Here, we present a hybrid method for accurate and robust prostate ultrasound image segmentation. The H-ProSeg method achieved superior performance compared with current state-of-the-art techniques. The knowledge of precise boundaries of the prostate is crucial for the conservation of risk structures. The proposed models have the potential to improve prostate cancer diagnosis and therapeutic outcomes.","This work was partly supported by Innovation and Technology Fund Projects, Hong Kong, No. ITS/080/19.",,Computer Methods and Programs in Biomedicine,,"Brachytherapy; Humans; Image Processing, Computer-Assisted; Male; Models, Theoretical; Prostate; Prostatic Neoplasms; Ultrasonography",2022-03-17,2022,2022-03-17,2022-06,219,,106752,Closed,Article,"Peng, Tao; Wu, Yiyun; Qin, Jing; Wu, Qingrong Jackie; Cai, Jing","Peng, Tao (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China.); Wu, Yiyun (Department of Medical Technology, Jiangsu Province Hospital, Nanjing, Jiangsu, China.); Qin, Jing (Department of Nursing, The Hong Kong Polytechnic University, Hong Kong, China.); Wu, Qingrong Jackie (Department of Radiation Oncology, Duke University Medical Center, Durham, NC, USA.); Cai, Jing (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China. Electronic address: jing.cai@polyu.edu.hk.)","Cai, Jing (Hong Kong Polytechnic University)","Peng, Tao (Hong Kong Polytechnic University); Wu, Yiyun (Jiangsu Province Hospital); Qin, Jing (Hong Kong Polytechnic University); Wu, Qingrong Jackie (Duke University Hospital); Cai, Jing (Hong Kong Polytechnic University)",5,5,,,,https://app.dimensions.ai/details/publication/pub.1146338404,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences, 4601 Applied Computing, 4603 Computer Vision and Multimedia Computation,,,,,,,,
4329,pub.1131472604,10.1016/j.media.2020.101845,33129147,PMC7725979,Biomechanically constrained non-rigid MR-TRUS prostate registration using deep learning based 3D point cloud matching,"A non-rigid MR-TRUS image registration framework is proposed for prostate interventions. The registration framework consists of a convolutional neural networks (CNN) for MR prostate segmentation, a CNN for TRUS prostate segmentation and a point-cloud based network for rapid 3D point cloud matching. Volumetric prostate point clouds were generated from the segmented prostate masks using tetrahedron meshing. The point cloud matching network was trained using deformation field that was generated by finite element analysis. Therefore, the network implicitly models the underlying biomechanical constraint when performing point cloud matching. A total of 50 patients' datasets were used for the network training and testing. Alignment of prostate shapes after registration was evaluated using three metrics including Dice similarity coefficient (DSC), mean surface distance (MSD) and Hausdorff distance (HD). Internal point-to-point registration accuracy was assessed using target registration error (TRE). Jacobian determinant and strain tensors of the predicted deformation field were calculated to analyze the physical fidelity of the deformation field. On average, the mean and standard deviation were 0.94±0.02, 0.90±0.23 mm, 2.96±1.00 mm and 1.57±0.77 mm for DSC, MSD, HD and TRE, respectively. Robustness of our method to point cloud noise was evaluated by adding different levels of noise to the query point clouds. Our results demonstrated that the proposed method could rapidly perform MR-TRUS image registration with good registration accuracy and robustness.","This research is supported in part by the National Cancer Institute of the National Institutes of Health under Award Number R01CA215718 (XY), the Department of Defense (DoD) Prostate Cancer Research Program (PCRP) Award W81XWH-17-1-0438 (TL) and W81XWH-17-1-0439 (AJ) and Dunwoody Golf Club Prostate Cancer Research Award (XY), a philanthropic award provided by the Winship Cancer Institute of Emory University.",,Medical Image Analysis,,"Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostatic Neoplasms; Ultrasonography",2020-10-07,2020,2020-10-07,2021-01,67,,101845,All OA, Bronze,Article,"Fu, Yabo; Lei, Yang; Wang, Tonghe; Patel, Pretesh; Jani, Ashesh B; Mao, Hui; Curran, Walter J; Liu, Tian; Yang, Xiaofeng","Fu, Yabo (Department of Radiation Oncology, Emory University, 1365 Clifton Road NE, Atlanta, GA 30322, United States.); Lei, Yang (Department of Radiation Oncology, Emory University, 1365 Clifton Road NE, Atlanta, GA 30322, United States.); Wang, Tonghe (Department of Radiation Oncology, Emory University, 1365 Clifton Road NE, Atlanta, GA 30322, United States; Winship Cancer Institute, Emory University, Atlanta, GA 30322, United States.); Patel, Pretesh (Department of Radiation Oncology, Emory University, 1365 Clifton Road NE, Atlanta, GA 30322, United States; Winship Cancer Institute, Emory University, Atlanta, GA 30322, United States.); Jani, Ashesh B (Department of Radiation Oncology, Emory University, 1365 Clifton Road NE, Atlanta, GA 30322, United States; Winship Cancer Institute, Emory University, Atlanta, GA 30322, United States.); Mao, Hui (Winship Cancer Institute, Emory University, Atlanta, GA 30322, United States; Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA 30322, United States.); Curran, Walter J (Department of Radiation Oncology, Emory University, 1365 Clifton Road NE, Atlanta, GA 30322, United States; Winship Cancer Institute, Emory University, Atlanta, GA 30322, United States.); Liu, Tian (Department of Radiation Oncology, Emory University, 1365 Clifton Road NE, Atlanta, GA 30322, United States; Winship Cancer Institute, Emory University, Atlanta, GA 30322, United States.); Yang, Xiaofeng (Department of Radiation Oncology, Emory University, 1365 Clifton Road NE, Atlanta, GA 30322, United States; Winship Cancer Institute, Emory University, Atlanta, GA 30322, United States. Electronic address: xiaofeng.yang@emory.edu.)","Yang, Xiaofeng (Emory University; Winship Cancer Institute)","Fu, Yabo (Emory University); Lei, Yang (Emory University); Wang, Tonghe (Emory University; Winship Cancer Institute); Patel, Pretesh (Emory University; Winship Cancer Institute); Jani, Ashesh B (Emory University; Winship Cancer Institute); Mao, Hui (Winship Cancer Institute; Emory University); Curran, Walter J (Emory University; Winship Cancer Institute); Liu, Tian (Emory University; Winship Cancer Institute); Yang, Xiaofeng (Emory University; Winship Cancer Institute)",23,22,5.53,9.8,https://www.sciencedirect.com/science/article/am/pii/S1361841520302097,https://app.dimensions.ai/details/publication/pub.1131472604,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4321,pub.1143560057,10.3390/s21237936,34883940,PMC8659893,Evolving Deep Architecture Generation with Residual Connections for Image Classification Using Particle Swarm Optimization,"Automated deep neural architecture generation has gained increasing attention. However, exiting studies either optimize important design choices, without taking advantage of modern strategies such as residual/dense connections, or they optimize residual/dense networks but reduce search space by eliminating fine-grained network setting choices. To address the aforementioned weaknesses, we propose a novel particle swarm optimization (PSO)-based deep architecture generation algorithm, to devise deep networks with residual connections, whilst performing a thorough search which optimizes important design choices. A PSO variant is proposed which incorporates a new encoding scheme and a new search mechanism guided by non-uniformly randomly selected neighboring and global promising solutions for the search of optimal architectures. Specifically, the proposed encoding scheme is able to describe convolutional neural network architecture configurations with residual connections. Evaluated using benchmark datasets, the proposed model outperforms existing state-of-the-art methods for architecture generation. Owing to the guidance of diverse non-uniformly selected neighboring promising solutions in combination with the swarm leader at fine-grained and global levels, the proposed model produces a rich assortment of residual architectures with great diversity. Our devised networks show better capabilities in tackling vanishing gradients with up to 4.34% improvement of mean accuracy in comparison with those of existing studies.","We appreciate the support and resources provided by Northumbria University and Ocucon Ltd. (Newcastle upon Tyne, UK).",,Sensors,,"Algorithms; Benchmarking; Data Collection; Neural Networks, Computer",2021-11-28,2021,2021-11-28,,21,23,7936,All OA, Gold,Article,"Lawrence, Tom; Zhang, Li; Rogage, Kay; Lim, Chee Peng","Lawrence, Tom (Department of Computer and Information Sciences, Faculty of Engineering and Environment, Northumbria University, Newcastle upon Tyne NE1 8ST, UK;, tom.lawrence@northumbria.ac.uk, (T.L.);, k.rogage@northumbria.ac.uk, (K.R.)); Zhang, Li (Department of Computer Science, Royal Holloway, University of London, Egham TW20 0EX, UK); Rogage, Kay (Department of Computer and Information Sciences, Faculty of Engineering and Environment, Northumbria University, Newcastle upon Tyne NE1 8ST, UK;, tom.lawrence@northumbria.ac.uk, (T.L.);, k.rogage@northumbria.ac.uk, (K.R.)); Lim, Chee Peng (Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds, VIC 3216, Australia;, chee.lim@deakin.edu.au)","Zhang, Li (Royal Holloway University of London)","Lawrence, Tom (Northumbria University); Zhang, Li (Royal Holloway University of London); Rogage, Kay (Northumbria University); Lim, Chee Peng (Deakin University)",5,5,0.7,4.02,https://www.mdpi.com/1424-8220/21/23/7936/pdf,https://app.dimensions.ai/details/publication/pub.1143560057,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
4315,pub.1147109719,10.1038/s41391-022-00537-2,35422101,PMC9385485,The promising role of new molecular biomarkers in prostate cancer: from coding and non-coding genes to artificial intelligence approaches,"BackgroundRisk stratification or progression in prostate cancer is performed with the support of clinical-pathological data such as the sum of the Gleason score and serum levels PSA. For several decades, methods aimed at the early detection of prostate cancer have included the determination of PSA serum levels. The aim of this systematic review is to provide an overview about recent advances in the discovery of new molecular biomarkers through transcriptomics, genomics and artificial intelligence that are expected to improve clinical management of the prostate cancer patient.MethodsAn exhaustive search was conducted by Pubmed, Google Scholar and Connected Papers using keywords relating to the genetics, genomics and artificial intelligence in prostate cancer, it includes “biomarkers”, “non-coding RNAs”, “lncRNAs”, “microRNAs”, “repetitive sequence”, “prognosis”, “prediction”, “whole-genome sequencing”, “RNA-Seq”, “transcriptome”, “machine learning”, and “deep learning”.ResultsNew advances, including the search for changes in novel biomarkers such as mRNAs, microRNAs, lncRNAs, and repetitive sequences, are expected to contribute to an earlier and accurate diagnosis for each patient in the context of precision medicine, thus improving the prognosis and quality of life of patients. We analyze several aspects that are relevant for prostate cancer including its new molecular markers associated with diagnosis, prognosis, and prediction to therapy and how bioinformatic approaches such as machine learning and deep learning can contribute to clinic. Furthermore, we also include current techniques that will allow an earlier diagnosis, such as Spatial Transcriptomics, Exome Sequencing, and Whole-Genome Sequencing.ConclusionTranscriptomic and genomic analysis have contributed to generate knowledge in the field of prostate carcinogenesis, new information about coding and non-coding genes as biomarkers has emerged. Synergies created by the implementation of artificial intelligence to analyze and understand sequencing data have allowed the development of clinical strategies that facilitate decision-making and improve personalized management in prostate cancer.","We thank to National Cancer Institute of Mexico (INCan) for their support. Rogelio Montiel-Manriquez is a PhD student in the “Programa de Doctorado en Ciencias Biomédicas, UNAM and received a fellowship from CONACYT with Currículum Vitae Único (CVU)- 581151. Figures were created with BioRender.com.",,Prostate Cancer and Prostatic Diseases,,"Artificial Intelligence; Biomarkers; Biomarkers, Tumor; Humans; Male; MicroRNAs; Prostate-Specific Antigen; Prostatic Neoplasms; Quality of Life",2022-04-14,2022,2022-04-14,2022-09,25,3,431-443,All OA, Hybrid,Article,"Alarcón-Zendejas, Ana Paula; Scavuzzo, Anna; Jiménez-Ríos, Miguel A.; Álvarez-Gómez, Rosa M.; Montiel-Manríquez, Rogelio; Castro-Hernández, Clementina; Jiménez-Dávila, Miguel A.; Pérez-Montiel, Delia; González-Barrios, Rodrigo; Jiménez-Trejo, Francisco; Arriaga-Canon, Cristian; Herrera, Luis A.","Alarcón-Zendejas, Ana Paula (Unidad de Investigación Biomédica en Cáncer, Instituto Nacional de Cancerología-Instituto de Investigaciones Biomédicas, UNAM, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Scavuzzo, Anna (Departamento de Urología, Instituto Nacional de Cancerología, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Jiménez-Ríos, Miguel A. (Departamento de Urología, Instituto Nacional de Cancerología, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Álvarez-Gómez, Rosa M. (Clínica de Cáncer Hereditario, Instituto Nacional de Cancerología, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Montiel-Manríquez, Rogelio (Unidad de Investigación Biomédica en Cáncer, Instituto Nacional de Cancerología-Instituto de Investigaciones Biomédicas, UNAM, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Castro-Hernández, Clementina (Unidad de Investigación Biomédica en Cáncer, Instituto Nacional de Cancerología-Instituto de Investigaciones Biomédicas, UNAM, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Jiménez-Dávila, Miguel A. (Departamento de Urología, Instituto Nacional de Cancerología, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Pérez-Montiel, Delia (Departamento de Patología, Instituto Nacional de Cancerología, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); González-Barrios, Rodrigo (Unidad de Investigación Biomédica en Cáncer, Instituto Nacional de Cancerología-Instituto de Investigaciones Biomédicas, UNAM, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Jiménez-Trejo, Francisco (Instituto Nacional de Pediatría, Insurgentes Sur No. 3700-C. Coyoacán. C.P., 04530, CDMX, México); Arriaga-Canon, Cristian (Unidad de Investigación Biomédica en Cáncer, Instituto Nacional de Cancerología-Instituto de Investigaciones Biomédicas, UNAM, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México); Herrera, Luis A. (Unidad de Investigación Biomédica en Cáncer, Instituto Nacional de Cancerología-Instituto de Investigaciones Biomédicas, UNAM, Avenida San Fernando No. 22 Col. Sección XVI, Tlalpan. C.P., 14080, CDMX, México; Instituto Nacional de Medicina Genómica, Periférico Sur 4809. Arenal Tepepan, Tlalpan. C.P., 14610, Mexico City, Mexico)","Arriaga-Canon, Cristian (National Autonomous University of Mexico); Herrera, Luis A. (National Autonomous University of Mexico; National Institute of Genomic Medicine)","Alarcón-Zendejas, Ana Paula (National Autonomous University of Mexico); Scavuzzo, Anna (Instituto Nacional de Cancerología); Jiménez-Ríos, Miguel A. (Instituto Nacional de Cancerología); Álvarez-Gómez, Rosa M. (Instituto Nacional de Cancerología); Montiel-Manríquez, Rogelio (National Autonomous University of Mexico); Castro-Hernández, Clementina (National Autonomous University of Mexico); Jiménez-Dávila, Miguel A. (Instituto Nacional de Cancerología); Pérez-Montiel, Delia (Instituto Nacional de Cancerología); González-Barrios, Rodrigo (National Autonomous University of Mexico); Jiménez-Trejo, Francisco (Instituto Nacional de Pediatria); Arriaga-Canon, Cristian (National Autonomous University of Mexico); Herrera, Luis A. (National Autonomous University of Mexico; National Institute of Genomic Medicine)",7,7,,,https://www.nature.com/articles/s41391-022-00537-2.pdf,https://app.dimensions.ai/details/publication/pub.1147109719,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,3 Good Health and Well Being,,,,,,,,
4305,pub.1117945641,10.1109/tmi.2019.2928056,31295109,,3D APA-Net: 3D Adversarial Pyramid Anisotropic Convolutional Network for Prostate Segmentation in MR Images,"Accurate and reliable segmentation of the prostate gland using magnetic resonance (MR) imaging has critical importance for the diagnosis and treatment of prostate diseases, especially prostate cancer. Although many automated segmentation approaches, including those based on deep learning have been proposed, the segmentation performance still has room for improvement due to the large variability in image appearance, imaging interference, and anisotropic spatial resolution. In this paper, we propose the 3D adversarial pyramid anisotropic convolutional deep neural network (3D APA-Net) for prostate segmentation in MR images. This model is composed of a generator (i.e., 3D PA-Net) that performs image segmentation and a discriminator (i.e., a six-layer convolutional neural network) that differentiates between a segmentation result and its corresponding ground truth. The 3D PA-Net has an encoder-decoder architecture, which consists of a 3D ResNet encoder, an anisotropic convolutional decoder, and multi-level pyramid convolutional skip connections. The anisotropic convolutional blocks can exploit the 3D context information of the MR images with anisotropic resolution, the pyramid convolutional blocks address both voxel classification and gland localization issues, and the adversarial training regularizes 3D PA-Net and thus enables it to generate spatially consistent and continuous segmentation results. We evaluated the proposed 3D APA-Net against several state-of-the-art deep learning-based segmentation approaches on two public databases and the hybrid of the two. Our results suggest that the proposed model outperforms the compared approaches on three databases and could be used in a routine clinical workflow.","This work was supported in part by the National Natural Science Foundation of China under Grant 61771397, in part by the Science and Technology Innovation Committee of Shenzhen Municipality, China, under Grant JCYJ20180306171334997, in part by the Northwestern Polytechnical University (NPU) through the Seed Foundation of Innovation and Creation for Graduate Students under Grant ZZ2019029, in part by the Synergy Innovation Foundation of the University and Enterprise for Graduate Students in the NPU under Grant XQ201911, and in part by the Project for Graduate Innovation Team of the NPU. We appreciate the efforts devoted by the organizers of the 2012 Prostate MR Image Segmentation Challenge and 2013 Automated Segmentation of Prostate Structures Challenge to collect and share the data for comparing interactive and (semi)-automatic segmentation algorithms for MRI of the prostate.",,IEEE Transactions on Medical Imaging,,"Algorithms; Anisotropy; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate",2019-07-11,2019,2019-07-11,2020-02,39,2,447-457,Closed,Article,"Jia, Haozhe; Xia, Yong; Song, Yang; Zhang, Donghao; Huang, Heng; Zhang, Yanning; Cai, Weidong","Jia, Haozhe (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xian, 710072, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xian, 710072, China); Xia, Yong (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xian, 710072, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xian, 710072, China; Research & Development Institute, Northwestern Polytechnical University in Shenzhen, Shenzhen, 518057, China); Song, Yang (School of Computer Science and Engineering, University of New South Wales, Sydney, NSW, 2052, Australia); Zhang, Donghao (School of Computer Science, The University of Sydney, Sydney, NSW, 2006, Australia); Huang, Heng (Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, 15261, USA); Zhang, Yanning (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xian, 710072, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xian, 710072, China); Cai, Weidong (School of Computer Science, The University of Sydney, Sydney, NSW, 2006, Australia)","Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University; Northwestern Polytechnical University)","Jia, Haozhe (Northwestern Polytechnical University; Northwestern Polytechnical University); Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University; Northwestern Polytechnical University); Song, Yang (UNSW Sydney); Zhang, Donghao (The University of Sydney); Huang, Heng (University of Pittsburgh); Zhang, Yanning (Northwestern Polytechnical University; Northwestern Polytechnical University); Cai, Weidong (The University of Sydney)",69,51,5.45,26.44,,https://app.dimensions.ai/details/publication/pub.1117945641,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
4301,pub.1112316638,10.1371/journal.pone.0211944,30794559,PMC6386287,Automatic classification of tissues on pelvic MRI based on relaxation times and support vector machine,"Tissue segmentation and classification in MRI is a challenging task due to a lack of signal intensity standardization. MRI signal is dependent on the acquisition protocol, the coil profile, the scanner type, etc. While we can compute quantitative physical tissue properties independent of the hardware and the sequence parameters, it is still difficult to leverage these physical properties to segment and classify pelvic tissues. The proposed method integrates quantitative MRI values (T1 and T2 relaxation times and pure synthetic weighted images) and machine learning (Support Vector Machine (SVM)) to segment and classify tissues in the pelvic region, i.e.: fat, muscle, prostate, bone marrow, bladder, and air. Twenty-two men with a mean age of 30±14 years were included in this prospective study. The images were acquired with a 3 Tesla MRI scanner. An inversion recovery-prepared turbo spin echo sequence was used to obtain T1-weighted images at different inversion times with a TR of 14000 ms. A 32-echo spin echo sequence was used to obtain the T2-weighted images at different echo times with a TR of 5000 ms. T1 and T2 relaxation times, synthetic T1- and T2-weighted images and anatomical probabilistic maps were calculated and used as input features of a SVM for segmenting and classifying tissues within the pelvic region. The mean SVM classification accuracy across subjects was calculated for the different tissues: prostate (94.2%), fat (96.9%), muscle (95.8%), bone marrow (91%) and bladder (82.1%) indicating an excellent classification performance. However, the segmentation and classification for air (within the rectum) may not always be successful (mean SVM accuracy 47.5%) due to the lack of air data in the training and testing sets. Our findings suggest that SVM can reliably segment and classify tissues in the pelvic region.","This work was supported by the Natural Sciences and Engineering Research Council of Canada (CA), Grant number: Discovery grant RGPIN-2018-05401 (http://www.nserc-crsng.gc.ca/index_eng.asp) to PMJ and the National Council of Science and Technology of Mexico, Grant number 218612-314201 (https://www.conacyt.gob.mx/) to JAZB. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.","This work was supported by the Natural Sciences and Engineering Research Council of Canada (CA), Grant number: Discovery grant RGPIN-2018-05401 (http://www.nserc-crsng.gc.ca/index_eng.asp) to PMJ and the National Council of Science and Technology of Mexico, Grant number 218612-314201 (https://www.conacyt.gob.mx/) to JAZB. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.",PLOS ONE,,"Adult; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Pelvis; Prospective Studies; Support Vector Machine",2019-02-22,2019,2019-02-22,,14,2,e0211944,All OA, Gold,Article,"Bojorquez, Jorge Arturo Zavala; Jodoin, Pierre-Marc; Bricq, Stéphanie; Walker, Paul Michael; Brunotte, François; Lalande, Alain","Bojorquez, Jorge Arturo Zavala (Le2i, Université Bourgogne Franche-Comte, Dijon, France); Jodoin, Pierre-Marc (VITAL, Université de Sherbrooke, Sherbrooke, Canada); Bricq, Stéphanie (Le2i, Université Bourgogne Franche-Comte, Dijon, France); Walker, Paul Michael (Le2i, Université Bourgogne Franche-Comte, Dijon, France; Centre Hospitalier Universitaire, Dijon, France); Brunotte, François (Le2i, Université Bourgogne Franche-Comte, Dijon, France; Centre Hospitalier Universitaire, Dijon, France); Lalande, Alain (Le2i, Université Bourgogne Franche-Comte, Dijon, France; Centre Hospitalier Universitaire, Dijon, France)","Lalande, Alain (Laboratoire d’Électronique, Informatique et Image; Centre Hospitalier Universitaire Dijon Bourgogne)","Bojorquez, Jorge Arturo Zavala (Laboratoire d’Électronique, Informatique et Image); Jodoin, Pierre-Marc (Université de Sherbrooke); Bricq, Stéphanie (Laboratoire d’Électronique, Informatique et Image); Walker, Paul Michael (Laboratoire d’Électronique, Informatique et Image; Centre Hospitalier Universitaire Dijon Bourgogne); Brunotte, François (Laboratoire d’Électronique, Informatique et Image; Centre Hospitalier Universitaire Dijon Bourgogne); Lalande, Alain (Laboratoire d’Électronique, Informatique et Image; Centre Hospitalier Universitaire Dijon Bourgogne)",8,4,0.97,3.36,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0211944&type=printable,https://app.dimensions.ai/details/publication/pub.1112316638,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
4301,pub.1028531893,10.1118/1.4914379,25832052,,The use of atlas registration and graph cuts for prostate segmentation in magnetic resonance images,"PURPOSE: An automatic method for 3D prostate segmentation in magnetic resonance (MR) images is presented for planning image-guided radiotherapy treatment of prostate cancer.
METHODS: A spatial prior based on intersubject atlas registration is combined with organ-specific intensity information in a graph cut segmentation framework. The segmentation is tested on 67 axial T2-weighted MR images in a leave-one-out cross validation experiment and compared with both manual reference segmentations and with multiatlas-based segmentations using majority voting atlas fusion. The impact of atlas selection is investigated in both the traditional atlas-based segmentation and the new graph cut method that combines atlas and intensity information in order to improve the segmentation accuracy. Best results were achieved using the method that combines intensity information, shape information, and atlas selection in the graph cut framework.
RESULTS: A mean Dice similarity coefficient (DSC) of 0.88 and a mean surface distance (MSD) of 1.45 mm with respect to the manual delineation were achieved.
CONCLUSIONS: This approaches the interobserver DSC of 0.90 and interobserver MSD 0f 1.15 mm and is comparable to other studies performing prostate segmentation in MR.",,,Medical Physics,,"Atlases as Topic; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Pattern Recognition, Automated; Prostate; Prostatic Neoplasms; Radiography; Radiotherapy Planning, Computer-Assisted",2015-03-17,2015,2015-03-17,2015-03-17,42,4,1614-1624,Closed,Article,"Korsager, Anne Sofie; Fortunati, Valerio; van der Lijn, Fedde; Carl, Jesper; Niessen, Wiro; Østergaard, Lasse Riis; van Walsum, Theo","Korsager, Anne Sofie (Department of Health Science and Technology, Aalborg University, Aalborg 9220, Denmark.); Fortunati, Valerio (Biomedical Imaging Group of Rotterdam, Department of Medical Informatics and Radiology, Erasmus MC, Rotterdam 3015 GE Rotterdam, The Netherlands.); van der Lijn, Fedde (Biomedical Imaging Group of Rotterdam, Department of Medical Informatics and Radiology, Erasmus MC, Rotterdam 3015 GE Rotterdam, The Netherlands.); Carl, Jesper (Department of Medical Physics, Oncology, Aalborg University Hospital, Aalborg 9220, Denmark.); Niessen, Wiro (Biomedical Imaging Group of Rotterdam, Department of Medical Informatics and Radiology, Erasmus MC, Rotterdam 3015 GE Rotterdam, The Netherlands.); Østergaard, Lasse Riis (Department of Health Science and Technology, Aalborg University, Aalborg 9220, Denmark.); van Walsum, Theo (Biomedical Imaging Group of Rotterdam, Department of Medical Informatics and Radiology, Erasmus MC, Rotterdam 3015 GE Rotterdam, The Netherlands.)",,"Korsager, Anne Sofie (Aalborg University); Fortunati, Valerio (Erasmus MC); van der Lijn, Fedde (Erasmus MC); Carl, Jesper (Aalborg University Hospital); Niessen, Wiro (Erasmus MC); Østergaard, Lasse Riis (Aalborg University); van Walsum, Theo (Erasmus MC)",26,5,1.15,16.0,,https://app.dimensions.ai/details/publication/pub.1028531893,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
4300,pub.1074246804,10.1002/mp.12141,28134979,,A multiresolution prostate representation for automatic segmentation in magnetic resonance images,"PURPOSE: Accurate prostate delineation is necessary in radiotherapy processes for concentrating the dose onto the prostate and reducing side effects in neighboring organs. Currently, manual delineation is performed over magnetic resonance imaging (MRI) taking advantage of its high soft tissue contrast property. Nevertheless, as human intervention is a consuming task with high intra- and interobserver variability rates, (semi)-automatic organ delineation tools have emerged to cope with these challenges, reducing the time spent for these tasks. This work presents a multiresolution representation that defines a novel metric and allows to segment a new prostate by combining a set of most similar prostates in a dataset.
METHODS: The proposed method starts by selecting the set of most similar prostates with respect to a new one using the proposed multiresolution representation. This representation characterizes the prostate through a set of salient points, extracted from a region of interest (ROI) that encloses the organ and refined using structural information, allowing to capture main relevant features of the organ boundary. Afterward, the new prostate is automatically segmented by combining the nonrigidly registered expert delineations associated to the previous selected similar prostates using a weighted patch-based strategy. Finally, the prostate contour is smoothed based on morphological operations.
RESULTS: The proposed approach was evaluated with respect to the expert manual segmentation under a leave-one-out scheme using two public datasets, obtaining averaged Dice coefficients of 82% ± 0.07 and 83% ± 0.06, and demonstrating a competitive performance with respect to atlas-based state-of-the-art methods.
CONCLUSIONS: The proposed multiresolution representation provides a feature space that follows a local salient point criteria and a global rule of the spatial configuration among these points to find out the most similar prostates. This strategy suggests an easy adaptation in the clinical routine, as supporting tool for annotation.",The two image databases used to prepare this article were obtained from MICCAI Grand Challenge: Prostate MR Image Segmentation 2012. This work was partially funded by the — Programa de becas de estudiantes sobrsalientes de posgrado — of the Universidad Nacional de Colombia.,,Medical Physics,,"Algorithms; Automation; Humans; Image Processing, Computer-Assisted; Linear Models; Magnetic Resonance Imaging; Male; Prostate",2017-04-12,2017,2017-04-12,2017-04,44,4,1312-1323,Closed,Article,"Alvarez, Charlens; Martínez, Fabio; Romero, Eduardo","Alvarez, Charlens (Computer Imaging and Medical Application Laboratory‐CIM@LAB, Universidad Nacional de Colombia, Bogotá, Colombia); Martínez, Fabio (Computer Imaging and Medical Application Laboratory‐CIM@LAB, Universidad Nacional de Colombia, Bogotá, Colombia; Escuela de Ingeniería de Sistemas e Informática, Universidad Industrial de Santander UIS, Bucaramanga, Colombia); Romero, Eduardo (Computer Imaging and Medical Application Laboratory‐CIM@LAB, Universidad Nacional de Colombia, Bogotá, Colombia)","Romero, Eduardo (National University of Colombia)","Alvarez, Charlens (National University of Colombia); Martínez, Fabio (National University of Colombia; Industrial University of Santander); Romero, Eduardo (National University of Colombia)",6,2,0.36,2.8,,https://app.dimensions.ai/details/publication/pub.1074246804,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
4300,pub.1013312135,10.1117/1.jmi.3.4.046002,27872873,PMC5097979,Postediting prostate magnetic resonance imaging segmentation consistency and operator time using manual and computer-assisted segmentation: multiobserver study,"Prostate segmentation on T2w MRI is important for several diagnostic and therapeutic procedures for prostate cancer. Manual segmentation is time-consuming, labor-intensive, and subject to high interobserver variability. This study investigated the suitability of computer-assisted segmentation algorithms for clinical translation, based on measurements of interoperator variability and measurements of the editing time required to yield clinically acceptable segmentations. A multioperator pilot study was performed under three pre- and postediting conditions: manual, semiautomatic, and automatic segmentation. We recorded the required editing time for each segmentation and measured the editing magnitude based on five different spatial metrics. We recorded average editing times of 213, 328, and 393 s for manual, semiautomatic, and automatic segmentation respectively, while an average fully manual segmentation time of 564 s was recorded. The reduced measured postediting interoperator variability of semiautomatic and automatic segmentations compared to the manual approach indicates the potential of computer-assisted segmentation for generating a clinically acceptable segmentation faster with higher consistency. The lack of strong correlation between editing time and the values of typically used error metrics ([Formula: see text]) implies that the necessary postsegmentation editing time needs to be measured directly in order to evaluate an algorithm's suitability for clinical translation.",,,Journal of Medical Imaging,,,2016-11-07,2016,2016-11-07,2016-11-07,3,4,046002-046002,All OA, Green,Article,"Shahedi, Maysam; Cool, Derek W.; Romagnoli, Cesare; Bauman, Glenn S.; Bastian-Jordan, Matthew; Rodrigues, George; Ahmad, Belal; Lock, Michael; Fenster, Aaron; Ward, Aaron D.","Shahedi, Maysam (London Regional Cancer Program, 790 Commissioners Road, London, Ontario N6A 4L6, Canada; University of Western Ontario, Robarts Research Institute, 1151 Richmond Street, London, Ontario N6A 5B7, Canada; University of Western Ontario, Graduate Program in Biomedical Engineering, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Cool, Derek W. (University of Western Ontario, Robarts Research Institute, 1151 Richmond Street, London, Ontario N6A 5B7, Canada; University of Western Ontario, Department of Medical Imaging, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Romagnoli, Cesare (University of Western Ontario, Department of Medical Imaging, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Bauman, Glenn S. (London Regional Cancer Program, 790 Commissioners Road, London, Ontario N6A 4L6, Canada; University of Western Ontario, Department of Medical Biophysics, 1151 Richmond Street, London, Ontario N6A 3K7, Canada; University of Western Ontario, Department of Oncology, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Bastian-Jordan, Matthew (University of Western Ontario, Department of Medical Imaging, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Rodrigues, George (London Regional Cancer Program, 790 Commissioners Road, London, Ontario N6A 4L6, Canada; University of Western Ontario, Department of Oncology, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Ahmad, Belal (London Regional Cancer Program, 790 Commissioners Road, London, Ontario N6A 4L6, Canada; University of Western Ontario, Department of Oncology, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Lock, Michael (London Regional Cancer Program, 790 Commissioners Road, London, Ontario N6A 4L6, Canada; University of Western Ontario, Department of Oncology, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Fenster, Aaron (University of Western Ontario, Robarts Research Institute, 1151 Richmond Street, London, Ontario N6A 5B7, Canada; University of Western Ontario, Graduate Program in Biomedical Engineering, 1151 Richmond Street, London, Ontario N6A 3K7, Canada; University of Western Ontario, Department of Medical Imaging, 1151 Richmond Street, London, Ontario N6A 3K7, Canada; University of Western Ontario, Department of Medical Biophysics, 1151 Richmond Street, London, Ontario N6A 3K7, Canada); Ward, Aaron D. (London Regional Cancer Program, 790 Commissioners Road, London, Ontario N6A 4L6, Canada; University of Western Ontario, Graduate Program in Biomedical Engineering, 1151 Richmond Street, London, Ontario N6A 3K7, Canada; University of Western Ontario, Department of Medical Biophysics, 1151 Richmond Street, London, Ontario N6A 3K7, Canada; University of Western Ontario, Department of Oncology, 1151 Richmond Street, London, Ontario N6A 3K7, Canada)","Shahedi, Maysam (London Health Sciences Centre; Western University; Western University)","Shahedi, Maysam (London Health Sciences Centre; Western University; Western University); Cool, Derek W. (Western University; Western University); Romagnoli, Cesare (Western University); Bauman, Glenn S. (London Health Sciences Centre; Western University; Western University); Bastian-Jordan, Matthew (Western University); Rodrigues, George (London Health Sciences Centre; Western University); Ahmad, Belal (London Health Sciences Centre; Western University); Lock, Michael (London Health Sciences Centre; Western University); Fenster, Aaron (Western University; Western University; Western University; Western University); Ward, Aaron D. (London Health Sciences Centre; Western University; Western University; Western University)",2,0,0.07,0.47,https://europepmc.org/articles/pmc5097979?pdf=render,https://app.dimensions.ai/details/publication/pub.1013312135,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4286,pub.1120960901,10.1016/j.media.2019.101558,31526965,PMC7985677,Automatic segmentation of prostate MRI using convolutional neural networks: Investigating the impact of network architecture on the accuracy of volume measurement and MRI-ultrasound registration,"Convolutional neural networks (CNNs) have recently led to significant advances in automatic segmentations of anatomical structures in medical images, and a wide variety of network architectures are now available to the research community. For applications such as segmentation of the prostate in magnetic resonance images (MRI), the results of the PROMISE12 online algorithm evaluation platform have demonstrated differences between the best-performing segmentation algorithms in terms of numerical accuracy using standard metrics such as the Dice score and boundary distance. These small differences in the segmented regions/boundaries outputted by different algorithms may potentially have an unsubstantial impact on the results of downstream image analysis tasks, such as estimating organ volume and multimodal image registration, which inform clinical decisions. This impact has not been previously investigated. In this work, we quantified the accuracy of six different CNNs in segmenting the prostate in 3D patient T2-weighted MRI scans and compared the accuracy of organ volume estimation and MRI-ultrasound (US) registration errors using the prostate segmentations produced by different networks. Networks were trained and tested using a set of 232 patient MRIs with labels provided by experienced clinicians. A statistically significant difference was found among the Dice scores and boundary distances produced by these networks in a non-parametric analysis of variance (p < 0.001 and p < 0.001, respectively), where the following multiple comparison tests revealed that the statistically significant difference in segmentation errors were caused by at least one tested network. Gland volume errors (GVEs) and target registration errors (TREs) were then estimated using the CNN-generated segmentations. Interestingly, there was no statistical difference found in either GVEs or TREs among different networks, (p = 0.34 and p = 0.26, respectively). This result provides a real-world example that these networks with different segmentation performances may potentially provide indistinguishably adequate registration accuracies to assist prostate cancer imaging applications. We conclude by recommending that the differences in the accuracy of downstream image analysis tasks that make use of data output by automatic segmentation methods, such as CNNs, within a clinical pipeline should be taken into account when selecting between different network architectures, in addition to reporting the segmentation accuracy.","We would like to acknowledge the UCL EPSRC Centre for Doctoral Training in Medical Imaging (Grant No. EP/L016478/1) for supporting Nooshin Ghavami in this work. Data used in this research comes from independent research by the Authors supported by the HIC Fund (Grant No. HICF-T4-310), a parallel funding partnership between the Department of Health and the Wellcome Trust. The views expressed in this publication are those of the author(s) and not necessarily those of the Department of Health or the Wellcome Trust. This work was also supported by the Wellcome/EPSRC Grant 203145Z/16/Z. Mark Emberton receives research support from the United Kingdom&#x27;s National Institute for Health Research (NIHR) UCLH/UCL Biomedical Research Centre. He is an NIHR Senior Investigator.",,Medical Image Analysis,,"Humans; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Pattern Recognition, Automated; Prostatic Neoplasms; Tumor Burden; Ultrasonography",2019-09-11,2019,2019-09-11,2019-12,58,,101558,All OA, Hybrid,Article,"Ghavami, Nooshin; Hu, Yipeng; Gibson, Eli; Bonmati, Ester; Emberton, Mark; Moore, Caroline M.; Barratt, Dean C.","Ghavami, Nooshin (Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, UK; Wellcome/EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK); Hu, Yipeng (Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, UK; Wellcome/EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK); Gibson, Eli (Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, UK; Siemens Healthineers, Princeton, USA); Bonmati, Ester (Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, UK; Wellcome/EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK); Emberton, Mark (Wellcome/EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK; Division of Surgery & Interventional Science, University College London, London, UK); Moore, Caroline M. (Wellcome/EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK; Division of Surgery & Interventional Science, University College London, London, UK); Barratt, Dean C. (Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, UK; Wellcome/EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK)","Ghavami, Nooshin (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences)","Ghavami, Nooshin (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Hu, Yipeng (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Gibson, Eli (University College London; Siemens Healthcare (United States)); Bonmati, Ester (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Emberton, Mark (Wellcome / EPSRC Centre for Interventional and Surgical Sciences; University College London); Moore, Caroline M. (Wellcome / EPSRC Centre for Interventional and Surgical Sciences; University College London); Barratt, Dean C. (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences)",40,30,2.65,14.13,https://doi.org/10.1016/j.media.2019.101558,https://app.dimensions.ai/details/publication/pub.1120960901,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4286,pub.1111780192,10.1002/mp.13416,30702759,PMC6453726,Deeply supervised 3D fully convolutional networks with group dilated convolution for automatic MRI prostate segmentation,"PURPOSE: Reliable automated segmentation of the prostate is indispensable for image-guided prostate interventions. However, the segmentation task is challenging due to inhomogeneous intensity distributions, variation in prostate anatomy, among other problems. Manual segmentation can be time-consuming and is subject to inter- and intraobserver variation. We developed an automated deep learning-based method to address this technical challenge.
METHODS: We propose a three-dimensional (3D) fully convolutional networks (FCN) with deep supervision and group dilated convolution to segment the prostate on magnetic resonance imaging (MRI). In this method, a deeply supervised mechanism was introduced into a 3D FCN to effectively alleviate the common exploding or vanishing gradients problems in training deep models, which forces the update process of the hidden layer filters to favor highly discriminative features. A group dilated convolution which aggregates multiscale contextual information for dense prediction was proposed to enlarge the effective receptive field of convolutional neural networks, which improve the prediction accuracy of prostate boundary. In addition, we introduced a combined loss function including cosine and cross entropy, which measures similarity and dissimilarity between segmented and manual contours, to further improve the segmentation accuracy. Prostate volumes manually segmented by experienced physicians were used as a gold standard against which our segmentation accuracy was measured.
RESULTS: The proposed method was evaluated on an internal dataset comprising 40 T2-weighted prostate MR volumes. Our method achieved a Dice similarity coefficient (DSC) of 0.86 ± 0.04, a mean surface distance (MSD) of 1.79 ± 0.46 mm, 95% Hausdorff distance (95%HD) of 7.98 ± 2.91 mm, and absolute relative volume difference (aRVD) of 15.65 ± 10.82. A public dataset (PROMISE12) including 50 T2-weighted prostate MR volumes was also employed to evaluate our approach. Our method yielded a DSC of 0.88 ± 0.05, MSD of 1.02 ± 0.35 mm, 95% HD of 9.50 ± 5.11 mm, and aRVD of 8.93 ± 7.56.
CONCLUSION: We developed a novel deeply supervised deep learning-based approach with a group dilated convolution to automatically segment the MRI prostate, demonstrated its clinical feasibility, and validated its accuracy against manual segmentation. The proposed technique could be a useful tool for image-guided interventions in prostate cancer.","This research is supported in part by the National Cancer Institute of the National Institutes of Health under Award Number R01CA215718 (XY), the Department of Defense (DoD) Prostate Cancer Research Program (PCRP) Award W81XWH‐13‐1‐0269 (XY), W81XWH‐17‐1‐0438 (TL), and W81XWH‐17‐1‐0439 (AJ) and Dunwoody Golf Club Prostate Cancer Research Award (XY), a philanthropic award provided by the Winship Cancer Institute of Emory University. We are also grateful for the GPU support from NVIDIA Corporation.",,Medical Physics,,"Algorithms; Automation; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Observer Variation; Prostate",2019-02-19,2019,2019-02-19,2019-04,46,4,1707-1718,All OA, Green,Article,"Wang, Bo; Lei, Yang; Tian, Sibo; Wang, Tonghe; Liu, Yingzi; Patel, Pretesh; Jani, Ashesh B.; Mao, Hui; Curran, Walter J.; Liu, Tian; Yang, Xiaofeng","Wang, Bo (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; School of Physics and Electronic‐Electrical Engineering, Ningxia University, Yinchuan, Ningxia, 750021, P.R. China); Lei, Yang (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Tian, Sibo (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Wang, Tonghe (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Liu, Yingzi (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Patel, Pretesh (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Jani, Ashesh B. (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Mao, Hui (Department of Radiology and Imaging Sciences and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Curran, Walter J. (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Liu, Tian (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA); Yang, Xiaofeng (Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA)","Yang, Xiaofeng (Emory University)","Wang, Bo (Emory University; Ningxia University); Lei, Yang (Emory University); Tian, Sibo (Emory University); Wang, Tonghe (Emory University); Liu, Yingzi (Emory University); Patel, Pretesh (Emory University); Jani, Ashesh B. (Emory University); Mao, Hui (Emory University); Curran, Walter J. (Emory University); Liu, Tian (Emory University); Yang, Xiaofeng (Emory University)",130,75,9.0,45.39,https://europepmc.org/articles/pmc6453726?pdf=render,https://app.dimensions.ai/details/publication/pub.1111780192,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
4286,pub.1037589143,10.1155/2014/789561,25525604,PMC4267002,Computer Aided-Diagnosis of Prostate Cancer on Multiparametric MRI: A Technical Review of Current Research,"Prostate cancer (PCa) is the most commonly diagnosed cancer among men in the United States. In this paper, we survey computer aided-diagnosis (CADx) systems that use multiparametric magnetic resonance imaging (MP-MRI) for detection and diagnosis of prostate cancer. We review and list mainstream techniques that are commonly utilized in image segmentation, registration, feature extraction, and classification. The performances of 15 state-of-the-art prostate CADx systems are compared through the area under their receiver operating characteristic curves (AUC). Challenges and potential directions to further the research of prostate CADx are discussed in this paper. Further improvements should be investigated to make prostate CADx systems useful in clinical practice.",This work was supported by the Intramural Research Programs of the NIH Clinical Center. The authors thank Kristine Evers for her help on the writing of the paper.,,BioMed Research International,,"Diagnosis, Computer-Assisted; Humans; Magnetic Resonance Imaging; Male; Prostate; Prostatic Neoplasms; Radiography",2014-12-01,2014,2014-12-01,2014,2014,,789561,All OA, Gold,Article,"Wang, Shijun; Burtt, Karen; Turkbey, Baris; Choyke, Peter; Summers, Ronald M.","Wang, Shijun (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Clinical Center, National Institutes of Health, Building 10, Room 1C224, Bethesda, MD 20892-1182, USA, nih.gov); Burtt, Karen (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Clinical Center, National Institutes of Health, Building 10, Room 1C224, Bethesda, MD 20892-1182, USA, nih.gov); Turkbey, Baris (Molecular Imaging Program, National Cancer Institute, National Institutes of Health, Building 10, Room B3B69F, Bethesda, MD 20892-1088, USA, nih.gov); Choyke, Peter (Molecular Imaging Program, National Cancer Institute, National Institutes of Health, Building 10, Room B3B69F, Bethesda, MD 20892-1088, USA, nih.gov); Summers, Ronald M. (Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Clinical Center, National Institutes of Health, Building 10, Room 1C224, Bethesda, MD 20892-1182, USA, nih.gov)","Summers, Ronald M. (National Institutes of Health Clinical Center)","Wang, Shijun (National Institutes of Health Clinical Center); Burtt, Karen (National Institutes of Health Clinical Center); Turkbey, Baris (National Cancer Institute); Choyke, Peter (National Cancer Institute); Summers, Ronald M. (National Institutes of Health Clinical Center)",101,24,2.8,20.34,https://downloads.hindawi.com/journals/bmri/2014/789561.pdf,https://app.dimensions.ai/details/publication/pub.1037589143,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,3 Good Health and Well Being,,,,,,,,
4284,pub.1093036022,10.1109/tmi.2017.2777870,29610082,,Online Robust Projective Dictionary Learning: Shape Modeling for MR-TRUS Registration,"Robust and effective shape prior modeling from a set of training data remains a challenging task, since the shape variation is complicated, and shape models should preserve local details as well as handle shape noises. To address these challenges, a novel robust projective dictionary learning (RPDL) scheme is proposed in this paper. Specifically, the RPDL method integrates the dimension reduction and dictionary learning into a unified framework for shape prior modeling, which can not only learn a robust and representative dictionary with the energy preservation of the training data, but also reduce the dimensionality and computational cost via the subspace learning. In addition, the proposed RPDL algorithm is regularized by using the norm to handle the outliers and noises, and is embedded in an online framework so that of memory and time efficiency. The proposed method is employed to model prostate shape prior for the application of magnetic resonance transrectal ultrasound registration. The experimental results demonstrate that our method provides more accurate and robust shape modeling than the state-of-the-art methods do. The proposed RPDL method is applicable for modeling other organs, and hence, a general solution for the problem of shape prior modeling.",,,IEEE Transactions on Medical Imaging,,"Algorithms; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostate; Prostatic Neoplasms; Supervised Machine Learning; Ultrasonography; Ultrasound, High-Intensity Focused, Transrectal",2018-03-30,2018,2018-03-30,2018-04,37,4,1067-1078,Closed,Article,"Wang, Yi; Zheng, Qingqing; Heng, Pheng Ann","Wang, Yi (Nat.-Regional Key Technol. Eng. Lab. for Med. Ultrasound, Shenzhen Univ., Shenzhen, China); Zheng, Qingqing (Dept. of Comput. Sci. &amp; Eng., Chinese Univ. of Hong Kong, Hong Kong, China); Heng, Pheng Ann (Dept. of Comput. Sci. &amp; Eng., Chinese Univ. of Hong Kong, Hong Kong, China)",,"Wang, Yi (Shenzhen University); Zheng, Qingqing (Chinese University of Hong Kong); Heng, Pheng Ann (Chinese University of Hong Kong)",11,3,0.66,3.72,,https://app.dimensions.ai/details/publication/pub.1093036022,40 Engineering, 4006 Communications Engineering, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,7 Affordable and Clean Energy,,,,,,,,
4269,pub.1109807854,10.1117/1.jmi.5.4.044501,30840739,PMC6228312,PROSTATEx Challenges for computerized classification of prostate lesions from multiparametric magnetic resonance images,"Grand challenges stimulate advances within the medical imaging research community; within a competitive yet friendly environment, they allow for a direct comparison of algorithms through a well-defined, centralized infrastructure. The tasks of the two-part PROSTATEx Challenges (the PROSTATEx Challenge and the PROSTATEx-2 Challenge) are (1) the computerized classification of clinically significant prostate lesions and (2) the computerized determination of Gleason Grade Group in prostate cancer, both based on multiparametric magnetic resonance images. The challenges incorporate well-vetted cases for training and testing, a centralized performance assessment process to evaluate results, and an established infrastructure for case dissemination, communication, and result submission. In the PROSTATEx Challenge, 32 groups apply their computerized methods (71 methods total) to 208 prostate lesions in the test set. The area under the receiver operating characteristic curve for these methods in the task of differentiating between lesions that are and are not clinically significant ranged from 0.45 to 0.87; statistically significant differences in performance among the top-performing methods, however, are not observed. In the PROSTATEx-2 Challenge, 21 groups apply their computerized methods (43 methods total) to 70 prostate lesions in the test set. When compared with the reference standard, the quadratic-weighted kappa values for these methods in the task of assigning a five-point Gleason Grade Group to each lesion range from - 0.24  to 0.27; superiority to random guessing can be established for only two methods. When approached with a sense of commitment and scientific rigor, challenges foster interest in the designated task and encourage innovation in the field.","The authors would like to express their appreciation to Angela Keyser and Shayna Knazik from the AAPM and Diane Cline and Robbine Waters from SPIE for their assistance in making these challenges a reality. The PROSTATEx Challenges were supported by SPIE, NCI, and AAPM. The authors are grateful to the groups that participated in the PROSTATEx Challenges, including groups led by Bejoy Abraham and Madhu S. Nair (University of Kerala), Jason Adleberg (Drexel University College of Medicine), Ruhallah Amandi and Mohammad Farhadi (Amirkabir University of Technology), Yoganand Balagurunathan (H.L. Moffitt Cancer Center), Adrian Barbu (Florida State University), Lei Bi (University of Sydney), Vy Bui (The Catholic University of America), Quan Chen (University of Virginia), King Chung Ho and Karthik Sarma (University of California, Los Angeles), Andy Kitchen, Panagiotis Korfiatis (Mayo Clinic), Peter S. LaViolette (Medical College of Wisconsin), Nathan Lay (National Institutes of Health), Chao Li (Huazhong University of Science and Technology), Qing Liang (Temple University), Saifeng Liu (The MRI Institute for Biomedical Research), Sean D. McGarry (Medical College of Wisconsin), Alireza Mehrtash (Brigham and Women’s Hospital), Mira Park (The University of Newcastle), N. Andres Parra (Moffitt Cancer Center), Yue Miao (University of Electronic Science and Technology of China), Hung Le Minh (Huazhong University of Science and Technology), Jin Qi and Miao Le (University of Electronic Science and Technology of China), Jarrel Chen Yi Seah (Alfred Health), Piotr Sobecki (Warsaw University of Technology), Radka Stoyanova (University of Miami), Yu Sun (Peter MacCallum Cancer Centre), Ning Wen (Henry Ford Health System), Xinran Zhong (University of California, Los Angeles), and Delong Zhu (Robotic Geometry Research Group CUHK). These (and the other) participating groups deserve recognition for their work to develop their prostate lesion classification and grading methods and for their contributions to the success of the challenges. This project has been funded in whole or in part with Federal funds from the National Cancer Institute, National Institutes of Health, under Contract No. HHSN261200800001E. The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government. The challenge platform and JKC were funded in part by NIH/NCI U01CA154601, U24CA180927, and U24CA180918 and a contract (HHSN26120080001E) from Leidos Biomedical Research. This manuscript has been authored in part by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a nonexclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan. The mention of commercial products, their sources, or their use in connection with material reported herein is not to be construed as either an actual or implied endorsement of such products by the Department of Health and Human Services.",,Journal of Medical Imaging,,,2018-10,2018,2018-11-10,2018-10,5,4,044501-044501,All OA, Green,Article,"Armato, Samuel G.; Huisman, Henkjan; Drukker, Karen; Hadjiiski, Lubomir; Kirby, Justin S.; Petrick, Nicholas; Redmond, George; Giger, Maryellen L.; Cha, Kenny; Mamonov, Artem; Kalpathy-Cramer, Jayashree; Farahani, Keyvan","Armato, Samuel G. (The University of Chicago, Department of Radiology, Chicago, Illinois, United States); Huisman, Henkjan (Radboud University Medical Center, Department of Radiology and Nuclear Medicine, Nijmegen, The Netherlands); Drukker, Karen (The University of Chicago, Department of Radiology, Chicago, Illinois, United States); Hadjiiski, Lubomir (University of Michigan, Department of Radiology, Ann Arbor, Michigan, United States); Kirby, Justin S. (Frederick National Laboratory for Cancer Research, Cancer Imaging Program, Frederick, Maryland, United States); Petrick, Nicholas (U.S. Food and Drug Administration, Center for Devices and Radiological Health, Silver Spring, Maryland, United States); Redmond, George (National Cancer Institute, Cancer Imaging Program, Division of Cancer Treatment and Diagnosis, Bethesda, Maryland, United States); Giger, Maryellen L. (The University of Chicago, Department of Radiology, Chicago, Illinois, United States); Cha, Kenny (University of Michigan, Department of Radiology, Ann Arbor, Michigan, United States; U.S. Food and Drug Administration, Center for Devices and Radiological Health, Silver Spring, Maryland, United States); Mamonov, Artem (MGH/Harvard Medical School, Boston, Massachusetts, United States); Kalpathy-Cramer, Jayashree (MGH/Harvard Medical School, Boston, Massachusetts, United States); Farahani, Keyvan (National Cancer Institute, Cancer Imaging Program, Division of Cancer Treatment and Diagnosis, Bethesda, Maryland, United States)","Armato, Samuel G. (University of Chicago)","Armato, Samuel G. (University of Chicago); Huisman, Henkjan (Radboud University Nijmegen Medical Centre); Drukker, Karen (University of Chicago); Hadjiiski, Lubomir (University of Michigan–Ann Arbor); Kirby, Justin S. (Frederick National Laboratory for Cancer Research); Petrick, Nicholas (United States Food and Drug Administration); Redmond, George (National Cancer Institute); Giger, Maryellen L. (University of Chicago); Cha, Kenny (University of Michigan–Ann Arbor; United States Food and Drug Administration); Mamonov, Artem (Harvard University); Kalpathy-Cramer, Jayashree (Harvard University); Farahani, Keyvan (National Cancer Institute)",90,67,4.31,26.27,https://europepmc.org/articles/pmc6228312?pdf=render,https://app.dimensions.ai/details/publication/pub.1109807854,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4265,pub.1113599425,10.1002/mp.13550,31002381,,Automated segmentation of prostate zonal anatomy on T2‐weighted (T2W) and apparent diffusion coefficient (ADC) map MR images using U‐Nets,"PURPOSE: Accurate regional segmentation of the prostate boundaries on magnetic resonance (MR) images is a fundamental requirement before automated prostate cancer diagnosis can be achieved. In this paper, we describe a novel methodology to segment prostate whole gland (WG), central gland (CG), and peripheral zone (PZ), where PZ + CG = WG, from T2W and apparent diffusion coefficient (ADC) map prostate MR images.
METHODS: We designed two similar models each made up of two U-Nets to delineate the WG, CG, and PZ from T2W and ADC map MR images, separately. The U-Net, which is a modified version of a fully convolutional neural network, includes contracting and expanding paths with convolutional, pooling, and upsampling layers. Pooling and upsampling layers help to capture and localize image features with a high spatial consistency. We used a dataset consisting of 225 patients (combining 153 and 72 patients with and without clinically significant prostate cancer) imaged with multiparametric MRI at 3 Tesla.
RESULTS AND CONCLUSION: Our proposed model for prostate zonal segmentation from T2W was trained and tested using 1154 and 1587 slices of 100 and 125 patients, respectively. Median of Dice similarity coefficient (DSC) on test dataset for prostate WG, CG, and PZ were 95.33 ± 7.77%, 93.75 ± 8.91%, and 86.78 ± 3.72%, respectively. Designed model for regional prostate delineation from ADC map images was trained and validated using 812 and 917 slices from 100 and 125 patients. This model yielded a median DSC of 92.09 ± 8.89%, 89.89 ± 10.69%, and 86.1 ± 9.56% for prostate WG, CG, and PZ on test samples, respectively. Further investigation indicated that the proposed algorithm reported high DSC for prostate WG segmentation from both T2W and ADC map MR images irrespective of WG size. In addition, segmentation accuracy in terms of DSC does not significantly vary among patients with or without significant tumors.
SIGNIFICANCE: We describe a method for automated prostate zonal segmentation using T2W and ADC map MR images independent of prostate size and the presence or absence of tumor. Our results are important in terms of clinical perspective as fully automated methods for ADC map images, which are considered as one of the most important sequences for prostate cancer detection in the PZ and CG, have not been reported previously.","The authors acknowledge Drs. Steven Currin MD and Abdullah Alayed MD (Department of Medical Imaging, The Ottawa Hospital) for their contribution to the performance of manual segmentation. The authors also acknowledge the Natural Sciences and Engineering Research Council of Canada (NSERC) for the opportunity and financial support provided for the research. Fatemeh Zabihollahy acknowledges the Ontario Graduate Scholarship (OGS). The authors of this paper have no conflict of interest to disclose.",,Medical Physics,,"Automation; Diffusion Magnetic Resonance Imaging; Humans; Image Processing, Computer-Assisted; Male; Prostate",2019-05-11,2019,2019-05-11,2019-07,46,7,3078-3090,Closed,Article,"Zabihollahy, Fatemeh; Schieda, Nicola; Jeyaraj, Satheesh Krishna; Ukwatta, Eranga","Zabihollahy, Fatemeh (Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada); Schieda, Nicola (Department of Radiology, University of Ottawa, Ottawa, ON, Canada); Jeyaraj, Satheesh Krishna (Department of Medical Imaging, University of Toronto, Toronto, ON, Canada); Ukwatta, Eranga (School of Engineering, University of Guelph, Guelph, ON, Canada)","Zabihollahy, Fatemeh (Carleton University)","Zabihollahy, Fatemeh (Carleton University); Schieda, Nicola (University of Ottawa); Jeyaraj, Satheesh Krishna (University of Toronto); Ukwatta, Eranga (University of Guelph)",33,21,2.34,11.52,,https://app.dimensions.ai/details/publication/pub.1113599425,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
4265,pub.1110953757,10.1016/j.cmpb.2018.12.031,30712600,,A propagation-DNN: Deep combination learning of multi-level features for MR prostate segmentation,"BACKGROUND AND OBJECTIVE: Prostate segmentation on Magnetic Resonance (MR) imaging is problematic because disease changes the shape and boundaries of the gland and it can be difficult to separate the prostate from surrounding tissues. We propose an automated model that extracts and combines multi-level features in a deep neural network to segment prostate on MR images.
METHODS: Our proposed model, the Propagation Deep Neural Network (P-DNN), incorporates the optimal combination of multi-level feature extraction as a single model. High level features from the convolved data using DNN are extracted for prostate localization and shape recognition, while labeling propagation, by low level cues, is embedded into a deep layer to delineate the prostate boundary.
RESULTS: A well-recognized benchmarking dataset (50 training data and 30 testing data from patients) was used to evaluate the P-DNN. When compared it to existing DNN methods, the P-DNN statistically outperformed the baseline DNN models with an average improvement in the DSC of 3.19%. When compared to the state-of-the-art non-DNN prostate segmentation methods, P-DNN was competitive by achieving 89.9 ± 2.8% DSC and 6.84 ± 2.5 mm HD on training sets and 84.13 ± 5.18% DSC and 9.74 ± 4.21 mm HD on testing sets.
CONCLUSION: Our results show that P-DNN maximizes multi-level feature extraction for prostate segmentation of MR images.",,,Computer Methods and Programs in Biomedicine,,"Algorithms; Deep Learning; Humans; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate",2018-12-29,2018,2018-12-29,2019-03,170,,11-21,All OA, Green,Article,"Yan, Ke; Wang, Xiuying; Kim, Jinman; Khadra, Mohamed; Fulham, Michael; Feng, Dagan","Yan, Ke (Biomedical and Multimedia Information Technology Research Group, School of Computer Science, University of Sydney, Sydney, Australia.); Wang, Xiuying (Biomedical and Multimedia Information Technology Research Group, School of Computer Science, University of Sydney, Sydney, Australia. Electronic address: xiu.wang@sydney.edu.au.); Kim, Jinman (Biomedical and Multimedia Information Technology Research Group, School of Computer Science, University of Sydney, Sydney, Australia.); Khadra, Mohamed (Department of Urology, Nepean Hospital, Kingswood, Australia.); Fulham, Michael (Department of Molecular Imaging, Royal Prince Alfred Hospital, Sydney, Australia.); Feng, Dagan (Biomedical and Multimedia Information Technology Research Group, School of Computer Science, University of Sydney, Sydney, Australia.)","Wang, Xiuying (The University of Sydney)","Yan, Ke (The University of Sydney); Wang, Xiuying (The University of Sydney); Kim, Jinman (The University of Sydney); Khadra, Mohamed (Nepean Hospital); Fulham, Michael (Royal Prince Alfred Hospital); Feng, Dagan (The University of Sydney)",36,21,1.78,11.36,https://ses.library.usyd.edu.au/bitstream/2123/20559/2/XWang-KeYan-CMPB2019-accepted.pdf,https://app.dimensions.ai/details/publication/pub.1110953757,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
4265,pub.1061696863,10.1109/tmi.2016.2643635,28026756,,Longitudinal Analysis of Pre-Term Neonatal Cerebral Ventricles From 3D Ultrasound Images Using Spatial-Temporal Deformable Registration,"Preterm neonates with a very low birth weight of less than 1,500 grams are at increased risk for developing intraventricular hemorrhage (IVH), which is a major cause of brain injury in preterm neonates. Quantitative measurements of ventricular dilatation or shrinkage play an important role in monitoring patients and evaluating treatment options. 3D ultrasound (US) has been developed to monitor ventricle volume as a biomarker for ventricular changes. However, ventricle volume as a global indicator does not allow for precise analysis of local ventricular changes, which could be linked to specific neurological problems often seen in the patient population later in life. In this work, a 3D+t spatial-temporal deformable registration approachis proposed, which is applied to the analysis of the detailed local changes of preterm IVH neonatal ventricles from 3D US images. In particular, a novel sequential convex/dual optimization algorithm is introduced to extract the optimal 3D+t spatial-temporal deformable field, which simultaneously optimizes the sequence of 3D deformation fieldswhile enjoying both efficiencyand simplicity in numerics. The developed registration technique was evaluated by comparing two manually extracted ventricle surfaces from the baseline and the registered follow-up images using the metrics of Dice similarity coefficient (DSC), mean absolute surface distance (MAD), and maximum absolute surface distance (MAXD). The performed experiments using 14 patients with 5 time-point images per patient show that the proposed 3D+t registration approach accurately recovered the longitudinal deformation of ventricle surfaces from 3D US images. The proposed approach may be potentially used to analyse the change pattern of cerebral ventricles of IVH patients, their response to different treatment options, and to elucidate the deficiencies that a patient could have later in life. To the best of our knowledge, this paper reports the first study on the longitudinalanalysis of neonatal ventricular system from 3D US images.",This work was supported by the Canadian Institutes of Health Research (CIHR: DCO150GP) and Academic Medical Organization of Southwestern Ontario (AMOSO).,,IEEE Transactions on Medical Imaging,,"Algorithms; Cerebral Ventricles; Humans; Imaging, Three-Dimensional; Reproducibility of Results; Ultrasonography",2016-12-22,2016,2016-12-22,2017-04,36,4,1016-1026,Closed,Article,"Qiu, Wu; Chen, Yimin; Kishimoto, Jessica; de Ribaupierre, Sandrine; Chiu, Bernard; Fenster, Aaron; Menon, Bijoy K.; Yuan, Jing","Qiu, Wu (Robarts Research Institute, University of Western Ontario, London, ON, N6A 5K8, Canada; Department of Clinical Neuroscience, University of Calgary, Calgary, AB, T2N 2T9, Canada); Chen, Yimin (Department of Electronic Engineering, City University of Hong Kong, China); Kishimoto, Jessica (Robarts Research Institute, University of Western Ontario, London, ON, N6A 5K8, Canada); de Ribaupierre, Sandrine (Robarts Research Institute, University of Western Ontario, London, ON, N6A 5K8, Canada); Chiu, Bernard (Department of Electronic Engineering, City University of Hong Kong, China); Fenster, Aaron (Robarts Research Institute, University of Western Ontario, London, ON, N6A 5K8, Canada); Menon, Bijoy K. (Department of Clinical Neuroscience, University of Calgary, Calgary, AB, T2N 2T9, Canada); Yuan, Jing (Robarts Research Institute, University of Western Ontario, London, ON, N6A 5K8, Canada; Mathematics and Statistics School, Xidian University, Xian, 710126, China)","Qiu, Wu (Western University; University of Calgary)","Qiu, Wu (Western University; University of Calgary); Chen, Yimin (City University of Hong Kong); Kishimoto, Jessica (Western University); de Ribaupierre, Sandrine (Western University); Chiu, Bernard (City University of Hong Kong); Fenster, Aaron (Western University); Menon, Bijoy K. (University of Calgary); Yuan, Jing (Western University; Xidian University)",7,4,0.41,,,https://app.dimensions.ai/details/publication/pub.1061696863,40 Engineering, 46 Information and Computing Sciences,,,,,,,,,,,
4157,pub.1146956010,10.1016/j.media.2022.102447,35509136,,Semi-supervised medical image segmentation via a tripled-uncertainty guided mean teacher model with contrastive learning,"Due to the difficulty in accessing a large amount of labeled data, semi-supervised learning is becoming an attractive solution in medical image segmentation. To make use of unlabeled data, current popular semi-supervised methods (e.g., temporal ensembling, mean teacher) mainly impose data-level and model-level consistency on unlabeled data. In this paper, we argue that in addition to these strategies, we could further utilize auxiliary tasks and consider task-level consistency to better excavate effective representations from unlabeled data for segmentation. Specifically, we introduce two auxiliary tasks, i.e., a foreground and background reconstruction task for capturing semantic information and a signed distance field (SDF) prediction task for imposing shape constraint, and explore the mutual promotion effect between the two auxiliary and the segmentation tasks based on mean teacher architecture. Moreover, to handle the potential bias of the teacher model caused by annotation scarcity, we develop a tripled-uncertainty guided framework to encourage the three tasks in the student model to learn more reliable knowledge from the teacher. When calculating uncertainty, we propose an uncertainty weighted integration (UWI) strategy for yielding the segmentation predictions of the teacher. In addition, following the advance of unsupervised learning in leveraging the unlabeled data, we also incorporate a contrastive learning based constraint to help the encoders extract more distinct representations to promote the medical image segmentation performance. Extensive experiments on the public 2017 ACDC dataset and the PROMISE12 dataset have demonstrated the effectiveness of our method.",This work is supported by National Natural Science Foundation of China (NSFC 62071314) and Sichuan Science and Technology Program (2021YFG0326).,,Medical Image Analysis,,"Humans; Image Processing, Computer-Assisted; Supervised Machine Learning; Uncertainty",2022-04-08,2022,2022-04-08,2022-07,79,,102447,Closed,Article,"Wang, Kaiping; Zhan, Bo; Zu, Chen; Wu, Xi; Zhou, Jiliu; Zhou, Luping; Wang, Yan","Wang, Kaiping (School of Computer Science, Sichuan University, Chengdu, China.); Zhan, Bo (School of Computer Science, Sichuan University, Chengdu, China.); Zu, Chen (Department of Risk Controlling Research, JD.COM, China.); Wu, Xi (School of Computer Science, Chengdu University of Information Technology, China.); Zhou, Jiliu (School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Chengdu University of Information Technology, China.); Zhou, Luping (School of Electrical and Information Engineering, University of Sydney, Australia.); Wang, Yan (School of Computer Science, Sichuan University, Chengdu, China. Electronic address: wangyanscu@hotmail.com.)","Wang, Yan (Sichuan University)","Wang, Kaiping (Sichuan University); Zhan, Bo (Sichuan University); Zu, Chen (Jingdong (China)); Wu, Xi (Chengdu University of Information Technology); Zhou, Jiliu (Sichuan University; Chengdu University of Information Technology); Zhou, Luping (The University of Sydney); Wang, Yan (Sichuan University)",14,14,,,,https://app.dimensions.ai/details/publication/pub.1146956010,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,,
4154,pub.1132730628,10.1016/j.media.2020.101907,33341496,,Test-time adaptable neural networks for robust medical image segmentation,"Convolutional Neural Networks (CNNs) work very well for supervised learning problems when the training dataset is representative of the variations expected to be encountered at test time. In medical image segmentation, this premise is violated when there is a mismatch between training and test images in terms of their acquisition details, such as the scanner model or the protocol. Remarkable performance degradation of CNNs in this scenario is well documented in the literature. To address this problem, we design the segmentation CNN as a concatenation of two sub-networks: a relatively shallow image normalization CNN, followed by a deep CNN that segments the normalized image. We train both these sub-networks using a training dataset, consisting of annotated images from a particular scanner and protocol setting. Now, at test time, we adapt the image normalization sub-network for each test image, guided by an implicit prior on the predicted segmentation labels. We employ an independently trained denoising autoencoder (DAE) in order to model such an implicit prior on plausible anatomical segmentation labels. We validate the proposed idea on multi-center Magnetic Resonance imaging datasets of three anatomies: brain, heart and prostate. The proposed test-time adaptation consistently provides performance improvement, demonstrating the promise and generality of the approach. Being agnostic to the architecture of the deep CNN, the second sub-network, the proposed design can be utilized with any segmentation network to increase robustness to variations in imaging scanners and protocols. Our code is available at: https://github.com/neerakara/test-time-adaptable-neural-networks-for-domain-generalization.","This work was supported by: 1. The Swiss Platform for Advanced Scientific Computing (PASC), that is coordinated by the Swiss National Supercomputing Centre (CSCS), 2. Clinical Research Priority Program Grant on Artificial Intelligence in Oncological Imaging Network from University of Zurich, and 3. Personalized Health and Related Technologies (PHRT), project number 222, ETH domain. We also thank NVIDIA corporation for their GPU donation.",,Medical Image Analysis,,"Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Prostate",2020-11-19,2020,2020-11-19,2021-02,68,,101907,All OA, Hybrid,Article,"Karani, Neerav; Erdil, Ertunc; Chaitanya, Krishna; Konukoglu, Ender","Karani, Neerav (Biomedical Image Computing Group, ETH Zurich, Zurich 8092, Switzerland. Electronic address: nkarani@vision.ee.ethz.ch.); Erdil, Ertunc (Biomedical Image Computing Group, ETH Zurich, Zurich 8092, Switzerland.); Chaitanya, Krishna (Biomedical Image Computing Group, ETH Zurich, Zurich 8092, Switzerland.); Konukoglu, Ender (Biomedical Image Computing Group, ETH Zurich, Zurich 8092, Switzerland.)","Karani, Neerav (ETH Zurich)","Karani, Neerav (ETH Zurich); Erdil, Ertunc (ETH Zurich); Chaitanya, Krishna (ETH Zurich); Konukoglu, Ender (ETH Zurich)",59,54,3.84,,https://doi.org/10.1016/j.media.2020.101907,https://app.dimensions.ai/details/publication/pub.1132730628,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
4125,pub.1139992095,10.21037/qims-21-175,34888197,PMC8611468,A review of deep learning-based three-dimensional medical image registration methods,"Medical image registration is a vital component of many medical procedures, such as image-guided radiotherapy (IGRT), as it allows for more accurate dose-delivery and better management of side effects. Recently, the successful implementation of deep learning (DL) in various fields has prompted many research groups to apply DL to three-dimensional (3D) medical image registration. Several of these efforts have led to promising results. This review summarized the progress made in DL-based 3D image registration over the past 5 years and identify existing challenges and potential avenues for further research. The collected studies were statistically analyzed based on the region of interest (ROI), image modality, supervision method, and registration evaluation metrics. The studies were classified into three categories: deep iterative registration, supervised registration, and unsupervised registration. The studies are thoroughly reviewed and their unique contributions are highlighted. A summary is presented following a review of each category of study, discussing its advantages, challenges, and trends. Finally, the common challenges for all categories are discussed, and potential future research topics are identified.",,,Quantitative Imaging in Medicine and Surgery,,,2021-12,2021,2021-12,2021-12,0,0,0-0,All OA, Gold,Article,"Xiao, Haonan; Teng, Xinzhi; Liu, Chenyang; Li, Tian; Ren, Ge; Yang, Ruijie; Shen, Dinggang; Cai, Jing","Xiao, Haonan (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China.); Teng, Xinzhi (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China.); Liu, Chenyang (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China.); Li, Tian (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China.); Ren, Ge (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China.); Yang, Ruijie (Department of Radiation Oncology, Peking University Third Hospital, Beijing, China.); Shen, Dinggang (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China.; Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China.; Department of Artificial Intelligence, Korea University, Seoul, Republic of Korea.); Cai, Jing (Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong, China.)",,"Xiao, Haonan (Hong Kong Polytechnic University); Teng, Xinzhi (Hong Kong Polytechnic University); Liu, Chenyang (Hong Kong Polytechnic University); Li, Tian (Hong Kong Polytechnic University); Ren, Ge (Hong Kong Polytechnic University); Yang, Ruijie (Peking University Third Hospital); Shen, Dinggang (ShanghaiTech University; Korea University); Cai, Jing (Hong Kong Polytechnic University)",14,14,1.79,17.53,https://qims.amegroups.com/article/viewFile/75304/pdf,https://app.dimensions.ai/details/publication/pub.1139992095,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
4119,pub.1153437437,10.3389/fonc.2022.1047215,36568171,PMC9768226,A review of deep learning-based deformable medical image registration,"The alignment of images through deformable image registration is vital to clinical applications (e.g., atlas creation, image fusion, and tumor targeting in image-guided navigation systems) and is still a challenging problem. Recent progress in the field of deep learning has significantly advanced the performance of medical image registration. In this review, we present a comprehensive survey on deep learning-based deformable medical image registration methods. These methods are classified into five categories: Deep Iterative Methods, Supervised Methods, Unsupervised Methods, Weakly Supervised Methods, and Latest Methods. A detailed review of each category is provided with discussions about contributions, tasks, and inadequacies. We also provide statistical analysis for the selected papers from the point of view of image modality, the region of interest (ROI), evaluation metrics, and method categories. In addition, we summarize 33 publicly available datasets that are used for benchmarking the registration algorithms. Finally, the remaining challenges, future directions, and potential trends are discussed in our review.",The work described in this paper is supported by two General Research Funds of Hong Kong Research Grants Council (project no. 15205919 and 15218521).,,Frontiers in Oncology,,,2022-12-07,2022,2022-12-07,,12,,1047215,All OA, Gold,Article,"Zou, Jing; Gao, Bingchen; Song, Youyi; Qin, Jing","Zou, Jing (Hong Kong Polytechnic University, Hong Kong, Hong Kong SAR, China.); Gao, Bingchen (Hong Kong Polytechnic University, Hong Kong, Hong Kong SAR, China.); Song, Youyi (Hong Kong Polytechnic University, Hong Kong, Hong Kong SAR, China.); Qin, Jing (Hong Kong Polytechnic University, Hong Kong, Hong Kong SAR, China.)","Zou, Jing (Hong Kong Polytechnic University)","Zou, Jing (Hong Kong Polytechnic University); Gao, Bingchen (Hong Kong Polytechnic University); Song, Youyi (Hong Kong Polytechnic University); Qin, Jing (Hong Kong Polytechnic University)",0,0,,,https://www.frontiersin.org/articles/10.3389/fonc.2022.1047215/pdf,https://app.dimensions.ai/details/publication/pub.1153437437,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4071,pub.1110009925,10.1016/j.media.2018.11.007,30476698,,GAS: A genetic atlas selection strategy in multi-atlas segmentation framework,"Multi-Atlas based Segmentation (MAS) algorithms have been successfully applied to many medical image segmentation tasks, but their success relies on a large number of atlases and good image registration performance. Choosing well-registered atlases for label fusion is vital for an accurate segmentation. This choice becomes even more crucial when the segmentation involves organs characterized by a high anatomical and pathological variability. In this paper, we propose a new genetic atlas selection strategy (GAS) that automatically chooses the best subset of atlases to be used for segmenting the target image, on the basis of both image similarity and segmentation overlap. More precisely, the key idea of GAS is that if two images are similar, the performances of an atlas for segmenting each image are similar. Since the ground truth of each atlas is known, GAS first selects a predefined number of similar images to the target, then, for each one of them, finds a near-optimal subset of atlases by means of a genetic algorithm. All these near-optimal subsets are then combined and used to segment the target image. GAS was tested on single-label and multi-label segmentation problems. In the first case, we considered the segmentation of both the whole prostate and of the left ventricle of the heart from magnetic resonance images. Regarding multi-label problems, the zonal segmentation of the prostate into peripheral and transition zone was considered. The results showed that the performance of MAS algorithms statistically improved when GAS is used.","Special thanks go to Catherine Scott for her overall assistance. This work is supported by EPSRC, the National Institute for Health Research University College London Hospitals Biomedical Research Centre (BRC), Cancer Research UK (CRUK), and by the Comprehensive Cancer Imaging Centre (CCIC).",,Medical Image Analysis,,"Algorithms; Heart Diseases; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Pattern Recognition, Automated; Prostatic Neoplasms",2018-11-19,2018,2018-11-19,2019-02,52,,97-108,All OA, Green,Article,"Antonelli, Michela; Cardoso, M Jorge; Johnston, Edward W; Appayya, Mrishta Brizmohun; Presles, Benoit; Modat, Marc; Punwani, Shonit; Ourselin, Sebastien","Antonelli, Michela (Centre for Medical Image Computing, University College London, U.K.. Electronic address: m.antonelli@ucl.ac.uk.); Cardoso, M Jorge (Dep. of Medical Physics and Biomedical Engineering, University College London, U.K.; School of Biomedical Engineering and Imaging Science, Kings College London, U.K.); Johnston, Edward W (Centre for Medical Imaging, University College London, U.K.); Appayya, Mrishta Brizmohun (Centre for Medical Imaging, University College London, U.K.); Presles, Benoit (Centre for Medical Image Computing, University College London, U.K.); Modat, Marc (Dep. of Medical Physics and Biomedical Engineering, University College London, U.K.; School of Biomedical Engineering and Imaging Science, Kings College London, U.K.); Punwani, Shonit (Centre for Medical Imaging, University College London, U.K.); Ourselin, Sebastien (Dep. of Medical Physics and Biomedical Engineering, University College London, U.K.; School of Biomedical Engineering and Imaging Science, Kings College London, U.K.)","Antonelli, Michela (University College London)","Antonelli, Michela (University College London); Cardoso, M Jorge (University College London; King's College London); Johnston, Edward W (University College London); Appayya, Mrishta Brizmohun (University College London); Presles, Benoit (University College London); Modat, Marc (University College London; King's College London); Punwani, Shonit (University College London); Ourselin, Sebastien (University College London; King's College London)",17,11,1.53,,https://discovery.ucl.ac.uk/10067168/1/prostateSegmentation.pdf,https://app.dimensions.ai/details/publication/pub.1110009925,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
4066,pub.1084024265,10.1007/s10278-017-9964-7,28342043,PMC5681466,Accuracy Validation of an Automated Method for Prostate Segmentation in Magnetic Resonance Imaging,"Three dimensional (3D) manual segmentation of the prostate on magnetic resonance imaging (MRI) is a laborious and time-consuming task that is subject to inter-observer variability. In this study, we developed a fully automatic segmentation algorithm for T2-weighted endorectal prostate MRI and evaluated its accuracy within different regions of interest using a set of complementary error metrics. Our dataset contained 42 T2-weighted endorectal MRI from prostate cancer patients. The prostate was manually segmented by one observer on all of the images and by two other observers on a subset of 10 images. The algorithm first coarsely localizes the prostate in the image using a template matching technique. Then, it defines the prostate surface using learned shape and appearance information from a set of training images. To evaluate the algorithm, we assessed the error metric values in the context of measured inter-observer variability and compared performance to that of our previously published semi-automatic approach. The automatic algorithm needed an average execution time of ∼60 s to segment the prostate in 3D. When compared to a single-observer reference standard, the automatic algorithm has an average mean absolute distance of 2.8 mm, Dice similarity coefficient of 82%, recall of 82%, precision of 84%, and volume difference of 0.5 cm3 in the mid-gland. Concordant with other studies, accuracy was highest in the mid-gland and lower in the apex and base. Loss of accuracy with respect to the semi-automatic algorithm was less than the measured inter-observer variability in manual segmentation for the same task.",The authors gratefully acknowledge the late Dr. Cesare Romagnoli for his support and scientific contribution to this work. This work was supported by the Ontario Institute for Cancer Research and the Ontario Research Fund. This work was also supported by Prostate Cancer Canada and is proudly funded by the Movember Foundation—Grant # RS2015-04. A. Fenster holds a Canada Research Chair in Biomedical Engineering and acknowledges the support of the Canada Research Chair Program. A. D. Ward holds a Cancer Care Ontario Research Chair in Cancer Imaging.,,Journal of Digital Imaging,,"Algorithms; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Observer Variation; Pattern Recognition, Automated; Prostate; Prostatic Neoplasms; Reproducibility of Results",2017-03-24,2017,2017-03-24,2017-12,30,6,782-795,All OA, Green,Article,"Shahedi, Maysam; Cool, Derek W.; Bauman, Glenn S.; Bastian-Jordan, Matthew; Fenster, Aaron; Ward, Aaron D.","Shahedi, Maysam (Baines Imaging Research Laboratory, London Regional Cancer Program, A3-123A, 790 Commissioners Rd E, N6A 4L6, London, ON, Canada; Robarts Research Institute, The University of Western Ontario, London, ON, Canada; Graduate Program in Biomedical Engineering, The University of Western Ontario, London, ON, Canada); Cool, Derek W. (Robarts Research Institute, The University of Western Ontario, London, ON, Canada; The Department of Medical Imaging, The University of Western Ontario, London, ON, Canada); Bauman, Glenn S. (Baines Imaging Research Laboratory, London Regional Cancer Program, A3-123A, 790 Commissioners Rd E, N6A 4L6, London, ON, Canada; The Department of Medical Biophysics, The University of Western Ontario, London, ON, Canada; The Department of Oncology, The University of Western Ontario, London, ON, Canada); Bastian-Jordan, Matthew (The Department of Medical Imaging, The University of Western Ontario, London, ON, Canada); Fenster, Aaron (Robarts Research Institute, The University of Western Ontario, London, ON, Canada; Graduate Program in Biomedical Engineering, The University of Western Ontario, London, ON, Canada; The Department of Medical Imaging, The University of Western Ontario, London, ON, Canada; The Department of Medical Biophysics, The University of Western Ontario, London, ON, Canada); Ward, Aaron D. (Baines Imaging Research Laboratory, London Regional Cancer Program, A3-123A, 790 Commissioners Rd E, N6A 4L6, London, ON, Canada; Graduate Program in Biomedical Engineering, The University of Western Ontario, London, ON, Canada; The Department of Medical Biophysics, The University of Western Ontario, London, ON, Canada; The Department of Oncology, The University of Western Ontario, London, ON, Canada)","Shahedi, Maysam (London Health Sciences Centre; Western University; Western University)","Shahedi, Maysam (London Health Sciences Centre; Western University; Western University); Cool, Derek W. (Western University; Western University); Bauman, Glenn S. (London Health Sciences Centre; Western University; Western University); Bastian-Jordan, Matthew (Western University); Fenster, Aaron (Western University; Western University; Western University; Western University); Ward, Aaron D. (London Health Sciences Centre; Western University; Western University; Western University)",18,7,1.17,4.53,https://europepmc.org/articles/pmc5681466?pdf=render,https://app.dimensions.ai/details/publication/pub.1084024265,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
4066,pub.1041819301,10.1016/j.media.2016.06.038,27428629,,Automatic segmentation approach to extracting neonatal cerebral ventricles from 3D ultrasound images,"Preterm neonates with a very low birth weight of less than 1,500 g are at increased risk for developing intraventricular hemorrhage (IVH). Progressive ventricle dilatation of IVH patients may cause increased intracranial pressure, leading to neurological damage, such as neurodevelopmental delay and cerebral palsy. The technique of 3D ultrasound (US) imaging has been used to quantitatively monitor the ventricular volume in IVH neonates, which may elucidate the ambiguity surrounding the timing of interventions in these patients as 2D clinical US imaging relies on linear measurement and visual estimation of ventricular dilation from a series of 2D slices. To translate 3D US imaging into the clinical setting, a fully automated segmentation algorithm is necessary to extract the ventricular system from 3D neonatal brain US images. In this paper, an automatic segmentation approach is proposed to delineate lateral ventricles of preterm neonates from 3D US images. The proposed segmentation approach makes use of phase congruency map, multi-atlas initialization technique, atlas selection strategy, and a multiphase geodesic level-sets (MGLS) evolution combined with a spatial shape prior derived from multiple pre-segmented atlases. Experimental results using 30 IVH patient images show that the proposed GPU-implemented approach is accurate in terms of the Dice similarity coefficient (DSC), the mean absolute surface distance (MAD), and maximum absolute surface distance (MAXD). To the best of our knowledge, this paper reports the first study on automatic segmentation of the ventricular system of premature neonatal brains from 3D US images.",The authors are grateful for the funding support from the Canadian Institutes of Health Research (CIHR) (DCO150GP) and Academic Medical Organization of Southwestern Ontario (AMOSO).,,Medical Image Analysis,,"Algorithms; Cerebral Ventricles; Humans; Imaging, Three-Dimensional; Infant, Newborn; Infant, Premature; Sensitivity and Specificity; Ultrasonography",2016-07-09,2016,2016-07-09,2017-01,35,,181-191,Closed,Article,"Qiu, Wu; Chen, Yimin; Kishimoto, Jessica; de Ribaupierre, Sandrine; Chiu, Bernard; Fenster, Aaron; Yuan, Jing","Qiu, Wu (Robarts Research Institute, University of Western Ontario, London, ON, Canada; Department of Clinical Neuroscience, University of Calgary, Calgary, AB, Canada); Chen, Yimin (Department of Electronic Engineering, City University of Hong Kong, Hong Kong, China); Kishimoto, Jessica (Robarts Research Institute, University of Western Ontario, London, ON, Canada; Medical Biophysics, University of Western Ontario, London, ON, Canada); de Ribaupierre, Sandrine (Neurosurgery, Department of Clinical Neurological Sciences, University of Western Ontario, London, ON, Canada); Chiu, Bernard (Department of Electronic Engineering, City University of Hong Kong, Hong Kong, China); Fenster, Aaron (Robarts Research Institute, University of Western Ontario, London, ON, Canada; Medical Biophysics, University of Western Ontario, London, ON, Canada); Yuan, Jing (Robarts Research Institute, University of Western Ontario, London, ON, Canada; Medical Biophysics, University of Western Ontario, London, ON, Canada; Mathematics and Statistics School, Xidian University, Xian, China)","Chen, Yimin (City University of Hong Kong); Yuan, Jing (Western University; Western University; Xidian University)","Qiu, Wu (Western University; University of Calgary); Chen, Yimin (City University of Hong Kong); Kishimoto, Jessica (Western University; Western University); de Ribaupierre, Sandrine (Western University); Chiu, Bernard (City University of Hong Kong); Fenster, Aaron (Western University; Western University); Yuan, Jing (Western University; Western University; Xidian University)",36,9,2.03,10.73,,https://app.dimensions.ai/details/publication/pub.1041819301,32 Biomedical and Clinical Sciences, 3213 Paediatrics,,,,,,,,,,,
4061,pub.1130456187,10.1007/s10916-020-01641-3,32862251,,Deep Learning in Radiation Oncology Treatment Planning for Prostate Cancer: A Systematic Review,"Radiation oncology for prostate cancer is important as it can decrease the morbidity and mortality associated with this disease. Planning for this modality of treatment is both fundamental, time-consuming and prone to human-errors, leading to potentially avoidable delays in start of treatment. A fundamental step in radiotherapy planning is contouring of radiation targets, where medical specialists contouring, i.e., segment, the boundaries of the structures to be irradiated. Automating this step can potentially lead to faster treatment planning without a decrease in quality, while increasing time available to physicians and also more consistent treatment results. This can be framed as an image segmentation task, which has been studied for many decades in the fields of Computer Vision and Machine Learning. With the advent of Deep Learning, there have been many proposals for different network architectures achieving high performance levels. In this review, we searched the literature for those methods and describe them briefly, grouping those based on Computed Tomography (CT) or Magnetic Resonance Imaging (MRI). This is a booming field, evidenced by the date of the publications found. However, most publications use data from a very limited number of patients, which presents an obstacle to deep learning models training. Although the performance of the models has achieved very satisfactory results, there is still room for improvement, and there is arguably a long way before these models can be used safely and effectively in clinical practice.",,"The authors would like to thank Fundação para a Ciência e Tecnologia (FCT) for the PhD grant (reference SFRH/BD/146887/2019) awarded to the first author, which this work is a part of.",Journal of Medical Systems,,"Deep Learning; Humans; Male; Prostatic Neoplasms; Radiation Oncology; Radiotherapy Planning, Computer-Assisted; Tomography, X-Ray Computed",2020-08-30,2020,2020-08-30,2020-10,44,10,179,All OA, Green,Article,"Almeida, Gonçalo; Tavares, João Manuel R.S.","Almeida, Gonçalo (Instituto de Ciência e Inovação em Engenharia Mecânica e Engenharia Industrial, Faculdade de Engenharia, Universidade do Porto, Porto, Portugal); Tavares, João Manuel R.S. (Instituto de Ciência e Inovação em Engenharia Mecânica e Engenharia Industrial, Departamento de Engenharia Mecânica, Faculdade de Engenharia, Universidade do Porto, Porto, Portugal)","Tavares, João Manuel R.S. (University of Porto)","Almeida, Gonçalo (University of Porto); Tavares, João Manuel R.S. (University of Porto)",14,13,1.28,7.16,https://repositorio-aberto.up.pt/bitstream/10216/129011/2/415537.pdf,https://app.dimensions.ai/details/publication/pub.1130456187,42 Health Sciences, 4203 Health Services and Systems,3 Good Health and Well Being,,,,,,,,,
4061,pub.1128664700,10.1007/s11517-020-02199-5,32566988,,Superpixel-based deep convolutional neural networks and active contour model for automatic prostate segmentation on 3D MRI scans,"Automatic and reliable prostate segmentation is an essential prerequisite for assisting the diagnosis and treatment, such as guiding biopsy procedure and radiation therapy. Nonetheless, automatic segmentation is challenging due to the lack of clear prostate boundaries owing to the similar appearance of prostate and surrounding tissues and the wide variation in size and shape among different patients ascribed to pathological changes or different resolutions of images. In this regard, the state-of-the-art includes methods based on a probabilistic atlas, active contour models, and deep learning techniques. However, these techniques have limitations that need to be addressed, such as MRI scans with the same spatial resolution, initialization of the prostate region with well-defined contours and a set of hyperparameters of deep learning techniques determined manually, respectively. Therefore, this paper proposes an automatic and novel coarse-to-fine segmentation method for prostate 3D MRI scans. The coarse segmentation step combines local texture and spatial information using the Intrinsic Manifold Simple Linear Iterative Clustering algorithm and probabilistic atlas in a deep convolutional neural networks model jointly with the particle swarm optimization algorithm to classify prostate and non-prostate tissues. Then, the fine segmentation uses the 3D Chan-Vese active contour model to obtain the final prostate surface. The proposed method has been evaluated on the Prostate 3T and PROMISE12 databases presenting a dice similarity coefficient of 84.86%, relative volume difference of 14.53%, sensitivity of 90.73%, specificity of 99.46%, and accuracy of 99.11%. Experimental results demonstrate the high performance potential of the proposed method compared to those previously published.",,The authors acknowledge CAPES and CNPq for their financial support.,Medical & Biological Engineering & Computing,,"Algorithms; Databases, Factual; Deep Learning; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Latent Class Analysis; Magnetic Resonance Imaging; Male; Models, Statistical; Neural Networks, Computer; Prostatic Neoplasms",2020-06-21,2020,2020-06-21,2020-09,58,9,1947-1964,Closed,Article,"da Silva, Giovanni L. F.; Diniz, Petterson S.; Ferreira, Jonnison L.; França, João V. F.; Silva, Aristófanes C.; de Paiva, Anselmo C.; de Cavalcanti, Elton A. A.","da Silva, Giovanni L. F. (Applied Computing Group - NCA, Federal University of Maranhão - UFMA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); Diniz, Petterson S. (Applied Computing Group - NCA, Federal University of Maranhão - UFMA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); Ferreira, Jonnison L. (Applied Computing Group - NCA, Federal University of Maranhão - UFMA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); França, João V. F. (Applied Computing Group - NCA, Federal University of Maranhão - UFMA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); Silva, Aristófanes C. (Applied Computing Group - NCA, Federal University of Maranhão - UFMA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); de Paiva, Anselmo C. (Applied Computing Group - NCA, Federal University of Maranhão - UFMA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); de Cavalcanti, Elton A. A. (Applied Computing Group - NCA, Federal University of Maranhão - UFMA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil)","da Silva, Giovanni L. F. (Federal University of Maranhão)","da Silva, Giovanni L. F. (Federal University of Maranhão); Diniz, Petterson S. (Federal University of Maranhão); Ferreira, Jonnison L. (Federal University of Maranhão); França, João V. F. (Federal University of Maranhão); Silva, Aristófanes C. (Federal University of Maranhão); de Paiva, Anselmo C. (Federal University of Maranhão); de Cavalcanti, Elton A. A. (Federal University of Maranhão)",13,13,1.34,6.44,,https://app.dimensions.ai/details/publication/pub.1128664700,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,,
4061,pub.1001679750,10.1118/1.4917481,25979040,,Biomechanical modeling constrained surface‐based image registration for prostate MR guided TRUS biopsy,"PURPOSE: Adding magnetic resonance (MR)-derived information to standard transrectal ultrasound (TRUS) images for guiding prostate biopsy is of substantial clinical interest. A tumor visible on MR images can be projected on ultrasound (US) by using MR-US registration. A common approach is to use surface-based registration. The authors hypothesize that biomechanical modeling will better control deformation inside the prostate than a regular nonrigid surface-based registration method. The authors developed a novel method by extending a nonrigid surface-based registration algorithm with biomechanical finite element (FE) modeling to better predict internal deformations of the prostate.
METHODS: Data were collected from ten patients and the MR and TRUS images were rigidly registered to anatomically align prostate orientations. The prostate was manually segmented in both images and corresponding surface meshes were generated. Next, a tetrahedral volume mesh was generated from the MR image. Prostate deformations due to the TRUS probe were simulated using the surface displacements as the boundary condition. A three-dimensional thin-plate spline deformation field was calculated by registering the mesh vertices. The target registration errors (TREs) of 35 reference landmarks determined by surface and volume mesh registrations were compared.
RESULTS: The median TRE of a surface-based registration with biomechanical regularization was 2.76 (0.81-7.96) mm. This was significantly different than the median TRE of 3.47 (1.05-7.80) mm for regular surface-based registration without biomechanical regularization.
CONCLUSIONS: Biomechanical FE modeling has the potential to improve the accuracy of multimodal prostate registration when comparing it to a regular nonrigid surface-based registration algorithm and can help to improve the effectiveness of MR guided TRUS biopsy procedures.",,,Medical Physics,,"Algorithms; Computer Simulation; Finite Element Analysis; Humans; Image-Guided Biopsy; Magnetic Resonance Imaging; Male; Models, Biological; Prostate; Prostatic Neoplasms; Ultrasonography",2015-04-21,2015,2015-04-21,2015-04-21,42,5,2470-2481,All OA, Green,Article,"van de Ven, Wendy J M; Hu, Yipeng; Barentsz, Jelle O; Karssemeijer, Nico; Barratt, Dean; Huisman, Henkjan J","van de Ven, Wendy J M (Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen 6525 GA, The Netherlands.); Hu, Yipeng (Centre for Medical Image Computing, University College London, London WC1E 6BT, United Kingdom.); Barentsz, Jelle O (Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen 6525 GA, The Netherlands.); Karssemeijer, Nico (Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen 6525 GA, The Netherlands.); Barratt, Dean (Centre for Medical Image Computing, University College London, London WC1E 6BT, United Kingdom.); Huisman, Henkjan J (Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen 6525 GA, The Netherlands.)",,"van de Ven, Wendy J M (Radboud University Nijmegen Medical Centre); Hu, Yipeng (University College London); Barentsz, Jelle O (Radboud University Nijmegen Medical Centre); Karssemeijer, Nico (Radboud University Nijmegen Medical Centre); Barratt, Dean (University College London); Huisman, Henkjan J (Radboud University Nijmegen Medical Centre)",17,3,0.74,10.46,https://repository.ubn.ru.nl/bitstream/handle/2066/151152/3/151152.pdf,https://app.dimensions.ai/details/publication/pub.1001679750,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
4047,pub.1059031501,10.1088/0031-9155/61/16/6085,27461085,,A combinatorial Bayesian and Dirichlet model for prostate MR image segmentation using probabilistic image features,"Blurred boundaries and heterogeneous intensities make accurate prostate MR image segmentation problematic. To improve prostate MR image segmentation we suggest an approach that includes: (a) an image patch division method to partition the prostate into homogeneous segments for feature extraction; (b) an image feature formulation and classification method, using the relevance vector machine, to provide probabilistic prior knowledge for graph energy construction; (c) a graph energy formulation scheme with Bayesian priors and Dirichlet graph energy and (d) a non-iterative graph energy minimization scheme, based on matrix differentiation, to perform the probabilistic pixel membership optimization. The segmentation output was obtained by assigning pixels with foreground and background labels based on derived membership probabilities. We evaluated our approach on the PROMISE-12 dataset with 50 prostate MR image volumes. Our approach achieved a mean dice similarity coefficient (DSC) of 0.90  ±  0.02, which surpassed the five best prior-based methods in the PROMISE-12 segmentation challenge.",,,Physics in Medicine and Biology,,"Algorithms; Bayes Theorem; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Models, Statistical; Prostate",2016-07-27,2016,2016-07-27,2016-08-21,61,16,6085-6104,All OA, Green,Article,"Li, Ang; Li, Changyang; Wang, Xiuying; Eberl, Stefan; Feng, Dagan; Fulham, Michael","Li, Ang (Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, Australia); Li, Changyang (Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, Australia); Wang, Xiuying (Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, Australia); Eberl, Stefan (Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, Australia; Department of PET and Nuclear Medicine, Royal Prince Alfred Hospital, Sydney, Australia); Feng, Dagan (Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, Australia); Fulham, Michael (Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, Australia; Department of PET and Nuclear Medicine, Royal Prince Alfred Hospital, Sydney, Australia)",,"Li, Ang (The University of Sydney); Li, Changyang (The University of Sydney); Wang, Xiuying (The University of Sydney); Eberl, Stefan (The University of Sydney; Royal Prince Alfred Hospital); Feng, Dagan (The University of Sydney); Fulham, Michael (The University of Sydney; Royal Prince Alfred Hospital)",4,0,0.19,3.13,https://ses.library.usyd.edu.au/bitstream/2123/21808/1/Li_2016_Phys._Med._Biol._61_6085.pdf,https://app.dimensions.ai/details/publication/pub.1059031501,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
4028,pub.1132740777,10.1055/a-1290-8070,33212541,,Comparison of Prostate MRI Lesion Segmentation Agreement Between Multiple Radiologists and a Fully Automatic Deep Learning System,"PURPOSE:  A recently developed deep learning model (U-Net) approximated the clinical performance of radiologists in the prediction of clinically significant prostate cancer (sPC) from prostate MRI. Here, we compare the agreement between lesion segmentations by U-Net with manual lesion segmentations performed by different radiologists.
MATERIALS AND METHODS:  165 patients with suspicion for sPC underwent targeted and systematic fusion biopsy following 3 Tesla multiparametric MRI (mpMRI). Five sets of segmentations were generated retrospectively: segmentations of clinical lesions, independent segmentations by three radiologists, and fully automated bi-parametric U-Net segmentations. Per-lesion agreement was calculated for each rater by averaging Dice coefficients with all overlapping lesions from other raters. Agreement was compared using descriptive statistics and linear mixed models.
RESULTS:  The mean Dice coefficient for manual segmentations showed only moderate agreement at 0.48-0.52, reflecting the difficult visual task of determining the outline of otherwise jointly detected lesions. U-net segmentations were significantly smaller than manual segmentations (p < 0.0001) and exhibited a lower mean Dice coefficient of 0.22, which was significantly lower compared to manual segmentations (all p < 0.0001). These differences remained after correction for lesion size and were unaffected between sPC and non-sPC lesions and between peripheral and transition zone lesions.
CONCLUSION:  Knowledge of the order of agreement of manual segmentations of different radiologists is important to set the expectation value for artificial intelligence (AI) systems in the task of prostate MRI lesion segmentation. Perfect agreement (Dice coefficient of one) should not be expected for AI. Lower Dice coefficients of U-Net compared to manual segmentations are only partially explained by smaller segmentation sizes and may result from a focus on the lesion core and a small relative lesion center shift. Although it is primarily important that AI detects sPC correctly, the Dice coefficient for overlapping lesions from multiple raters can be used as a secondary measure for segmentation quality in future studies.
KEY POINTS:   · Intermediate human Dice coefficients reflect the difficulty of outlining jointly detected lesions.. · Lower Dice coefficients of deep learning motivate further research to approximate human perception.. · Comparable predictive performance of deep learning appears independent of Dice agreement.. · Dice agreement independent of significant cancer presence indicates indistinguishability of some benign imaging findings.. · Improving DWI to T2 registration may improve the observed U-Net Dice coefficients..
CITATION FORMAT: · Schelb P, Tavakoli AA, Tubtawee T et al. Comparison of Prostate MRI Lesion Segmentation Agreement Between Multiple Radiologists and a Fully Automatic Deep Learning System. Fortschr Röntgenstr 2021; 193: 559 - 573.",,,RöFo,,Artificial Intelligence, Deep Learning, Humans, Magnetic Resonance Imaging, Male, Prostate, Radiologists, Retrospective Studies,2020-11-19,2020,2020-11-19,2021-05,193,5,559-573,All OA, Bronze,Article,"Schelb, Patrick; Tavakoli, Anoshirwan; Tubtawee, Teeravut; Hielscher, Thomas; Radtke, Jan-Philipp; Görtz, Magdalena; Schütz, Viktoria; Kuder, Tristan; Schimmöller, Lars; Stenzinger, Albrecht; Hohenfellner, Markus; Schlemmer, Heinz-Peter; Bonekamp, David","Schelb, Patrick (Division of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany); Tavakoli, Anoshirwan (Division of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany); Tubtawee, Teeravut (Division of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany); Hielscher, Thomas (Division of Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany); Radtke, Jan-Philipp (Department of Urology, University of Heidelberg Medical Center, Heidelberg, Germany); Görtz, Magdalena (Department of Urology, University of Heidelberg Medical Center, Heidelberg, Germany); Schütz, Viktoria (Department of Urology, University of Heidelberg Medical Center, Heidelberg, Germany); Kuder, Tristan (Division of Medical Physics, German Cancer Research Center (DKFZ), Heidelberg, Germany); Schimmöller, Lars (University Dusseldorf, Medical Faculty, Department of Diagnostic and Interventional Radiology, Dusseldorf, Germany); Stenzinger, Albrecht (Institute of Pathology, University of Heidelberg Medical Center, Heidelberg, Germany); Hohenfellner, Markus (Department of Urology, University of Heidelberg Medical Center, Heidelberg, Germany); Schlemmer, Heinz-Peter (Division of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany); Bonekamp, David (Division of Radiology, German Cancer Research Center (DKFZ), Heidelberg, Germany)",,"Schelb, Patrick (German Cancer Research Center); Tavakoli, Anoshirwan (German Cancer Research Center); Tubtawee, Teeravut (German Cancer Research Center); Hielscher, Thomas (German Cancer Research Center); Radtke, Jan-Philipp (University Hospital Heidelberg); Görtz, Magdalena (University Hospital Heidelberg); Schütz, Viktoria (University Hospital Heidelberg); Kuder, Tristan (German Cancer Research Center); Schimmöller, Lars (Heinrich Heine University Düsseldorf); Stenzinger, Albrecht (University Hospital Heidelberg); Hohenfellner, Markus (University Hospital Heidelberg); Schlemmer, Heinz-Peter (German Cancer Research Center); Bonekamp, David (German Cancer Research Center)",17,17,3.8,7.24,http://www.thieme-connect.de/products/ejournals/pdf/10.1055/a-1290-8070.pdf,https://app.dimensions.ai/details/publication/pub.1132740777,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,
4028,pub.1020582697,10.1016/j.ultrasmedbio.2014.09.019,25542486,,User-Guided Segmentation of Preterm Neonate Ventricular System from 3-D Ultrasound Images Using Convex Optimization,"A three-dimensional (3-D) ultrasound (US) system has been developed to monitor the intracranial ventricular system of preterm neonates with intraventricular hemorrhage (IVH) and the resultant dilation of the ventricles (ventriculomegaly). To measure ventricular volume from 3-D US images, a semi-automatic convex optimization-based approach is proposed for segmentation of the cerebral ventricular system in preterm neonates with IVH from 3-D US images. The proposed semi-automatic segmentation method makes use of the convex optimization technique supervised by user-initialized information. Experiments using 58 patient 3-D US images reveal that our proposed approach yielded a mean Dice similarity coefficient of 78.2% compared with the surfaces that were manually contoured, suggesting good agreement between these two segmentations. Additional metrics, the mean absolute distance of 0.65 mm and the maximum absolute distance of 3.2 mm, indicated small distance errors for a voxel spacing of 0.22 × 0.22 × 0.22 mm(3). The Pearson correlation coefficient (r = 0.97, p < 0.001) indicated a significant correlation of algorithm-generated ventricular system volume (VSV) with the manually generated VSV. The calculated minimal detectable difference in ventricular volume change indicated that the proposed segmentation approach with 3-D US images is capable of detecting a VSV difference of 6.5 cm(3) with 95% confidence, suggesting that this approach might be used for monitoring IVH patients' ventricular changes using 3-D US imaging. The mean segmentation times of the graphics processing unit (GPU)- and central processing unit-implemented algorithms were 50 ± 2 and 205 ± 5 s for one 3-D US image, respectively, in addition to 120 ± 10 s for initialization, less than the approximately 35 min required by manual segmentation. In addition, repeatability experiments indicated that the intra-observer variability ranges from 6.5% to 7.5%, and the inter-observer variability is 8.5% in terms of the coefficient of variation of the Dice similarity coefficient. The intra-class correlation coefficient for ventricular system volume measurements for each independent observer ranged from 0.988 to 0.996 and was 0.945 for three different observers. The coefficient of variation and intra-class correlation coefficient revealed that the intra- and inter-observer variability of the proposed approach introduced by the user initialization was small, indicating good reproducibility, independent of different users.","The authors are grateful for the funding support from the Ontario Research Fund (ORF), the Canada Research Chairs (CRC) Program, and Academic Medical Organization of Southwestern Ontario (AMOSO).",,Ultrasound in Medicine & Biology,,"Algorithms; Cerebral Hemorrhage; Cerebral Ventricles; Humans; Imaging, Three-Dimensional; Infant, Newborn; Infant, Premature; Observer Variation; Reproducibility of Results; Ultrasonography",2014-12-23,2014,2014-12-23,2015-02,41,2,542-556,Closed,Article,"Qiu, Wu; Yuan, Jing; Kishimoto, Jessica; McLeod, Jonathan; Chen, Yimin; de Ribaupierre, Sandrine; Fenster, Aaron","Qiu, Wu (Robarts Research Institute, University of Western Ontario, London, Ontario, Canada); Yuan, Jing (Robarts Research Institute, University of Western Ontario, London, Ontario, Canada); Kishimoto, Jessica (Robarts Research Institute, University of Western Ontario, London, Ontario, Canada); McLeod, Jonathan (Robarts Research Institute, University of Western Ontario, London, Ontario, Canada); Chen, Yimin (Department of Electronic Engineering, City University of Hong Kong, Hong Kong, China); de Ribaupierre, Sandrine (Neurosurgery, Department of Clinical Neurologic Sciences, University of Western Ontario, London, Ontario, Canada); Fenster, Aaron (Robarts Research Institute, University of Western Ontario, London, Ontario, Canada)","Qiu, Wu (Western University)","Qiu, Wu (Western University); Yuan, Jing (Western University); Kishimoto, Jessica (Western University); McLeod, Jonathan (Western University); Chen, Yimin (City University of Hong Kong); de Ribaupierre, Sandrine (Western University); Fenster, Aaron (Western University)",27,3,1.28,7.82,,https://app.dimensions.ai/details/publication/pub.1020582697,32 Biomedical and Clinical Sciences, 3213 Paediatrics,,,,,,,,,,,
3940,pub.1152680879,10.1016/j.zemedi.2022.10.005,36376203,,The use of deep learning in interventional radiotherapy (brachytherapy): A review with a focus on open source and open data,"Deep learning advanced to one of the most important technologies in almost all medical fields. Especially in areas, related to medical imaging it plays a big role. However, in interventional radiotherapy (brachytherapy) deep learning is still in an early phase. In this review, first, we investigated and scrutinised the role of deep learning in all processes of interventional radiotherapy and directly related fields. Additionally, we summarised the most recent developments. For better understanding, we provide explanations of key terms and approaches to solving common deep learning problems. To reproduce results of deep learning algorithms both source code and training data must be available. Therefore, a second focus of this work is on the analysis of the availability of open source, open data and open models. In our analysis, we were able to show that deep learning plays already a major role in some areas of interventional radiotherapy, but is still hardly present in others. Nevertheless, its impact is increasing with the years, partly self-propelled but also influenced by closely related fields. Open source, data and models are growing in number but are still scarce and unevenly distributed among different research groups. The reluctance in publishing code, data and models limits reproducibility and restricts evaluation to mono-institutional datasets. The conclusion of our analysis is that deep learning can positively change the workflow of interventional radiotherapy but there is still room for improvements when it comes to reproducible results and standardised evaluation methods.",,,Zeitschrift für Medizinische Physik,,,2022-11-11,2022,2022-11-11,2022-11,,,,All OA, Hybrid,Article,"Fechter, Tobias; Sachpazidis, Ilias; Baltas, Dimos","Fechter, Tobias (Division of Medical Physics, Department of Radiation Oncology, Medical Center University of Freiburg, Germany; Faculty of Medicine, University of Freiburg, Germany; German Cancer Consortium (DKTK), Partner Site Freiburg, Germany. Electronic address: tobias.fechter@uniklinik-freiburg.de.); Sachpazidis, Ilias (Division of Medical Physics, Department of Radiation Oncology, Medical Center University of Freiburg, Germany; Faculty of Medicine, University of Freiburg, Germany; German Cancer Consortium (DKTK), Partner Site Freiburg, Germany.); Baltas, Dimos (Division of Medical Physics, Department of Radiation Oncology, Medical Center University of Freiburg, Germany; Faculty of Medicine, University of Freiburg, Germany; German Cancer Consortium (DKTK), Partner Site Freiburg, Germany.)","Fechter, Tobias (University Medical Center Freiburg; University of Freiburg; German Cancer Research Center)","Fechter, Tobias (University Medical Center Freiburg; University of Freiburg; German Cancer Research Center); Sachpazidis, Ilias (University Medical Center Freiburg; University of Freiburg; German Cancer Research Center); Baltas, Dimos (University Medical Center Freiburg; University of Freiburg; German Cancer Research Center)",0,0,,,https://doi.org/10.1016/j.zemedi.2022.10.005,https://app.dimensions.ai/details/publication/pub.1152680879,32 Biomedical and Clinical Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
3856,pub.1110670776,10.1117/1.jmi.5.4.044507,30840728,PMC6294844,Prostate cancer detection from multi-institution multiparametric MRIs using deep convolutional neural networks,"Multiparametric magnetic resonance imaging (mpMRI) of the prostate aids in early diagnosis of prostate cancer, but is difficult to interpret and subject to interreader variability. Our objective is to generate probability maps, overlaid on original mpMRI images to help radiologists identify where a cancer is suspected as a computer-aided diagnostic (CAD). We optimized the holistically nested edge detection (HED) deep convolutional neural network. Our dataset contains T2, apparent diffusion coefficient, and high b  -value images from 186 patients across six institutions worldwide: 92 with an endorectal coil (ERC) and 94 without. Ground-truth was based on tumor segmentations manually drawn by expert radiologists based on histologic evidence of cancer. The training set consisted of 120 patients and the validation set and test set included 19 and 47, respectively. Slice-level probability maps are evaluated at the lesion level of analysis. The best model: HED using 5 × 5  convolutional kernels, batch normalization, and optimized using Adam. This CAD performed significantly better ( p < 0.001  ) in the peripheral zone ( AUC = 0.94 ± 0.01  ) than the transition zone. It outperforms a previous CAD from our group in a head-to-head comparison on the same ERC-only test cases ( AUC = 0.97 ± 0.01  ; p < 0.001  ). Our CAD establishes a state-of-the-art performance for predicting prostate cancer lesions on mpMRIs.","This research was funded by the Intramural Research Programs of the National Institutes of Health, Clinical Center and National Cancer Institute. We thank NVIDIA for GPU card donation.",,Journal of Medical Imaging,,,2018-10,2018,2018-12-15,2018-10,5,4,044507-044507,All OA, Green,Article,"Sumathipala, Yohan; Lay, Nathan; Turkbey, Baris; Smith, Clayton; Choyke, Peter L.; Summers, Ronald M.","Sumathipala, Yohan (National Institutes of Health Clinical Center, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Bethesda, Maryland, United States); Lay, Nathan (National Institutes of Health Clinical Center, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Bethesda, Maryland, United States); Turkbey, Baris (National Institutes of Health, National Cancer Institute, Molecular Imaging Program, Bethesda, Maryland, United States); Smith, Clayton (National Institutes of Health, National Cancer Institute, Molecular Imaging Program, Bethesda, Maryland, United States); Choyke, Peter L. (National Institutes of Health, National Cancer Institute, Molecular Imaging Program, Bethesda, Maryland, United States); Summers, Ronald M. (National Institutes of Health Clinical Center, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Bethesda, Maryland, United States)","Summers, Ronald M. (National Institutes of Health Clinical Center)","Sumathipala, Yohan (National Institutes of Health Clinical Center); Lay, Nathan (National Institutes of Health Clinical Center); Turkbey, Baris (National Cancer Institute); Smith, Clayton (National Cancer Institute); Choyke, Peter L. (National Cancer Institute); Summers, Ronald M. (National Institutes of Health Clinical Center)",42,25,1.71,12.26,https://europepmc.org/articles/pmc6294844?pdf=render,https://app.dimensions.ai/details/publication/pub.1110670776,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
3852,pub.1107516224,10.1109/tnnls.2018.2870182,30307879,PMC6550324,STRAINet: Spatially Varying sTochastic Residual AdversarIal Networks for MRI Pelvic Organ Segmentation,"Accurate segmentation of pelvic organs is important for prostate radiation therapy. Modern radiation therapy starts to use a magnetic resonance image (MRI) as an alternative to computed tomography image because of its superior soft tissue contrast and also free of risk from radiation exposure. However, segmentation of pelvic organs from MRI is a challenging problem due to inconsistent organ appearance across patients and also large intrapatient anatomical variations across treatment days. To address such challenges, we propose a novel deep network architecture, called ""Spatially varying sTochastic Residual AdversarIal Network"" (STRAINet), to delineate pelvic organs from MRI in an end-to-end fashion. Compared to the traditional fully convolutional networks (FCN), the proposed architecture has two main contributions: 1) inspired by the recent success of residual learning, we propose an evolutionary version of the residual unit, i.e., stochastic residual unit, and use it to the plain convolutional layers in the FCN. We further propose long-range stochastic residual connections to pass features from shallow layers to deep layers; and 2) we propose to integrate three previously proposed network strategies to form a new network for better medical image segmentation: a) we apply dilated convolution in the smallest resolution feature maps, so that we can gain a larger receptive field without overly losing spatial information; b) we propose a spatially varying convolutional layer that adapts convolutional filters to different regions of interest; and c) an adversarial network is proposed to further correct the segmented organ structures. Finally, STRAINet is used to iteratively refine the segmentation probability maps in an autocontext manner. Experimental results show that our STRAINet achieved the state-of-the-art segmentation accuracy. Further analysis also indicates that our proposed network components contribute most to the performance.",This work was supported by the National Institutes of Health under Grant R01 CA206100. The authors would like to thank our colleague Dr. Shaun Stone for helping check the grammar issues.,,IEEE Transactions on Neural Networks and Learning Systems,,"Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Stochastic Processes",2018-10-09,2018,2018-10-09,2019-05,30,5,1552-1564,All OA, Green,Article,"Nie, Dong; Wang, Li; Gao, Yaozong; Lian, Jun; Shen, Dinggang","Nie, Dong (Department of Computer Science, The University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA; Department of Radiology, BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Wang, Li (Department of Radiology, BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Gao, Yaozong (Shanghai United Imaging Intelligence Company Ltd., Shanghai, 201807, China); Lian, Jun (Department of Radiation Oncology, The University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA); Shen, Dinggang (Department of Radiology, BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA; Department of Brain and Cognitive Engineering, Korea University, Seoul, 02841, South Korea)","Nie, Dong (University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill)","Nie, Dong (University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill); Wang, Li (University of North Carolina at Chapel Hill); Gao, Yaozong (); Lian, Jun (University of North Carolina at Chapel Hill); Shen, Dinggang (University of North Carolina at Chapel Hill; Korea University)",36,23,2.13,11.36,https://europepmc.org/articles/pmc6550324?pdf=render,https://app.dimensions.ai/details/publication/pub.1107516224,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
3852,pub.1059030569,10.1088/0031-9155/59/23/7245,25383566,,Automatic hip cartilage segmentation from 3D MR images using arc-weighted graph searching,"Accurate segmentation of hip joint cartilage from magnetic resonance (MR) images offers opportunities for quantitative investigations of pathoanatomical conditions such as osteoarthritis. In this paper, we present a fully automatic scheme for the segmentation of the individual femoral and acetabular cartilage plates in the human hip joint from high-resolution 3D MR images. The developed scheme uses an improved optimal multi-object multi-surface graph search framework with an arc-weighted graph representation that incorporates prior morphological knowledge as a basis for segmentation of the individual femoral and acetabular cartilage plates despite weak or incomplete boundary interfaces. This automated scheme was validated against manual segmentations from 3D true fast imaging with steady-state precession (TrueFISP) MR examinations of the right hip joints in 52 asymptomatic volunteers. Compared with expert manual segmentations of the combined, femoral and acetabular cartilage volumes, the automatic scheme obtained mean (± standard deviation) Dice's similarity coefficients of 0.81 (± 0.03), 0.79 (± 0.03) and 0.72 (± 0.05). The corresponding mean absolute volume difference errors were 8.44% (± 6.36), 9.44% (± 7.19) and 9.05% (± 8.02). The mean absolute differences between manual and automated measures of cartilage thickness for femoral and acetabular cartilage plates were 0.13 mm (± 0.12) and 0.11 mm (± 0.11), respectively.",This research was supported by funding from the Australian Research Council’s Linkage Projects scheme LP100200422. The authors would like to thank Drs Jason Dowling and Soumya Ghose for their help with this work.,,Physics in Medicine and Biology,,"Algorithms; Cartilage, Articular; Hip Joint; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Osteoarthritis",2014-11-10,2014,2014-11-10,2014-12-07,59,23,7245-7266,Closed,Article,"Xia, Ying; Chandra, Shekhar S; Engstrom, Craig; Strudwick, Mark W; Crozier, Stuart; Fripp, Jurgen","Xia, Ying (School of Information Technology and Electrical Engineering, The University of Queensland, St. Lucia, QLD 4027, Australia; CSIRO Digital Productivity and Services Flagship, The Australian e-Health Research Centre, Brisbane QLD 4029, Australia); Chandra, Shekhar S (School of Information Technology and Electrical Engineering, The University of Queensland, St. Lucia, QLD 4027, Australia); Engstrom, Craig (School of Human Movement Studies, The University of Queensland, St. Lucia, QLD 4027, Australia); Strudwick, Mark W (School of Information Technology and Electrical Engineering, The University of Queensland, St. Lucia, QLD 4027, Australia); Crozier, Stuart (School of Information Technology and Electrical Engineering, The University of Queensland, St. Lucia, QLD 4027, Australia); Fripp, Jurgen (CSIRO Digital Productivity and Services Flagship, The Australian e-Health Research Centre, Brisbane QLD 4029, Australia)",,"Xia, Ying (University of Queensland; Australian e-Health Research Centre); Chandra, Shekhar S (University of Queensland); Engstrom, Craig (University of Queensland); Strudwick, Mark W (University of Queensland); Crozier, Stuart (University of Queensland); Fripp, Jurgen (Australian e-Health Research Centre)",32,7,1.76,20.58,,https://app.dimensions.ai/details/publication/pub.1059030569,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
3849,pub.1061696770,10.1109/tmi.2016.2578680,27305669,,Cloud-Based Evaluation of Anatomical Structure Segmentation and Landmark Detection Algorithms: VISCERAL Anatomy Benchmarks,"Variations in the shape and appearance of anatomical structures in medical images are often relevant radiological signs of disease. Automatic tools can help automate parts of this manual process. A cloud-based evaluation framework is presented in this paper including results of benchmarking current state-of-the-art medical imaging algorithms for anatomical structure segmentation and landmark detection: the VISCERAL Anatomy benchmarks. The algorithms are implemented in virtual machines in the cloud where participants can only access the training data and can be run privately by the benchmark administrators to objectively compare their performance in an unseen common test set. Overall, 120 computed tomography and magnetic resonance patient volumes were manually annotated to create a standard Gold Corpus containing a total of 1295 structures and 1760 landmarks. Ten participants contributed with automatic algorithms for the organ segmentation task, and three for the landmark localization task. Different algorithms obtained the best scores in the four available imaging modalities and for subsets of anatomical structures. The annotation framework, resulting data set, evaluation setup, results and performance analysis from the three VISCERAL Anatomy benchmarks are presented in this article. Both the VISCERAL data set and Silver Corpus generated with the fusion of the participant algorithms on a larger set of non-manually-annotated medical images are available to the research community.","The research leading to these results received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no 318068 (VISCERAL) and 257528 (KHRESMOI). The authors would like to thank Microsoft research for their support on using the Azure cloud for our benchmarks. We also thank all research groups contributing to this work by participating at the VISCERAL Anatomy benchmarks. I. Eggel, A. Foncubierta–Rodríguez, O. Goksel, K. Gruenberg, A. Hanbury, A. Jakab, O. Jimenez-del-Toro, G. Kontokotsios, M. Krenn, G. Langs, B. H. Menze, H. Müller, T. Salas Fernandez, R. Schaer, A. Aziz Taha, A. Walleyo, M.-A. Weber, and M. Winterstein organized the VISCERAL Anatomy benchmarks. All other authors contributed as participants of the three Anatomy benchmarks. O. Jimenez-del-Toro, A. Aziz Taha, M. Krenn, K. Gruenberg, M. Winterstein, and O. Goksel analyzed the results from the benchmarks. O. Jimenez-del-Toro, and A. Aziz Taha wrote the manuscript.",,IEEE Transactions on Medical Imaging,,"Aged; Algorithms; Anatomic Landmarks; Anatomy; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Tomography, X-Ray Computed",2016-06-09,2016,2016-06-09,2016-11,35,11,2459-2475,Closed,Article,"Jimenez-del-Toro, Oscar; Müller, Henning; Krenn, Markus; Gruenberg, Katharina; Taha, Abdel Aziz; Winterstein, Marianne; Eggel, Ivan; Foncubierta-Rodríguez, Antonio; Goksel, Orcun; Jakab, András; Kontokotsios, Georgios; Langs, Georg; Menze, Bjoern H.; Fernandez, Tomàs Salas; Schaer, Roger; Walleyo, Anna; Weber, Marc-André; Cid, Yashin Dicente; Gass, Tobias; Heinrich, Mattias; Jia, Fucang; Kahl, Fredrik; Kechichian, Razmig; Mai, Dominic; Spanier, Assaf B.; Vincent, Graham; Wang, Chunliang; Wyeth, Daniel; Hanbury, Allan","Jimenez-del-Toro, Oscar (University of Applied Sciences Western Switzerland, 3960, Sierre, Switzerland; University Hospital and University of Geneva, 1205, Geneva, Switzerland); Müller, Henning (University of Applied Sciences Western Switzerland, 3960, Sierre, Switzerland; University Hospital and University of Geneva, 1205, Geneva, Switzerland); Krenn, Markus (Medical University of Vienna, 1090, Vienna, Austria); Gruenberg, Katharina (Heidelberg University Hospital, 69120, Heidelberg, Germany); Taha, Abdel Aziz (Vienna University of Technology, 1040, Vienna, Austria); Winterstein, Marianne (Heidelberg University Hospital, 69120, Heidelberg, Germany); Eggel, Ivan (University of Applied Sciences Western Switzerland, 3960, Sierre, Switzerland); Foncubierta-Rodríguez, Antonio (Swiss Federal Institute of Technology (ETH) Zurich, 8092, Zurich, Switzerland); Goksel, Orcun (Swiss Federal Institute of Technology (ETH) Zurich, 8092, Zurich, Switzerland); Jakab, András (Medical University of Vienna, 1090, Vienna, Austria); Kontokotsios, Georgios (Vienna University of Technology, 1040, Vienna, Austria); Langs, Georg (Medical University of Vienna, 1090, Vienna, Austria); Menze, Bjoern H. (Swiss Federal Institute of Technology (ETH) Zurich, 8092, Zurich, Switzerland); Fernandez, Tomàs Salas (Agency for Health Quality and Assessment of Catalonia, 08005, Barcelona, Spain); Schaer, Roger (University of Applied Sciences Western Switzerland, 3960, Sierre, Switzerland); Walleyo, Anna (Heidelberg University Hospital, 69120, Heidelberg, Germany); Weber, Marc-André (Heidelberg University Hospital, 69120, Heidelberg, Germany); Cid, Yashin Dicente (University of Applied Sciences Western Switzerland, 3960, Sierre, Switzerland; University Hospital and University of Geneva, 1205, Geneva, Switzerland); Gass, Tobias (Swiss Federal Institute of Technology (ETH) Zurich, 8092, Zurich, Switzerland); Heinrich, Mattias (University of Lübeck, 23562, Lübeck, Germany); Jia, Fucang (Shenzhen Intitutes of Advanced Technology, Chinese Academy of Sciences, Beijing, 100864, China); Kahl, Fredrik (Chalmers University of Technology, 412 58, Göteborg, Sweden); Kechichian, Razmig (University of Lyon, 69007, Lyon, France); Mai, Dominic (University of Freiburg, 79085, Freiburg, Germany); Spanier, Assaf B. (The Hebrew University of Jerusalem, 9190401, Jerusalem, Israel); Vincent, Graham (Imorphics, M15 6SE, Manchester, U.K.); Wang, Chunliang (KTH–Royal Institute of Technology, 114 28, Stockholm, Sweden); Wyeth, Daniel (Toshiba Medical Visualization Systems Europe, EH6 5NP, Edinburgh, U.K.); Hanbury, Allan (Vienna University of Technology, 1040, Vienna, Austria)","Jimenez-del-Toro, Oscar (HES-SO Valais-Wallis; University Hospital of Geneva)","Jimenez-del-Toro, Oscar (HES-SO Valais-Wallis; University Hospital of Geneva); Müller, Henning (HES-SO Valais-Wallis; University Hospital of Geneva); Krenn, Markus (Medical University of Vienna); Gruenberg, Katharina (University Hospital Heidelberg); Taha, Abdel Aziz (TU Wien); Winterstein, Marianne (University Hospital Heidelberg); Eggel, Ivan (HES-SO Valais-Wallis); Foncubierta-Rodríguez, Antonio (ETH Zurich); Goksel, Orcun (ETH Zurich); Jakab, András (Medical University of Vienna); Kontokotsios, Georgios (TU Wien); Langs, Georg (Medical University of Vienna); Menze, Bjoern H. (ETH Zurich); Fernandez, Tomàs Salas (); Schaer, Roger (HES-SO Valais-Wallis); Walleyo, Anna (University Hospital Heidelberg); Weber, Marc-André (University Hospital Heidelberg); Cid, Yashin Dicente (HES-SO Valais-Wallis; University Hospital of Geneva); Gass, Tobias (ETH Zurich); Heinrich, Mattias (University of Lübeck); Jia, Fucang (Chinese Academy of Sciences); Kahl, Fredrik (Chalmers University of Technology); Kechichian, Razmig (University of Lyon System); Mai, Dominic (University of Freiburg); Spanier, Assaf B. (Hebrew University of Jerusalem); Vincent, Graham (); Wang, Chunliang (Royal Institute of Technology); Wyeth, Daniel (); Hanbury, Allan (TU Wien)",119,44,2.84,32.08,,https://app.dimensions.ai/details/publication/pub.1061696770,46 Information and Computing Sciences, 4602 Artificial Intelligence,,,,,,,,,,,
3848,pub.1114311936,10.1038/s41585-019-0193-3,31092914,,A new era: artificial intelligence and machine learning in prostate cancer,"Artificial intelligence (AI) — the ability of a machine to perform cognitive tasks to achieve a particular goal based on provided data — is revolutionizing and reshaping our health-care systems. The current availability of ever-increasing computational power, highly developed pattern recognition algorithms and advanced image processing software working at very high speeds has led to the emergence of computer-based systems that are trained to perform complex tasks in bioinformatics, medical imaging and medical robotics. Accessibility to ‘big data’ enables the ‘cognitive’ computer to scan billions of bits of unstructured information, extract the relevant information and recognize complex patterns with increasing confidence. Computer-based decision-support systems based on machine learning (ML) have the potential to revolutionize medicine by performing complex tasks that are currently assigned to specialists to improve diagnostic accuracy, increase efficiency of throughputs, improve clinical workflow, decrease human resource costs and improve treatment choices. These characteristics could be especially helpful in the management of prostate cancer, with growing applications in diagnostic imaging, surgical interventions, skills training and assessment, digital pathology and genomics. Medicine must adapt to this changing world, and urologists, oncologists, radiologists and pathologists, as high-volume users of imaging and pathology, need to understand this burgeoning science and acknowledge that the development of highly accurate AI-based decision-support applications of ML will require collaboration between data scientists, computer researchers and engineers.",,,Nature Reviews Urology,,Artificial Intelligence, Humans, Machine Learning, Male, Prostatic Neoplasms,2019-05-15,2019,2019-05-15,2019-07,16,7,391-403,Closed,Article,"Goldenberg, S. Larry; Nir, Guy; Salcudean, Septimiu E.","Goldenberg, S. Larry (Department of Urologic Sciences and the Vancouver Prostate Centre, University of British Columbia, Vancouver, British Columbia, Canada); Nir, Guy (Department of Urologic Sciences and the Vancouver Prostate Centre, University of British Columbia, Vancouver, British Columbia, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, British Columbia, Canada); Salcudean, Septimiu E. (Department of Urologic Sciences and the Vancouver Prostate Centre, University of British Columbia, Vancouver, British Columbia, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, British Columbia, Canada)","Goldenberg, S. Larry (University of British Columbia)","Goldenberg, S. Larry (University of British Columbia); Nir, Guy (University of British Columbia; University of British Columbia); Salcudean, Septimiu E. (University of British Columbia; University of British Columbia)",206,161,11.02,72.77,,https://app.dimensions.ai/details/publication/pub.1114311936,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,3 Good Health and Well Being,,,,,
3830,pub.1004042111,10.1118/1.4906129,25652500,,Rotationally resliced 3D prostate TRUS segmentation using convex optimization with shape priors,"PURPOSE: Efficient and accurate segmentations of 3D end-firing transrectal ultrasound (TRUS) images play an important role in planning of 3D TRUS guided prostate biopsy. However, poor image quality of the input 3D TRUS images, such as strong imaging artifacts and speckles, often makes it a challenging task to extract the prostate boundaries accurately and efficiently.
METHODS: In this paper, the authors propose a novel convex optimization-based approach to delineate the prostate surface from a given 3D TRUS image, which reduces the original 3D segmentation problem to a sequence of simple 2D segmentation subproblems over the rotational reslices of the 3D TRUS volume. Essentially, the authors introduce a novel convex relaxation-based contour evolution approach to each 2D slicewise image segmentation with the joint optimization of shape information, where the learned 2D nonlinear statistical shape prior is incorporated to segment the initial slice, its result is propagated as a shape constraint to the segmentation of the following slices. In practice, the proposed segmentation algorithm is implemented on a GPU to achieve the high computational performance.
RESULTS: Experimental results using 30 patient 3D TRUS images show that the proposed method can achieve a mean Dice similarity coefficient of 93.4% ± 2.2% in 20 s for one 3D image, outperforming the existing local-optimization-based methods, e.g., level-set and active-contour, in terms of accuracy and efficiency. In addition, inter- and intraobserver variability experiments show its good reproducibility.
CONCLUSIONS: A semiautomatic segmentation approach is proposed and evaluated to extract the prostate boundary from 3D TRUS images acquired by a 3D end-firing TRUS guided prostate biopsy system. Experimental results suggest that it may be suitable for the clinical use involving the image guided prostate biopsy procedures.",,,Medical Physics,,"Algorithms; Artifacts; Artificial Intelligence; Humans; Imaging, Three-Dimensional; Male; Principal Component Analysis; Prostate; Reproducibility of Results; Ultrasonography",2015-01-26,2015,2015-01-26,2015-01-26,42,2,877-891,Closed,Article,"Qiu, Wu; Yuan, Jing; Ukwatta, Eranga; Fenster, Aaron","Qiu, Wu (Imaging Research Laboratories, Robarts Research Institute, The University of Western Ontario, London, Ontario N64 5K8, Canada.); Yuan, Jing (Imaging Research Laboratories, Robarts Research Institute, The University of Western Ontario, London, Ontario N64 5K8, Canada.); Ukwatta, Eranga (Department of Biomedical Engineering, Johns Hopkins University, Baltimore, Maryland 21205.); Fenster, Aaron (Imaging Research Laboratories, Robarts Research Institute, The Department of Medical Biophysics, The University of Western Ontario, London, Ontario N64 5K8, Canada.)",,"Qiu, Wu (Western University); Yuan, Jing (Western University); Ukwatta, Eranga (Johns Hopkins University); Fenster, Aaron (Western University)",13,2,0.43,8.0,,https://app.dimensions.ai/details/publication/pub.1004042111,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
3816,pub.1061814637,10.1109/tvcg.2015.2501813,26595923,,Comparative Local Quality Assessment of 3D Medical Image Segmentations with Focus on Statistical Shape Model-Based Algorithms,"The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.",,,IEEE Transactions on Visualization and Computer Graphics,,,2015-11-19,2015,2015-11-19,2016-12,22,12,2537-2549,Closed,Article,"Von Landesberger, Tatiana; Basgier, Dennis; Becker, Meike","Von Landesberger, Tatiana (Tech. Univ. Darmstadt, Darmstadt, Germany); Basgier, Dennis (Tech. Univ. Darmstadt, Darmstadt, Germany); Becker, Meike (Tech. Univ. Darmstadt, Darmstadt, Germany)",,"Von Landesberger, Tatiana (TU Darmstadt); Basgier, Dennis (TU Darmstadt); Becker, Meike (TU Darmstadt)",13,5,0.19,3.57,,https://app.dimensions.ai/details/publication/pub.1061814637,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,,
3816,pub.1052124452,10.1118/1.4899182,25370674,,Spatially varying accuracy and reproducibility of prostate segmentation in magnetic resonance images using manual and semiautomated methods,"PURPOSE: Three-dimensional (3D) prostate image segmentation is useful for cancer diagnosis and therapy guidance, but can be time-consuming to perform manually and involves varying levels of difficulty and interoperator variability within the prostatic base, midgland (MG), and apex. In this study, the authors measured accuracy and interobserver variability in the segmentation of the prostate on T2-weighted endorectal magnetic resonance (MR) imaging within the whole gland (WG), and separately within the apex, midgland, and base regions.
METHODS: The authors collected MR images from 42 prostate cancer patients. Prostate border delineation was performed manually by one observer on all images and by two other observers on a subset of ten images. The authors used complementary boundary-, region-, and volume-based metrics [mean absolute distance (MAD), Dice similarity coefficient (DSC), recall rate, precision rate, and volume difference (ΔV)] to elucidate the different types of segmentation errors that they observed. Evaluation for expert manual and semiautomatic segmentation approaches was carried out. Compared to manual segmentation, the authors' semiautomatic approach reduces the necessary user interaction by only requiring an indication of the anteroposterior orientation of the prostate and the selection of prostate center points on the apex, base, and midgland slices. Based on these inputs, the algorithm identifies candidate prostate boundary points using learned boundary appearance characteristics and performs regularization based on learned prostate shape information.
RESULTS: The semiautomated algorithm required an average of 30 s of user interaction time (measured for nine operators) for each 3D prostate segmentation. The authors compared the segmentations from this method to manual segmentations in a single-operator (mean whole gland MAD = 2.0 mm, DSC = 82%, recall = 77%, precision = 88%, and ΔV = - 4.6 cm(3)) and multioperator study (mean whole gland MAD = 2.2 mm, DSC = 77%, recall = 72%, precision = 86%, and ΔV = - 4.0 cm(3)). These results compared favorably with observed differences between manual segmentations and a simultaneous truth and performance level estimation reference for this data set (whole gland differences as high as MAD = 3.1 mm, DSC = 78%, recall = 66%, precision = 77%, and ΔV = 15.5 cm(3)). The authors found that overall, midgland segmentation was more accurate and repeatable than the segmentation of the apex and base, with the base posing the greatest challenge.
CONCLUSIONS: The main conclusions of this study were that (1) the semiautomated approach reduced interobserver segmentation variability; (2) the segmentation accuracy of the semiautomated approach, as well as the accuracies of recently published methods from other groups, were within the range of observed expert variability in manual prostate segmentation; and (3) further efforts in the development of computer-assisted segmentation would be most productive if focused on improvement of segmentation accuracy and reduction of variability within the prostatic apex and base.",,,Medical Physics,,"Algorithms; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Observer Variation; Pattern Recognition, Automated; Prostate; Prostatic Neoplasms; Reproducibility of Results; Software",2014-11-03,2014,2014-11-03,2014-11-03,41,11,113503,Closed,Article,"Shahedi, Maysam; Cool, Derek W; Romagnoli, Cesare; Bauman, Glenn S; Bastian-Jordan, Matthew; Gibson, Eli; Rodrigues, George; Ahmad, Belal; Lock, Michael; Fenster, Aaron; Ward, Aaron D","Shahedi, Maysam (London Regional Cancer Program, London, Ontario N6A 5W9, Canada; Robarts Research Institute, The University of Western Ontario, London, Ontario N6A 3K7, Canada; and Graduate Program in Biomedical Engineering, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Cool, Derek W (Robarts Research Institute, The University of Western Ontario, London, Ontario N6A 3K7, Canadaand The Department of Medical Imaging, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Romagnoli, Cesare (The Department of Medical Imaging, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Bauman, Glenn S (London Regional Cancer Program, London, Ontario N6A 5W9, Canada; The Department of Medical Biophysics, The University of Western Ontario, London, Ontario N6A 3K7, Canada; and The Department of Oncology, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Bastian-Jordan, Matthew (The Department of Medical Imaging, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Gibson, Eli (Robarts Research Institute, The University of Western Ontario, London, Ontario N6A 3K7, Canada and Graduate Program in Biomedical Engineering, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Rodrigues, George (London Regional Cancer Program, London, Ontario N6A 5W9, Canada and The Department of Oncology, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Ahmad, Belal (London Regional Cancer Program, London, Ontario N6A 5W9, Canada and The Department of Oncology, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Lock, Michael (London Regional Cancer Program, London, Ontario N6A 5W9, Canada and The Department of Oncology, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Fenster, Aaron (Robarts Research Institute, The University of Western Ontario, London, Ontario N6A 3K7, Canada; Graduate Program in Biomedical Engineering, The University of Western Ontario, London, Ontario N6A 3K7, Canada; The Department of Medical Biophysics, The University of Western Ontario, London, Ontario N6A 3K7, Canada; and The Department of Medical Imaging, The University of Western Ontario, London, Ontario N6A 3K7, Canada.); Ward, Aaron D (London Regional Cancer Program, London, Ontario N6A 5W9, Canada; Graduate Program in Biomedical Engineering, The University of Western Ontario, London, Ontario N6A 3K7, Canada; The Department of Medical Biophysics, The University of Western Ontario, London, Ontario N6A 3K7, Canada; and The Department of Oncology, The University of Western Ontario, London, Ontario N6A 3K7, Canada.)",,"Shahedi, Maysam (London Health Sciences Centre; Western University); Cool, Derek W (Western University); Romagnoli, Cesare (Western University); Bauman, Glenn S (London Health Sciences Centre; Western University); Bastian-Jordan, Matthew (Western University); Gibson, Eli (Western University); Rodrigues, George (London Health Sciences Centre; Western University); Ahmad, Belal (London Health Sciences Centre; Western University); Lock, Michael (London Health Sciences Centre; Western University); Fenster, Aaron (Western University); Ward, Aaron D (London Health Sciences Centre; Western University)",14,7,0.39,9.0,,https://app.dimensions.ai/details/publication/pub.1052124452,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
3815,pub.1142805592,10.3390/diagnostics11111964,34829310,PMC8625809,Automatic Segmentation of Pelvic Cancers Using Deep Learning: State-of-the-Art Approaches and Challenges,"The recent rise of deep learning (DL) and its promising capabilities in capturing non-explicit detail from large datasets have attracted substantial research attention in the field of medical image processing. DL provides grounds for technological development of computer-aided diagnosis and segmentation in radiology and radiation oncology. Amongst the anatomical locations where recent auto-segmentation algorithms have been employed, the pelvis remains one of the most challenging due to large intra- and inter-patient soft-tissue variabilities. This review provides a comprehensive, non-systematic and clinically-oriented overview of 74 DL-based segmentation studies, published between January 2016 and December 2020, for bladder, prostate, cervical and rectal cancers on computed tomography (CT) and magnetic resonance imaging (MRI), highlighting the key findings, challenges and limitations.",,,Diagnostics,,,2021-10-22,2021,2021-10-22,,11,11,1964,All OA, Gold,Article,"Kalantar, Reza; Lin, Gigin; Winfield, Jessica M.; Messiou, Christina; Lalondrelle, Susan; Blackledge, Matthew D.; Koh, Dow-Mu","Kalantar, Reza (Division of Radiotherapy and Imaging, The Institute of Cancer Research, London SM2 5NG, UK;, reza.kalantar@icr.ac.uk, (R.K.);, jessica.winfield@icr.ac.uk, (J.M.W.);, christina.messiou@icr.ac.uk, (C.M.);, susan.lalondrelle@rmh.nhs.uk, (S.L.);, mu.koh@icr.ac.uk, (D.-M.K.)); Lin, Gigin (Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital at Linkou and Chang Gung University, 5 Fuhsing St., Guishan, Taoyuan 333, Taiwan;, giginlin@cgmh.org.tw); Winfield, Jessica M. (Division of Radiotherapy and Imaging, The Institute of Cancer Research, London SM2 5NG, UK;, reza.kalantar@icr.ac.uk, (R.K.);, jessica.winfield@icr.ac.uk, (J.M.W.);, christina.messiou@icr.ac.uk, (C.M.);, susan.lalondrelle@rmh.nhs.uk, (S.L.);, mu.koh@icr.ac.uk, (D.-M.K.); Department of Radiology, The Royal Marsden Hospital, London SW3 6JJ, UK); Messiou, Christina (Division of Radiotherapy and Imaging, The Institute of Cancer Research, London SM2 5NG, UK;, reza.kalantar@icr.ac.uk, (R.K.);, jessica.winfield@icr.ac.uk, (J.M.W.);, christina.messiou@icr.ac.uk, (C.M.);, susan.lalondrelle@rmh.nhs.uk, (S.L.);, mu.koh@icr.ac.uk, (D.-M.K.); Department of Radiology, The Royal Marsden Hospital, London SW3 6JJ, UK); Lalondrelle, Susan (Division of Radiotherapy and Imaging, The Institute of Cancer Research, London SM2 5NG, UK;, reza.kalantar@icr.ac.uk, (R.K.);, jessica.winfield@icr.ac.uk, (J.M.W.);, christina.messiou@icr.ac.uk, (C.M.);, susan.lalondrelle@rmh.nhs.uk, (S.L.);, mu.koh@icr.ac.uk, (D.-M.K.); Department of Radiology, The Royal Marsden Hospital, London SW3 6JJ, UK); Blackledge, Matthew D. (Division of Radiotherapy and Imaging, The Institute of Cancer Research, London SM2 5NG, UK;, reza.kalantar@icr.ac.uk, (R.K.);, jessica.winfield@icr.ac.uk, (J.M.W.);, christina.messiou@icr.ac.uk, (C.M.);, susan.lalondrelle@rmh.nhs.uk, (S.L.);, mu.koh@icr.ac.uk, (D.-M.K.)); Koh, Dow-Mu (Division of Radiotherapy and Imaging, The Institute of Cancer Research, London SM2 5NG, UK;, reza.kalantar@icr.ac.uk, (R.K.);, jessica.winfield@icr.ac.uk, (J.M.W.);, christina.messiou@icr.ac.uk, (C.M.);, susan.lalondrelle@rmh.nhs.uk, (S.L.);, mu.koh@icr.ac.uk, (D.-M.K.); Department of Radiology, The Royal Marsden Hospital, London SW3 6JJ, UK)","Blackledge, Matthew D. (Institute of Cancer Research)","Kalantar, Reza (Institute of Cancer Research); Lin, Gigin (Chang Gung University); Winfield, Jessica M. (Institute of Cancer Research; Royal Marsden Hospital); Messiou, Christina (Institute of Cancer Research; Royal Marsden Hospital); Lalondrelle, Susan (Institute of Cancer Research; Royal Marsden Hospital); Blackledge, Matthew D. (Institute of Cancer Research); Koh, Dow-Mu (Institute of Cancer Research; Royal Marsden Hospital)",12,12,2.22,8.69,https://www.mdpi.com/2075-4418/11/11/1964/pdf?version=1634902331,https://app.dimensions.ai/details/publication/pub.1142805592,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
3812,pub.1152771610,10.3390/cancers14225595,36428686,PMC9688370,Artificial Intelligence for Clinical Diagnosis and Treatment of Prostate Cancer,"As medical science and technology progress towards the era of ""big data"", a multi-dimensional dataset pertaining to medical diagnosis and treatment is becoming accessible for mathematical modelling. However, these datasets are frequently inconsistent, noisy, and often characterized by a significant degree of redundancy. Thus, extensive data processing is widely advised to clean the dataset before feeding it into the mathematical model. In this context, Artificial intelligence (AI) techniques, including machine learning (ML) and deep learning (DL) algorithms based on artificial neural networks (ANNs) and their types, are being used to produce a precise and cross-sectional illustration of clinical data. For prostate cancer patients, datasets derived from the prostate-specific antigen (PSA), MRI-guided biopsies, genetic biomarkers, and the Gleason grading are primarily used for diagnosis, risk stratification, and patient monitoring. However, recording diagnoses and further stratifying risks based on such diagnostic data frequently involves much subjectivity. Thus, implementing an AI algorithm on a PC's diagnostic data can reduce the subjectivity of the process and assist in decision making. In addition, AI is used to cut down the processing time and help with early detection, which provides a superior outcome in critical cases of prostate cancer. Furthermore, this also facilitates offering the service at a lower cost by reducing the amount of human labor. Herein, the prime objective of this review is to provide a deep analysis encompassing the existing AI algorithms that are being deployed in the field of prostate cancer (PC) for diagnosis and treatment. Based on the available literature, AI-powered technology has the potential for extensive growth and penetration in PC diagnosis and treatment to ease and expedite the existing medical process.",The authors would like to thank King Abdulaziz City for Science and Technology (KACST) for its support.,This research received no external funding.,Cancers,,,2022-11-14,2022,2022-11-14,,14,22,5595,All OA, Gold,Article,"Rabaan, Ali A.; Bakhrebah, Muhammed A.; AlSaihati, Hajir; Alhumaid, Saad; Alsubki, Roua A.; Turkistani, Safaa A.; Al-Abdulhadi, Saleh; Aldawood, Yahya; Alsaleh, Abdulmonem A.; Alhashem, Yousef N.; Almatouq, Jenan A.; Alqatari, Ahlam A.; Alahmed, Hejji E.; Sharbini, Dalal A.; Alahmadi, Arwa F.; Alsalman, Fatimah; Alsayyah, Ahmed; Mutair, Abbas Al","Rabaan, Ali A. (Molecular Diagnostic Laboratory, Johns Hopkins Aramco Healthcare, Dhahran 31311, Saudi Arabia; College of Medicine, Alfaisal University, Riyadh 11533, Saudi Arabia; Department of Public Health and Nutrition, The University of Haripur, Haripur 22610, Pakistan); Bakhrebah, Muhammed A. (Life Science and Environment Research Institute, King Abdulaziz City for Science and Technology (KACST), Riyadh 11442, Saudi Arabia); AlSaihati, Hajir (Department of Clinical Laboratory Sciences, College of Applied Medical Sciences, University of Hafr Al Batin, Hafr Al Batin 39831, Saudi Arabia); Alhumaid, Saad (Administration of Pharmaceutical Care, Al-Ahsa Health Cluster, Ministry of Health, Al-Ahsa 31982, Saudi Arabia); Alsubki, Roua A. (Department of Clinical Laboratory Sciences, College of Applied Medical Sciences, King Saud University, Riyadh 11362, Saudi Arabia); Turkistani, Safaa A. (Department of Medical Laboratory Sciences, Fakeeh College for Medical Science, Jeddah 21134, Saudi Arabia); Al-Abdulhadi, Saleh (Department of Medical Laboratory Sciences, College of Applied Medical Sciences, Prince Sattam Bin Abdulaziz University, Riyadh 11942, Saudi Arabia); Aldawood, Yahya (Clinical Laboratory Science Department, Mohammed Al-Mana College for Medical Sciences, Dammam 34222, Saudi Arabia); Alsaleh, Abdulmonem A. (Clinical Laboratory Science Department, Mohammed Al-Mana College for Medical Sciences, Dammam 34222, Saudi Arabia); Alhashem, Yousef N. (Clinical Laboratory Science Department, Mohammed Al-Mana College for Medical Sciences, Dammam 34222, Saudi Arabia); Almatouq, Jenan A. (Clinical Laboratory Science Department, Mohammed Al-Mana College for Medical Sciences, Dammam 34222, Saudi Arabia); Alqatari, Ahlam A. (Hematopathology Department, Clinical Pathology, Al-Dorr Specialist Medical Center, Qatif 31911, Saudi Arabia); Alahmed, Hejji E. (Department of Laboratory and Blood Bank, King Fahad Hospital Hofuf, Al Hofuf 36441, Saudi Arabia); Sharbini, Dalal A. (Immunology and Serology Laboratory, King Fahd Military Medical Complex Dhahran, Dhahran 31932, Saudi Arabia); Alahmadi, Arwa F. (Department of Clinical Laboratory Sciences, College of Applied Medical Sciences, Imam Abdulrahman Bin Faisal University, Dammam 31441, Saudi Arabia); Alsalman, Fatimah (Department of Emergency Medicine, Oyun City Hospital, Al-Ahsa 36312, Saudi Arabia); Alsayyah, Ahmed (Department of Pathology, College of Medicine, Imam Abdulrahman Bin Faisal University, Dammam 31441, Saudi Arabia); Mutair, Abbas Al (Research Center, Almoosa Specialist Hospital, Al-Ahsa 36342, Saudi Arabia; College of Nursing, Princess Norah Bint Abdulrahman University, Riyadh 11564, Saudi Arabia; School of Nursing, Wollongong University, Wollongong, NSW 2522, Australia; Nursing Department, Prince Sultan Military College of Health Sciences, Dhahran 33048, Saudi Arabia)","Rabaan, Ali A. (Johns Hopkins Aramco Healthcare; Alfaisal University; University of Haripur)","Rabaan, Ali A. (Johns Hopkins Aramco Healthcare; Alfaisal University; University of Haripur); Bakhrebah, Muhammed A. (King Abdulaziz City for Science and Technology); AlSaihati, Hajir (University of Hafr Al-Batin); Alhumaid, Saad (Ministry of Health); Alsubki, Roua A. (King Saud University); Turkistani, Safaa A. (); Al-Abdulhadi, Saleh (Prince Sattam Bin Abdulaziz University); Aldawood, Yahya (); Alsaleh, Abdulmonem A. (); Alhashem, Yousef N. (); Almatouq, Jenan A. (); Alqatari, Ahlam A. (); Alahmed, Hejji E. (King Fahad Hospital Hufūf); Sharbini, Dalal A. (King Fahd Military Medical Complex); Alahmadi, Arwa F. (Imam Abdulrahman Bin Faisal University); Alsalman, Fatimah (); Alsayyah, Ahmed (Imam Abdulrahman Bin Faisal University); Mutair, Abbas Al (University of Wollongong)",1,1,,,https://www.mdpi.com/2072-6694/14/22/5595/pdf?version=1668590752,https://app.dimensions.ai/details/publication/pub.1152771610,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
3791,pub.1125498262,10.1016/j.neunet.2020.03.007,32203876,,AdaEn-Net: An ensemble of adaptive 2D–3D Fully Convolutional Networks for medical image segmentation,"Fully Convolutional Networks (FCNs) have emerged as powerful segmentation models but are usually designed manually, which requires extensive time and can result in large and complex architectures. There is a growing interest to automatically design efficient architectures that can accurately segment 3D medical images. However, most approaches either do not fully exploit volumetric information or do not optimize the model's size. To address these problems, we propose a self-adaptive 2D-3D ensemble of FCNs called AdaEn-Net for 3D medical image segmentation that incorporates volumetric data and adapts to a particular dataset by optimizing both the model's performance and size. The AdaEn-Net consists of a 2D FCN that extracts intra-slice information and a 3D FCN that exploits inter-slice information. The architecture and hyperparameters of the 2D and 3D architectures are found through a multiobjective evolutionary based algorithm that maximizes the expected segmentation accuracy and minimizes the number of parameters in the network. The main contribution of this work is a model that fully exploits volumetric information and automatically searches for a high-performing and efficient architecture. The AdaEn-Net was evaluated for prostate segmentation on the PROMISE12 Grand Challenge and for cardiac segmentation on the MICCAI ACDC challenge. In the first challenge, the AdaEn-Net ranks 9 out of 297 submissions and surpasses the performance of an automatically-generated segmentation network while producing an architecture with 13× fewer parameters. In the second challenge, the proposed model is ranked within the top 8 submissions and outperforms an architecture designed with reinforcement learning while having 1.25× fewer parameters.",,,Neural Networks,,"Humans; Image Enhancement; Imaging, Three-Dimensional; Neural Networks, Computer",2020-03-10,2020,2020-03-10,2020-06,126,,76-94,Closed,Article,"Baldeon Calisto, Maria; Lai-Yuen, Susana K","Baldeon Calisto, Maria (University of South Florida, 4202 E. Fowler Avenue, Tampa, FL 33620, USA.); Lai-Yuen, Susana K (University of South Florida, 4202 E. Fowler Avenue, Tampa, FL 33620, USA. Electronic address: laiyuen@usf.edu.)",,"Baldeon Calisto, Maria (University of South Florida); Lai-Yuen, Susana K (University of South Florida)",62,58,4.09,26.37,,https://app.dimensions.ai/details/publication/pub.1125498262,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,,
3785,pub.1149686743,10.1007/s10334-022-01031-5,35867236,,Emerging MR methods for improved diagnosis of prostate cancer by multiparametric MRI,"Current challenges of using serum prostate-specific antigen (PSA) level-based screening, such as the increased false positive rate, inability to detect clinically significant prostate cancer (PCa) with random biopsy, multifocality in PCa, and the molecular heterogeneity of PCa, can be addressed by integrating advanced multiparametric MR imaging (mpMRI) approaches into the diagnostic workup of PCa. The standard method for diagnosing PCa is a transrectal ultrasonography (TRUS)-guided systematic prostate biopsy, but it suffers from sampling errors and frequently fails to detect clinically significant PCa. mpMRI not only increases the detection of clinically significant PCa, but it also helps to reduce unnecessary biopsies because of its high negative predictive value. Furthermore, non-Cartesian image acquisition and compressed sensing have resulted in faster MR acquisition with improved signal-to-noise ratio, which can be used in quantitative MRI methods such as dynamic contrast-enhanced (DCE)-MRI. With the growing emphasis on the role of pre-biopsy mpMRI in the evaluation of PCa, there is an increased demand for innovative MRI methods that can improve PCa grading, detect clinically significant PCa, and biopsy guidance. To meet these demands, in addition to routine T1-weighted, T2-weighted, DCE-MRI, diffusion MRI, and MR spectroscopy, several new MR methods such as restriction spectrum imaging, vascular, extracellular, and restricted diffusion for cytometry in tumors (VERDICT) method, hybrid multi-dimensional MRI, luminal water imaging, and MR fingerprinting have been developed for a better characterization of the disease. Further, with the increasing interest in combining MR data with clinical and genomic data, there is a growing interest in utilizing radiomics and radiogenomics approaches. These big data can also be utilized in the development of computer-aided diagnostic  tools, including automatic segmentation and the detection of clinically significant PCa using machine learning methods.","The authors would like to thank the students and collaborators for many fruitful discussions, help, and support. NRJ thanks the Science and Engineering Research Board, Department of Science and Technology, Government of India for the award of J. C. Bose Fellowship.",,"Magnetic Resonance Materials in Physics, Biology and Medicine",,Humans, Image-Guided Biopsy, Magnetic Resonance Imaging, Male, Multiparametric Magnetic Resonance Imaging, Prostate, Prostatic Neoplasms,2022-07-22,2022,2022-07-22,2022-08,35,4,587-608,Closed,Article,"Dwivedi, Durgesh Kumar; Jagannathan, Naranamangalam R.","Dwivedi, Durgesh Kumar (Department of Radiodiagnosis, King George Medical University, 226 003, Lucknow, UP, India); Jagannathan, Naranamangalam R. (Department of Radiology, Chettinad Hospital and Research Institute, Chettinad Academy of Research and Education, 603 103, Kelambakkam, TN, India; Department of Radiology, Sri Ramachandra Institute of Higher Education and Research, 600 116, Chennai, TN, India; Department of Electrical Engineering, Indian Institute Technology Madras, 600 036, Chennai, TN, India)","Dwivedi, Durgesh Kumar (King George's Medical University); Jagannathan, Naranamangalam R. (Chettinad Academy of Research and Education; Chettinad Health City; Sri Ramachandra Institute of Higher Education and Research; Indian Institute of Technology Madras)","Dwivedi, Durgesh Kumar (King George's Medical University); Jagannathan, Naranamangalam R. (Chettinad Academy of Research and Education; Chettinad Health City; Sri Ramachandra Institute of Higher Education and Research; Indian Institute of Technology Madras)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1149686743,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,
3785,pub.1144994979,10.3390/diagnostics12020289,35204380,PMC8870978,Machine Learning in Prostate MRI for Prostate Cancer: Current Status and Future Opportunities,"Advances in our understanding of the role of magnetic resonance imaging (MRI) for the detection of prostate cancer have enabled its integration into clinical routines in the past two decades. The Prostate Imaging Reporting and Data System (PI-RADS) is an established imaging-based scoring system that scores the probability of clinically significant prostate cancer on MRI to guide management. Image fusion technology allows one to combine the superior soft tissue contrast resolution of MRI, with real-time anatomical depiction using ultrasound or computed tomography. This allows the accurate mapping of prostate cancer for targeted biopsy and treatment. Machine learning provides vast opportunities for automated organ and lesion depiction that could increase the reproducibility of PI-RADS categorisation, and improve co-registration across imaging modalities to enhance diagnostic and treatment methods that can then be individualised based on clinical risk of malignancy. In this article, we provide a comprehensive and contemporary review of advancements, and share insights into new opportunities in this field.",,,Diagnostics,,,2022-01-24,2022,2022-01-24,,12,2,289,All OA, Gold,Article,"Li, Huanye; Lee, Chau Hung; Chia, David; Lin, Zhiping; Huang, Weimin; Tan, Cher Heng","Li, Huanye (School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798, Singapore;, li0002ye@e.ntu.edu.sg, (H.L.);, ezplin@ntu.edu.sg, (Z.L.)); Lee, Chau Hung (Department of Diagnostic Radiology, Tan Tock Seng Hospital, Singapore 308433, Singapore;, chau_hung_lee@ttsh.com.sg); Chia, David (Department of Radiation Oncology, National University Cancer Institute (NUH), Singapore 119074, Singapore;, david_chia@nuhs.edu.sg); Lin, Zhiping (School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798, Singapore;, li0002ye@e.ntu.edu.sg, (H.L.);, ezplin@ntu.edu.sg, (Z.L.)); Huang, Weimin (Institute for Infocomm Research, A*Star, Singapore 138632, Singapore;, wmhuang@i2r.a-star.edu.sg); Tan, Cher Heng (Department of Diagnostic Radiology, Tan Tock Seng Hospital, Singapore 308433, Singapore;, chau_hung_lee@ttsh.com.sg; Lee Kong Chian School of Medicine, Nanyang Technological University, Singapore 639798, Singapore)","Tan, Cher Heng (Tan Tock Seng Hospital; Nanyang Technological University)","Li, Huanye (Nanyang Technological University); Lee, Chau Hung (Tan Tock Seng Hospital); Chia, David (National University Cancer Institute); Lin, Zhiping (Nanyang Technological University); Huang, Weimin (Institute for Infocomm Research); Tan, Cher Heng (Tan Tock Seng Hospital; Nanyang Technological University)",7,7,,,https://www.mdpi.com/2075-4418/12/2/289/pdf?version=1643024351,https://app.dimensions.ai/details/publication/pub.1144994979,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
3655,pub.1051651093,10.1002/cnm.2726,26009857,,Prostate contours delineation using interactive directional active contours model and parametric shape prior model,"Prostate contours delineation on Magnetic Resonance (MR) images is a challenging and important task in medical imaging with applications of guiding biopsy, surgery and therapy. While a fully automated method is highly desired for this application, it can be a very difficult task due to the structure and surrounding tissues of the prostate gland. Traditional active contours-based delineation algorithms are typically quite successful for piecewise constant images. Nevertheless, when MR images have diffuse edges or multiple similar objects (e.g. bladder close to prostate) within close proximity, such approaches have proven to be unsuccessful. In order to mitigate these problems, we proposed a new framework for bi-stage contours delineation algorithm based on directional active contours (DAC) incorporating prior knowledge of the prostate shape. We first explicitly addressed the prostate contour delineation problem based on fast globally DAC that incorporates both statistical and parametric shape prior model. In doing so, we were able to exploit the global aspects of contour delineation problem by incorporating a user feedback in contours delineation process where it is shown that only a small amount of user input can sometimes resolve ambiguous scenarios raised by DAC. In addition, once the prostate contours have been delineated, a cost functional is designed to incorporate both user feedback interaction and the parametric shape prior model. Using data from publicly available prostate MR datasets, which includes several challenging clinical datasets, we highlighted the effectiveness and the capability of the proposed algorithm. Besides, the algorithm has been compared with several state-of-the-art methods.",,,International Journal for Numerical Methods in Biomedical Engineering,,"Algorithms; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Models, Biological; Models, Statistical; Pelvic Bones; Prostate; Urinary Bladder",2015-06-24,2015,2015-06-24,2015-11,31,11,,Closed,Article,"Derraz, Foued; Forzy, Gérard; Delebarre, Arnaud; Taleb-Ahmed, Abdelmalik; Oussalah, Mourad; Peyrodie, Laurent; Verclytte, Sebastien","Derraz, Foued (Telecommunications Laboratory, Technology Faculty, Abou Bekr Belkaïd University, Tlemcen, 13000, Algeria.; Université Nord de France, F-59000, Lille, France.; Unité de Traitement de Signaux Biomédicaux, Faculté de médecine et maïeutique, Lille, France.; LAMIH UMR CNRS 8201, Le Mont Houy, Université de Valenciennes et Cambresis, 59313, Valenciennes, France.); Forzy, Gérard (Unité de Traitement de Signaux Biomédicaux, Faculté de médecine et maïeutique, Lille, France.; Groupement des Hopitaux de l'́Institut Catholique de Lille, France.); Delebarre, Arnaud (Groupement des Hopitaux de l'́Institut Catholique de Lille, France.); Taleb-Ahmed, Abdelmalik (Université Nord de France, F-59000, Lille, France.; LAMIH UMR CNRS 8201, Le Mont Houy, Université de Valenciennes et Cambresis, 59313, Valenciennes, France.); Oussalah, Mourad (School of Electronics, Electrical and Computer Engineering, University of Birmingham, Edgbaston, Birmingham, B15 2TT, UK.); Peyrodie, Laurent (Université Nord de France, F-59000, Lille, France.; Hautes Etudes dÍngénieur, 13 rue de Toul, 59000, Lille, France.); Verclytte, Sebastien (Groupement des Hopitaux de l'́Institut Catholique de Lille, France.)","Derraz, Foued (University of Abou Bekr Belkaïd; ; ; Laboratory of Industrial and Human Automation Control, Mechanical Engineering and Computer Science)","Derraz, Foued (University of Abou Bekr Belkaïd; Laboratory of Industrial and Human Automation Control, Mechanical Engineering and Computer Science); Forzy, Gérard (Groupe Hospitalier de l'Institut Catholique de Lille); Delebarre, Arnaud (Groupe Hospitalier de l'Institut Catholique de Lille); Taleb-Ahmed, Abdelmalik (Laboratory of Industrial and Human Automation Control, Mechanical Engineering and Computer Science); Oussalah, Mourad (University of Birmingham); Peyrodie, Laurent (Hautes Études d'Ingénieur); Verclytte, Sebastien (Groupe Hospitalier de l'Institut Catholique de Lille)",2,0,0.08,,,https://app.dimensions.ai/details/publication/pub.1051651093,40 Engineering, 49 Mathematical Sciences,,,,,,,,,,,
3637,pub.1118153008,10.1109/tmi.2019.2930068,31329113,,Reducing the Hausdorff Distance in Medical Image Segmentation With Convolutional Neural Networks,"The Hausdorff Distance (HD) is widely used in evaluating medical image segmentation methods. However, the existing segmentation methods do not attempt to reduce HD directly. In this paper, we present novel loss functions for training convolutional neural network (CNN)-based segmentation methods with the goal of reducing HD directly. We propose three methods to estimate HD from the segmentation probability map produced by a CNN. One method makes use of the distance transform of the segmentation boundary. Another method is based on applying morphological erosion on the difference between the true and estimated segmentation maps. The third method works by applying circular/spherical convolution kernels of different radii on the segmentation probability maps. Based on these three methods for estimating HD, we suggest three loss functions that can be used for training to reduce HD. We use these loss functions to train CNNs for segmentation of the prostate, liver, and pancreas in ultrasound, magnetic resonance, and computed tomography images and compare the results with commonly-used loss functions. Our results show that the proposed loss functions can lead to approximately 18-45% reduction in HD without degrading other segmentation performance criteria such as the Dice similarity coefficient. The proposed loss functions can be used for training medical image segmentation methods in order to reduce the large segmentation errors.","Thisworkwas supported in part by the Natural Science and Engineering Research Council of Canada (NSERC), in part by the Canadian Institutes of Health Research (CIHR), and in part by the Prostate Cancer Canada (PCC). The authors would like to thank the support from the Charles Laszlo Chair in Biomedical Engineering held by Professor S. Salcudean.",,IEEE Transactions on Medical Imaging,,"Algorithms; Diagnostic Imaging; Humans; Image Processing, Computer-Assisted; Liver; Male; Neural Networks, Computer; Prostate",2019-07-19,2019,2019-07-19,2020-02,39,2,499-513,All OA, Green,Article,"Karimi, Davood; Salcudean, Septimiu E.","Karimi, Davood (Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, V6T 1Z4, Canada); Salcudean, Septimiu E. (Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, V6T 1Z4, Canada)","Karimi, Davood (University of British Columbia)","Karimi, Davood (University of British Columbia); Salcudean, Septimiu E. (University of British Columbia)",209,178,11.46,80.1,http://arxiv.org/pdf/1904.10030,https://app.dimensions.ai/details/publication/pub.1118153008,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
3637,pub.1110321361,10.1038/s41467-018-07619-7,30523263,PMC6284017,Why rankings of biomedical image analysis competitions should be interpreted with care,"International challenges have become the standard for validation of biomedical image analysis methods. Given their scientific impact, it is surprising that a critical analysis of common practices related to the organization of challenges has not yet been performed. In this paper, we present a comprehensive analysis of biomedical image analysis challenges conducted up to now. We demonstrate the importance of challenges and show that the lack of quality control has critical consequences. First, reproducibility and interpretation of the results is often hampered as only a fraction of relevant information is typically provided. Second, the rank of an algorithm is generally not robust to a number of variables such as the test data used for validation, the ranking scheme applied and the observers that make the reference annotations. To overcome these problems, we recommend best practice guidelines and define open research questions to be addressed in the future.","We thank all organizers of the 2015 segmentation challenges who are not co-authoring this paper (a list is provided as Supplementary Note 3) and all participants of the international questionnaire (a list is provided as Supplementary Note 4). We further thank Angelika Laha, Diana Mindroc-Filimon, Bünyamin Pekdemir, and Jenshika Yoganathan (DKFZ, Germany) for helping with the comprehensive challenge capturing. Many thanks also go to Janina Dunning and Stefanie Strzysch (DKFZ, Germany) for their support of the project. Finally, we acknowledge support from the European Research Council (ERC) (ERC starting grant COMBIOSCOPY under the New Horizon Framework Programme grant agreement ERC-2015-StG-37960 as well as Seventh Framework Programme (FP7/2007-2013) under grant agreement no 318068 (VISCERAL)), the German Research Foundation (DFG) (grant MA 6340/10-1 and grant MA 6340/12-1), the Ministry of Science and Technology, Taiwan (MOST 106-3114-8-011-002, 106-2622-8-011-001-TE2, and 105-2221-E-011-121-MY2), the US National Institute of Health (NIH) (grants R01-NS070906, RG-1507-05243, and R01-EB017230 (NIBIB)), the Australian Research Council (DP140102794 and FT110100623), the Swiss National Science Foundation (grant 205321_157207), the Czech Science Foundation (grant P302/12/G157), the Czech Ministry of Education, Youth and Sports (grant LTC17016 in the frame of EU COST NEUBIAS project), the Engineering and Physical Sciences Research Council (EPSRC) (MedIAN UK Network (EP/N026993/1) and EP/P012841/1), the Wellcome Trust (NS/A000050/1), the Canadian Natural Science and Engineering Research Council (RGPIN-2015-05471), the UK Medical Research Council (MR/P015476/1), and the Heidelberg Collaboratory for Image Processing (HCI) including matching funds from the industry partners of the HCI.",,Nature Communications,,"Biomedical Research; Biomedical Technology; Diagnostic Imaging; Humans; Image Processing, Computer-Assisted; Reproducibility of Results; Surveys and Questionnaires; Technology Assessment, Biomedical",2018-12-06,2018,2018-12-06,,9,1,5217,All OA, Gold,Article,"Maier-Hein, Lena; Eisenmann, Matthias; Reinke, Annika; Onogur, Sinan; Stankovic, Marko; Scholz, Patrick; Arbel, Tal; Bogunovic, Hrvoje; Bradley, Andrew P.; Carass, Aaron; Feldmann, Carolin; Frangi, Alejandro F.; Full, Peter M.; van Ginneken, Bram; Hanbury, Allan; Honauer, Katrin; Kozubek, Michal; Landman, Bennett A.; März, Keno; Maier, Oskar; Maier-Hein, Klaus; Menze, Bjoern H.; Müller, Henning; Neher, Peter F.; Niessen, Wiro; Rajpoot, Nasir; Sharp, Gregory C.; Sirinukunwattana, Korsuk; Speidel, Stefanie; Stock, Christian; Stoyanov, Danail; Taha, Abdel Aziz; van der Sommen, Fons; Wang, Ching-Wei; Weber, Marc-André; Zheng, Guoyan; Jannin, Pierre; Kopp-Schneider, Annette","Maier-Hein, Lena (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Eisenmann, Matthias (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Reinke, Annika (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Onogur, Sinan (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Stankovic, Marko (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Scholz, Patrick (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Arbel, Tal (Centre for Intelligent Machines, McGill University, H3A0G4, Montreal, QC, Canada); Bogunovic, Hrvoje (Christian Doppler Laboratory for Ophthalmic Image Analysis, Department of Ophthalmology, Medical University Vienna, 1090, Vienna, Austria); Bradley, Andrew P. (Science and Engineering Faculty, Queensland University of Technology, 4001, Brisbane, QLD, Australia); Carass, Aaron (Department of Electrical and Computer Engineering, Department of Computer Science, Johns Hopkins University, 21218, Baltimore, MD, USA); Feldmann, Carolin (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Frangi, Alejandro F. (CISTIB - Center for Computational Imaging & Simulation Technologies in Biomedicine, The University of Leeds, LS2 9JT, Leeds, Yorkshire, UK); Full, Peter M. (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); van Ginneken, Bram (Department of Radiology and Nuclear Medicine, Medical Image Analysis, Radboud University Center, 6525 GA, Nijmegen, The Netherlands); Hanbury, Allan (Institute of Information Systems Engineering, TU Wien, 1040, Vienna, Austria; Complexity Science Hub Vienna, 1080, Vienna, Austria); Honauer, Katrin (Heidelberg Collaboratory for Image Processing (HCI), Heidelberg University, 69120, Heidelberg, Germany); Kozubek, Michal (Centre for Biomedical Image Analysis, Masaryk University, 60200, Brno, Czech Republic); Landman, Bennett A. (Electrical Engineering, Vanderbilt University, 37235-1679, Nashville, TN, USA); März, Keno (Division of Computer Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Maier, Oskar (Institute of Medical Informatics, Universität zu Lübeck, 23562, Lübeck, Germany); Maier-Hein, Klaus (Division of Medical Image Computing (MIC), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Menze, Bjoern H. (Institute for Advanced Studies, Department of Informatics, Technical University of Munich, 80333, Munich, Germany); Müller, Henning (Information System Institute, HES-SO, 3960, Sierre, Switzerland); Neher, Peter F. (Division of Medical Image Computing (MIC), German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Niessen, Wiro (Departments of Radiology, Nuclear Medicine and Medical Informatics, Erasmus MC, 3015 GD, Rotterdam, The Netherlands); Rajpoot, Nasir (Department of Computer Science, University of Warwick, CV4 7AL, Coventry, UK); Sharp, Gregory C. (Department of Radiation Oncology, Massachusetts General Hospital, 02114, Boston, MA, USA); Sirinukunwattana, Korsuk (Institute of Biomedical Engineering, University of Oxford, OX3 7DQ, Oxford, UK); Speidel, Stefanie (Division of Translational Surgical Oncology (TCO), National Center for Tumor Diseases Dresden, 01307, Dresden, Germany); Stock, Christian (Division of Clinical Epidemiology and Aging Research, German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany); Stoyanov, Danail (Centre for Medical Image Computing (CMIC) & Department of Computer Science, University College London, W1W 7TS, London, UK); Taha, Abdel Aziz (Data Science Studio, Research Studios Austria FG, 1090, Vienna, Austria); van der Sommen, Fons (Department of Electrical Engineering, Eindhoven University of Technology, 5600 MB, Eindhoven, The Netherlands); Wang, Ching-Wei (AIExplore, NTUST Center of Computer Vision and Medical Imaging, Graduate Institute of Biomedical Engineering, National Taiwan University of Science and Technology, 106, Taipei, Taiwan); Weber, Marc-André (Institute of Diagnostic and Interventional Radiology, University Medical Center Rostock, 18051, Rostock, Germany); Zheng, Guoyan (Institute for Surgical Technology and Biomechanics, University of Bern, 3014, Bern, Switzerland); Jannin, Pierre (Univ Rennes, Inserm, LTSI (Laboratoire Traitement du Signal et de l’Image) - UMR_S 1099, 35043, Rennes, Cedex, France); Kopp-Schneider, Annette (Division of Biostatistics, German Cancer Research Center (DKFZ), 69120, Heidelberg, Germany)","Maier-Hein, Lena (German Cancer Research Center)","Maier-Hein, Lena (German Cancer Research Center); Eisenmann, Matthias (German Cancer Research Center); Reinke, Annika (German Cancer Research Center); Onogur, Sinan (German Cancer Research Center); Stankovic, Marko (German Cancer Research Center); Scholz, Patrick (German Cancer Research Center); Arbel, Tal (McGill University); Bogunovic, Hrvoje (Medical University of Vienna); Bradley, Andrew P. (Queensland University of Technology); Carass, Aaron (Johns Hopkins University); Feldmann, Carolin (German Cancer Research Center); Frangi, Alejandro F. (University of Leeds); Full, Peter M. (German Cancer Research Center); van Ginneken, Bram (Radboud University Nijmegen); Hanbury, Allan (TU Wien; Complexity Science Hub Vienna); Honauer, Katrin (Heidelberg University); Kozubek, Michal (Masaryk University); Landman, Bennett A. (Vanderbilt University); März, Keno (German Cancer Research Center); Maier, Oskar (University of Lübeck); Maier-Hein, Klaus (German Cancer Research Center); Menze, Bjoern H. (Technical University of Munich); Müller, Henning (); Neher, Peter F. (German Cancer Research Center); Niessen, Wiro (Erasmus MC); Rajpoot, Nasir (University of Warwick); Sharp, Gregory C. (Massachusetts General Hospital); Sirinukunwattana, Korsuk (University of Oxford); Speidel, Stefanie (National Center for Tumor Diseases); Stock, Christian (German Cancer Research Center); Stoyanov, Danail (University College London); Taha, Abdel Aziz (); van der Sommen, Fons (Eindhoven University of Technology); Wang, Ching-Wei (National Taiwan University of Science and Technology); Weber, Marc-André (Universitätsmedizin Rostock); Zheng, Guoyan (University of Bern); Jannin, Pierre (University of Rennes 1); Kopp-Schneider, Annette (German Cancer Research Center)",178,107,8.14,57.96,https://www.nature.com/articles/s41467-018-07619-7.pdf,https://app.dimensions.ai/details/publication/pub.1110321361,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
3629,pub.1008397369,10.1016/j.neuroimage.2015.05.099,26070262,,3D MR ventricle segmentation in pre-term infants with post-hemorrhagic ventricle dilatation (PHVD) using multi-phase geodesic level-sets,"Intraventricular hemorrhage (IVH) or bleed within the cerebral ventricles is a common condition among very low birth weight pre-term neonates. The prognosis for these patients is worsened should they develop progressive ventricular dilatation, i.e., post-hemorrhagic ventricle dilatation (PHVD), which occurs in 10-30% of IVH patients. Accurate measurement of ventricular volume would be valuable information and could be used to predict PHVD and determine whether that specific patient with ventricular dilatation requires treatment. While the monitoring of PHVD in infants is typically done by repeated transfontanell 2D ultrasound (US) and not MRI, once the patient's fontanels have closed around 12-18months of life, the follow-up patient scans are done by MRI. Manual segmentation of ventricles from MR images is still seen as a gold standard. However, it is extremely time- and labor-consuming, and it also has observer variability. This paper proposes an accurate multiphase geodesic level-set segmentation algorithm for the extraction of the cerebral ventricle system of pre-term PHVD neonates from 3D T1 weighted MR images. The proposed segmentation algorithm makes use of multi-region segmentation technique associated with spatial priors built from a multi-atlas registration scheme. The leave-one-out cross validation with 19 patients with mild enlargement of ventricles and 7 hydrocephalus patients shows that the proposed method is accurate, suggesting that the proposed approach could be potentially used for volumetric and morphological analysis of the ventricle system of IVH neonatal brains in clinical practice.",The authors are grateful for the funding support from the Canadian Institutes of Health Research (CIHR) and Academic Medical Organization of Southwestern Ontario (AMOSO).,,NeuroImage,,"Algorithms; Brain; Brain Mapping; Cerebral Ventricles; Dilatation; Humans; Hydrocephalus; Imaging, Three-Dimensional; Infant, Newborn; Infant, Premature; Infant, Premature, Diseases; Intracranial Hemorrhages; Magnetic Resonance Imaging",2015-06-10,2015,2015-06-10,2015-09,118,,13-25,Closed,Article,"Qiu, Wu; Yuan, Jing; Rajchl, Martin; Kishimoto, Jessica; Chen, Yimin; de Ribaupierre, Sandrine; Chiu, Bernard; Fenster, Aaron","Qiu, Wu (Robarts Research Institute, University of Western Ontario, London, ON, Canada); Yuan, Jing (Robarts Research Institute, University of Western Ontario, London, ON, Canada); Rajchl, Martin (Robarts Research Institute, University of Western Ontario, London, ON, Canada; Biomedical Engineering Graduate Program, University of Western Ontario, London, ON, Canada); Kishimoto, Jessica (Robarts Research Institute, University of Western Ontario, London, ON, Canada; Medical Biophysics, University of Western Ontario, London, ON, Canada); Chen, Yimin (Department of Electronic Engineering, City University of Hong Kong, PR China); de Ribaupierre, Sandrine (Neurosurgery, Department of Clinical Neurological Sciences, University of Western Ontario, London, ON, Canada); Chiu, Bernard (Department of Electronic Engineering, City University of Hong Kong, PR China); Fenster, Aaron (Robarts Research Institute, University of Western Ontario, London, ON, Canada; Biomedical Engineering Graduate Program, University of Western Ontario, London, ON, Canada; Medical Biophysics, University of Western Ontario, London, ON, Canada)","Qiu, Wu (Western University)","Qiu, Wu (Western University); Yuan, Jing (Western University); Rajchl, Martin (Western University; Western University); Kishimoto, Jessica (Western University; Western University); Chen, Yimin (City University of Hong Kong); de Ribaupierre, Sandrine (Western University); Chiu, Bernard (City University of Hong Kong); Fenster, Aaron (Western University; Western University; Western University)",18,6,0.79,4.79,,https://app.dimensions.ai/details/publication/pub.1008397369,32 Biomedical and Clinical Sciences, 3213 Paediatrics,,,,,,,,,,,
3622,pub.1042949781,10.1016/j.media.2016.07.009,27475911,PMC5099118,ISLES 2015 - A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI,"Ischemic stroke is the most common cerebrovascular disease, and its diagnosis, treatment, and study relies on non-invasive imaging. Algorithms for stroke lesion segmentation from magnetic resonance imaging (MRI) volumes are intensely researched, but the reported results are largely incomparable due to different datasets and evaluation schemes. We approached this urgent problem of comparability with the Ischemic Stroke Lesion Segmentation (ISLES) challenge organized in conjunction with the MICCAI 2015 conference. In this paper we propose a common evaluation framework, describe the publicly available datasets, and present the results of the two sub-challenges: Sub-Acute Stroke Lesion Segmentation (SISS) and Stroke Perfusion Estimation (SPES). A total of 16 research groups participated with a wide range of state-of-the-art automatic segmentation algorithms. A thorough analysis of the obtained data enables a critical evaluation of the current state-of-the-art, recommendations for further developments, and the identification of remaining challenges. The segmentation of acute perfusion lesions addressed in SPES was found to be feasible. However, algorithms applied to sub-acute lesion segmentation in SISS still lack accuracy. Overall, no algorithmic characteristic of any method was found to perform superior to the others. Instead, the characteristics of stroke lesion appearances, their evolution, and the observed challenges should be studied in detail. The annotated ISLES image datasets continue to be publicly available through an online evaluation system to serve as an ongoing benchmarking resource (www.isles-challenge.org).","Image 11 CN-Neu This work was supported by the Fundamental Research Funds for the Central Universities of China under grant N140403006 and the Postdoctoral Scientific Research Funds of Northeastern University under grant No. 20150310. Image 7 US-Jhu This work was funded by the Epidemiology and Biostatistics training grant from the NIH (T32AG021334). Image 2 US-Imp1 This work was supported by NIHR Grant i4i: Decision-assist software for management of acute ischaemic stroke using brain-imaging machine-learning (Ref: II-LA-0814-20007). Image 9 US-Odu This work was partially supported through a grant from NCI/NIH (R15CA115464). Image 6 US-Imp2 This work was partially supported by the Imperial PhD Scholarship Programme and the Framework 7 program of the EU in the context of CENTER-TBI (https://www.center-tbi.eu). Image 15 BE-Kul2 This work was financially supported by the KU Leuven Concerted Research Action GOA/11/006. David Robben is supported by a Ph.D. fellowship of the Research Foundation - Flanders (FWO). Daan Christiaens is supported by Ph.D. grant SB 121013 of the Agency for Innovation by Science and Technology (IWT). Janaki Raman Rangarajan is supported by IWT SBO project MIRIAD (Molecular Imaging Research Initiative for Application in Drug Development, SBO-130065). Image 13 CA-USher This work was supported by NSERC Discovery Grant 371951. Image 10 NW-Ntust This work was supported by the Ministry of Science and Technology of Taiwan under the Grant (MOST104-2221-E-011-085). Appendix A Participating algorithms This section includes short descriptions of the participating algorithms. For a more detailed description please refer to the workshop’s postproceeding volume (Crimi et al., 2016) or the challenge proceedings (Maier et al., 2015a). Used abbreviations are: white matter (WM), gray matter (GM), cerebral spinal fluid (CSF), random forest (RF), extremely randomized trees (ET), contextual clustering (CC), gaussian mixture models (GMM), convolutional neural network (CNN), Markov Random Field (MRF), Conditional Random Field (CRF) and expectation maximization (EM). A.1 Image 2 UK-Imp1 (Liang Chen et al.) We propose a multi-scale patch-based random forest algorithm for sub-acute stroke lesion segmentation. In the first step, we perform an intensity normalization under the exclusion of outliers. Second, we extract features from all images: Patch-wise intensities of each modality are extracted at multiple scales obtained with Gaussian smoothing. We parcellate the whole brain into three parts, including top, middle, and bottom. To keep an equilibrated class balance in the training set, only a subset of background patches is samples from locations all over the brain. Subsequently, we train three standard RF (Breiman, 2001) classifiers based on the patches selected from three parts of the brain. Finally, we perform some postprocessing operations, including smoothing the outputs of the RFs, applying a threshold, and performing some morphological operations to obtain the binary lesion map. A.2 Image 3 DE-Dkfz (Michael Götz et al.) The basic idea of this approach is that a single classifier might not be able to learn all possible appearances of stroke lesions. We therefore use ‘Input-Data Adaptive Learning’ to train an individual classifier for every input image. The learning is done in two steps: First, we learn the similarity between two images to be able to find similar images for unseen data. We define the similarity between two images as the DC that can be achieved by a classifier trained on the first image with the second image. Neighborhood Approximation Forests (NAF) (Konukoglu et al., 2013) are used to predict similar images for images without a ground-truth label (e.g. without the possibility to calculate the DC). We use first-order statistic description of the complete images as features for the learning algorithm. While the first step is done offline, the second step is done online, when a new and unlabeled image should be segmented. A specific, voxel-wise classifier is trained from the closest three images, selected by the previous trained NAF. For the voxel classifier we use ETs (Geurts et al., 2006) which incorporate DALSA to show the general applicability of our approach (Goetz et al., 2016). In addition to the intensity values we use Gaussian, Difference of Gaussian, Laplacian of Gaussian (3 directions), and Hessian of Gaussian with Gaussian sigmas of 1, 2, 3mm for every modality, leading to 82 features per voxel. A.3 Image 4 FI-Hus (Hanna-Leena Halme et al.) The method performs lesion segmentation with a RF algorithm and subsequent CC (Salli et al., 2001). We utilize the training data to build statistical templates and use them for calculation of individual voxel-wise differences from the voxel-wise cross-subject mean. First, all image volumes are warped to a common template space using Advanced Normalization Tools (ANTS). Mean and standard deviation over subjects are calculated voxel-by-voxel, separately for T1, T2, FLAIR and DWI images; these constitute the statistical templates. The initial lesion segmentation is calculated using RF classification and 16 image features. The features include normalized voxel intensity, spatially filtered voxel intensity, intensity deviation from the mean specified by the template, and voxel-wise asymmetry in intensities across hemispheres, calculated separately for each imaging sequence. For RF training, we only use a random subset of voxels in order to decrease computational time and avoid classifier overfitting, As a last phase, the lesion probability maps given by the RF classifier are subjected to CC to spatially regularize the segmentation. The CC algorithm takes the neighborhood of each voxel into account by using a Markov random field prior and iterated conditional modes algorithm. A.4 Image 5 CA-McGill The authors of this method decided against participating in this article. A description of their approach can be found in the challenge’s proceedings on http://www.isles-challenge.org/ISLES2015/ A.5 Image 6 UK-Imp2 (Konstantinos Kamnitsas et al.) We developed an automatic segmentation system, based on a 11-layers deep, multi-scale, 3D CNN. The network classifies voxels after processing a multi-modal 3D patch around them. To achieve efficient processing of greater image context, we developed a network architecture with two parallel convolutional pathways that processes the image at different scales. To train our system we build upon the work in Urban et al. (2014) and form batches with large image segments, equally sampled from the two classes. We exploit our network’s fully convolutional nature to densely train on multiple voxels in the central part of the segments. By utilizing small 33 kernels that lead to deeper architectures with less trainable parameters, as well as adopting Dropout, Batch Normalization (Ioffe and Szegedy, 2015) and augmenting the database using reflection along the sagittal axis, we heavily regularize our network and show that it is possible to train such a deep and wide network on a limited database. Training our CNN takes approximately one day on a GeForce GTX Titan Black, while inference on a brain volume requires 3 minutes. We applied only minimum preprocessing, normalizing the modalities of each patient to zero mean and unit variance. For our final submission in the testing phase of the challenge, the outputs of 3 similar CNNs were averaged, to reduce noise caused by randomness during training. Additionally, we implemented a 3D, densely connected CRF by extending the work of Krähenbühl and Koltun (2012), which can efficiently postprocess a multi-modal scan in 2 minutes. Finally, connected components smaller than 20 voxels are eliminated. A.6 Image 7 US-Jhu (John Muschelli) As rigid registration may not correct local differences between spatial locations across sequences, we re-register images to the FLAIR using Symmetric Normalization (Avants et al., 2008). We normalize the voxel intensities to a z-score using the 20% trimmed mean and standard deviation from each image. To train an algorithm, we create a series of predictors, including the x-y flipped voxel intensity, local moments (mean, sd, skew, kurtosis), and the images smoothed with large Gaussian filters. We trained a RF from 9 images, downsampled to 300,000 voxels, with the manual segmentation as the outcome (Breiman, 2001). From the RF, we obtained the probability of lesion and determined the threshold for these probabilities using the out-of-sample voxels from the training images, optimizing for the DC. A.7 Image 8 SE-Cth (Qaiser Mahmood et al.) The proposed framework takes the multi-spectral MRI brain images as input and includes two preprocessing steps: (1) Correction of bias field using the N3 bias field correction algorithm (Sled et al., 1998) and (2) normalization of the intensity values of each MRI modality to the interval [0, 1], done by applying linear histogram stretching. For each voxel of multi-spectral MRI images, the following set of meaningful features is extracted: intensities, smooth intensities, median intensities, gradient, magnitude of the gradient and local entropy. All these features were normalized to zero mean and unit deviation. These features are then employed to train the RF (Criminisi and Shotton, 2013) classifier and segment the sub-acute ischemic stroke lesion. In this work, we set the RF parameters to: number of trees=150 and depth of each tree=50. A total of 999,000 data samples (i.e. 37,000 randomly selected from each training case) is used to train the RF classifier. Finally, the postprocessing is performed using dilation and erosion operations in order to remove small objects falsely classified as stroke lesion. A.8 Image 9 US-Odu (Syed M S Reza et al.) This work proposes fully automatic ischemic stroke lesion segmentation in multispectral brain MRI by innovating on our prior brain tumor segmentation work (Reza and Iftekharuddin, 2014). The method starts with the standard MRI preprocessing steps: intensity inhomogeneity correction and normalization. Next step involves two primary sets of feature extraction from T1, T2, FLAIR and DWI imaging sequences. The first set of features includes the pixel intensities (IFL, I T1, I T2, IDWI ) and differences of intensities ( d 1 = I F L − I T 1 , d 2 = I F L − I T 2 , d 3 = I F L − I D W I ) that represents the global characteristics of brain tissues. In the second set, local texture features such as piece-wise triangular prism surface area, multi-fractal Brownian motion (Islam et al., 2013) and structure tensor based local gradients are extracted to capture the surface variation of the brain tissues. We use a mutual information based implementation of minimum redundancy maximum relevance feature ranking technique and choose the 19 top ranked features. A classical RF classifier is employed to classify the brain tissues as lesion or background. Finally, a binary morphological filter is used to reduce the false positives from the original detections. We observe a few remaining false positives that compromise the overall performance. Our future works will include the study of more effective features, sophisticated feature selection techniques and an effective false positive reduction technique. A.9 Image 10 TW-Ntust (Ching-Wei Wang et al.) A fully automatic machine learning based stroke lesion three-dimensions segmentation system is built, which consists of a feature selection method, a multi-level RF model and a simple 3D registration approach. Only the FLAIR sequence was used and 275 features, which can be categorized into 24 types, are extracted for building RF models. To deal with the three dimensional data, a multi-RF model is developed and for stacks of five slices in the Z direction, a random forest model is built. The RF model generates probability maps. After obtaining the potential candidates from the RFs, we build a three-dimensional registration framework with backward and forward searching (Wang et al., 2015). It is applied to generate optimal three-dimensional predictions and too remove larger outliers. The system finds the largest object among all stacks and uses the stack with the largest object as the referenced stack. Then, the system performs backward and forward registration to maintain spatial consistency and remove the objects with no overlap to the detected objects in the neighboring stacks. A.10 Image 11 CN-Neu (Chaolu Feng) We propose a framework to automatically extract ischemic lesions from multi-spectral MRI images. We suppose that the input images of different modalities have already been rigidly registered in the same coordinate system and non-brain tissues have already been removed from the images (Gao et al., 2014). Lesion segmentation is then performed by the proposed framework in three major steps: 1) preliminary segmentation, 2) segmentation fusion, and 3) boundary refinement. No training data is needed and no preprocessing and postprocessing steps involved. In the proposed framework, MRI images of each modality are first segmented into brain tissues (WM, GM and CSF) and ischemic lesions by weighting suppressed fuzzy c-means. Preliminary lesion segmentation results are then fused among all the imaging modalities by majority voting. The judge rule is that candidate voxels are regarded as lesions only if 1) they are considered as brain lesions in FLAIR images, and 2) they are viewed as brain lesions in more than 1 imaging modality beside FLAIR. The fused segmentation results are finally refined by a three phase level set method. The level set formulation is defined on multi-spectral images with the capability of dealing with intensity inhomogeneities (Feng et al., 2013). A.11 Image 12 BE-Kul1 (Tom Haeck et al.) We present a fully-automated generative method that can be applied to individual patient images without need for a training data set. An EM-approach is used for estimating intensity models (GMMs) for both normal and pathological tissue. The segmentation is represented by a level-set that is iteratively updated to label voxels as either normal or pathological, based on which intensity model explains the voxels’ intensity the best. A convex level-set formulation is adopted (Goldstein et al., 2009), that eliminates the need for manual initialization of the level-set. For each iteration to update the level-set, a full EM-estimation of the GMM parameters is done. As a preprocessing step, spatial priors of WM, GM and CSF are non-rigidly registered to the patient image. The prior information is relaxed by smoothing the spatial priors with a Gaussian kernel. For SPES, we make use of the T2-weighted and TTP-weighted MR images and for SISS the diffusion weighted and FLAIR-weighted MR images. For SPES, the modalities are used in a completely multivariate way, i.e., with bivariate Gaussian models. For SISS, the modalities are segmented separately and a voxel is only labeled as lesion if it is a lesion in both modalities. A.12 Image 13 CA-USher (Francis Dutil et al.) We propose a fully-automatic CNN approach which is accurate while also being computationally efficient, a balance that existing methods have struggled to achieve. We approach the problem by solving it slice by slice from the axial view. The segmentation problem is then treated by predicting the label of the center of all the overlapping patches. We propose an architecture with two pathways: one which focuses on small details of the tissues and one focusing on the larger context. We also propose a two-phase patch-wise training procedure allowing us to train models in a few hours and to account for the imbalanced classes. We first train the model with a balanced dataset which allows us to learn features impartial to the distribution of classes. We then train the second phase by only training on the classification layer with a distribution closer to the ground truth’s. This way we learn good features and introduce the correct class prior to the model. Fully exploiting the convolutional nature of our model also allows to segment a complete brain image in 25 seconds. To test the ability of CNNs to learn useful features from scratch, we employ only minimal preprocessing. We truncate the 1% highest and lowest intensities and applied N4ITK bias correction. The input data is then normalized by subtracting the channel mean and dividing by its standard deviation. A postprocessing method based on connected components is also implemented to remove small blobs which might appear in the predictions. A.13 Image 14 DE-UzL (Oskar Maier et al.) We propose a novel voxel-wise RF classification method with features chosen to model a human observers discriminative criteria when segmenting a brain lesion. They are based on intensity, hemispheric difference, local histograms and center distances as detailed in (Maier et al., 2015c; 2016). First, the already co-registered, isotropic voxel-spacing and skull-stripped sequences are preprocessed with bias field correction and intensity range standardization (Maier, 2016) (SISS) resp. the Tmax capped at 10s (SPES). A total of 1,000,000 voxels are randomly sampled, keeping each case’s class ratio intact (i.e. imbalanced). With this training set, 50 trees are trained using Gini impurity and 163 features for node optimization. For SISS, the a-posteriori forest probability map is thresholded at 0.4 and objects smaller than 1ml removed. For SPES, the threshold is 0.35 and only the largest connected component is kept. Both are followed by an hole closing in sagittal slices. The proposed method was equally successfully applied to BRATS challenge data (Maier et al., 2016), underlining the generality of our approach. A.14 Image 15 BE-Kul2 (David Robben et al.) A single segmentation method for both the SISS and SPES sub-challenges is proposed (Robben et al., 2016). First, all data is preprocessed, including bias-field correction, linear intensity standardization, and affine registration to MNI space. Then, each voxel is probabilistically classified as lesion or background within the native image space. The classifier consists of 3 cascaded levels, in which each level extends the feature set and uses a more complex extremely randomized forest (Geurts et al., 2006). The first level only uses the T1 intensity. The second level uses all modalities, smoothed in a local neighborhood at different radii, as well as voxel coordinates in atlas space. The third level additionally uses the probabilities estimated in level 2, smoothed locally. Classifier hyperparameters were tuned using 5-fold cross-validation. Testing data is preprocessed similarly and the voxelwise probabilities are predicted by the classifier. A technique to select the threshold that optimizes the DC is presented and applied to the predicted probability map in order to obtain the final binary segmentation. A.15 Image 16 DE-Ukf (Elias Kellner et al.) In almost all cases of acute embolic anterior circulation stroke only one hemisphere is affected. We exploit this fact to (i) restrict the segmentation to only the affected hemisphere and (ii) to preselect the potential lesion by comparing local histograms of the affected side with the contralateral counterpart used as reference. Our approach is based on the evaluation of just the Tmax and ADC-maps. First, we automatically find the plane which separates the left and right hemisphere by co-registration with a mirrored Tmax-image, and identify the affected hemisphere as the one with the higher median value. For each voxel at position x → , a normalized, regional histogram H ( x → , t i ) is calculated in a 20 × 20 × 12mm3 neighborhood with a bin-width of t i + 1 − t i = 1.5 s . The difference to the corresponding contralateral histogram H ˜ ( x → , t i ) , taken from the mirrored part of the brain is calculated via D ( x → ) = 1 / 2 ∑ i | H ( x → , t i ) − H ˜ ( x → , t i ) | . The resulting map of histogram differences is thresholded by 0.5 to find the regions with unusual Tmax values. This preselection is thresholded with the generally accepted value of Tmax 6s. The histogram neighborhood size and the morphological operation parameters are globally fine-tuned based on the training dataset. To clean the mask, morphological erosion and dilation is applied. Finally, the segmentation is multiplied with ADC 1700mm2/s to remove CSF voxels. A.16 Image 17 CH-Insel (Richard McKinley et al.) The model is trained only using data from the SPES dataset; no additional data is used. The method makes use of all seven imaging modalities. Before learning takes place, the following preprocessing steps are employed: TMax values are censored below zero and above 100, and all imaging modalities are then scaled to lie in the interval [0, 256]. Simple image texture features, based on those first used in Porz et al. (2014) are extracted from each imaging modality. The resulting data points are used to train a decision forest model which assigns to each volume element a label indicating if it should be considered part of the perfusion lesion. The training algorithm is a modification of RF (Breiman, 2001), in which bootstrapping of the training data is performed first at the patient level, and only then at the voxel level. This avoids the effects of patient-level clustering and leads to out-of-sample patients. This out-of-sample data is then used to empirically discover a threshold at which the DC of the segmentation is maximized, avoiding the need for holding out training data to tune the classifier. After segmenting with this threshold, no further postprocessing was applied. The method takes approximately six minutes to segment a new case. Appendix B Ranking schema Our ranking system builds on the concept that a rank reveals only the direction of a relationship between two items (i.e. higher, lower, equal), but not its magnitude. After obtaining from each participating team the segmentation results for each case, the following steps are executed: 1. Compute the DC, ASSD HD values for each case 2. Establish each team’s rank for DC, ASSD HD separately for each case 3. Compute the mean rank over all three evaluation measures/case to obtain the team’s rank for the case 4. Compute the mean over all case-specific ranks to obtain the team’s final rank Graphically, the schema looks like displayed in Fig. B.11 . The outcome of the procedure is a final rank (real number) for each participant, which defines its standing in the leaderboard relative to all others. For SISS, with two ground truth sets for the testing dataset, their respective final ranks are averaged. For SPES, only the DC and the ASSD were used. This approach can be applied to any number of measures, independent of their range, type or direction. Its outcome denotes only the differences between algorithms and hence serves its purpose. For any interpretation of the results, the distinct evaluation measure values obtained have to be considered too. A challenge with winners requires an absolute ranking; an ongoing benchmark does not. For the online, ongoing leaderboard, the rank is not computed. Rather, each user is invited to sort the result table according to their favorite evaluation measure. Failed cases and resolving ties In one step of our algorithm, we have to rank the performance of each team on one case regarding a single evaluation metric. Such a situation can lead to ties, which have to be handled specially. We chose to decorate both tied teams with the upper rank and leaving the following empty (see Table B.10 for an example). This behavior has an interesting effect for very difficult cases, where most teams fail to produce a valid segmentation, as can be seen in the example of Table B.11 . Thus, difficult cases do not alter the mean as they would do when simply averaging, e.g., the DC values over all cases. Instead, only the performance relative to all other algorithms is compared, resulting in a more expressive ranking. Beside resolving ties, we decided to introduce a concept of failed cases: When faced with (1) a missing segmentation mask or (2) a DC value of 0.00 (i.e. no overlap at all), the concerned case was declared failed and all metric evaluation values subsequently set to infinity. Combined with the employed ranking approach and above described treatment of ties, this allows to incorporate missing segmentations in the ranking in a natural and fair manner. It could be argued that a DC of 0.00 could well mean that another part of the brain has been segmented. But the case has nevertheless to be considered a failed one, as the target structure has not been detected. Not declaring the case a failure would lead methods submitting a single random voxel segmentation to be ranked higher than an empty segmentation mask. Supplementary material Supplementary material associated with this article can be found, in the online version, at 10.1016/j.media.2016.07.009 Appendix C Supplementary materials Supplementary Data S1 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S1 Supplementary Data S2 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S2 Supplementary Data S3 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S3 Supplementary Data S4 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S4 Supplementary Data S5 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S5 Supplementary Data S6 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S6 Supplementary Data S7 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S7 Supplementary Data S8 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S8 Supplementary Data S9 Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/ Supplementary Data S9 References Albers, Thijs, Wechsler, et al., 2006 G.W. Albers V.N. Thijs L.R. Wechsler Magnetic resonance imaging profiles predict clinical response to early reperfusion: the diffusion and perfusion imaging evaluation for understanding stroke evolution (DEFUSE) study Ann. Neurol. 60 5 2006 508 517 10.1002/ana.20976 Artzi, Aizenstein, Jonas-Kimchi, et al., 2013 M. Artzi O. Aizenstein T. Jonas-Kimchi FLAIR lesion segmentation: application in patients with brain tumors and acute ischemic stroke Eur. J. Radiol. 82 9 2013 1512 1518 10.1016/j.ejrad.2013.05.029 Avants, Epstein, Grossman, Gee, 2008 B.B. Avants C. Epstein M. Grossman J. Gee Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain Med. Image Anal. 12 1 2008 26 41 10.1016/j.media.2007.06.004 Bauer, Fejes, Reyes, 2013 S. Bauer T. Fejes M. Reyes A skull-stripping filter for ITK Insight J 2013 Breiman, 2001 L. Breiman Random forests Mach. Learn. 45 1 2001 5 32 10.1023/A:1010933404324 Christensen, Campbell, de la Ossa, Lansberg, Straka, De Silva, Nagakane, Ogata, Mlynash, Bammer, Olivot, Desmond, Albers, Donnan, Davis, 2010 S. Christensen B.C. Campbell N.P. de la Ossa M.G. Lansberg M. Straka D.A. De Silva Y. Nagakane T. Ogata M. Mlynash R. Bammer J.-M. Olivot P. Desmond G.W. Albers G.A. Donnan S.M. Davis Optimal perfusion thresholds for prediction of tissue destined for infarction in the combined EPITHET and DEFUSE dataset Int. Stroke Conf 2010 Crimi, Maier, Menze, Reyes, Handels, 2016 Crimi, A., Maier, O., Menze, B., Reyes, M., Handels, H. (Eds.), 2016. In: LNCS Brainlesion: Glioma, MS, Stroke and Traumatic Brain Injuries - First International BrainLes Workshop MICCAI 2015. Springer. Criminisi, Shotton, 2013 Criminisi, A., Shotton, J. (Eds.), 2013. In: Decision forests for computer vision and medical image analysis. Springer. Dastidar, Heinonen, Ahonen, Jehkonen, Molnár, 2000 P. Dastidar T. Heinonen J.-P. Ahonen M. Jehkonen G. Molnár Volumetric measurements of right cerebral hemisphere infarction: use of a semiautomatic MRI segmentation technique Comput. Biol. Med. 30 1 2000 41 54 10.1016/S0010-4825(99)00022-0 Derntl, Plant, Gruber, et al., 2015 A. Derntl C. Plant P. Gruber Stroke lesion segmentation using a probabilistic atlas of cerebral vascular territories Crimi A. Maier O. Menze B. Reyes M. Handels H. LNCS Brainlesion Glioma, MS, Stroke Trauma. Brain Inj. - First Int. BrainLes Work. MICCAI 2015 2015 Springer Berlin Heidelberg 11 Feng, Li, Zhao, Davatzikos, Litt, 2013 C. Feng C. Li D. Zhao C. Davatzikos H. Litt Segmentation of the left ventricle using distance regularized two-layer level set approach Med. Image Comput. Comput. Interv. 16 2013 477 484 Fiez, Damasio, Grabowski, 2000 J.A. Fiez H. Damasio T.J. Grabowski Lesion segmentation and manual warping to a reference brain: intra- and interobserver reliability Hum. Brain Mapp. 9 4 2000 192 211 Forbes, Doyle, Garcia-Lorenzo, Barillot, Dojat, 2010 F. Forbes S. Doyle D. Garcia-Lorenzo C. Barillot M. Dojat Adaptive weighted fusion of multiple MR sequences for brain lesion segmentation IEEE Int. Symp. Biomed. Imaging From Nano to Macro 2010 IEEE 69 72 10.1109/ISBI.2010.5490413 Forkert, Kaesemann, Treszl, et al., 2013 N.D. Forkert P. Kaesemann A. Treszl Comparison of 10 TTP and Tmax estimation techniques for MR perfusion-diffusion mismatch quantification in acute stroke Am. J. Neuroradiol. 34 9 2013 1697 1703 Galar, Fernández, Barrenechea, Bustince, Herrera, 2013 M. Galar A. Fernández E. Barrenechea H. Bustince F. Herrera Dynamic classifier selection for One-vs-One strategy: avoiding non-competent classifiers Pattern Recognit. 46 12 2013 3412 3424 10.1016/j.patcog.2013.04.018 Gao, Li, Feng, et al., 2014 J. Gao C. Li C. Feng Non-locally regularized segmentation of multiple sclerosis lesion from multi-channel MRI data Magn. Reson. Imaging 32 8 2014 1058 1066 10.1016/j.mri.2014.03.006 Geurts, Ernst, Wehenkel, 2006 P. Geurts D. Ernst L. Wehenkel Extremely randomized trees Mach. Learn. 63 1 2006 3 42 10.1007/s10994-006-6226-1 Ghosh, Sun, Bhanu, Ashwal, Obenaus, 2014 N. Ghosh Y. Sun B. Bhanu S. Ashwal A. Obenaus Automated detection of brain abnormalities in neonatal hypoxia ischemic injury from MR images Med. Image Anal. 18 7 2014 1059 1069 10.1016/j.media.2014.05.002 Goetz, Weber, Binczyk, et al., 2016 M. Goetz C. Weber F. Binczyk DALSA: domain adaptation for supervised learning from sparsely annotated MR images IEEE Trans. Med. Imag. 35 1 2016 184 196 10.1109/TMI.2015.2463078 Goldstein, Bresson, Osher, 2009 T. Goldstein X. Bresson S. Osher Geometric applications of the split Bregman method: segmentation and surface reconstruction J. Sci. Comput. 45 1–3 2009 272 293 10.1007/s10915-009-9331-z González, Hirsch, Lev, Schaefer, Schwamm, 2011 Acute Ischemic Stroke - Imaging and Intervention González R.G. Hirsch J.A. Lev M.H. Schaefer P.W. Schwamm L.H. 2 2011 Springer Berlin Heidelberg 10.1007/978-3-642-12751-9 Han, Jovicich, Salat, et al., 2006 X. Han J. Jovicich D. Salat Reliability of MRI-derived measurements of human cerebral cortical thickness: the effects of field strength, scanner upgrade and manufacturer Neuroimage 32 1 2006 180 194 10.1016/j.neuroimage.2006.02.051 Ioffe, Szegedy, 2015 S. Ioffe C. Szegedy Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 2015 1502.03167 Išgum, Benders, Avants, et al., 2015 I. Išgum M.J.N.L. Benders B.B. Avants Evaluation of automatic neonatal brain segmentation algorithms: the NeoBrainS12 challenge Med. Image Anal. 20 1 2015 135 151 10.1016/j.media.2014.11.001 Islam, Reza, Iftekharuddin, 2013 A. Islam S.M.S. Reza K.M. Iftekharuddin Multifractal texture estimation for detection and segmentation of brain tumors IEEE Trans. Biomed. Eng. 60 11 2013 3204 3215 10.1109/TBME.2013.2271383 James, Yoder, Osuntokun, et al., 2006 J.R. James K.K. Yoder O. Osuntokun A supervised method for calculating perfusion/diffusion mismatch volume in acute ischemic stroke Comput. Biol. Med. 36 11 2006 1268 1287 10.1016/j.compbiomed.2005.05.007 Jenkinson, Pechaud, Smith, 2005 M. Jenkinson M. Pechaud S. Smith BET2: MR-based estimation of brain, skull and scalp surfaces Eleventh Annual Meeting of the Organization for Human Brain Mapping 17 2005 167 citeulike-article-id:1179617 Jovicich, Czanner, Han, et al., 2009 J. Jovicich S. Czanner X. Han MRI-derived measurements of human subcortical, ventricular and intracranial brain volumes: reliability effects of scan sessions, acquisition sequences, data analyses, scanner upgrade, scanner vendors and field strengths Neuroimage 46 1 2009 177 192 10.1016/j.neuroimage.2009.02.010 Kabir, Dojat, Scherrer, Forbes, Garbay, 2007 Y. Kabir M. Dojat B. Scherrer F. Forbes C. Garbay Multimodal MRI segmentation of ischemic stroke lesions IEEE Eng. Med. Biol. Soc 2007 IEEE 1595 1598 10.1109/IEMBS.2007.4352610 Kemmling, Flottmann, Forkert, et al., 2015 A. Kemmling F. Flottmann N.D. Forkert Multivariate dynamic prediction of ischemic infarction and tissue salvage as a function of time and degree of recanalization J. Cereb. Blood Flow Metab. 35 9 2015 1397 1405 10.1038/jcbfm.2015.144 Kistler, Bonaretti, Pfahrer, Niklaus, Büchler, 2013 M. Kistler S. Bonaretti M. Pfahrer R. Niklaus P. Büchler The virtual skeleton database: an open access repository for biomedical research and collaboration J. Med. Internet Res. 15 11 2013 e245 10.2196/jmir.2930 Klein, Staring, Murphy, Viergever, Pluim, 2010 S. Klein M. Staring K. Murphy M.A. Viergever J.P.W. Pluim elastix: a toolbox for intensity-based medical image registration IEEE Trans. Med. Imag. 29 1 2010 196 205 10.1109/TMI.2009.2035616 Konukoglu, Glocker, Zikic, Criminisi, 2013 E. Konukoglu B. Glocker D. Zikic A. Criminisi Neighbourhood approximation using randomized forests Med. Image Anal. 17 7 2013 790 804 10.1016/j.media.2013.04.013 Krähenbühl, Koltun, 2012 P. Krähenbühl V. Koltun Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials 2012 1210.5644 Langerak, Van Der Heide, Kotte, et al., 2010 T.R. Langerak U.A. Van Der Heide A.N.T.J. Kotte Label fusion in atlas-based segmentation using a selective and iterative method for performance level estimation (SIMPLE) Med. Imag. IEEE Trans. 29 12 2010 2000 2008 Lansberg, Straka, Kemp, et al., 2012 M.G. Lansberg M. Straka S. Kemp MRI profile and response to endovascular reperfusion after stroke (DEFUSE 2): a prospective cohort study Lancet. Neurol. 11 10 2012 860 867 10.1016/S1474-4422(12)70203-X Li, Tian, Li, Dai, 2004 W. Li J. Tian E. Li J. Dai Robust unsupervised segmentation of infarct lesion from diffusion tensor MR images using multiscale statistical classification and partial volume voxel reclassification Neuroimage 23 4 2004 1507 1518 10.1016/j.neuroimage.2004.08.009 Litjens, Toth, van de Ven, et al., 2014 G. Litjens R. Toth W. van de Ven Evaluation of prostate segmentation algorithms for MRI: the PROMISE12 challenge Med. Image Anal. 18 2 2014 359 373 10.1016/j.media.2013.12.002 Mah, Jager, Kennard, Husain, Nachev, 2014 Y.-H. Mah R. Jager C. Kennard M. Husain P. Nachev A new method for automated high-dimensional lesion segmentation evaluated in vascular injury and applied to the human occipital lobe Cortex 56 2014 51 63 10.1016/j.cortex.2012.12.008 Maier, 2016 O. Maier MedPy - Medical image processing in Python 2016 Maier, Reyes, Menze, Handels, 2015 Maier, O., Reyes, M., Menze, B., Handels, H. (Eds.), 2015. In: ISLES 2015: Ischemic Stroke Lesion Segmentation - Proceedings. Maier, Schröder, Forkert, Martinetz, Handels, 2015 O. Maier C. Schröder N.D. Forkert T. Martinetz H. Handels Classifiers for ischemic stroke lesion segmentation: a comparison study PLoS One 10 12 2015 e0145118 10.1371/journal.pone.0145118 Maier, Wilms, von der Gablentz, Krämer, Handels, 2014 O. Maier M. Wilms J. von der Gablentz U.M. Krämer H. Handels Ischemic stroke lesion segmentation in multi-spectral MR images with support vector machine classifiers Aylward S. Hadjiiski L.M. SPIE Med. Imaging 2014 International Society for Optics and Photonics 903504 10.1117/12.2043494 Maier, Wilms, von der Gablentz, et al., 2015c O. Maier M. Wilms J. von der Gablentz Extra tree forests for sub-acute ischemic stroke lesion segmentation in MR sequences J. Neurosci. Methods 240 2015 89 100 10.1016/j.jneumeth.2014.11.011 Maier, Wilms, Handels, 2016 O. Maier M. Wilms H. Handels Image features for brain lesion segmentation using random forests Crimi A. Maier O. Menze B. Reyes M. Handels H. LNCS Brainlesion Glioma, MS, Stroke Trauma. Brain Inj. - First Int. BrainLes Work. MICCAI 2015 2016 Springer Berlin Heidelberg Martel, Allder, Delay, Morgan, Moody, 1999 A.L. Martel S.J. Allder G.S. Delay P.S. Morgan A.R. Moody Measurement of infarct volume in stroke patients using adaptive segmentation of diffusion Weighted MR Images Taylor C. Colchester A. Med. Image Comput. Comput. Interv 1999 Springer Berlin Heidelberg Berlin, Heidelberg 22 31 10.1007/10704282 Menze, Jakab, Bauer, et al., 2015 B.H. Menze A. Jakab S. Bauer The multimodal brain tumor image segmentation benchmark (BRATS) IEEE Trans. Med. Imag. 34 10 2015 1993 2024 10.1109/TMI.2014.2377694 Muda, Saad, Abu-Bakar, Muda, Abdullah, 2015 A.F. Muda N.M. Saad S.A.R. Abu-Bakar A.S. Muda A.R. Abdullah Brain lesion segmentation using fuzzy C-means on diffusion-weighted imaging ARPN J. Eng. Appl. Sci. 10 3 2015 Mujumdar, Varma, Kishore, 2012 S. Mujumdar R. Varma L.T. Kishore A novel framework for segmentation of stroke lesions in diffusion weighted MRI using multiple b-value data Int. Conf. Pattern Recognit 2012 IEEE 3762 3765 Murphy, 2011 K. Murphy Development and evaluation of automated image analysis techniques in thoracic CT 2011 Utrecht University Ph.D. thesis Murphy, van Ginneken, Reinhardt, et al., 2011 K. Murphy B. van Ginneken J.M. Reinhardt Evaluation of registration methods on thoracic CT: the EMPIRE10 challenge IEEE Trans. Med. Imag. 30 11 2011 1901 1920 10.1109/TMI.2011.2158349 Nabizadeh, John, Wright, 2014 N. Nabizadeh N.M. John C. Wright Histogram-based gravitational optimization algorithm on single MR modality for automatic brain lesion detection and segmentation Expert Syst. Appl. 41 17 2014 7820 7836 10.1016/j.eswa.2014.06.043 Neumann, Jonsdottir, Mouridsen, et al., 2009 A.B. Neumann K.Y. Jonsdottir K. Mouridsen Interrater agreement for final infarct MRI lesion delineation Stroke 40 12 2009 3768 3771 10.1161/STROKEAHA.108.545368 Olivot, Mlynash, Thijs, et al., 2009a J.-M. Olivot M. Mlynash V.N. Thijs Optimal Tmax threshold for predicting penumbral tissue in acute stroke Stroke 40 2 2009 469 475 10.1161/STROKEAHA.108.526954 Olivot, Mlynash, Zaharchuk, et al., 2009b J.-M. Olivot M. Mlynash G. Zaharchuk Perfusion MRI (Tmax and MTT) correlation with xenon CT cerebral blood flow in stroke patients Neurology 72 13 2009 1140 1145 10.1212/01.wnl.0000345372.49233.e3 Petitjean, Zuluaga, Bai, et al., 2015 C. Petitjean M.A. Zuluaga W. Bai Right ventricle segmentation from cardiac MRI: a collation study Med. Image Anal. 19 1 2015 187 202 10.1016/j.media.2014.10.004 Porz, Bauer, Pica, et al., 2014 N. Porz S. Bauer A. Pica Multi-modal glioblastoma segmentation: man versus machine PLoS One 9 5 2014 e96873 10.1371/journal.pone.0096873 Prakash, Gupta, Bilello, Beauchamp, Nowinski, 2006 K.N.B. Prakash V. Gupta M. Bilello N.J. Beauchamp W.L. Nowinski Identification, segmentation, and image property study of acute infarcts in diffusion-weighted images by using a probabilistic neural network and adaptive Gaussian mixture model Acad. Radiol. 13 12 2006 1474 1484 10.1016/j.acra.2006.09.045 Rekik, Allassonnière, Carpenter, Wardlaw, 2012 I. Rekik S. Allassonnière T.K. Carpenter J.M. Wardlaw Medical image analysis methods in MR/CT-imaged acute-subacute ischemic stroke lesion: segmentation, prediction and insights into dynamic evolution simulation models. a critical appraisal NeuroImage Clin. 1 1 2012 164 178 10.1016/j.nicl.2012.10.003 Reza, Iftekharuddin, 2014 S.M.S. Reza K.M. Iftekharuddin Multi-fractal texture features for brain tumor and edema segmentation Aylward S. Hadjiiski L.M. SPIE Med. Imaging 2014 International Society for Optics and Photonics 903503 10.1117/12.2044264 Robben, Christiaens, Rangarajan, et al., 2016 D. Robben D. Christiaens J.R. Rangarajan A Voxel-wise, cascaded classification approach to ischemic stroke lesion segmentation Crimi A. Maier O. Menze B. Reyes M. Handels H. LNCS Brainlesion Glioma, MS, Stroke Trauma. Brain Inj. - First Int. BrainLes Work. MICCAI 2015 2016 Springer accepted Rudyanto, Kerkstra, van Rikxoort, et al., 2014 R.D. Rudyanto S. Kerkstra E.M. van Rikxoort Comparing algorithms for automated vessel segmentation in computed tomography scans of the lung: the VESSEL12 study Med. Image Anal. 18 7 2014 1217 1232 10.1016/j.media.2014.07.003 Saad, Abu-Bakar, Muda, Mokji, Salahuddin, 2011 N.M. Saad S.A.R. Abu-Bakar S. Muda M.M. Mokji L. Salahuddin Brain lesion segmentation of Diffusion-weighted MRI using gray level co-occurrence matrix IEEE Int. Conf. Imaging Syst. Tech 2011 IEEE 284 289 10.1109/IST.2011.5962179 Salli, Aronen, Savolainen, Korvenoja, Visa, 2001 E. Salli H.J. Aronen S. Savolainen A. Korvenoja A. Visa Contextual clustering for analysis of functional MRI data IEEE Trans. Med. Imag. 20 5 2001 403 414 10.1109/42.925293 Seghier, Ramlackhansingh, Crinion, Leff, Price, 2008 M.L. Seghier A. Ramlackhansingh J. Crinion A.P. Leff C.J. Price Lesion identification using unified segmentation-normalisation models and fuzzy clustering Neuroimage 41 4 2008 1253 1266 10.1016/j.neuroimage.2008.03.028 Shattuck, Prasad, Mirza, Narr, Toga, 2009 D.W. Shattuck G. Prasad M. Mirza K.L. Narr A.W. Toga Online resource for validation of brain segmentation methods Neuroimage 45 2 2009 431 439 10.1016/j.neuroimage.2008.10.066 Sled, Zijdenbos, Evans, 1998 J.G. Sled A.P. Zijdenbos A.C. Evans A nonparametric method for automatic correction of intensity nonuniformity in MRI data IEEE Trans. Med. Imag. 17 1 1998 87 97 10.1109/42.668698 Soltanian-Zadeh, Bagher-Ebadian, Ewing, et al., 2007 H. Soltanian-Zadeh H. Bagher-Ebadian J.R. Ewing Multiparametric iterative self-organizing data analysis of ischemic lesions using pre- or post-Gd T1 MRI Cerebrovasc. Dis. 23 2–3 2007 91 102 10.1159/000097044 Straka, Albers, Bammer, 2010 M. Straka G.W. Albers R. Bammer Real-time diffusion-perfusion mismatch analysis in acute stroke J. Magn. Reson. Imag. 32 5 2010 1024 1037 10.1002/jmri.22338 Styner, Lee, Chin, et al., 2008 M. Styner J. Lee B. Chin 3D segmentation in the clinic: a grand challenge II: MS lesion segmentation Midas J 2008 Takasawa, Jones, Guadagno, et al., 2008 M. Takasawa P.S. Jones J.V. Guadagno How reliable is perfusion MR in acute stroke? Validation and determination of the penumbra threshold against quantitative PET Stroke 39 3 2008 870 877 10.1161/STROKEAHA.107.500090 Tobon-Gomez, De Craene, McLeod, et al., 2013 C. Tobon-Gomez M. De Craene K. McLeod Benchmarking framework for myocardial tracking and deformation algorithms: an open access database Med. Image Anal. 17 6 2013 632 648 10.1016/j.media.2013.03.008 Tsai, Peng, Chen, et al., 2014 J.-Z. Tsai S.-J. Peng Y.-W. Chen Automatic detection and quantification of acute cerebral infarct by fuzzy clustering and histographic characterization on diffusion weighted MR imaging and apparent diffusion coefficient map Biomed. Res. Int. 2014 2014 13 10.1155/2014/963032 Urban, Bendszus, Hamprecht, Kleesiek, 2014 G. Urban M. Bendszus F.A. Hamprecht J. Kleesiek Multi-modal brain tumor segmentation using deep convolutional neural networks MICCAI BraTS (Brain Tumor Segmentation) Challenge. Proceedings, Win. Contrib. 2014 31 35 Wang, Budiman Gosno, Li, 2015 C.-W. Wang E. Budiman Gosno Y.-S. Li Fully automatic and robust 3D registration of serial-section microscopic images Sci. Rep. 5 2015 15051 10.1038/srep15051 Warfield, Zou, Wells, 2004 S.K. Warfield K.H. Zou W.M. Wells Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation Med. Imag. IEEE Trans. 23 7 2004 903 921 Wheeler, Mlynash, Inoue, et al., 2013 H.M. Wheeler M. Mlynash M. Inoue Early diffusion-weighted imaging and perfusion-weighted imaging lesion volumes forecast final infarct size in DEFUSE 2 Stroke 44 3 2013 681 685 10.1161/STROKEAHA.111.000135 WHO, 2012 WHO Cause-specific mortality - estimates for 2000–2012 Technical Report 2012 Wilcoxon, 1945 F. Wilcoxon Individual comparisons by ranking methods Biometrics Bull. 1 6 1945 80 83 Woloszynski, Kurzynski, 2011 T. Woloszynski M. Kurzynski A probabilistic model of classifier competence for dynamic ensemble selection Pattern Recognit. 44 10–11 2011 2656 2668 10.1016/j.patcog.2011.03.020 Xu, Krzyzak, Suen, 1992 L. Xu A. Krzyzak C.Y. Suen Methods of combining multiple classifiers and their applications to handwriting recognition Syst. Man Cybern. IEEE Trans. 22 3 1992 418 435",,Medical Image Analysis,,"Algorithms; Benchmarking; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Stroke",2016-07-21,2016,2016-07-21,2017-01,35,,250-269,All OA, Green,Article,"Maier, Oskar; Menze, Bjoern H.; von der Gablentz, Janina; Häni, Levin; Heinrich, Mattias P.; Liebrand, Matthias; Winzeck, Stefan; Basit, Abdul; Bentley, Paul; Chen, Liang; Christiaens, Daan; Dutil, Francis; Egger, Karl; Feng, Chaolu; Glocker, Ben; Götz, Michael; Haeck, Tom; Halme, Hanna-Leena; Havaei, Mohammad; Iftekharuddin, Khan M.; Jodoin, Pierre-Marc; Kamnitsas, Konstantinos; Kellner, Elias; Korvenoja, Antti; Larochelle, Hugo; Ledig, Christian; Lee, Jia-Hong; Maes, Frederik; Mahmood, Qaiser; Maier-Hein, Klaus H.; McKinley, Richard; Muschelli, John; Pal, Chris; Pei, Linmin; Rangarajan, Janaki Raman; Reza, Syed M.S.; Robben, David; Rueckert, Daniel; Salli, Eero; Suetens, Paul; Wang, Ching-Wei; Wilms, Matthias; Kirschke, Jan S.; Krämer, Ulrike M.; Münte, Thomas F.; Schramm, Peter; Wiest, Roland; Handels, Heinz; Reyes, Mauricio","Maier, Oskar (Institute of Medical Informatics, University of Lübeck, Lübeck, Germany; Graduate School for Computing in Medicine and Live Science, University of Lübeck, Germany); Menze, Bjoern H. (Institute for Advanced Study and Department of Computer Science, Technische Universität München, Munich, Germany); von der Gablentz, Janina (Department of Neurology, University of Lübeck, Germany); Häni, Levin (Institute for Surgical Technology and Biomechanics, University of Bern, Bern, Switzerland); Heinrich, Mattias P. (Institute of Medical Informatics, University of Lübeck, Lübeck, Germany); Liebrand, Matthias (Department of Neurology, University of Lübeck, Germany); Winzeck, Stefan (Institute for Advanced Study and Department of Computer Science, Technische Universität München, Munich, Germany); Basit, Abdul (Pakistan Institute of Nuclear Science and Technology, Islamabad, Pakistan); Bentley, Paul (Division of Brain Sciences, Department of Medicine, Imperial College London, UK); Chen, Liang (Biomedical Image Analysis Group, Department of Computing, Imperial College London, UK; Division of Brain Sciences, Department of Medicine, Imperial College London, UK); Christiaens, Daan (ESAT/PSI, Department of Electrical Engineering, KU Leuven, Belgium; iMinds, Medical IT Department, KU Leuven, Belgium; Medical Imaging Research Center, UZ Leuven, Belgium); Dutil, Francis (Université de Sherbrooke, Sherbrooke, Qc, Canada); Egger, Karl (Department of Neuroradiology, University Medical Center Freiburg, Germany); Feng, Chaolu (College of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China); Glocker, Ben (Biomedical Image Analysis Group, Department of Computing, Imperial College London, UK); Götz, Michael (Junior Group Medical Image Computing, German Cancer Research Center, Heidelberg, Germany); Haeck, Tom (ESAT/PSI, Department of Electrical Engineering, KU Leuven, Belgium; iMinds, Medical IT Department, KU Leuven, Belgium; Medical Imaging Research Center, UZ Leuven, Belgium); Halme, Hanna-Leena (HUS Medical Imaging Center, Radiology, University of Helsinki and Helsinki University Hospital, Helsinki, Finland; Department of Neuroscience and Biomedical Engineering NBE, Aalto University School of Science, Aalto, Finland); Havaei, Mohammad (Université de Sherbrooke, Sherbrooke, Qc, Canada); Iftekharuddin, Khan M. (Vision Lab, Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA); Jodoin, Pierre-Marc (Université de Sherbrooke, Sherbrooke, Qc, Canada); Kamnitsas, Konstantinos (Biomedical Image Analysis Group, Department of Computing, Imperial College London, UK); Kellner, Elias (Department of Radiology, Medical Physics, University Medical Center Freiburg, Germany); Korvenoja, Antti (HUS Medical Imaging Center, Radiology, University of Helsinki and Helsinki University Hospital, Helsinki, Finland); Larochelle, Hugo (Université de Sherbrooke, Sherbrooke, Qc, Canada); Ledig, Christian (Biomedical Image Analysis Group, Department of Computing, Imperial College London, UK); Lee, Jia-Hong (Graduate Institute of Biomedical Engineering, National Taiwan University of Science and Technology, Taipei City, Taiwan); Maes, Frederik (ESAT/PSI, Department of Electrical Engineering, KU Leuven, Belgium; iMinds, Medical IT Department, KU Leuven, Belgium; Medical Imaging Research Center, UZ Leuven, Belgium); Mahmood, Qaiser (Signals and Systems, Chalmers University of Technology, Gothenburg, Sweden; Pakistan Institute of Nuclear Science and Technology, Islamabad, Pakistan); Maier-Hein, Klaus H. (Junior Group Medical Image Computing, German Cancer Research Center, Heidelberg, Germany); McKinley, Richard (Department of Diagnostic and Interventional Neuroradiology, Inselspital Bern, Switzerland); Muschelli, John (Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD, USA); Pal, Chris (Ecole Polytechnique de Montréal, Canada); Pei, Linmin (Vision Lab, Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA); Rangarajan, Janaki Raman (ESAT/PSI, Department of Electrical Engineering, KU Leuven, Belgium; iMinds, Medical IT Department, KU Leuven, Belgium; Medical Imaging Research Center, UZ Leuven, Belgium); Reza, Syed M.S. (Vision Lab, Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA); Robben, David (ESAT/PSI, Department of Electrical Engineering, KU Leuven, Belgium; iMinds, Medical IT Department, KU Leuven, Belgium; Medical Imaging Research Center, UZ Leuven, Belgium); Rueckert, Daniel (Biomedical Image Analysis Group, Department of Computing, Imperial College London, UK); Salli, Eero (HUS Medical Imaging Center, Radiology, University of Helsinki and Helsinki University Hospital, Helsinki, Finland); Suetens, Paul (ESAT/PSI, Department of Electrical Engineering, KU Leuven, Belgium; iMinds, Medical IT Department, KU Leuven, Belgium; Medical Imaging Research Center, UZ Leuven, Belgium); Wang, Ching-Wei (Graduate Institute of Biomedical Engineering, National Taiwan University of Science and Technology, Taipei City, Taiwan); Wilms, Matthias (Institute of Medical Informatics, University of Lübeck, Lübeck, Germany); Kirschke, Jan S. (Department of Neuroradiology, Klinikum rechts der Isar, Technische Universität München, Munich, Germany); Krämer, Ulrike M. (Department of Neurology, University of Lübeck, Germany; Institute of Psychology II, University of Lübeck, Germany); Münte, Thomas F. (Department of Neurology, University of Lübeck, Germany); Schramm, Peter (Institute of Neuroradiology, University Medical Center Lübeck, Germany); Wiest, Roland (Department of Diagnostic and Interventional Neuroradiology, Inselspital Bern, Switzerland); Handels, Heinz (Institute of Medical Informatics, University of Lübeck, Lübeck, Germany); Reyes, Mauricio (Institute for Surgical Technology and Biomechanics, University of Bern, Bern, Switzerland)","Maier, Oskar (University of Lübeck; University of Lübeck)","Maier, Oskar (University of Lübeck; University of Lübeck); Menze, Bjoern H. (Technical University of Munich); von der Gablentz, Janina (University of Lübeck); Häni, Levin (University of Bern); Heinrich, Mattias P. (University of Lübeck); Liebrand, Matthias (University of Lübeck); Winzeck, Stefan (Technical University of Munich); Basit, Abdul (Pakistan Institute of Nuclear Science and Technology); Bentley, Paul (Imperial College London); Chen, Liang (Imperial College London; Imperial College London); Christiaens, Daan (KU Leuven; KU Leuven; Universitair Ziekenhuis Leuven); Dutil, Francis (Université de Sherbrooke); Egger, Karl (University Medical Center Freiburg); Feng, Chaolu (Northeastern University); Glocker, Ben (Imperial College London); Götz, Michael (German Cancer Research Center); Haeck, Tom (KU Leuven; KU Leuven; Universitair Ziekenhuis Leuven); Halme, Hanna-Leena (Helsinki University Hospital; Aalto University); Havaei, Mohammad (Université de Sherbrooke); Iftekharuddin, Khan M. (Old Dominion University); Jodoin, Pierre-Marc (Université de Sherbrooke); Kamnitsas, Konstantinos (Imperial College London); Kellner, Elias (University Medical Center Freiburg); Korvenoja, Antti (Helsinki University Hospital); Larochelle, Hugo (Université de Sherbrooke); Ledig, Christian (Imperial College London); Lee, Jia-Hong (National Taiwan University of Science and Technology); Maes, Frederik (KU Leuven; KU Leuven; Universitair Ziekenhuis Leuven); Mahmood, Qaiser (Chalmers University of Technology; Pakistan Institute of Nuclear Science and Technology); Maier-Hein, Klaus H. (German Cancer Research Center); McKinley, Richard (University Hospital of Bern); Muschelli, John (Johns Hopkins University); Pal, Chris (Polytechnique Montréal); Pei, Linmin (Old Dominion University); Rangarajan, Janaki Raman (KU Leuven; KU Leuven; Universitair Ziekenhuis Leuven); Reza, Syed M.S. (Old Dominion University); Robben, David (KU Leuven; KU Leuven; Universitair Ziekenhuis Leuven); Rueckert, Daniel (Imperial College London); Salli, Eero (Helsinki University Hospital); Suetens, Paul (KU Leuven; KU Leuven; Universitair Ziekenhuis Leuven); Wang, Ching-Wei (National Taiwan University of Science and Technology); Wilms, Matthias (University of Lübeck); Kirschke, Jan S. (Rechts der Isar Hospital; Technical University of Munich); Krämer, Ulrike M. (University of Lübeck; University of Lübeck); Münte, Thomas F. (University of Lübeck); Schramm, Peter (); Wiest, Roland (University Hospital of Bern); Handels, Heinz (University of Lübeck); Reyes, Mauricio (University of Bern)",314,117,9.58,,https://helda.helsinki.fi/bitstream/10138/229842/1/1_s2.0_S1361841516301268_main.pdf,https://app.dimensions.ai/details/publication/pub.1042949781,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
3522,pub.1151796264,10.1177/17562872221128791,36249889,PMC9554123,A review of artificial intelligence in prostate cancer detection on imaging,"A multitude of studies have explored the role of artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists in prostate cancer detection, risk-stratification, and management. This review provides a comprehensive overview of relevant literature regarding the use of AI models in (1) detecting prostate cancer on radiology images (magnetic resonance and ultrasound imaging), (2) detecting prostate cancer on histopathology images of prostate biopsy tissue, and (3) assisting in supporting tasks for prostate cancer detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both the potential of these AI models to assist in the clinical workflow of prostate cancer diagnosis, as well as the current limitations including variability in training data sets, algorithms, and evaluation criteria. We also discuss ongoing challenges and what is needed to bridge the gap between academic research on AI for prostate cancer and commercial solutions that improve routine clinical care.",None.,"Funding The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: Research reported in this publication was supported by the National Cancer Institute of the National Institutes of Health (award no. R37CA260346). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The authors acknowledge the following funding sources: Departments of Radiology and Urology, Stanford University, GE Healthcare Blue Sky Award, National Institutes of Health, National Cancer Institute (grant no. U01CA196387, to JDB), and the generous philanthropic support of our patients (GS).",Therapeutic Advances in Urology,,,2022-01,2022,2022-10-10,2022-01,14,,"1,75628722211288E+016",All OA, Gold,Article,"Bhattacharya, Indrani; Khandwala, Yash S.; Vesal, Sulaiman; Shao, Wei; Yang, Qianye; Soerensen, Simon J.C.; Fan, Richard E.; Ghanouni, Pejman; Kunder, Christian A.; Brooks, James D.; Hu, Yipeng; Rusu, Mirabela; Sonn, Geoffrey A.","Bhattacharya, Indrani (Department of Radiology, Stanford University School of Medicine, 1201 Welch Road, Stanford, CA 94305, USA; Department of Urology, Stanford University School of Medicine, Stanford, CA, USA); Khandwala, Yash S. (Department of Urology, Stanford University School of Medicine, Stanford, CA, USA); Vesal, Sulaiman (Department of Urology, Stanford University School of Medicine, Stanford, CA, USA); Shao, Wei (Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA); Yang, Qianye (Centre for Medical Image Computing, University College London, London, UK; Wellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK); Soerensen, Simon J.C. (Department of Urology, Stanford University School of Medicine, Stanford, CA, USA; Department of Epidemiology & Population Health, Stanford University School of Medicine, Stanford, CA, USA); Fan, Richard E. (Department of Urology, Stanford University School of Medicine, Stanford, CA, USA); Ghanouni, Pejman (Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA; Department of Urology, Stanford University School of Medicine, Stanford, CA, USA); Kunder, Christian A. (Department of Pathology, Stanford University School of Medicine, Stanford, CA, USA); Brooks, James D. (Department of Urology, Stanford University School of Medicine, Stanford, CA, USA); Hu, Yipeng (Centre for Medical Image Computing, University College London, London, UK; Wellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK); Rusu, Mirabela (Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA); Sonn, Geoffrey A. (Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA; Department of Urology, Stanford University School of Medicine, Stanford, CA, USA)","Bhattacharya, Indrani (Stanford University; Stanford University)","Bhattacharya, Indrani (Stanford University; Stanford University); Khandwala, Yash S. (Stanford University); Vesal, Sulaiman (Stanford University); Shao, Wei (Stanford University); Yang, Qianye (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Soerensen, Simon J.C. (Stanford University; Stanford University); Fan, Richard E. (Stanford University); Ghanouni, Pejman (Stanford University; Stanford University); Kunder, Christian A. (Stanford University); Brooks, James D. (Stanford University); Hu, Yipeng (University College London; Wellcome / EPSRC Centre for Interventional and Surgical Sciences); Rusu, Mirabela (Stanford University); Sonn, Geoffrey A. (Stanford University; Stanford University)",0,0,,,https://doi.org/10.1177/17562872221128791,https://app.dimensions.ai/details/publication/pub.1151796264,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
3515,pub.1140592380,10.1016/j.artmed.2021.102154,34531013,,EMONAS-Net: Efficient multiobjective neural architecture search using surrogate-assisted evolutionary algorithm for 3D medical image segmentation,"Deep learning plays a critical role in medical image segmentation. Nevertheless, manually designing a neural network for a specific segmentation problem is a very difficult and time-consuming task due to the massive hyperparameter search space, long training time and large volumetric data. Therefore, most designed networks are highly complex, task specific and over-parametrized. Recently, multiobjective neural architecture search (NAS) methods have been proposed to automate the design of accurate and efficient segmentation architectures. However, they only search for either the micro- or macro-structure of the architecture, do not use the information produced during the optimization process to increase the efficiency of the search, or do not consider the volumetric nature of medical images. In this work, we present EMONAS-Net, an Efficient MultiObjective NAS framework for 3D medical image segmentation that optimizes both the segmentation accuracy and size of the network. EMONAS-Net has two key components, a novel search space that considers the configuration of the micro- and macro-structure of the architecture and a Surrogate-assisted Multiobjective Evolutionary based Algorithm (SaMEA algorithm) that efficiently searches for the best hyperparameter values. The SaMEA algorithm uses the information collected during the initial generations of the evolutionary process to identify the most promising subproblems and select the best performing hyperparameter values during mutation to improve the convergence speed. Furthermore, a Random Forest surrogate model is incorporated to accelerate the fitness evaluation of the candidate architectures. EMONAS-Net is tested on the tasks of prostate segmentation from the MICCAI PROMISE12 challenge, hippocampus segmentation from the Medical Segmentation Decathlon challenge, and cardiac segmentation from the MICCAI ACDC challenge. In all the benchmarks, the proposed framework finds architectures that perform better or comparable with competing state-of-the-art NAS methods while being considerably smaller and reducing the architecture search time by more than 50%.",,"This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.",Artificial Intelligence in Medicine,,"Algorithms; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Male; Neural Networks, Computer",2021-08-24,2021,2021-08-24,2021-09,119,,102154,Closed,Article,"Baldeon Calisto, Maria; Lai-Yuen, Susana K","Baldeon Calisto, Maria (Departamento de Ingeniería Industrial, Instituto de Innovación en Productividad y Logística CATENA-USFQ, Colegio de Ciencias e Ingeniería, Universidad San Francisco de Quito, Diego de Robles s/n y Vía Interoceánica, Quito 170901, Ecuador.); Lai-Yuen, Susana K (Department of Industrial and Management Systems Engineering, University of South Florida, 4202 E. Fowler Avenue, Tampa, FL 33620, USA. Electronic address: laiyuen@usf.edu.)","Lai-Yuen, Susana K (University of South Florida)","Baldeon Calisto, Maria (Universidad San Francisco de Quito); Lai-Yuen, Susana K (University of South Florida)",11,11,0.17,8.71,,https://app.dimensions.ai/details/publication/pub.1140592380,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,,
3463,pub.1129798581,10.1117/1.jmi.7.4.046501,32743017,PMC7390892,Machine learning for the prediction of pseudorealistic pediatric abdominal phantoms for radiation dose reconstruction,"Purpose: Current phantoms used for the dose reconstruction of long-term childhood cancer survivors lack individualization. We design a method to predict highly individualized abdominal three-dimensional (3-D) phantoms automatically. Approach: We train machine learning (ML) models to map (2-D) patient features to 3-D organ-at-risk (OAR) metrics upon a database of 60 pediatric abdominal computed tomographies with liver and spleen segmentations. Next, we use the models in an automatic pipeline that outputs a personalized phantom given the patient's features, by assembling 3-D imaging from the database. A step to improve phantom realism (i.e., avoid OAR overlap) is included. We compare five ML algorithms, in terms of predicting OAR left-right (LR), anterior-posterior (AP), inferior-superior (IS) positions, and surface Dice-Sørensen coefficient (sDSC). Furthermore, two existing human-designed phantom construction criteria and two additional control methods are investigated for comparison. Results: Different ML algorithms result in similar test mean absolute errors: ∼ 8    mm  for liver LR, IS, and spleen AP, IS; ∼ 5    mm  for liver AP and spleen LR; ∼ 80 %  for abdomen sDSC; and ∼ 60 %  to 65% for liver and spleen sDSC. One ML algorithm (GP-GOMEA) significantly performs the best for 6/9 metrics. The control methods and the human-designed criteria in particular perform generally worse, sometimes substantially ( + 5 - mm  error for spleen IS, - 10 %  sDSC for liver). The automatic step to improve realism generally results in limited metric accuracy loss, but fails in one case (out of 60). Conclusion: Our ML-based pipeline leads to phantoms that are significantly and substantially more individualized than currently used human-designed criteria.","The authors acknowledge the Kinderen Kankervrij Foundation for financial support (Project #187) and the Maurits and Anna de Kock Foundation for financing a high-performance computing system. We thank Dr. Brian V. Balgobind, Dr. Irma W. E. M. van Dijk, and Dr. Jan Wiersma from the Department of Radiation Oncology of Amsterdam UMC, location AMC, Amsterdam, the Netherlands, and Dr. Geert O. R. Janssens and Dr. Petra Kroon from the Department of Radiation Oncology of UMC Utrecht Cancer Center, Utrecht, the Netherlands, for providing help in the collection and/or in the assessment of the imaging data used in this work. The authors are grateful to Elekta for providing ADMIRE research software for automatic organ segmentation. We further acknowledge Dr. Choonsik Lee from the National Cancer Institute, Division of Cancer Epidemiology and Genetics, Rockville, Maryland, USA, for details on the phantom library of the University of Florida/National Cancer Institute. This article is based upon and extends (by introducing the anatomical inconsistency correction method and related results), an SPIE proceedings paper of which an abstract has been recently submitted for consideration to the SPIE conference on Medical Imaging (2020), titled “Machine Learning for Automatic Construction of Pediatric Abdominal Phantoms for Radiation Dose Reconstruction,” authored by the same authors of this article.",,Journal of Medical Imaging,,,2020-07-30,2020,2020-07-30,2020-07,7,4,046501-046501,Closed,Article,"Virgolin, Marco; Wang, Ziyuan; Alderliesten, Tanja; Bosman, Peter A. N.","Virgolin, Marco (Centrum Wiskunde and Informatica, Life Sciences and Health Group, Amsterdam, The Netherlands); Wang, Ziyuan (Amsterdam UMC, University of Amsterdam, Department of Radiation Oncology, Amsterdam, The Netherlands); Alderliesten, Tanja (Amsterdam UMC, University of Amsterdam, Department of Radiation Oncology, Amsterdam, The Netherlands; Leiden University Medical Center, Department of Radiation Oncology, Leiden, The Netherlands); Bosman, Peter A. N. (Centrum Wiskunde and Informatica, Life Sciences and Health Group, Amsterdam, The Netherlands; Delft University of Technology, Algorithmics Group, Delft, The Netherlands)","Virgolin, Marco (Centrum Wiskunde & Informatica)","Virgolin, Marco (Centrum Wiskunde & Informatica); Wang, Ziyuan (University of Amsterdam); Alderliesten, Tanja (University of Amsterdam; Leiden University Medical Center); Bosman, Peter A. N. (Centrum Wiskunde & Informatica; Delft University of Technology)",2,2,0.36,0.74,,https://app.dimensions.ai/details/publication/pub.1129798581,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,,
3457,pub.1125815259,10.1007/s11263-020-01321-2,34149167,PMC8211108,Adversarial Confidence Learning for Medical Image Segmentation and Synthesis,"Generative adversarial networks (GAN) are widely used in medical image analysis tasks, such as medical image segmentation and synthesis. In these works, adversarial learning is directly applied to the original supervised segmentation (synthesis) networks. The usage of adversarial learning is effective in improving visual perception performance since adversarial learning works as realistic regularization for supervised generators. However, the quantitative performance often cannot improve as much as the qualitative performance, and it can even become worse in some cases. In this paper, we explore how we can take better advantage of adversarial learning in supervised segmentation (synthesis) models and propose an adversarial confidence learning framework to better model these problems. We analyze the roles of discriminator in the classic GANs and compare them with those in supervised adversarial systems. Based on this analysis, we propose adversarial confidence learning, i.e., besides the adversarial learning for emphasizing visual perception, we use the confidence information provided by the adversarial network to enhance the design of supervised segmentation (synthesis) network. In particular, we propose using a fully convolutional adversarial network for confidence learning to provide voxel-wise and region-wise confidence information for the segmentation (synthesis) network. With these settings, we propose a difficulty-aware attention mechanism to properly handle hard samples or regions by taking structural information into consideration so that we can better deal with the irregular distribution of medical data. Furthermore, we investigate the loss functions of various GANs and propose using the binary cross entropy loss to train the proposed adversarial system so that we can retain the unlimited modeling capacity of the discriminator. Experimental results on clinical and challenge datasets show that our proposed network can achieve state-of-the-art segmentation (synthesis) accuracy. Further analysis also indicates that adversarial confidence learning can both improve the visual perception performance and the quantitative performance.",This work was supported by the National Institutes of Health under Grant R01 CA206100.,,International Journal of Computer Vision,,,2020-03-21,2020,2020-03-21,2020-11,128,10-11,2494-2513,All OA, Green,Article,"Nie, Dong; Shen, Dinggang","Nie, Dong (Department of Computer Science, University of North Carolina at Chapel Hill, 27514, Chapel Hill, NC, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, 27514, Chapel Hill, NC, USA); Shen, Dinggang (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, 27514, Chapel Hill, NC, USA; Department of Brain and Cognitive Engineering, Korea University, 02841, Seoul, Republic of Korea)","Nie, Dong (University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill); Shen, Dinggang (University of North Carolina at Chapel Hill; Korea University)","Nie, Dong (University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill); Shen, Dinggang (University of North Carolina at Chapel Hill; Korea University)",26,23,1.18,13.4,https://europepmc.org/articles/pmc8211108?pdf=render,https://app.dimensions.ai/details/publication/pub.1125815259,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
3323,pub.1120871333,10.1155/2019/1464592,31582963,PMC6748179,A Semi-Automated Usability Evaluation Framework for Interactive Image Segmentation Systems,"For complex segmentation tasks, the achievable accuracy of fully automated systems is inherently limited. Specifically, when a precise segmentation result is desired for a small amount of given data sets, semi-automatic methods exhibit a clear benefit for the user. The optimization of human computer interaction (HCI) is an essential part of interactive image segmentation. Nevertheless, publications introducing novel interactive segmentation systems (ISS) often lack an objective comparison of HCI aspects. It is demonstrated that even when the underlying segmentation algorithm is the same throughout interactive prototypes, their user experience may vary substantially. As a result, users prefer simple interfaces as well as a considerable degree of freedom to control each iterative step of the segmentation. In this article, an objective method for the comparison of ISS is proposed, based on extensive user studies. A summative qualitative content analysis is conducted via abstraction of visual and verbal feedback given by the participants. A direct assessment of the segmentation system is executed by the users via the system usability scale (SUS) and AttrakDiff-2 questionnaires. Furthermore, an approximation of the findings regarding usability aspects in those studies is introduced, conducted solely from the system-measurable user actions during their usage of interactive segmentation prototypes. The prediction of all questionnaire results has an average relative error of 8.9%, which is close to the expected precision of the questionnaire results themselves. This automated evaluation scheme may significantly reduce the resources necessary to investigate each variation of a prototype's user interface (UI) features and segmentation methodologies.",Thanks are due to Christian Kisker and Carina Lehle for their hard work with the data collection.,,International Journal of Biomedical Imaging,,,2019-09-05,2019,2019-09-05,2019-09-05,2019,,1464592,All OA, Gold,Article,"Amrehn, Mario; Steidl, Stefan; Kortekaas, Reinier; Strumia, Maddalena; Weingarten, Markus; Kowarschik, Markus; Maier, Andreas","Amrehn, Mario (The Pattern Recognition Lab, Computer Science Department, Friedrich-Alexander University Erlangen-Nuremberg, Germany); Steidl, Stefan (The Pattern Recognition Lab, Computer Science Department, Friedrich-Alexander University Erlangen-Nuremberg, Germany); Kortekaas, Reinier (Siemens Healthineers AG, Forchheim, Germany); Strumia, Maddalena (Siemens Healthineers AG, Forchheim, Germany); Weingarten, Markus (Siemens Healthineers AG, Forchheim, Germany); Kowarschik, Markus (Siemens Healthineers AG, Forchheim, Germany); Maier, Andreas (The Pattern Recognition Lab, Computer Science Department, Friedrich-Alexander University Erlangen-Nuremberg, Germany)","Amrehn, Mario (University of Erlangen-Nuremberg)","Amrehn, Mario (University of Erlangen-Nuremberg); Steidl, Stefan (University of Erlangen-Nuremberg); Kortekaas, Reinier (Siemens Healthcare (Germany)); Strumia, Maddalena (Siemens Healthcare (Germany)); Weingarten, Markus (Siemens Healthcare (Germany)); Kowarschik, Markus (Siemens Healthcare (Germany)); Maier, Andreas (University of Erlangen-Nuremberg)",5,3,0.08,2.11,https://downloads.hindawi.com/journals/ijbi/2019/1464592.pdf,https://app.dimensions.ai/details/publication/pub.1120871333,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4608 Human-Centred Computing",,,,,,,,,,,
3182,pub.1113836002,10.1016/j.media.2019.04.008,31096134,PMC6554617,An algorithm for learning shape and appearance models without annotations,"This paper presents a framework for automatically learning shape and appearance models for medical (and certain other) images. The algorithm was developed with the aim of eventually enabling distributed privacy-preserving analysis of brain image data, such that shared information (shape and appearance basis functions) may be passed across sites, whereas latent variables that encode individual images remain secure within each site. These latent variables are proposed as features for privacy-preserving data mining applications. The approach is demonstrated qualitatively on the KDEF dataset of 2D face images, showing that it can align images that traditionally require shape and appearance models trained using manually annotated data (manually defined landmarks etc.). It is applied to the MNIST dataset of handwritten digits to show its potential for machine learning applications, particularly when training data is limited. The model is able to handle ""missing data"", which allows it to be cross-validated according to how well it can predict left-out voxels. The suitability of the derived features for classifying individuals into patient groups was assessed by applying it to a dataset of over 1900 segmented T1-weighted MR images, which included images from the COBRE and ABIDE datasets.","This project has received funding from the European Unions Horizon 2020 Research and Innovation Programme under Grant Agreement No. 720270 (HBP SGA1). YB has been supported by the MRC and Spinal Research Charity through the ERA-NET Neuron joint call (MR/R000050/1). The Wellcome Centre for Human Neuroimaging is supported by core funding from the Wellcome Trust [grant number 203147/Z/16/Z]. Funding for the OASIS dataset came from the following grants: P50 AG05681, P01 AG03991, R01 AG021910, P20 MH071616, U24 RR021382. Funding for the ABIDE I dataset came from a variety of sources, which include NIMH (K23MH087770 and R03MH096321), the Leon Levy Foundation, Joseph P. Healy and the Stavros Niarchos Foundation to the Child Mind Institute. The imaging data and phenotypic information of the COBRE dataset was collected and shared by the Mind Research Network and the University of New Mexico funded by a National Institute of Health Center of Biomedical Research Excellence (COBRE) grant 1P20RR021938-01A2.",,Medical Image Analysis,,"Algorithms; Brain; Face; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Machine Learning; Magnetic Resonance Imaging",2019-04-30,2019,2019-04-30,2019-07,55,,197-215,All OA, Hybrid,Article,"Ashburner, John; Brudfors, Mikael; Bronik, Kevin; Balbastre, Yaël","Ashburner, John (Wellcome Centre for Human NeuroimagingUCL Queen Square Institute of Neurology12 Queen Square, London, WC1N 3AR, UK. Electronic address: j.ashburner@ucl.ac.uk.); Brudfors, Mikael (Wellcome Centre for Human NeuroimagingUCL Queen Square Institute of Neurology12 Queen Square, London, WC1N 3AR, UK.); Bronik, Kevin (Wellcome Centre for Human NeuroimagingUCL Queen Square Institute of Neurology12 Queen Square, London, WC1N 3AR, UK.); Balbastre, Yaël (Wellcome Centre for Human NeuroimagingUCL Queen Square Institute of Neurology12 Queen Square, London, WC1N 3AR, UK.)","Ashburner, John (National Hospital for Neurology and Neurosurgery)","Ashburner, John (National Hospital for Neurology and Neurosurgery); Brudfors, Mikael (National Hospital for Neurology and Neurosurgery); Bronik, Kevin (National Hospital for Neurology and Neurosurgery); Balbastre, Yaël (National Hospital for Neurology and Neurosurgery)",10,5,0.14,,https://doi.org/10.1016/j.media.2019.04.008,https://app.dimensions.ai/details/publication/pub.1113836002,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
3157,pub.1090869220,10.1016/j.media.2017.07.004,28772163,PMC5666910,"Designing image segmentation studies: Statistical power, sample size and reference standard quality","Segmentation algorithms are typically evaluated by comparison to an accepted reference standard. The cost of generating accurate reference standards for medical image segmentation can be substantial. Since the study cost and the likelihood of detecting a clinically meaningful difference in accuracy both depend on the size and on the quality of the study reference standard, balancing these trade-offs supports the efficient use of research resources. In this work, we derive a statistical power calculation that enables researchers to estimate the appropriate sample size to detect clinically meaningful differences in segmentation accuracy (i.e. the proportion of voxels matching the reference standard) between two algorithms. Furthermore, we derive a formula to relate reference standard errors to their effect on the sample sizes of studies using lower-quality (but potentially more affordable and practically available) reference standards. The accuracy of the derived sample size formula was estimated through Monte Carlo simulation, demonstrating, with 95% confidence, a predicted statistical power within 4% of simulated values across a range of model parameters. This corresponds to sample size errors of less than 4 subjects and errors in the detectable accuracy difference less than 0.6%. The applicability of the formula to real-world data was assessed using bootstrap resampling simulations for pairs of algorithms from the PROMISE12 prostate MR segmentation challenge data set. The model predicted the simulated power for the majority of algorithm pairs within 4% for simulated experiments using a high-quality reference standard and within 6% for simulated experiments using a low-quality reference standard. A case study, also based on the PROMISE12 data, illustrates using the formulae to evaluate whether to use a lower-quality reference standard in a prostate segmentation study.","This work was supported by the UK Medical Research Council, Radboud University Medical Centre and the Canadian Institutes of Health Research. Yipeng Hu is funded by Cancer Research UK and the UK Engineering and Physical Sciences Research Council (EPSRC) as part of the UCL-KCL Comprehensive Cancer Imaging Centre.",,Medical Image Analysis,,"Algorithms; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Models, Statistical; Prostatic Neoplasms; Reference Standards; Reproducibility of Results; Sample Size; Sensitivity and Specificity",2017-07-22,2017,2017-07-22,2017-12,42,,44-59,All OA, Hybrid,Article,"Gibson, Eli; Hu, Yipeng; Huisman, Henkjan J.; Barratt, Dean C.","Gibson, Eli (Department of Radiology, Radboud University Medical Center, Nijmegen, The Netherlands; Department of Medical Physics and Biomedical Engineering, University College London, London, United Kingdom; Centre for Medical Image Computing, The Engineering Front Building, University College London, Malet Place, London, WC1E 6BT, United Kingdom); Hu, Yipeng (Department of Medical Physics and Biomedical Engineering, University College London, London, United Kingdom); Huisman, Henkjan J. (Department of Radiology, Radboud University Medical Center, Nijmegen, The Netherlands); Barratt, Dean C. (Department of Medical Physics and Biomedical Engineering, University College London, London, United Kingdom)","Gibson, Eli (Radboud University Nijmegen Medical Centre; University College London; University College London)","Gibson, Eli (Radboud University Nijmegen Medical Centre; University College London; University College London); Hu, Yipeng (University College London); Huisman, Henkjan J. (Radboud University Nijmegen Medical Centre); Barratt, Dean C. (University College London)",12,5,0.74,,https://doi.org/10.1016/j.media.2017.07.004,https://app.dimensions.ai/details/publication/pub.1090869220,32 Biomedical and Clinical Sciences, 40 Engineering,,,,,,,,,,
3152,pub.1059031425,10.1088/0031-9155/61/13/4796,27272935,,Computer-aided detection of prostate cancer in T2-weighted MRI within the peripheral zone,"In this paper we propose a prostate cancer computer-aided diagnosis (CAD) system and suggest a set of discriminant texture descriptors extracted from T2-weighted MRI data which can be used as a good basis for a multimodality system. For this purpose, 215 texture descriptors were extracted and eleven different classifiers were employed to achieve the best possible results. The proposed method was tested based on 418 T2-weighted MR images taken from 45 patients and evaluated using 9-fold cross validation with five patients in each fold. The results demonstrated comparable results to existing CAD systems using multimodality MRI. We achieved an area under the receiver operating curve (A z ) values equal to [Formula: see text], [Formula: see text], [Formula: see text] and [Formula: see text] for Bayesian networks, ADTree, random forest and multilayer perceptron classifiers, respectively, while a meta-voting classifier using average probability as a combination rule achieved [Formula: see text].",A Rampun is grateful for the awards given by Aberystwyth University under the Departmental Overseas Scholarship (DOS) and Doctoral Career Development Scholarships (DCDS). This work was funded in part by the NISCHR Biomedical Research Unit for Advanced Medical Imaging and Visualization.,,Physics in Medicine and Biology,,"Algorithms; Bayes Theorem; Diagnosis, Computer-Assisted; Humans; Magnetic Resonance Imaging; Male; Probability; Prostatic Neoplasms; ROC Curve",2016-06-08,2016,2016-06-08,2016-07-07,61,13,4796-4825,All OA, Green,Article,"Rampun, Andrik; Zheng, Ling; Malcolm, Paul; Tiddeman, Bernie; Zwiggelaar, Reyer","Rampun, Andrik (Department of Computer Science, Aberystwyth University, Aberystwyth SY23 3DB, UK); Zheng, Ling (Department of Computer Science, Aberystwyth University, Aberystwyth SY23 3DB, UK); Malcolm, Paul (Department of Radiology, Norfolk Norwich University Hospital, Norwich NR4 7UY, UK); Tiddeman, Bernie (Department of Computer Science, Aberystwyth University, Aberystwyth SY23 3DB, UK); Zwiggelaar, Reyer (Department of Computer Science, Aberystwyth University, Aberystwyth SY23 3DB, UK)",,"Rampun, Andrik (Aberystwyth University); Zheng, Ling (Aberystwyth University); Malcolm, Paul (Norfolk and Norwich University Hospital); Tiddeman, Bernie (Aberystwyth University); Zwiggelaar, Reyer (Aberystwyth University)",32,7,0.98,25.03,https://pure.aber.ac.uk/portal/files/9271217/PMB.pdf,https://app.dimensions.ai/details/publication/pub.1059031425,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
2817,pub.1120604563,10.3390/cancers11091235,31450799,PMC6770116,Cancer Diagnosis Using Deep Learning: A Bibliographic Review,"In this paper, we first describe the basics of the field of cancer diagnosis, which includes steps of cancer diagnosis followed by the typical classification methods used by doctors, providing a historical idea of cancer classification techniques to the readers. These methods include Asymmetry, Border, Color and Diameter (ABCD) method, seven-point detection method, Menzies method, and pattern analysis. They are used regularly by doctors for cancer diagnosis, although they are not considered very efficient for obtaining better performance. Moreover, considering all types of audience, the basic evaluation criteria are also discussed. The criteria include the receiver operating characteristic curve (ROC curve), Area under the ROC curve (AUC), F1 score, accuracy, specificity, sensitivity, precision, dice-coefficient, average accuracy, and Jaccard index. Previously used methods are considered inefficient, asking for better and smarter methods for cancer diagnosis. Artificial intelligence and cancer diagnosis are gaining attention as a way to define better diagnostic tools. In particular, deep neural networks can be successfully used for intelligent image analysis. The basic framework of how this machine learning works on medical imaging is provided in this study, i.e., pre-processing, image segmentation and post-processing. The second part of this manuscript describes the different deep learning techniques, such as convolutional neural networks (CNNs), generative adversarial models (GANs), deep autoencoders (DANs), restricted Boltzmann's machine (RBM), stacked autoencoders (SAE), convolutional autoencoders (CAE), recurrent neural networks (RNNs), long short-term memory (LTSM), multi-scale convolutional neural network (M-CNN), multi-instance learning convolutional neural network (MIL-CNN). For each technique, we provide Python codes, to allow interested readers to experiment with the cited algorithms on their own diagnostic problems. The third part of this manuscript compiles the successfully applied deep learning models for different types of cancers. Considering the length of the manuscript, we restrict ourselves to the discussion of breast cancer, lung cancer, brain cancer, and skin cancer. The purpose of this bibliographic review is to provide researchers opting to work in implementing deep learning and artificial neural networks for cancer diagnosis a knowledge from scratch of the state-of-the-art achievements.",,This research received no external funding.,Cancers,,,2019-08-23,2019,2019-08-23,,11,9,1235,All OA, Gold,Article,"Munir, Khushboo; Elahi, Hassan; Ayub, Afsheen; Frezza, Fabrizio; Rizzi, Antonello","Munir, Khushboo (Department of Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome, Via Eudossiana 18, 00184 Rome, Italy); Elahi, Hassan (Department of Mechanical and Aerospace Engineering (DIMA), Sapienza University of Rome, Via Eudossiana 18, 00184 Rome, Italy); Ayub, Afsheen (Department of Basic and Applied Science for Engineering (SBAI), Sapienza University of Rome, Via Antonio Scarpa 14/16, 00161 Rome, Italy); Frezza, Fabrizio (Department of Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome, Via Eudossiana 18, 00184 Rome, Italy); Rizzi, Antonello (Department of Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome, Via Eudossiana 18, 00184 Rome, Italy)","Munir, Khushboo (Sapienza University of Rome)","Munir, Khushboo (Sapienza University of Rome); Elahi, Hassan (Sapienza University of Rome); Ayub, Afsheen (Sapienza University of Rome); Frezza, Fabrizio (Sapienza University of Rome); Rizzi, Antonello (Sapienza University of Rome)",207,171,7.84,62.04,https://www.mdpi.com/2072-6694/11/9/1235/pdf,https://app.dimensions.ai/details/publication/pub.1120604563,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
2817,pub.1031644012,10.1016/j.media.2015.11.003,26716720,,"Joint optimization of segmentation and shape prior from level-set-based statistical shape model, and its application to the automated segmentation of abdominal organs","The goal of this study is to provide a theoretical framework for accurately optimizing the segmentation energy considering all of the possible shapes generated from the level-set-based statistical shape model (SSM). The proposed algorithm solves the well-known open problem, in which a shape prior may not be optimal in terms of an objective functional that needs to be minimized during segmentation. The algorithm allows the selection of an optimal shape prior from among all possible shapes generated from an SSM by conducting a branch-and-bound search over an eigenshape space. The proposed algorithm does not require predefined shape templates or the construction of a hierarchical clustering tree before graph-cut segmentation. It jointly optimizes an objective functional in terms of both the shape prior and segmentation labeling, and finds an optimal solution by considering all possible shapes generated from an SSM. We apply the proposed algorithm to both pancreas and spleen segmentation using multiphase computed tomography volumes, and we compare the results obtained with those produced by a conventional algorithm employing a branch-and-bound search over a search tree of predefined shapes, which were sampled discretely from an SSM. The proposed algorithm significantly improves the segmentation performance in terms of the Jaccard index and Dice similarity index. In addition, we compare the results with the state-of-the-art multiple abdominal organs segmentation algorithm, and confirmed that the performances of both algorithms are comparable to each other. We discuss the high computational efficiency of the proposed algorithm, which was determined experimentally using a normalized number of traversed nodes in a search tree, and the extensibility of the proposed algorithm to other SSMs or energy functionals.","The authors wish to acknowledge the assistance provided by Dr. Okada of the University of Tsukuba, and Prof. Sato of the Nara Institute of Science and Technology, for providing their segmentation algorithm with the training software and for helpful discussions. Part of this research was conducted under a Grant-in-aid for scientific research from the Japanese Ministry of Education, Culture, Sports, Science, and Technology (No. 21103007).",,Medical Image Analysis,,"Algorithms; Computer Simulation; Data Interpretation, Statistical; Humans; Imaging, Three-Dimensional; Models, Statistical; Pancreas; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Abdominal; Reproducibility of Results; Sensitivity and Specificity; Spleen; Subtraction Technique; Tomography, X-Ray Computed; Viscera",2015-12-04,2015,2015-12-04,2016-02,28,,46-65,Closed,Article,"Saito, Atsushi; Nawano, Shigeru; Shimizu, Akinobu","Saito, Atsushi (Tokyo University of Agriculture and Technology, 2-24-16 Nakacho, Koganei, Tokyo 184-8588, Japan); Nawano, Shigeru (Center for Radiological Sciences, International University of Health and Welfare, 1-4-3 Mita, Minato-ku, Tokyo 108-8329, Japan); Shimizu, Akinobu (Tokyo University of Agriculture and Technology, 2-24-16 Nakacho, Koganei, Tokyo 184-8588, Japan)","Saito, Atsushi (Tokyo University of Agriculture and Technology)","Saito, Atsushi (Tokyo University of Agriculture and Technology); Nawano, Shigeru (International University of Health and Welfare); Shimizu, Akinobu (Tokyo University of Agriculture and Technology)",45,15,1.61,,,https://app.dimensions.ai/details/publication/pub.1031644012,40 Engineering,7 Affordable and Clean Energy,,,,,,,,,,,
2817,pub.1001043710,10.1016/j.acra.2016.03.010,27133005,PMC5355004,Computer-aided Detection of Prostate Cancer with MRI Technology and Applications,"One in six men will develop prostate cancer in his lifetime. Early detection and accurate diagnosis of the disease can improve cancer survival and reduce treatment costs. Recently, imaging of prostate cancer has greatly advanced since the introduction of multiparametric magnetic resonance imaging (mp-MRI). Mp-MRI consists of T2-weighted sequences combined with functional sequences including dynamic contrast-enhanced MRI, diffusion-weighted MRI, and magnetic resonance spectroscopy imaging. Because of the big data and variations in imaging sequences, detection can be affected by multiple factors such as observer variability and visibility and complexity of the lesions. To improve quantitative assessment of the disease, various computer-aided detection systems have been designed to help radiologists in their clinical practice. This review paper presents an overview of literatures on computer-aided detection of prostate cancer with mp-MRI, which include the technology and its applications. The aim of the survey is threefold: an introduction for those new to the field, an overview for those working in the field, and a reference for those searching for literature on a specific application.","This work was partially supported by NIH grants R01CA156775, R21CA176684, and P50CA128301. Z. Zhang was supported by the National Natural Science Foundation of China (81372274). The work was also partially supported by a Georgia Research Alliance Distinguished Scientists Award.",,Academic Radiology,,"Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostate; Prostatic Neoplasms",2016-04-25,2016,2016-04-25,2016-08,23,8,1024-1046,All OA, Green,Article,"Liu, Lizhi; Tian, Zhiqiang; Zhang, Zhenfeng; Fei, Baowei","Liu, Lizhi (Department of Radiology and Imaging Sciences, Emory University School of Medicine, 1841 Clifton Road NE, Atlanta, GA 30329; Center of Medical Imaging and Image-guided Therapy, Sun Yat-sen University Cancer Center, State Key Laboratory of Oncology Collaborative Innovation Center for Cancer Medicine, 651 Dongfeng Road East, Guangzhou, 510060, China); Tian, Zhiqiang (Department of Radiology and Imaging Sciences, Emory University School of Medicine, 1841 Clifton Road NE, Atlanta, GA 30329); Zhang, Zhenfeng (Center of Medical Imaging and Image-guided Therapy, Sun Yat-sen University Cancer Center, State Key Laboratory of Oncology Collaborative Innovation Center for Cancer Medicine, 651 Dongfeng Road East, Guangzhou, 510060, China); Fei, Baowei (Department of Radiology and Imaging Sciences, Emory University School of Medicine, 1841 Clifton Road NE, Atlanta, GA 30329; Department of Biomedical Engineering, Emory University and Georgia Institute of Technology, 1841 Clifton Road NE, Atlanta, Georgia 30329; Winship Cancer Institute of Emory University, 1841 Clifton Road NE, Atlanta, Georgia 30329)","Fei, Baowei (Emory University; Emory University; Winship Cancer Institute)","Liu, Lizhi (Emory University; Sun Yat-sen University Cancer Center); Tian, Zhiqiang (Emory University); Zhang, Zhenfeng (Sun Yat-sen University Cancer Center); Fei, Baowei (Emory University; Emory University; Winship Cancer Institute)",48,17,1.85,11.18,https://europepmc.org/articles/pmc5355004?pdf=render,https://app.dimensions.ai/details/publication/pub.1001043710,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
2812,pub.1112098321,10.1007/s13246-019-00730-z,30762223,,Multiparametric MRI and radiomics in prostate cancer: a review,"Multiparametric MRI (mpMRI) is an imaging modality that combines anatomical MR imaging with one or more functional MRI sequences. It has become a versatile tool for detecting and characterising prostate cancer (PCa). The traditional role of mpMRI was confined to PCa staging, but due to the advanced imaging techniques, its role has expanded to various stages in clinical practises including tumour detection, disease monitor during active surveillance and sequential imaging for patient follow-up. Meanwhile, with the growing speed of data generation and the increasing volume of imaging data, it is highly demanded to apply computerised methods to process mpMRI data and extract useful information. Hence quantitative analysis for imaging data using radiomics has become an emerging paradigm. The application of radiomics approaches in prostate cancer has not only enabled automatic localisation of the disease but also provided a non-invasive solution to assess tumour biology (e.g. aggressiveness and the presence of hypoxia). This article reviews mpMRI and its expanding role in PCa detection, staging and patient management. Following that, an overview of prostate radiomics will be provided, with a special focus on its current applications as well as its future directions.","The authors would like to acknowledge Peter MacCallum Cancer Centre, The University of Melbourne and Cancer Therapeutics CRC for providing the resources to perform the literature survey. The authors would also like to show their gratitude to Courtney Savill and Lauren Caspersz for their contributions in data collection.","This study is supported by PdCCRS grant 628592 with funding partners: Prostate Cancer Foundation of Australia, and the Radiation Oncology Section of the Australian Government of Health and Aging and Cancer Australia. Yu Sun is funded by the Melbourne International Research Scholarship, a Movember Young Investigator Grant through Prostate Cancer Foundation of Australia (PCFA) and Cancer Therapeutics Top-up Funding. Hayley Reynolds is funded by a Movember Young Investigator Grant through PCFA’s Research Program.",Physical and Engineering Sciences in Medicine,,"Algorithms; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostatic Neoplasms",2019-02-14,2019,2019-02-14,2019-03,42,1,3-25,Closed,Article,"Sun, Yu; Reynolds, Hayley M.; Parameswaran, Bimal; Wraith, Darren; Finnegan, Mary E.; Williams, Scott; Haworth, Annette","Sun, Yu (University of Sydney, Sydney, Australia; Peter MacCallum Cancer Centre, Melbourne, Australia); Reynolds, Hayley M. (Peter MacCallum Cancer Centre, Melbourne, Australia); Parameswaran, Bimal (Imaging Associates, Melbourne, Australia); Wraith, Darren (Queensland University of Technology, Brisbane, Australia); Finnegan, Mary E. (Imperial College Healthcare NHS Trust, London, UK; Imperial College London, London, UK); Williams, Scott (Peter MacCallum Cancer Centre, Melbourne, Australia); Haworth, Annette (University of Sydney, Sydney, Australia)","Sun, Yu (The University of Sydney; Peter MacCallum Cancer Centre)","Sun, Yu (The University of Sydney; Peter MacCallum Cancer Centre); Reynolds, Hayley M. (Peter MacCallum Cancer Centre); Parameswaran, Bimal (); Wraith, Darren (Queensland University of Technology); Finnegan, Mary E. (Imperial College Healthcare NHS Trust; Imperial College London); Williams, Scott (Peter MacCallum Cancer Centre); Haworth, Annette (The University of Sydney)",68,51,6.12,23.74,,https://app.dimensions.ai/details/publication/pub.1112098321,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,3 Good Health and Well Being,,,,,,,,
2665,pub.1143817480,10.1186/s40658-021-00426-y,34897550,PMC8665861,Artificial intelligence with deep learning in nuclear medicine and radiology,"The use of deep learning in medical imaging has increased rapidly over the past few years, finding applications throughout the entire radiology pipeline, from improved scanner performance to automatic disease detection and diagnosis. These advancements have resulted in a wide variety of deep learning approaches being developed, solving unique challenges for various imaging modalities. This paper provides a review on these developments from a technical point of view, categorizing the different methodologies and summarizing their implementation. We provide an introduction to the design of neural networks and their training procedure, after which we take an extended look at their uses in medical imaging. We cover the different sections of the radiology pipeline, highlighting some influential works and discussing the merits and limitations of deep learning approaches compared to other traditional methods. As such, this review is intended to provide a broad yet concise overview for the interested reader, facilitating adoption and interdisciplinary research of deep learning in the field of medical imaging.",The authors would like to thank Roland Hustinx from the University of Liege for reading the manuscript and making useful suggestions for further improvements.,,EJNMMI Physics,,,2021-12-11,2021,2021-12-11,2021-12,8,1,81,All OA, Gold,Article,"Decuyper, Milan; Maebe, Jens; Van Holen, Roel; Vandenberghe, Stefaan","Decuyper, Milan (Department of Electronics and Information Systems, Ghent University, Ghent, Belgium); Maebe, Jens (Department of Electronics and Information Systems, Ghent University, Ghent, Belgium); Van Holen, Roel (Department of Electronics and Information Systems, Ghent University, Ghent, Belgium); Vandenberghe, Stefaan (Department of Electronics and Information Systems, Ghent University, Ghent, Belgium)","Maebe, Jens (Ghent University)","Decuyper, Milan (Ghent University); Maebe, Jens (Ghent University); Van Holen, Roel (Ghent University); Vandenberghe, Stefaan (Ghent University)",10,10,0.85,7.41,https://ejnmmiphys.springeropen.com/counter/pdf/10.1186/s40658-021-00426-y,https://app.dimensions.ai/details/publication/pub.1143817480,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,
2421,pub.1032306523,10.1016/j.compbiomed.2015.02.009,25747341,,Computer-Aided Detection and diagnosis for prostate cancer based on mono and multi-parametric MRI: A review,"Prostate cancer is the second most diagnosed cancer of men all over the world. In the last few decades, new imaging techniques based on Magnetic Resonance Imaging (MRI) have been developed to improve diagnosis. In practise, diagnosis can be affected by multiple factors such as observer variability and visibility and complexity of the lesions. In this regard, computer-aided detection and computer-aided diagnosis systems have been designed to help radiologists in their clinical practice. Research on computer-aided systems specifically focused for prostate cancer is a young technology and has been part of a dynamic field of research for the last 10 years. This survey aims to provide a comprehensive review of the state-of-the-art in this lapse of time, focusing on the different stages composing the work-flow of a computer-aided system. We also provide a comparison between studies and a discussion about the potential avenues for future research. In addition, this paper presents a new public online dataset which is made available to the research community with the aim of providing a common evaluation framework to overcome some of the current limitations identified in this survey.",G. Lemaître was supported by the Generalitat de Catalunya (Grant no. FI-DGR2012) and partly by the Mediterranean Office for Youth (Grant no. 2011/018/06). We would like to acknowledge Sharad Nagappa for all the discussions involved and his precious advices and corrections regarding the reduction of this entire manuscript.,,Computers in Biology and Medicine,,"Carcinoma; Diagnosis, Computer-Assisted; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Mass Screening; Medical Informatics; Neoplasm Grading; Neural Networks, Computer; Observer Variation; Prostatic Neoplasms; Reproducibility of Results; Research Design; Software; Time Factors",2015-02-20,2015,2015-02-20,2015-05,60,,8-31,All OA, Green,Article,"Lemaître, Guillaume; Martí, Robert; Freixenet, Jordi; Vilanova, Joan C.; Walker, Paul M.; Meriaudeau, Fabrice","Lemaître, Guillaume (LE2I-UMR CNRS 6306, Université de Bourgogne, 12 rue de la Fonderie, 71200 Le Creusot, France; ViCOROB, Universitat de Girona, Campus Montilivi, Edifici P4, 17071 Girona, Spain); Martí, Robert (ViCOROB, Universitat de Girona, Campus Montilivi, Edifici P4, 17071 Girona, Spain); Freixenet, Jordi (ViCOROB, Universitat de Girona, Campus Montilivi, Edifici P4, 17071 Girona, Spain); Vilanova, Joan C. (Department of Magnetic Resonance, Clínica Girona, Lorenzana 36, 17002 Girona, Spain); Walker, Paul M. (LE2I-UMR CNRS 6306, Université de Bourgogne, Avenue Alain Savary, 21000 Dijon, France); Meriaudeau, Fabrice (LE2I-UMR CNRS 6306, Université de Bourgogne, 12 rue de la Fonderie, 71200 Le Creusot, France)","Lemaître, Guillaume (University of Burgundy; University of Girona)","Lemaître, Guillaume (University of Burgundy; University of Girona); Martí, Robert (University of Girona); Freixenet, Jordi (University of Girona); Vilanova, Joan C. (Clínica Girona); Walker, Paul M. (University of Burgundy); Meriaudeau, Fabrice (University of Burgundy)",182,63,4.53,38.79,https://hal-univ-bourgogne.archives-ouvertes.fr/hal-01235868/file/g_lemaitre_state_of_the_art.pdf,https://app.dimensions.ai/details/publication/pub.1032306523,31 Biological Sciences, 3102 Bioinformatics and Computational Biology, 42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences, 4601 Applied Computing,,,,,,
2410,pub.1139649257,10.3390/s21144758,34300498,PMC8309939,"Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past, Present and Future","With the advances of data-driven machine learning research, a wide variety of prediction problems have been tackled. It has become critical to explore how machine learning and specifically deep learning methods can be exploited to analyse healthcare data. A major limitation of existing methods has been the focus on grid-like data; however, the structure of physiological recordings are often irregular and unordered, which makes it difficult to conceptualise them as a matrix. As such, graph neural networks have attracted significant attention by exploiting implicit information that resides in a biological system, with interacting nodes connected by edges whose weights can be determined by either temporal associations or anatomical junctions. In this survey, we thoroughly review the different types of graph architectures and their applications in healthcare. We provide an overview of these methods in a systematic manner, organized by their domain of application including functional connectivity, anatomical structure, and electrical-based analysis. We also outline the limitations of existing techniques and discuss potential directions for future research.",Not applicable.,"This research was funded by the Imaging and Computer Vision group at CSIRO Data61 Canberra, Australia.",Sensors,,"Attention; Deep Learning; Machine Learning; Neural Networks, Computer",2021-07-12,2021,2021-07-12,,21,14,4758,All OA, Gold,Article,"Ahmedt-Aristizabal, David; Armin, Mohammad Ali; Denman, Simon; Fookes, Clinton; Petersson, Lars","Ahmedt-Aristizabal, David (Imaging and Computer Vision Group, CSIRO Data61, Canberra 2601, Australia;, ali.armin@data61.csiro.au, (M.A.A.);, lars.petersson@data61.csiro.au, (L.P.); Signal Processing, Artificial Intelligence and Vision Technologies (SAIVT) Research Program, Queensland University of Technology, Brisbane 4000, Australia;, s.denman@qut.edu.au, (S.D.);, c.fookes@qut.edu.au, (C.F.)); Armin, Mohammad Ali (Imaging and Computer Vision Group, CSIRO Data61, Canberra 2601, Australia;, ali.armin@data61.csiro.au, (M.A.A.);, lars.petersson@data61.csiro.au, (L.P.)); Denman, Simon (Signal Processing, Artificial Intelligence and Vision Technologies (SAIVT) Research Program, Queensland University of Technology, Brisbane 4000, Australia;, s.denman@qut.edu.au, (S.D.);, c.fookes@qut.edu.au, (C.F.)); Fookes, Clinton (Signal Processing, Artificial Intelligence and Vision Technologies (SAIVT) Research Program, Queensland University of Technology, Brisbane 4000, Australia;, s.denman@qut.edu.au, (S.D.);, c.fookes@qut.edu.au, (C.F.)); Petersson, Lars (Imaging and Computer Vision Group, CSIRO Data61, Canberra 2601, Australia;, ali.armin@data61.csiro.au, (M.A.A.);, lars.petersson@data61.csiro.au, (L.P.))","Ahmedt-Aristizabal, David (Commonwealth Scientific and Industrial Research Organisation; Queensland University of Technology; )","Ahmedt-Aristizabal, David (Commonwealth Scientific and Industrial Research Organisation; Queensland University of Technology); Armin, Mohammad Ali (Commonwealth Scientific and Industrial Research Organisation); Denman, Simon (Queensland University of Technology); Fookes, Clinton (Queensland University of Technology); Petersson, Lars (Commonwealth Scientific and Industrial Research Organisation)",31,31,2.77,25.37,https://www.mdpi.com/1424-8220/21/14/4758/pdf?version=1626182636,https://app.dimensions.ai/details/publication/pub.1139649257,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1785,pub.1084688689,10.1117/12.2254621,,,Deep convolutional neural network for prostate MR segmentation,,,,Proceedings of SPIE,"Medical Imaging 2017: Image-Guided Procedures, Robotic Interventions, and Modeling",,2017-03-03,2017,2017-03-03,2017-03-03,10135,,101351l-101351l-6,All OA, Green,Proceeding,"Tian, Zhiqiang; Liu, Lizhi; Fei, Baowei","Tian, Zhiqiang (Emory Univ. (United States)); Liu, Lizhi (Emory Univ. (United States)); Fei, Baowei (Emory Univ. (United States))",,"Tian, Zhiqiang (Emory University); Liu, Lizhi (Emory University); Fei, Baowei (Emory University)",25,13,,7.97,https://europepmc.org/articles/pmc6177294?pdf=render,https://app.dimensions.ai/details/publication/pub.1084688689,"40 Engineering; 4006 Communications Engineering; 4009 Electronics, Sensors and Digital Hardware; 51 Physical Sciences; 5102 Atomic, Molecular and Optical Physics",,,,,,,,,,,
1780,pub.1122023718,10.48550/arxiv.1910.10470,,,Neural Ordinary Differential Equations for Semantic Segmentation of  Individual Colon Glands,"Automated medical image segmentation plays a key role in quantitative
research and diagnostics. Convolutional neural networks based on the U-Net
architecture are the state-of-the-art. A key disadvantage is the hard-coding of
the receptive field size, which requires architecture optimization for each
segmentation task. Furthermore, increasing the receptive field results in an
increasing number of weights. Recently, Neural Ordinary Differential Equations
(NODE) have been proposed, a new type of continuous depth deep neural network.
This framework allows for a dynamic receptive field at a fixed memory cost and
a smaller amount of parameters. We show on a colon gland segmentation dataset
(GlaS) that these NODEs can be used within the U-Net framework to improve
segmentation results while reducing memory load and parameter counts.",,,arXiv,,,2019-10-23,2019,,,,,,All OA, Green,Preprint,"Pinckaers, Hans; Litjens, Geert","Pinckaers, Hans (); Litjens, Geert ()",,"Pinckaers, Hans (); Litjens, Geert ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1122023718,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1738,pub.1127839888,10.1109/isbi45749.2020.9098643,,,V-Net Light - Parameter-Efficient 3-D Convolutional Neural Network for Prostate MRI Segmentation,"Prostate MRI segmentation has become an important tool for quantitative estimation of the gland volume during diagnostic imaging. It is also a critical step in the fusion between MRI and transrectal ultrasound (TRUS) for fusion guided biopsy or therapy. 3-D neural networks have demonstrated strong potential for this task, but require substantial computational resources due to their large number of parameters. In this work, we focus on the efficiency of the segmentation network in terms of speed and memory requirements. Specifically, we aim at reaching state-of-the-art results with smaller networks, involving significantly fewer parameters, thus making the network easier to train and operate. A novel 3-D network architecture, called V-net Light (VnL) is proposed, based on an efficient 3-D Module called 3-D Light, that minimizes the number of network parameters while maintaining state-of-the-art segmentation results. The proposed method is validated on the PROMISE12 challenge data [1]. The proposed VnL has only 9.1% of V-net's parameters, 3.2% of its floating point operations (FLOPs) and uses only 9.1% of hard-disk storage compared to V-net, yet V-net and VnL has comparable accuracy.",,,,2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI),,2020-04-03,2020,,2020-04-03,0,,442-445,Closed,Proceeding,"Yaniv, Ophir; Portnoy, Orith; Talmon, Amit; Kiryati, Nahum; Konen, Eli; Mayer, Arnaldo","Yaniv, Ophir (School of Electric Engineering, Tel Aviv University, Tel Aviv, Israel); Portnoy, Orith (Diagnostic Imaging Unit, Sheba Medical Center, Affiliated to the Sackler, School of Medicine Tel-Aviv University, Tel Aviv, Israel); Talmon, Amit (Diagnostic Imaging Unit, Sheba Medical Center, Affiliated to the Sackler, School of Medicine Tel-Aviv University, Tel Aviv, Israel); Kiryati, Nahum (The Raquel and Manuel Klachky Chair of Image Processing, School of Electrical Engineering, Tel Aviv University); Konen, Eli (Diagnostic Imaging Unit, Sheba Medical Center, Affiliated to the Sackler, School of Medicine Tel-Aviv University, Tel Aviv, Israel); Mayer, Arnaldo (Diagnostic Imaging Unit, Sheba Medical Center, Affiliated to the Sackler, School of Medicine Tel-Aviv University, Tel Aviv, Israel)",,"Yaniv, Ophir (Tel Aviv University); Portnoy, Orith (Sheba Medical Center); Talmon, Amit (Sheba Medical Center); Kiryati, Nahum (Tel Aviv University); Konen, Eli (Sheba Medical Center); Mayer, Arnaldo (Sheba Medical Center)",8,8,,4.12,,https://app.dimensions.ai/details/publication/pub.1127839888,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1726,pub.1124296068,10.1109/ubmyk48245.2019.8965456,,,Prostate Segmentation via Fusing the Nested-V-net3d and V-net2d,"Prostate cancer is caused by uncontrolled growth of cells in the prostate gland. Prostate cancer, unlike benign prostate gland enlargement, is not from the center of the prostate, but from the site of the tumor to the center of the capsule. Therefore, the patient experiences urinary tract complaints at the last stage. Therefore, until the last stage, the patient does not have any finding. For this reason, Magnetic Resonance Imaging (MRI) is used in regular examinations after a certain age or in various diagnostic imaging methods of patients diagnosed with this disease. Proper localization of the prostate is an important step in assisting diagnosis and treatment, such as guiding the biopsy procedure and radiation therapy. However, manual segmentation of the prostate is tedious and time-consuming. It also varies in inter-rater evaluation. The two main challenges for correct MR prostate localization are; nonhomogeneous and inconsistent appearance around the prostate border, wide prostate shape variability in different patients. In this study, Fusing the Nested 3D Dimensional Volumetric Convolutional Neural Network (Nested-Vnet3d) and 2D Volumetric Convolutional Neural Network (V-net2d) models are compared with other V-net based models. In the training conducted on the PROMISE12 dataset, 0.92 validation dice score was achieved. This study showed that the proposed model is a robust deep learning model for prostate segmentation.",,,,2019 1st International Informatics and Software Engineering Conference (UBMYK),,2019-11-07,2019,,2019-11-07,0,,1-4,Closed,Proceeding,"Öcal, Hakan; Barışçı, Necaattin","Öcal, Hakan (Computer Engineering / School of Natural and Applied Sciences, Gazi University, Ankara, Turkey); Barışçı, Necaattin (Computer Engineering / School of Natural and Applied Sciences, Gazi University, Ankara, Turkey)","Öcal, Hakan (Gazi University)","Öcal, Hakan (Gazi University); Barışçı, Necaattin (Gazi University)",4,3,,1.41,,https://app.dimensions.ai/details/publication/pub.1124296068,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
1699,pub.1138337604,10.1109/isbi48211.2021.9434108,,,Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning,"Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) for PCa on mp-MRI based on deep learning techniques has gained much attention and shown promising progress. The key for the success of deep learning based PCa diagnosis is to obtain a large amount of high quality PCa region annotation on mp-MRI such that the network can accurately learn the large variation of PCa lesions. In order to precisely annotate the PCa region on mp-MRI, the pathological whole mount data of the patient is normally required as reference, which is often difficult to obtain in real world clinical situations. Therefore, we are motivated to propose a new deep learning based method to integrate different levels of information available in the PCa screening workflow through a multitask hierarchical weakly supervised framework for PCa detection on mp-MRI. Experimental results show that our method achieves promising PCa detection and segmentation results.",This work was sponsored by Shanghai Pujiang Program (grant No. 19PJ1431900).,,,2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI),,2021-04-16,2021,,2021-04-16,0,,316-319,Closed,Proceeding,"Yang, Haibo; Wu, GuangYu; Shen, Dinggang; Liao, Shu","Yang, Haibo (Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China); Wu, GuangYu (Department of Radiology, Renji Hospital, School of Medicine, Shanghai Jiao Tong University, China); Shen, Dinggang (Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China); Liao, Shu (Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China)","Yang, Haibo ","Yang, Haibo (); Wu, GuangYu (Renji Hospital; Shanghai Jiao Tong University); Shen, Dinggang (); Liao, Shu ()",4,4,,2.42,,https://app.dimensions.ai/details/publication/pub.1138337604,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,,
1697,pub.1123609521,10.1145/3364836.3364854,,,3D Fully Convolutional Network Incorporating Savitzky-Golay Filtering for Prostate Segmentation,"In this paper, we proposed a 3D fully convolutional network (FCN) incorporating Savitzky-Golay (SG) filtering for prostate segmentation using magnetic resonance images (MRIs). Deep learning methods have achieved promising results in the field of segmentation, especially in semantic segmentation. However, it is not fully applicable to 3D medical images. To better extract the spatial information encoded in the 3D volumetric data, we designed a 3D FCN with long skip connection and the Parametric Rectified Linear Unit (PReLU) being the activation function. To further polish the deep learning based segmentation results, we employed SG filtering as a post-processing step. The SG filter was applied for smoothing and denoising, wherein second-order partial derivatives were taken to extract the edge information and achieve hole filling. In comparison with the 3D FCN without SG filtering, the post-processed results were more smooth, accurate and robust. The proposed method performed superiorly for prostate segmentation over several other state-of-the-art methods.",,,,Proceedings of the Third International Symposium on Image Computing and Digital Medicine,,2019-08-24,2019,2019-08-24,2019-08-24,,,88-91,Closed,Proceeding,"Zhong, Pinyuan; Wu, Jiong; Yuan, Zhe; Zhang, Yue; Tang, Xiaoying","Zhong, Pinyuan (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China); Wu, Jiong (School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China); Yuan, Zhe (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China); Zhang, Yue (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China); Tang, Xiaoying (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China)",,"Zhong, Pinyuan (Southern University of Science and Technology); Wu, Jiong (Sun Yat-sen University); Yuan, Zhe (Southern University of Science and Technology); Zhang, Yue (Southern University of Science and Technology); Tang, Xiaoying (Southern University of Science and Technology)",1,1,,0.37,,https://app.dimensions.ai/details/publication/pub.1123609521,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
1697,pub.1117944765,10.1109/isbi.2019.8759554,,,Prostate Segmentation Using Z-Net,"In this paper, we proposed a novel architecture of convolutional neural network (CNN), namely Z-net, for segmenting prostate from magnetic resonance images (MRIs). In the proposed Z-net, 5 pairs of Z-block and decoder Z-block with different sizes and numbers of feature maps were assembled in a way similar to that of U-net. The proposed architecture can capture more multi-level features by using concatenation and dense connection. A total of 45 training images were used to train the proposed Z-net and the evaluations were conducted qualitatively on 5 validation images and quantitatively on 30 testing images. In addition, three approaches including pad and cut, 2D resize, and 3D resize for uniforming the size of samples were evaluated and compared. The experimental results demonstrated that the 2D resize is the most suitable approach for the proposed Z-net. Compared to the other two classical CNN architectures, the proposed method was observed with superior performance for segmenting prostate.","This study was supported by the National Natural Science Foundation of China (81501546, 61871207), the National Key R&amp;D Program of China (2017YFC0112404), Guangdong Natural Science Funds for Distinguished Young Scholar (2015A030306032), Talent Support Project of Guangdong (2016TQ03X839), and Shenzhen Science and Technology Innovation Committee funds (KQJSCX20160226193445, JCYJ20160301113918121, JSGG20160427105120572).",,,2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),,2019-04-11,2019,,2019-04-11,0,,11-14,All OA, Green,Proceeding,"Zhang, Yue; Wu, Jiong; Chen, Wanli; Chen, Yifan; Tang, Xiaoying","Zhang, Yue (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, China); Wu, Jiong (School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China); Chen, Wanli (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China); Chen, Yifan (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Faculty of Science and Engineering, The University of Waikato, Hamilton, New Zealand); Tang, Xiaoying (Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China)","Zhang, Yue (Southern University of Science and Technology; University of Hong Kong)","Zhang, Yue (Southern University of Science and Technology; University of Hong Kong); Wu, Jiong (Sun Yat-sen University); Chen, Wanli (Southern University of Science and Technology); Chen, Yifan (Southern University of Science and Technology; University of Waikato); Tang, Xiaoying (Southern University of Science and Technology)",26,14,,9.71,http://arxiv.org/pdf/1901.06115,https://app.dimensions.ai/details/publication/pub.1117944765,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1696,pub.1101172511,10.1109/icip.2017.8296847,,,Prostate Detection and Segmentation Based on Convolutional Neural Network and Topological Derivative,"The topological derivative (TD) for shape analysis has been employed in image segmentation, and machine learning schames based on convolutional neural network (CNN) provide the high performance in the image processing. The supervised and unsupervised approaches have different roles and advantages according to their concepts. To maximize the benefits of two approaches, we propose CNN-TD based segmentation approach. A CNN-based segmentation scheme is employed to faithfully consider the characteristics of an object to be segmented in a given image, and we refine the CNN results using a TD-based scheme. Experimental results show that the proposed scheme produces better performance for the prostate segmentation than the refined results by level set − based schemes. Therefore, we believe that the proposed scheme can be a useful tool for effective medical image segmentation.","This work supported by the IT R&amp;D program of MSIP/IITP [2017-0-00255, Autonomous digital companion development] and this work was partially supported by the NRF funded by the Ministry of Education, Science and Technology[NRF-2014R1A2A1A11049986]. This work supported by the IT R&amp;D program of MSIP/IITP [2017-0-00255, Autonomous digital companion development]. and this work was partially supported by the NRF funded by the Ministry of Education, Science and Technology[NRF-2014R1A2A1A11049986].",,,2017 IEEE International Conference on Image Processing (ICIP),,2017-01-01,2017,,2017-01-01,,,3071-3074,Closed,Proceeding,"Ch, Choongsang; Lee, Young Han; Lee, Sangkeun","Ch, Choongsang (Artificial Intelligence Research Center, Korea Electronics Technology Institute, Seongnam, Korea); Lee, Young Han (Artificial Intelligence Research Center, Korea Electronics Technology Institute, Seongnam, Korea); Lee, Sangkeun (Graduate School of Advanced Imaging Science, Multimedia & Film Chung-Ang University, Seoul, Korea)",,"Ch, Choongsang (Korea Electronics Technology Institute); Lee, Young Han (Korea Electronics Technology Institute); Lee, Sangkeun (Chung-Ang University)",7,2,,2.01,,https://app.dimensions.ai/details/publication/pub.1101172511,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
1696,pub.1012973922,10.1117/12.2217162,,,Deeply learnt hashing forests for content based image retrieval in prostate MR images,,,,Proceedings of SPIE,Medical Imaging 2016: Image Processing,,2016-03-21,2016,2016-03-21,2016-03-21,9784,,978414-978414-6,Closed,Proceeding,"Shah, Amit; Conjeti, Sailesh; Navab, Nassir; Katouzian, Amin","Shah, Amit (Technische Univ. München (Germany)); Conjeti, Sailesh (Technische Univ. München (Germany)); Navab, Nassir (Technische Univ. München (Germany)); Katouzian, Amin (Technische Univ. München (Germany))",,"Shah, Amit (Technical University of Munich); Conjeti, Sailesh (Technical University of Munich); Navab, Nassir (Technical University of Munich); Katouzian, Amin (Technical University of Munich)",24,5,,7.85,,https://app.dimensions.ai/details/publication/pub.1012973922,"40 Engineering; 4006 Communications Engineering; 4009 Electronics, Sensors and Digital Hardware; 51 Physical Sciences; 5102 Atomic, Molecular and Optical Physics",,,,,,,,,,,,
1687,pub.1033410348,10.1117/12.2217079,,,3D prostate MR-TRUS non-rigid registration using dual optimization with volume-preserving constraint,,,,Proceedings of SPIE,Medical Imaging 2016: Image Processing,,2016-03-21,2016,2016-03-21,2016-03-21,9784,,97841p-97841p-6,Closed,Proceeding,"Qiu, Wu; Yuan, Jing; Fenster, Aaron","Qiu, Wu (Western Univ. (Canada)); Yuan, Jing (Western Univ. (Canada)); Fenster, Aaron (Western Univ. (Canada))",,"Qiu, Wu (Western University); Yuan, Jing (Western University); Fenster, Aaron (Western University)",2,1,,0.65,,https://app.dimensions.ai/details/publication/pub.1033410348,"40 Engineering; 4006 Communications Engineering; 4009 Electronics, Sensors and Digital Hardware; 51 Physical Sciences; 5102 Atomic, Molecular and Optical Physics",,,,,,,,,,,,
1685,pub.1119434599,10.48550/arxiv.1901.06115,,,Prostate segmentation using Z-net,"In this paper, we proposed a novel architecture of convolutional neural
network (CNN), namely Z-net, for segmenting prostate from magnetic resonance
images (MRIs). In the proposed Z-net, 5 pairs of Z-block and decoder Z-block
with different sizes and numbers of feature maps were assembled in a way
similar to that of U-net. The proposed architecture can capture more
multi-level features by using concatenation and dense connection. A total of 45
training images were used to train the proposed Z-net and the evaluations were
conducted qualitatively on 5 validation images and quantitatively on 30 testing
images. In addition, three approaches including pad and cut, 2D resize, and 3D
resize for uniforming the size of samples were evaluated and compared. The
experimental results demonstrated that the 2D resize is the most suitable
approach for the proposed Z-net. Compared to the other two classical CNN
architectures, the proposed method was observed with superior performance for
segmenting prostate.",,,arXiv,,,2019-01-18,2019,,,,,,All OA, Green,Preprint,"Zhang, Yue; Wu, Jiong; Chen, Wanli; Chen, Yifan; Tang, Xiaoying","Zhang, Yue (); Wu, Jiong (); Chen, Wanli (); Chen, Yifan (); Tang, Xiaoying ()",,"Zhang, Yue (); Wu, Jiong (); Chen, Wanli (); Chen, Yifan (); Tang, Xiaoying ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119434599,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1663,pub.1135528950,10.1117/12.2582130,,,Improved segmentation by adversarial U-Net,"Medical image segmentation has a fundamental role in many computer-aided diagnosis (CAD) applications. Accurate segmentation of medical images is a key step in tracking changes over time, contouring during radiotherapy planning, and more. One of the state-of-the-art models for medical image segmentation is the U–Net that consists of an encoder-decoder based architecture. Many variations exist to the U–Net architecture. In this work, we present a new training procedure that combines U–Net with an adversarial training we refer to as Adversarial U–Net. We show that Adversarial U–Net outperformes the conventional U–Net in three versatile domains that differ in the acquisition method as well as the physical characteristics and yields smooth and improved segmentation maps.",,,Progress in Biomedical Optics and Imaging,Medical Imaging 2021: Computer-Aided Diagnosis,,2021-02-15,2021,,,11597,,1159719-1159719-6,Closed,Proceeding,"Sriker, David; Cohen, Dana; Cahan, Noa; Greenspan, Hayit","Sriker, David (Tel Aviv Univ. (Israel)); Cohen, Dana (Tel Aviv Univ. (Israel)); Cahan, Noa (Tel Aviv Univ. (Israel)); Greenspan, Hayit (Tel Aviv Univ. (Israel))",,"Sriker, David (Tel Aviv University); Cohen, Dana (Tel Aviv University); Cahan, Noa (Tel Aviv University); Greenspan, Hayit (Tel Aviv University)",1,1,,1.25,,https://app.dimensions.ai/details/publication/pub.1135528950,46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,
1663,pub.1110815626,10.1109/ultsym.2018.8580157,,,Zonal Segmentation in Transrectal Ultrasound Images of the Prostate Through Deep Learning,"Segmentation of both prostatic and zonal boundaries in transrectal ultrasound images is of great value in current clinical practice and for advancing techniques in computer-assisted diagnosis and inter-modality fusion. In this work, we propose a deep-learning approach to automatically segment the prostate and its main zones. In comparison with conventional methods, this method shows increased accuracy and Dice coefficients for full-prostate delineation. Moreover, the mean deviation between the annotated and predicted contours decreased substantially. Albeit the method still requires validation for different scanners and configurations, its real-time inference rate highlights the potential of this technique to be applied in clinical practice.",This research was performed in association with the IM-PULS2 program within the Eindhoven University in collaboration with Philips. This study was also funded by an unrestricted grant from the Dutch Cancer Society (#UVA2013-5941) and a Starting Grant from the European Research Council (#280209). We acknowledge the NVIDIA Corporation for the donation of a TITAN XP GPU.,,,2018 IEEE International Ultrasonics Symposium (IUS),,2018-10-22,2018,,2018-10-22,0,,1-4,Closed,Proceeding,"van Sloun, R.J.G.; Wildeboer, R.R.; Postema, A.W.; Mannaerts, C.K.; Gayer, M.; Wiikstra, H.; Mischi, M.","van Sloun, R.J.G. (Dept. of Electrical Engineering, Eindhoven University of Technology, The Netherlands); Wildeboer, R.R. (Dept. of Electrical Engineering, Eindhoven University of Technology, The Netherlands); Postema, A.W. (Dept. of Urology, Academic Medical Center University Hospital, Amsterdam, The Netherlands); Mannaerts, C.K. (Dept. of Urology, Academic Medical Center University Hospital, Amsterdam, The Netherlands); Gayer, M. (Dept. of Urology, Jeroen Bosch Hospital, Hertogenbosch, The Netherlands); Wiikstra, H. (Dept. of Electrical Engineering, Eindhoven University of Technology, The Netherlands; Dept. of Urology, Academic Medical Center University Hospital, Amsterdam, The Netherlands); Mischi, M. (Dept. of Electrical Engineering, Eindhoven University of Technology, The Netherlands)",,"van Sloun, R.J.G. (Eindhoven University of Technology); Wildeboer, R.R. (Eindhoven University of Technology); Postema, A.W. (Amsterdam UMC Location VUmc); Mannaerts, C.K. (Amsterdam UMC Location VUmc); Gayer, M. (Jeroen Bosch Ziekenhuis); Wiikstra, H. (Eindhoven University of Technology; Amsterdam UMC Location VUmc); Mischi, M. (Eindhoven University of Technology)",6,5,,1.49,,https://app.dimensions.ai/details/publication/pub.1110815626,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,,
1658,pub.1152818550,10.48550/arxiv.2211.08840,,,Semi-Supervised and Self-Supervised Collaborative Learning for Prostate  3D MR Image Segmentation,"Volumetric magnetic resonance (MR) image segmentation plays an important role
in many clinical applications. Deep learning (DL) has recently achieved
state-of-the-art or even human-level performance on various image segmentation
tasks. Nevertheless, manually annotating volumetric MR images for DL model
training is labor-exhaustive and time-consuming. In this work, we aim to train
a semi-supervised and self-supervised collaborative learning framework for
prostate 3D MR image segmentation while using extremely sparse annotations, for
which the ground truth annotations are provided for just the central slice of
each volumetric MR image. Specifically, semi-supervised learning and
self-supervised learning methods are used to generate two independent sets of
pseudo labels. These pseudo labels are then fused by Boolean operation to
extract a more confident pseudo label set. The images with either manual or
network self-generated labels are then employed to train a segmentation model
for target volume extraction. Experimental results on a publicly available
prostate MR image dataset demonstrate that, while requiring significantly less
annotation effort, our framework generates very encouraging segmentation
results. The proposed framework is very useful in clinical applications when
training data with dense annotations are difficult to obtain.",,,arXiv,,,2022-11-16,2022,,,,,,All OA, Green,Preprint,"Osman, Yousuf Babiker M.; Li, Cheng; Huang, Weijian; Elsayed, Nazik; Xue, Zhenzhen; Zheng, Hairong; Wang, Shanshan","Osman, Yousuf Babiker M. (); Li, Cheng (); Huang, Weijian (); Elsayed, Nazik (); Xue, Zhenzhen (); Zheng, Hairong (); Wang, Shanshan ()",,"Osman, Yousuf Babiker M. (); Li, Cheng (); Huang, Weijian (); Elsayed, Nazik (); Xue, Zhenzhen (); Zheng, Hairong (); Wang, Shanshan ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152818550,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1658,pub.1121662799,10.1109/eurocon.2019.8861636,,,Skin lesion segmentation with deep learning,"Skin lesion segmentation is an important process in skin diagnostics because it improves manual and computeraided diagnostics by focusing the medical personnel on specific parts of the skin. Image segmentation is a common task in computer vision that partitions a digital image into multiple segments, for which deep neural networks have been proven to be reliable. In this paper, we investigate the applicability of deep learning methods for skin lesion segmentation evaluating three architectures: a pre-trained VGG16 encoder combined with SegNet decoder, TernausNet, and DeepLabV3+. The data set consists of images with RGB skin lesions and the ground truth of their segmentation. All the image sizes vary from hundreds to thousands of pixels per dimension. We evaluated the approaches with the Jaccard index and the computational efficiency of the training. The results show that the three deep neural network architectures achieve Jaccard Index scores of above 0.82, while the DeeplabV3+ outperforms the other approaches with a score of 0.876. The results are encouraging and can lead to fully-fledged automated approaches for skin lesion segmentation.","This work was partially financed by the Faculty of Computer Science and Engineering at the Ss. Cyril and Methodius University, Skopje, Macedonia. We also the support of Microsoft Azure for Research and the NVIDIA Corporation through grants providing GPU resources for this work. This work was partially financed by the Faculty of Computer Science and Engineering at the Ss. Cyril and Methodius University, Skopje, Macedonia. We also gratefully acknowledge the support of Microsoft Azure for Research and the NVIDIA Corporation through grants providing GPU resources for this work.",,,IEEE EUROCON 2019 -18th International Conference on Smart Technologies,,2019-07-04,2019,,2019-07-04,0,,1-5,Closed,Proceeding,"Lameski, Jane; Jovanov, Andrej; Zdravevski, Eftim; Lameski, Petre; Gievska, Sonja","Lameski, Jane (Faculty of Computer Science and Engineering, Ss.Cyril and Methodius University, Skopje, Macedonia); Jovanov, Andrej (Faculty of Computer Science and Engineering, Ss.Cyril and Methodius University, Skopje, Macedonia); Zdravevski, Eftim (Faculty of Computer Science and Engineering, Ss.Cyril and Methodius University, Skopje, Macedonia); Lameski, Petre (Faculty of Computer Science and Engineering, Ss.Cyril and Methodius University, Skopje, Macedonia); Gievska, Sonja (Faculty of Computer Science and Engineering, Ss.Cyril and Methodius University, Skopje, Macedonia)","Lameski, Jane (Saints Cyril and Methodius University of Skopje)","Lameski, Jane (Saints Cyril and Methodius University of Skopje); Jovanov, Andrej (Saints Cyril and Methodius University of Skopje); Zdravevski, Eftim (Saints Cyril and Methodius University of Skopje); Lameski, Petre (Saints Cyril and Methodius University of Skopje); Gievska, Sonja (Saints Cyril and Methodius University of Skopje)",16,13,,6.13,,https://app.dimensions.ai/details/publication/pub.1121662799,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1650,pub.1121202743,10.48550/arxiv.1909.09541,,,A Transfer Learning Approach for Automated Segmentation of Prostate  Whole Gland and Transition Zone in Diffusion Weighted MRI,"The segmentation of prostate whole gland and transition zone in Diffusion
Weighted MRI (DWI) are the first step in designing computer-aided detection
algorithms for prostate cancer. However, variations in MRI acquisition
parameters and scanner manufacturing result in different appearances of
prostate tissue in the images. Convolutional neural networks (CNNs) which have
shown to be successful in various medical image analysis tasks including
segmentation are typically sensitive to the variations in imaging parameters.
This sensitivity leads to poor segmentation performance of CNNs trained on a
source cohort and tested on a target cohort from a different scanner and hence,
it limits the applicability of CNNs for cross-cohort training and testing.
Contouring prostate whole gland and transition zone in DWI images are
time-consuming and expensive. Thus, it is important to enable CNNs pretrained
on images of source domain, to segment images of target domain with minimum
requirement for manual segmentation of images from the target domain. In this
work, we propose a transfer learning method based on a modified U-net
architecture and loss function, for segmentation of prostate whole gland and
transition zone in DWIs using a CNN pretrained on a source dataset and tested
on the target dataset. We explore the effect of the size of subset of target
dataset used for fine-tuning the pre-trained CNN on the overall segmentation
accuracy. Our results show that with a fine-tuning data as few as 30 patients
from the target domain, the proposed transfer learning-based algorithm can
reach dice score coefficient of 0.80 for both prostate whole gland and
transition zone segmentation. Using a fine-tuning data of 115 patients from the
target domain, dice score coefficient of 0.85 and 0.84 are achieved for
segmentation of whole gland and transition zone, respectively, in the target
domain.",,,arXiv,,,2019-09-20,2019,,,,,,All OA, Green,Preprint,"Motamed, Saman; Gujrathi, Isha; Deniffel, Dominik; Oentoro, Anton; Haider, Masoom A.; Khalvati, Farzad","Motamed, Saman (); Gujrathi, Isha (); Deniffel, Dominik (); Oentoro, Anton (); Haider, Masoom A. (); Khalvati, Farzad ()",,"Motamed, Saman (); Gujrathi, Isha (); Deniffel, Dominik (); Oentoro, Anton (); Haider, Masoom A. (); Khalvati, Farzad ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1121202743,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,
1648,pub.1094197780,10.1109/isbi.2017.7950630,,,Prostate Segmentation in MR Images Using Ensemble Deep Convolutional Neural Networks,"The automated segmentation of the prostate gland from MR images is increasingly used for clinical diagnosis. Since deep learning demonstrates superior performance in computer vision applications, we propose a coarse-to-fine segmentation strategy using ensemble deep convolutional neural networks (DCNNs) to address prostate segmentation in MR images. First, we use registration-based coarse segmentation on preprocessed prostate MR images to define the potential boundary region. We then train four DCNNs as voxel-based classifiers and classify the voxel in the potential region is a prostate voxel when at least three DCNNs made that decision. Finally, we use boundary refinement to eliminate the outliers and smooth the boundary. We evaluated our approach on the MICCAI PROMIS12 challenge dataset and our experimental results verify the effectiveness of the proposed algorithms.","This work was supported in part by the National Natural Science Foundation of China under Grants 61471297, in part by the Natural Science Foundation of Shaanxi Province, China under Grant 2015JM6287, and in part by the Australian Research Council (ARC) Grants. We appreciate the efforts devoted by the organizers of the 2012 Prostate MR Image Segmentation Challenge to collect and share the data for This work was supported in part by the National Natural Science Foundation of China under Grants 61471297, in part by the Natural Science Foundation of Shaanxi Province, China under Grant 2015JM6287, and in part by the Australian Research Council (ARC) Grants. We appreciate the efforts devoted by the organizers of the 2012 Prostate MR Image Segmentation Challenge to collect and share the data for comparing interactive and (semi)-automatic segmentation algorithms for MRI of the prostate.",,,2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),,2017-04-01,2017,,2017-04-01,,,762-765,All OA, Green,Proceeding,"Jia, Haazhe; Xia, Yang; Cai, Weidang; Fulham, Michael; Feng, David Dagan","Jia, Haazhe (Shaanxi Key Lab of Speech & Image Information Processing (SAIIP), School of Computer Science, Northwestern Polytechnical University, Xi'an, 710072, China); Xia, Yang (Shaanxi Key Lab of Speech & Image Information Processing (SAIIP), School of Computer Science, Northwestern Polytechnical University, Xi'an, 710072, China); Cai, Weidang (Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, NSW, 2006, Australia); Fulham, Michael (Shaanxi Key Lab of Speech & Image Information Processing (SAIIP), School of Computer Science, Northwestern Polytechnical University, Xi'an, 710072, China; Department of Molecular Imaging, Royal Prince Alfred Hospital, NSW, 2050, Australia; Sydney Medical School, University of Sydney, NSW, 2006, Australia); Feng, David Dagan (Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, NSW, 2006, Australia)",,"Jia, Haazhe (Northwestern Polytechnical University); Xia, Yang (Northwestern Polytechnical University); Cai, Weidang (The University of Sydney); Fulham, Michael (Northwestern Polytechnical University; Royal Prince Alfred Hospital; The University of Sydney); Feng, David Dagan (The University of Sydney)",21,7,,5.66,https://ses.library.usyd.edu.au/bitstream/2123/20529/2/Cai08_ISBI-2017_Haozhe-Jia.pdf,https://app.dimensions.ai/details/publication/pub.1094197780,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1647,pub.1119437287,10.48550/arxiv.1903.02500,,,Prostate Segmentation from 3D MRI Using a Two-Stage Model and  Variable-Input Based Uncertainty Measure,"This paper proposes a two-stage segmentation model, variable-input based
uncertainty measures and an uncertainty-guided post-processing method for
prostate segmentation on 3D magnetic resonance images (MRI). The two-stage
model was based on 3D dilated U-Nets with the first stage to localize the
prostate and the second stage to obtain an accurate segmentation from cropped
images. For data augmentation, we proposed the variable-input method which
crops the region of interest with additional random variations. Similar to
other deep learning models, the proposed model also faced the challenge of
suboptimal performance in certain testing cases due to varied training and
testing image characteristics. Therefore, it is valuable to evaluate the
confidence and performance of the network using uncertainty measures, which are
often calculated from the probability maps or their standard deviations with
multiple model outputs for the same testing case. However, few studies have
quantitatively compared different methods of uncertainty calculation.
Furthermore, unlike the commonly used Bayesian dropout during testing, we
developed uncertainty measures based on the variable input images at the second
stage and evaluated its performance by calculating the correlation with
ground-truth-based performance metrics, such as Dice score. For performance
estimation, we predicted Dice scores and Hausdorff distance with the most
correlated uncertainty measure. For post-processing, we performed Gaussian
filter on the underperformed slices to improve segmentation quality. Using
PROMISE-12 data, we demonstrated the robustness of the two-stage model and
showed high correlation of the proposed variable-input based uncertainty
measures with GT-based performance. The uncertainty-guided post-processing
method significantly improved label smoothness.",,,arXiv,,,2019-03-06,2019,,,,,,All OA, Green,Preprint,"Pan, Huitong; Feng, Yushan; Chen, Quan; Meyer, Craig; Feng, Xue","Pan, Huitong (); Feng, Yushan (); Chen, Quan (); Meyer, Craig (); Feng, Xue ()",,"Pan, Huitong (); Feng, Yushan (); Chen, Quan (); Meyer, Craig (); Feng, Xue ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119437287,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
1647,pub.1117944548,10.1109/isbi.2019.8759300,,,Prostate Segmentation From 3D Mri Using A Two-Stage Model and Variable-Input Based Uncertainty Measure,"This paper proposes a two-stage segmentation model, variable-input based uncertainty measures and an uncertainty-guided post-processing method for prostate segmentation on 3D magnetic resonance images (MRI). The two-stage model was based on 3D dilated U-Nets with the first stage to localize the prostate and the second stage to obtain an accurate segmentation from cropped images. For data augmentation, we proposed the variable-input method which crops the region of interest with additional random variations. Similar to other deep learning models, the proposed model also faced the challenge of suboptimal performance in certain testing cases due to varied training and testing image characteristics. Therefore, it is valuable to evaluate the confidence and performance of the network using uncertainty measures, which are often calculated from the probability maps or their standard deviations with multiple model outputs for the same testing case. However, few studies have quantitatively compared different methods of uncertainty calculation. Furthermore, unlike the commonly used Bayesian dropout during testing, we developed uncertainty measures based on the variable input images at the second stage and evaluated its performance by calculating the correlation with ground-truth-based performance metrics, such as Dice score. For performance estimation, we predicted Dice scores and Hausdorff distance with the most correlated uncertainty measure. For post-processing, we performed Gaussian filter on the underperformed slices to improve segmentation quality. Using PROMISE-12 data, we demonstrated the robustness of the two-stage model and showed high correlation of the proposed variable-input based uncertainty measures with GT-based performance. The uncertainty-guided post-processing method significantly improved label smoothness.",,,,2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),,2019-04-11,2019,,2019-04-11,0,,468-471,All OA, Green,Proceeding,"Pan, Huitong; Feng, Yushan; Chen, Quan; Meyer, Craig; Feng, Xue","Pan, Huitong (Springbok, Inc, Charlottesville, VA, USA); Feng, Yushan (Springbok, Inc, Charlottesville, VA, USA); Chen, Quan (Radiation Medicine, University of Kentucky, Lexington, KY, USA); Meyer, Craig (Springbok, Inc, Charlottesville, VA, USA; Biomedical Engineering, University of Virginia, Charlottesville, VA, USA); Feng, Xue (Springbok, Inc, Charlottesville, VA, USA; Biomedical Engineering, University of Virginia, Charlottesville, VA, USA)","Pan, Huitong ","Pan, Huitong (); Feng, Yushan (); Chen, Quan (University of Kentucky); Meyer, Craig (University of Virginia); Feng, Xue (University of Virginia)",11,8,,4.61,http://arxiv.org/pdf/1903.02500,https://app.dimensions.ai/details/publication/pub.1117944548,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
1634,pub.1153015277,10.1016/j.media.2022.102704,36473414,,Data synthesis and adversarial networks: A review and meta-analysis in cancer imaging,"Despite technological and medical advances, the detection, interpretation, and treatment of cancer based on imaging data continue to pose significant challenges. These include inter-observer variability, class imbalance, dataset shifts, inter- and intra-tumour heterogeneity, malignancy determination, and treatment effect uncertainty. Given the recent advancements in image synthesis, Generative Adversarial Networks (GANs), and adversarial training, we assess the potential of these technologies to address a number of key challenges of cancer imaging. We categorise these challenges into (a) data scarcity and imbalance, (b) data access and privacy, (c) data annotation and segmentation, (d) cancer detection and diagnosis, and (e) tumour profiling, treatment planning and monitoring. Based on our analysis of 164 publications that apply adversarial training techniques in the context of cancer imaging, we highlight multiple underexplored solutions with research potential. We further contribute the Synthesis Study Trustworthiness Test (SynTRUST), a meta-analysis framework for assessing the validation rigour of medical image synthesis studies. SynTRUST is based on 26 concrete measures of thoroughness, reproducibility, usefulness, scalability, and tenability. Based on SynTRUST, we analyse 16 of the most promising cancer imaging challenge solutions and observe a high validation rigour in general, but also several desirable improvements. With this work, we strive to bridge the gap between the needs of the clinical cancer imaging community and the current and prospective research on data synthesis and adversarial networks in the artificial intelligence community.",This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 952103.,,Medical Image Analysis,,"Humans; Image Processing, Computer-Assisted; Artificial Intelligence; Reproducibility of Results; Prospective Studies; Neoplasms; Magnetic Resonance Imaging",2022-11-24,2022,2022-11-24,2023-02,84,,102704,All OA, Green,Article,"Osuala, Richard; Kushibar, Kaisar; Garrucho, Lidia; Linardos, Akis; Szafranowska, Zuzanna; Klein, Stefan; Glocker, Ben; Diaz, Oliver; Lekadir, Karim","Osuala, Richard (Artificial Intelligence in Medicine Lab (BCN-AIM), Facultat de Matemàtiques i Informàtica, Universitat de Barcelona, Spain. Electronic address: richard.osuala@ub.edu.); Kushibar, Kaisar (Artificial Intelligence in Medicine Lab (BCN-AIM), Facultat de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.); Garrucho, Lidia (Artificial Intelligence in Medicine Lab (BCN-AIM), Facultat de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.); Linardos, Akis (Artificial Intelligence in Medicine Lab (BCN-AIM), Facultat de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.); Szafranowska, Zuzanna (Artificial Intelligence in Medicine Lab (BCN-AIM), Facultat de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.); Klein, Stefan (Biomedical Imaging Group Rotterdam, Department of Radiology & Nuclear Medicine, Erasmus MC, Rotterdam, The Netherlands.); Glocker, Ben (Biomedical Image Analysis Group, Department of Computing, Imperial College London, UK.); Diaz, Oliver (Artificial Intelligence in Medicine Lab (BCN-AIM), Facultat de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.); Lekadir, Karim (Artificial Intelligence in Medicine Lab (BCN-AIM), Facultat de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.)","Osuala, Richard (University of Barcelona)","Osuala, Richard (University of Barcelona); Kushibar, Kaisar (University of Barcelona); Garrucho, Lidia (University of Barcelona); Linardos, Akis (University of Barcelona); Szafranowska, Zuzanna (University of Barcelona); Klein, Stefan (Erasmus MC); Glocker, Ben (Imperial College London); Diaz, Oliver (University of Barcelona); Lekadir, Karim (University of Barcelona)",2,2,,,https://pure.eur.nl/ws/files/77475022/1_s2.0_S1361841522003322_main.pdf,https://app.dimensions.ai/details/publication/pub.1153015277,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
1629,pub.1138337289,10.1109/isbi48211.2021.9433793,,,Improving Prostate Whole Gland Segmentation In T2-Weighted MRI With Synthetically Generated Data,"Whole gland (WG) segmentation of the prostate plays a crucial role in detection, staging and treatment planning of prostate cancer (PCa). Despite promise shown by deep learning (DL) methods, they rely on the availability of a considerable amount of annotated data. Augmentation techniques such as translation and rotation of images present an alternative to increase data availability. Nevertheless, the amount of information provided by the transformed data is limited due to the correlation between the generated data and the original. Based on the recent success of generative adversarial networks (GAN) in producing synthetic images for other domains as well as in the medical domain, we present a pipeline to generate WG segmentation masks and synthesize T2-weighted MRI of the prostate based on a publicly available multi-center dataset. Following, we use the generated data as a form of data augmentation. Results show an improvement in the quality of the WG segmentation when compared to standard augmentation techniques.",This work has been funded by the University of Stavanger. This work has been funded by the University of Stavanger. The authors have no relevant financial or non-financial interests to disclose.,,,2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI),,2021-04-16,2021,,2021-04-16,0,,1915-1919,All OA, Green,Proceeding,"Fernandez-Quilez, Alvaro; Larsen, Steinar Valle; Goodwin, Morten; Gulsrud, Thor Ole; Kjosavik, Svein Reidar; Oppedal, Ketil","Fernandez-Quilez, Alvaro (Department of Quality and Health Technology, University of Stavanger, Norway; Stavanger Medical Imaging Laboratory (SMIL), Stavanger University Hospital, Norway); Larsen, Steinar Valle (Stavanger Medical Imaging Laboratory (SMIL), Stavanger University Hospital, Norway; Department of Electrical Engineering and Computer Science, University of Stavanger, Norway); Goodwin, Morten (Department of ICT, University of Agder, Grimstad, Norway); Gulsrud, Thor Ole (Department of Quality and Health Technology, University of Stavanger, Norway); Kjosavik, Svein Reidar (General Practice and Care Coordination Research Group, Stavanger University Hospital, Norway); Oppedal, Ketil (Stavanger Medical Imaging Laboratory (SMIL), Stavanger University Hospital, Norway; Department of Electrical Engineering and Computer Science, University of Stavanger, Norway; Centre for Age-Related Medicine, Stavanger University Hospital, Norway)","Fernandez-Quilez, Alvaro (University of Stavanger; Stavanger University Hospital)","Fernandez-Quilez, Alvaro (University of Stavanger; Stavanger University Hospital); Larsen, Steinar Valle (Stavanger University Hospital; University of Stavanger); Goodwin, Morten (University of Agder); Gulsrud, Thor Ole (University of Stavanger); Kjosavik, Svein Reidar (Stavanger University Hospital); Oppedal, Ketil (Stavanger University Hospital; University of Stavanger; Stavanger University Hospital)",9,9,,6.51,http://arxiv.org/pdf/2103.14955,https://app.dimensions.ai/details/publication/pub.1138337289,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,
1629,pub.1086387459,10.1007/978-3-319-59876-5_52,,,Segmentation of Prostate in Diffusion MR Images via Clustering,"Automatic segmentation of prostate gland in magnetic resonance (MR) images is a challenging task due to large variations of prostate shapes and indistinct boundaries with adjacent tissues. In this paper, we propose an automatic pipeline to segment prostate gland in diffusion magnetic resonance images (dMRI). The most common approach for segmenting prostate in MR images is based on image registration, which is computationally expensive and solely relies on the pre-segmented images (also known as atlas). In contrast, the proposed method uses a clustering method applied to the dMRI to separate prostate gland from the surrounding tissues followed by a postprocessing stage via active contours. The proposed pipeline was validated on prostate MR images of 25 patients and the segmentation results were compared to manually delineated prostate contours. The proposed method achieves an overall accuracy with mean Dice Similarity Coefficient (DSC) of 0.84\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ \pm \ $$\end{document}0.04, while being the most effective in the middle prostate gland producing a mean DSC of 0.91\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ \pm \ $$\end{document}0.03. The proposed method has the potential to be integrated into clinical decision support systems that aid radiologists in monitoring prostate cancer.",,,Lecture Notes in Computer Science,Image Analysis and Recognition,,2017-06-02,2017,2017-06-02,2017,10317,,471-478,Closed,Chapter,"Zhang, Junjie; Baig, Sameer; Wong, Alexander; Haider, Masoom A.; Khalvati, Farzad","Zhang, Junjie (Medical Imaging, Sunnybrook Research Institute, University of Toronto, Toronto, Canada); Baig, Sameer (Medical Imaging, Sunnybrook Research Institute, University of Toronto, Toronto, Canada); Wong, Alexander (Systems Design Engineering, University of Waterloo, Waterloo, Canada); Haider, Masoom A. (Medical Imaging, Sunnybrook Research Institute, University of Toronto, Toronto, Canada); Khalvati, Farzad (Medical Imaging, Sunnybrook Research Institute, University of Toronto, Toronto, Canada)","Khalvati, Farzad (University of Toronto)","Zhang, Junjie (University of Toronto); Baig, Sameer (University of Toronto); Wong, Alexander (University of Waterloo); Haider, Masoom A. (University of Toronto); Khalvati, Farzad (University of Toronto)",2,0,,,,https://app.dimensions.ai/details/publication/pub.1086387459,46 Information and Computing Sciences,,,,,,,,,,,,
1629,pub.1041723656,10.1007/978-3-319-13117-7_107,,,Automatic Design of Window Operators for the Segmentation of the Prostate Gland in Magnetic Resonance Images,"W-operators are nonlinear image operators that are translation invariant and locally defined inside a finite spatial window. In this work, we consider the problem of automatic design of W-operators for the segmentation of magnetic resonance (MR) volumes as a problem of classifier design. We propose to segment the objects of interest in an MR volume by classifying each pixel of its slices as either part of the objects of interest or background. The classifiers used here are the artificial feed-forward neural networks. The proposed method is applied to the segmentation of the two main regions of the prostate gland: the peripheral zone and the central gland. Performance evaluation was carried out on the volumes of the Prostate-3T collection of the NCI-ISBI 2013 Challenge. The results obtained show the suitability of our approach as a marker detector of the prostate gland.",,,IFMBE Proceedings,"VI Latin American Congress on Biomedical Engineering CLAIB 2014, Paraná, Argentina 29, 30 & 31 October 2014",,2015,2015,,2015,49,,417-420,Closed,Chapter,"Benalcázar, Marco E.; Brun, Marcel; Ballarin, Virginia","Benalcázar, Marco E. (Grupo de Procesamiento Digital de Imágenes, Universidad Nacional de Mar del Plata (UNMDP), Buenos Aires, Argentina; Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación (SENESCYT), Guayaquil, Ecuador; Consejo Nacional de Investigaciones Científicas y Técnicas (CONICET), Buenos Aires, Argentina); Brun, Marcel (Grupo de Procesamiento Digital de Imágenes, Universidad Nacional de Mar del Plata (UNMDP), Buenos Aires, Argentina); Ballarin, Virginia (Grupo de Procesamiento Digital de Imágenes, Universidad Nacional de Mar del Plata (UNMDP), Buenos Aires, Argentina)","Benalcázar, Marco E. (National University of Mar del Plata; ; National Scientific and Technical Research Council)","Benalcázar, Marco E. (National University of Mar del Plata; National Scientific and Technical Research Council); Brun, Marcel (National University of Mar del Plata); Ballarin, Virginia (National University of Mar del Plata)",5,1,,1.23,,https://app.dimensions.ai/details/publication/pub.1041723656,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,,
1624,pub.1138337347,10.1109/isbi48211.2021.9433851,,,Multi-Task Curriculum Learning For Semi-Supervised Medical Image Segmentation,"The lack of annotated data is a common problem in medical image segmentation tasks. In this paper, we present a novel multi-task semi-supervised segmentation algorithm with a curriculum-style learning strategy. The proposed method includes a segmentation task and an auxiliary regression task. Concretely, the auxiliary regression task aims to learn image-level properties such as the size and centroid position of target region to regularize the segmentation network, enforcing the pixel-level segmentation result match the distributions of these regressions. In addition, these regressions are treated as pseudo labels for the learning of unlabeled data. For the purpose of decreasing noise from the deviation of inferred labels, we adopt the inequality constraint for the learning of unlabeled data, which would generate a tolerance interval where the prediction within it would not be published to reduce the impact of prediction deviation of regression network. Experimental results on both 2017 ACDC dataset and PROMISE12 dataset demonstrate the effectiveness of our method.","This work is supported by National Natural Science Foundation of China (NSFC 62071314) and Sichuan Science and Technology Program (2021YFG0326, 2020YFG0079). This work is supported by National Natural Science Foundation of China (NSFC 62071314) and Sichuan Science and Technology Program (2021YFG0326, 2020YFG0079).",,,2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI),,2021-04-16,2021,,2021-04-16,0,,925-928,Closed,Proceeding,"Wang, Kaiping; Zhan, Bo; Luo, Yanmei; Zhou, Jiliu; Wu, Xi; Wang, Yan","Wang, Kaiping (College of Computer Science, Sichuan University, China); Zhan, Bo (College of Computer Science, Sichuan University, China); Luo, Yanmei (College of Computer Science, Sichuan University, China); Zhou, Jiliu (College of Computer Science, Sichuan University, China; College of Computer Science, Chengdu University of Information Technology, China); Wu, Xi (College of Computer Science, Chengdu University of Information Technology, China); Wang, Yan (College of Computer Science, Sichuan University, China)","Wang, Kaiping (Sichuan University)","Wang, Kaiping (Sichuan University); Zhan, Bo (Sichuan University); Luo, Yanmei (Sichuan University); Zhou, Jiliu (Sichuan University; Chengdu University of Information Technology); Wu, Xi (Chengdu University of Information Technology); Wang, Yan (Sichuan University)",2,2,,1.59,,https://app.dimensions.ai/details/publication/pub.1138337347,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,,
1623,pub.1147396344,10.1109/isbi52829.2022.9761564,,,A Feature Regularization Based Meta-Learning Framework for Generalizing Prostate Mri Segmentation,"Magnetic Resonance Imaging acquired by different operators and devices often vary greatly, causing the domain shift problem, where deep learning models trained from existing data sources perform poorly on other data sources. This paper proposes a novel feature regularization based meta learning framework to address this problem. In particular, we design a domain discriminator module to regularize the encoder to extract domain-invariant features, and an image reconstruction module to regularize the shape compactness of predictions for target domain data. We evaluate our method on three public prostate MRI datasets. Experimental results show that our approach has better segmentation performance and more powerful generalization performance.","This work was supported in part by the National Natural Science Foundation of China (No.61802027 and 61802022), the Beijing Natural Science Foundation-Haidian Original Innovation Joint Fund Project (No.L182034), the Fundamental Research Funds for the Central Universities (No.2019XD-A12 and 2020RC07), and the Scientific Research Seed Fund Of Peking University First Hospital (No.2021SF45).","This work was supported in part by the National Natural Science Foundation of China (No.61802027 and 61802022), the Beijing Natural Science Foundation-Haidian Original Innovation Joint Fund Project (No.L182034), the Fundamental Research Funds for the Central Universities (No.2019XD-A12 and 2020RC07), and the Scientific Research Seed Fund Of Peking University First Hospital (No.2021SF45)",,2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),,2022-03-31,2022,,2022-03-31,0,,1-4,Closed,Proceeding,"Wang, Hui; Zhang, Zheng; Zhang, Bo; Mi, Yue; Wu, Jingyun; Huang, Haiwen; Ma, Zibo; Wang, Wendong","Wang, Hui (State Key Laboratory of Networking and Switching Technology, Beijing, 100876, China; Beijing University of Posts and Telecommunications, Beijing, 100876, China); Zhang, Zheng (Beijing University of Posts and Telecommunications, Beijing, 100876, China); Zhang, Bo (State Key Laboratory of Networking and Switching Technology, Beijing, 100876, China; Beijing University of Posts and Telecommunications, Beijing, 100876, China); Mi, Yue (Peking University First Hospital, Beijing, 100034, China); Wu, Jingyun (Peking University First Hospital, Beijing, 100034, China); Huang, Haiwen (Peking University First Hospital, Beijing, 100034, China); Ma, Zibo (State Key Laboratory of Networking and Switching Technology, Beijing, 100876, China; Beijing University of Posts and Telecommunications, Beijing, 100876, China); Wang, Wendong (State Key Laboratory of Networking and Switching Technology, Beijing, 100876, China; Beijing University of Posts and Telecommunications, Beijing, 100876, China)","Zhang, Bo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Wang, Wendong (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications)","Wang, Hui (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Zhang, Zheng (Beijing University of Posts and Telecommunications); Zhang, Bo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Mi, Yue (Peking University First Hospital); Wu, Jingyun (Peking University First Hospital); Huang, Haiwen (Peking University First Hospital); Ma, Zibo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Wang, Wendong (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1147396344,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,,
1623,pub.1119401279,10.48550/arxiv.1902.03459,,,Super-realtime facial landmark detection and shape fitting by deep  regression of shape model parameters,"We present a method for highly efficient landmark detection that combines
deep convolutional neural networks with well established model-based fitting
algorithms. Motivated by established model-based fitting methods such as active
shapes, we use a PCA of the landmark positions to allow generative modeling of
facial landmarks. Instead of computing the model parameters using iterative
optimization, the PCA is included in a deep neural network using a novel layer
type. The network predicts model parameters in a single forward pass, thereby
allowing facial landmark detection at several hundreds of frames per second.
Our architecture allows direct end-to-end training of a model-based landmark
detection method and shows that deep neural networks can be used to reliably
predict model parameters directly without the need for an iterative
optimization. The method is evaluated on different datasets for facial landmark
detection and medical image segmentation. PyTorch code is freely available at
https://github.com/justusschock/shapenet",,,arXiv,,,2019-02-09,2019,,,,,,All OA, Green,Preprint,"Kopaczka, Marcin; Schock, Justus; Merhof, Dorit","Kopaczka, Marcin (); Schock, Justus (); Merhof, Dorit ()",,"Kopaczka, Marcin (); Schock, Justus (); Merhof, Dorit ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119401279,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1621,pub.1112737819,10.1117/12.2512551,,,Automatic MRI prostate segmentation using 3D deeply supervised FCN with concatenated atrous convolution,"Prostate segmentation of MR volumes is a very important task for treatment planning and image-guided brachytherapy and radiotherapy. Manual delineation of prostate in MR image is very time-consuming and depends on the subjective experience of the physicians. On the other hand, automatic prostate segmentation becomes a reasonable and attractive choice for its speed, even though the task is very challenging because of inhomogeneous intensity and variability of prostate appearance and shape. In this paper, we propose a method to automatically segment MR prostate image based on 3D deeply supervised FCN with concatenated atrous convolution (3D DSA-FCN). More discriminative features provide explicit convergence acceleration in training stage using straightforward dense predictions as deep supervision and the concatenated atrous convolution extract more global contextual information for accurate predictions. The presented method was evaluated on the internal dataset comprising 15 T2-weighted prostate MR volumes from Winship Cancer Institute and obtained a mean Dice similarity coefficient (DSC) of 0.8520.031, 95% Hausdorff distance (95%HD) 7.1891.953 mm and mean surface distance (MSD) of 1.5970.360 mm. The experimental results show that our 3D DSA-FCN could yield satisfied MR prostate segmentation, which can be used for image-guided radiotherapy.",,,Progress in Biomedical Optics and Imaging,Medical Imaging 2019: Computer-Aided Diagnosis,,2019-03-13,2019,,,10950,,109503x-109503x-8,Closed,Proceeding,"Wang, Bo; Lei, Yang; Jeong, Jiwoong Jason; Wang, Tonghe; Liu, Yingzi; Tian, Sibo; Patel, Pretesh; Jiang, Xiaojun; Jani, Ashesh B.; Mao, Hui; Curran, Walter J.; Liu, Tian; Yang, Xiaofeng","Wang, Bo (Ningxia Univ. (China)); Lei, Yang (Emory Univ. (United States)); Jeong, Jiwoong Jason (Emory Univ. (United States)); Wang, Tonghe (Emory Univ. (United States)); Liu, Yingzi (Emory Univ. (United States)); Tian, Sibo (Emory Univ. (United States)); Patel, Pretesh (Emory Univ. (United States)); Jiang, Xiaojun (Emory Univ. (United States)); Jani, Ashesh B. (Emory Univ. (United States)); Mao, Hui (Georgia Institute of Technology (United States)); Curran, Walter J. (Emory Univ. (United States)); Liu, Tian (Emory Univ. (United States)); Yang, Xiaofeng (Emory Univ. (United States))",,"Wang, Bo (Ningxia University); Lei, Yang (Emory University); Jeong, Jiwoong Jason (Emory University); Wang, Tonghe (Emory University); Liu, Yingzi (Emory University); Tian, Sibo (Emory University); Patel, Pretesh (Emory University); Jiang, Xiaojun (Emory University); Jani, Ashesh B. (Emory University); Mao, Hui (Georgia Institute of Technology); Curran, Walter J. (Emory University); Liu, Tian (Emory University); Yang, Xiaofeng (Emory University)",14,7,,6.11,,https://app.dimensions.ai/details/publication/pub.1112737819,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
1621,pub.1136783518,10.48550/arxiv.2103.14955,,,Improving prostate whole gland segmentation in t2-weighted MRI with  synthetically generated data,"Whole gland (WG) segmentation of the prostate plays a crucial role in
detection, staging and treatment planning of prostate cancer (PCa). Despite
promise shown by deep learning (DL) methods, they rely on the availability of a
considerable amount of annotated data. Augmentation techniques such as
translation and rotation of images present an alternative to increase data
availability. Nevertheless, the amount of information provided by the
transformed data is limited due to the correlation between the generated data
and the original. Based on the recent success of generative adversarial
networks (GAN) in producing synthetic images for other domains as well as in
the medical domain, we present a pipeline to generate WG segmentation masks and
synthesize T2-weighted MRI of the prostate based on a publicly available
multi-center dataset. Following, we use the generated data as a form of data
augmentation. Results show an improvement in the quality of the WG segmentation
when compared to standard augmentation techniques.",,,arXiv,,,2021-03-27,2021,,,,,,All OA, Green,Preprint,"Fernandez-Quilez, Alvaro; Larsen, Steinar Valle; Goodwin, Morten; Gulsurd, Thor Ole; Kjosavik, Svein Reidar; Oppedal, Ketil","Fernandez-Quilez, Alvaro (); Larsen, Steinar Valle (); Goodwin, Morten (); Gulsurd, Thor Ole (); Kjosavik, Svein Reidar (); Oppedal, Ketil ()",,"Fernandez-Quilez, Alvaro (); Larsen, Steinar Valle (); Goodwin, Morten (); Gulsurd, Thor Ole (); Kjosavik, Svein Reidar (); Oppedal, Ketil ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136783518,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,
1621,pub.1032101903,10.1007/978-3-319-24888-2_2,,,Visual Saliency Based Active Learning for Prostate MRI Segmentation,We propose an active learning (AL) approach for prostate segmentation from magnetic resonance (MR) images. Our label query strategy is inspired from the principles of visual saliency that has similar considerations for choosing the most salient region. These similarities are encoded in a graph using classification maps and low level features. Random walks identify the most informative node which is equivalent to the label query sample in AL. Experimental results on the MICCAI 2012 Prostate segmentation challenge show the superior performance of our approach to conventional methods using fully supervised learning.,,,Lecture Notes in Computer Science,Machine Learning in Medical Imaging,,2015-10-02,2015,2015-10-02,2015,9352,,9-16,Closed,Chapter,"Mahapatra, Dwarikanath; Buhmann, Joachim M.","Mahapatra, Dwarikanath (Department of Computer Science, ETH Zurich, Zürich, Switzerland); Buhmann, Joachim M. (Department of Computer Science, ETH Zurich, Zürich, Switzerland)","Mahapatra, Dwarikanath (ETH Zurich)","Mahapatra, Dwarikanath (ETH Zurich); Buhmann, Joachim M. (ETH Zurich)",13,7,,3.69,,https://app.dimensions.ai/details/publication/pub.1032101903,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
1612,pub.1123958994,10.48550/arxiv.2001.02387,,,A context based deep learning approach for unbalanced medical image  segmentation,"Automated medical image segmentation is an important step in many medical
procedures. Recently, deep learning networks have been widely used for various
medical image segmentation tasks, with U-Net and generative adversarial nets
(GANs) being some of the commonly used ones. Foreground-background class
imbalance is a common occurrence in medical images, and U-Net has difficulty in
handling class imbalance because of its cross entropy (CE) objective function.
Similarly, GAN also suffers from class imbalance because the discriminator
looks at the entire image to classify it as real or fake. Since the
discriminator is essentially a deep learning classifier, it is incapable of
correctly identifying minor changes in small structures. To address these
issues, we propose a novel context based CE loss function for U-Net, and a
novel architecture Seg-GLGAN. The context based CE is a linear combination of
CE obtained over the entire image and its region of interest (ROI). In
Seg-GLGAN, we introduce a novel context discriminator to which the entire image
and its ROI are fed as input, thus enforcing local context. We conduct
extensive experiments using two challenging unbalanced datasets: PROMISE12 and
ACDC. We observe that segmentation results obtained from our methods give
better segmentation metrics as compared to various baseline methods.",,,arXiv,,,2020-01-08,2020,,,,,,All OA, Green,Preprint,"Murugesan, Balamurali; Sarveswaran, Kaushik; S, Vijaya Raghavan; Shankaranarayana, Sharath M; Ram, Keerthi; Sivaprakasam, Mohanasankar","Murugesan, Balamurali (); Sarveswaran, Kaushik (); S, Vijaya Raghavan (); Shankaranarayana, Sharath M (); Ram, Keerthi (); Sivaprakasam, Mohanasankar ()",,"Murugesan, Balamurali (); Sarveswaran, Kaushik (); S, Vijaya Raghavan (); Shankaranarayana, Sharath M (); Ram, Keerthi (); Sivaprakasam, Mohanasankar ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1123958994,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1612,pub.1122952417,10.1007/978-3-030-34110-7_45,,,Scale Normalization Cascaded Dense-Unet for Prostate Segmentation in MR Images,"Automated and accurate prostate segmentation technique from magnetic resonance images plays an important role in diagnostic and radiological planning. However, this task faces the challenge of extreme scale variation of prostate glands presented in the slices at different locations of MRI volumes. To alleviate problems arising from scale variation. We propose a cascaded prostate segmentation model that includes three stages: Coarse segmentation, segmentation result refinement, and scale normalization segmentation. Segmentation result refinement can remove the coarse segmentation results that do not contain prostates. More importantly, it normalizes the scale of the prostate region on different slice images of the same nuclear magnetic resonance volume according to the result of the coarse segmentation, thereby making the scale normalization segmentation network obtain scale-invariant magnetic resonance images as input. The experimental results demonstrate that this design can significantly reduce the degradation of segmentation performance arising from large scale variation.",This work is supported by Shanghai Science and Technology Commission (grant No. 17511104203) and NSFC (grant NO. 61472087).,,Lecture Notes in Computer Science,Image and Graphics,,2019-11-28,2019,2019-11-28,2019,11902,,538-547,Closed,Chapter,"Chen, Yuxuan; Li, Suiyi; Yang, Su; Luo, Wuyang","Chen, Yuxuan (Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, 201203, Shanghai, China); Li, Suiyi (Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, 201203, Shanghai, China); Yang, Su (Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, 201203, Shanghai, China); Luo, Wuyang (Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, 201203, Shanghai, China)","Yang, Su (Fudan University)","Chen, Yuxuan (Fudan University); Li, Suiyi (Fudan University); Yang, Su (Fudan University); Luo, Wuyang (Fudan University)",3,2,,,,https://app.dimensions.ai/details/publication/pub.1122952417,46 Information and Computing Sciences,,,,,,,,,,,,
1612,pub.1122546770,10.1007/978-3-030-35817-4_21,,,CNS: CycleGAN-Assisted Neonatal Segmentation Model for Cross-Datasets,"Abstract
Accurate segmentation of neonatal brain MR images is critical for studying early brain development. Recently, supervised learning-based methods, i.e., using convolutional neural networks (CNNs), have been successfully applied to infant brain segmentation. Although these CNN-based methods have achieved reasonable segmentation results on the testing subjects acquired with similar imaging protocol as the training subjects, they are typically not able to produce reasonable results for the testing subjects acquired with different imaging protocols. To address this practical issue, in this paper, we propose leveraging a cycle-consistent generative adversarial network (CycleGAN) to transfer each testing image (of a new dataset/cross-dataset) into the domain of training data, thus obtaining the transferred testing image with similar intensity appearance as the training images. Then, a densely-connected U-Net based segmentation model, which has been trained on the training data, can be utilized to robustly segment each transferred testing image. Experimental results demonstrate the superior performance of our proposed method, over existing methods, on segmenting cross-dataset of neonatal brain MR images.",,,Lecture Notes in Computer Science,Graph Learning in Medical Imaging,,2019-11-14,2019,2019-11-14,2019,11849,,172-179,Closed,Chapter,"Chen, Jian; Fang, Zhenghan; Xiao, Deqiang; Bui, Duc Toan; Thung, Kim-Han; Li, Xianjun; Yang, Jian; Lin, Weili; Li, Gang; Shen, Dinggang; Wang, Li","Chen, Jian (School of Information Science and Engineering, Fujian University of Technology, Fuzhou, China); Fang, Zhenghan (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Xiao, Deqiang (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Bui, Duc Toan (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Thung, Kim-Han (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Li, Xianjun (Department of Diagnostic Radiology, The First Affiliated Hospital of Xi’an Jiaotong University, Xi’an, China); Yang, Jian (Department of Diagnostic Radiology, The First Affiliated Hospital of Xi’an Jiaotong University, Xi’an, China); Lin, Weili (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Li, Gang (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Shen, Dinggang (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Wang, Li (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA)","Shen, Dinggang (University of North Carolina at Chapel Hill); Wang, Li (University of North Carolina at Chapel Hill)","Chen, Jian (Fujian University of Technology); Fang, Zhenghan (University of North Carolina at Chapel Hill); Xiao, Deqiang (University of North Carolina at Chapel Hill); Bui, Duc Toan (University of North Carolina at Chapel Hill); Thung, Kim-Han (University of North Carolina at Chapel Hill); Li, Xianjun (First Affiliated Hospital of Xi'an Jiaotong University); Yang, Jian (First Affiliated Hospital of Xi'an Jiaotong University); Lin, Weili (University of North Carolina at Chapel Hill); Li, Gang (University of North Carolina at Chapel Hill); Shen, Dinggang (University of North Carolina at Chapel Hill); Wang, Li (University of North Carolina at Chapel Hill)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1122546770,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1611,pub.1120980417,10.1109/bhi.2019.8834494,,,Classification before Segmentation: Improved U-Net Prostate Segmentation,"Prostate segmentation is a necessary pre-processing step for computer-aided detection and diagnosis algorithms for prostate disorders and associated cancers. Deep learning models like U-Net offer the potential for performing classification and segmentation in a single step. We evaluated the U-Net model for prostate segmentation on 1,235 Magnetic Resonance (MR) images from 39 patients with prostate cancer. On our data set, the U-Net models generated a substantial number of false-positive predictions (average precision of 67.6%). We propose separating classification and segmentation into distinct tasks implemented via a pipeline. Our classifier achieved an average ROC AUC of 97.7% (SD 1.0%) in four-fold cross-fold validation. Compared to the U-Net model alone, our pipeline increased overall agreement between the predicted and human-annotated masks (Dice score of 0.907 vs 0.758 and precision of 83.2% vs 67.6%) without a significant loss in recall (83.4% vs 85.6%). Performing classification and segmentation in a single step is a desirable goal. Our results suggest, however, that improvements are still needed before U-Net offers comparable precision to using a separate classifier.",Research reported in this publication was supported by National Cancer Institute of the National Institutes of Health under award numbers R01CA218144 and R21CA231892. We would like to thank Derek Riley and Christopher R. Beal for feedback on the manuscript. Research reported in this publication was supported by National Cancer Institute of the National Institutes of Health under award numbers R01CA218144 and R21CA231892.,,,2019 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),,2019-05-19,2019,,2019-05-19,0,,1-4,Closed,Proceeding,"Nowling, Ronald J.; Bukowy, John; McGarry, Sean D.; Nencka, Andrew S.; Blasko, Oliver; Urbain, Jay; Lowman, Allison; Barrington, Alexander; Banerjee, Anjishnu; Iczkowski, Kenneth A.; LaViolette, Peter S.","Nowling, Ronald J. (Milwaukee School of Engineering, Milwaukee, WI, 53202); Bukowy, John (Medical College of Wisconsin, Milwaukee, WI, 53226); McGarry, Sean D. (Medical College of Wisconsin, Milwaukee, WI, 53226); Nencka, Andrew S. (Medical College of Wisconsin, Milwaukee, WI, 53226); Blasko, Oliver (Milwaukee School of Engineering, Milwaukee, WI, 53202); Urbain, Jay (Milwaukee School of Engineering, Milwaukee, WI, 53202; Medical College of Wisconsin, Milwaukee, WI, 53226); Lowman, Allison (Medical College of Wisconsin, Milwaukee, WI, 53226); Barrington, Alexander (Medical College of Wisconsin, Milwaukee, WI, 53226); Banerjee, Anjishnu (Medical College of Wisconsin, Milwaukee, WI, 53226); Iczkowski, Kenneth A. (Medical College of Wisconsin, Milwaukee, WI, 53226); LaViolette, Peter S. (Medical College of Wisconsin, Milwaukee, WI, 53226)",,"Nowling, Ronald J. (Milwaukee School of Engineering); Bukowy, John (Medical College of Wisconsin); McGarry, Sean D. (Medical College of Wisconsin); Nencka, Andrew S. (Medical College of Wisconsin); Blasko, Oliver (Milwaukee School of Engineering); Urbain, Jay (Milwaukee School of Engineering; Medical College of Wisconsin); Lowman, Allison (Medical College of Wisconsin); Barrington, Alexander (Medical College of Wisconsin); Banerjee, Anjishnu (Medical College of Wisconsin); Iczkowski, Kenneth A. (Medical College of Wisconsin); LaViolette, Peter S. (Medical College of Wisconsin)",8,7,,2.4,,https://app.dimensions.ai/details/publication/pub.1120980417,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,,
1594,pub.1147392916,10.1109/isbi52829.2022.9761487,,,Adaptation of a Multi-Site Network to a New Clinical Site Via Batch-Normalization Similarity,"This paper tackles the challenging problem of medical site adaptation; i.e., learning a model from multi-site source data such that it can be modified and adapted to a new site using only unlabeled data from the new site. The method is based on Domain Specific Batch Normalization architecture and uses the Batch Normalization statistics of the new site to find the most similar internal site. The similarity measure is computed in an embedded space of the BN parameters. We evaluated our method on the task of MRI prostate segmentation. Public datasets from six different institutions were used, containing distribution shifts. The experimental results show that the proposed approach outperforms other generalization and adaptation methods.",,"This research was supported by the Ministry of Science & Technology, Israel.",,2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),,2022-03-31,2022,,2022-03-31,0,,1-5,Closed,Proceeding,"Serlin, Shira Kasten; Goldberger, Jacob; Greenspan, Hayit","Serlin, Shira Kasten (Department of Biomedical Engineering, Tel Aviv University, Tel Aviv, Israel); Goldberger, Jacob (Faculty of Engineering, Bar-Ilan University, Ramat-Gan, Israel); Greenspan, Hayit (Department of Biomedical Engineering, Tel Aviv University, Tel Aviv, Israel)","Serlin, Shira Kasten (Tel Aviv University)","Serlin, Shira Kasten (Tel Aviv University); Goldberger, Jacob (Bar-Ilan University); Greenspan, Hayit (Tel Aviv University)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1147392916,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1589,pub.1107037508,10.1007/978-3-030-00919-9_42,,,Iterative Interaction Training for Segmentation Editing Networks,"Automatic segmentation has great potential to facilitate morphological measurements while simultaneously increasing efficiency. Nevertheless often users want to edit the segmentation to their own needs and will need different tools for this. There has been methods developed to edit segmentations of automatic methods based on the user input, primarily for binary segmentations. Here however, we present an unique training strategy for convolutional neural networks (CNNs) trained on top of an automatic method to enable interactive segmentation editing that is not limited to binary segmentation. By utilizing a robot-user during training, we closely mimic realistic use cases to achieve optimal editing performance. In addition, we show that an increase of the iterative interactions during the training process up to ten improves the segmentation editing performance substantially. Furthermore, we compare our segmentation editing CNN (interCNN) to state-of-the-art interactive segmentation algorithms and show a superior or on par performance.",We thank the Swiss Data Science Center (project C17-04 deepMICROIA) for funding and acknowledge NVIDIA for GPU support.,,Lecture Notes in Computer Science,Machine Learning in Medical Imaging,,2018-09-15,2018,2018-09-15,2018,11046,,363-370,All OA, Green,Chapter,"Bredell, Gustav; Tanner, Christine; Konukoglu, Ender","Bredell, Gustav (Computer Vision Laboratory, ETH Zurich, Zurich, Switzerland); Tanner, Christine (Computer Vision Laboratory, ETH Zurich, Zurich, Switzerland); Konukoglu, Ender (Computer Vision Laboratory, ETH Zurich, Zurich, Switzerland)","Bredell, Gustav (ETH Zurich)","Bredell, Gustav (ETH Zurich); Tanner, Christine (ETH Zurich); Konukoglu, Ender (ETH Zurich)",17,14,,6.24,http://arxiv.org/pdf/1807.08555,https://app.dimensions.ai/details/publication/pub.1107037508,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
1589,pub.1084919114,10.1007/978-3-319-46726-9_25,,,Multi-task Shape Regression for Medical Image Segmentation,"In this paper, we propose a general segmentation framework of Multi-Task Shape Regression (MTSR) which formulates segmentation as multi-task learning to leverage its strength of jointly solving multiple tasks enhanced by capturing task correlations. The MTSR entirely estimates coordinates of all points on shape contours by multi-task regression, where estimation of each coordinate corresponds to a regression task; the MTSR can jointly handle nonlinear relationships between image appearance and shapes while capturing holistic shape information by encoding coordinate correlations, which enables estimation of highly variable shapes, even with vague edge or region inhomogeneity. The MTSR achieves a long-desired general framework without relying on any specific assumptions or initialization, which enables flexible and fully automatic segmentation of multiple objects simultaneously, for different applications irrespective of modalities. The MTSR is validated on six representative applications of diverse images, achieves consistently high performance with dice similarity coefficient (DSC) up to 0.93 and largely outperforms state of the arts in each application, which demonstrates its effectiveness and generality for medical image segmentation.",This work was supported by the NSFC Joint Fund with Guangdong under Key Project (Grant No. U1201258) and NSFC (Grant No. 61571147).,,Lecture Notes in Computer Science,Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016,,2016-10-02,2016,2016-10-02,2016,9902,,210-218,Closed,Chapter,"Zhen, Xiantong; Yin, Yilong; Bhaduri, Mousumi; Nachum, Ilanit Ben; Laidley, David; Li, Shuo","Zhen, Xiantong (Digital Imaging Group (DIG), London, Ontario, Canada; The University of Western Ontario, London, Ontario, Canada); Yin, Yilong (Shandong University, Shandong, China); Bhaduri, Mousumi (London Health Sciences Centre, London, Ontario, Canada); Nachum, Ilanit Ben (Digital Imaging Group (DIG), London, Ontario, Canada; The University of Western Ontario, London, Ontario, Canada); Laidley, David (London Health Sciences Centre, London, Ontario, Canada); Li, Shuo (Digital Imaging Group (DIG), London, Ontario, Canada; The University of Western Ontario, London, Ontario, Canada)","Zhen, Xiantong (; Western University)","Zhen, Xiantong (Western University); Yin, Yilong (Shandong University); Bhaduri, Mousumi (London Health Sciences Centre); Nachum, Ilanit Ben (Western University); Laidley, David (London Health Sciences Centre); Li, Shuo (Western University)",7,0,,2.0,,https://app.dimensions.ai/details/publication/pub.1084919114,"46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,,
1588,pub.1122023787,10.48550/arxiv.1910.10542,,,Deep generative model-driven multimodal prostate segmentation in  radiotherapy,"Deep learning has shown unprecedented success in a variety of applications,
such as computer vision and medical image analysis. However, there is still
potential to improve segmentation in multimodal images by embedding prior
knowledge via learning-based shape modeling and registration to learn the
modality invariant anatomical structure of organs. For example, in radiotherapy
automatic prostate segmentation is essential in prostate cancer diagnosis,
therapy, and post-therapy assessment from T2-weighted MR or CT images. In this
paper, we present a fully automatic deep generative model-driven multimodal
prostate segmentation method using convolutional neural network (DGMNet). The
novelty of our method comes with its embedded generative neural network for
learning-based shape modeling and its ability to adapt for different imaging
modalities via learning-based registration. The proposed method includes a
multi-task learning framework that combines a convolutional feature extraction
and an embedded regression and classification based shape modeling. This
enables the network to predict the deformable shape of an organ. We show that
generative neural networkbased shape modeling trained on a reliable contrast
imaging modality (such as MRI) can be directly applied to low contrast imaging
modality (such as CT) to achieve accurate prostate segmentation. The method was
evaluated on MRI and CT datasets acquired from different clinical centers with
large variations in contrast and scanning protocols. Experimental results
reveal that our method can be used to automatically and accurately segment the
prostate gland in different imaging modalities.",,,arXiv,,,2019-10-23,2019,,,,,,All OA, Green,Preprint,"Girum, Kibrom Berihu; Créhange, Gilles; Hussain, Raabid; Walker, Paul Michael; Lalande, Alain","Girum, Kibrom Berihu (); Créhange, Gilles (); Hussain, Raabid (); Walker, Paul Michael (); Lalande, Alain ()",,"Girum, Kibrom Berihu (); Créhange, Gilles (); Hussain, Raabid (); Walker, Paul Michael (); Lalande, Alain ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1122023787,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 4611 Machine Learning, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,
1586,pub.1119958880,10.1007/978-3-030-26763-6_46,,,Cascade Dense-Unet for Prostate Segmentation in MR Images,"Automatic prostate segmentation from magnetic resonance images can assist in diagnosis and radiological planning. The extensive clinical application of this task has attracted the attention of researchers. However, due to noise, blurred boundaries and scale variation, it is very challenging to segment prostate from magnetic resonance images. We propose a cascade method for prostate segmentation. The model consists of two stage. In the first stage, a dense-unet model are used to obtain the initial segmentation results. In the second stage, the segmentation result of the first stage is used as prior knowledge, and another dense-unet is used to obtain more accurate segmentation results. The experimental results show that the proposed method can obtain more accurate segmentation results.",This work is supported by Shanghai Science and Technology Commission (grant No. 17511104203) and NSFC (grant NO. 61472087).,,Lecture Notes in Computer Science,Intelligent Computing Theories and Application,,2019-07-24,2019,2019-07-24,2019,11643,,481-490,Closed,Chapter,"Li, Suiyi; Chen, Yuxuan; Yang, Su; Luo, Wuyang","Li, Suiyi (Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, 201203, Shanghai, China); Chen, Yuxuan (Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, 201203, Shanghai, China); Yang, Su (Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, 201203, Shanghai, China); Luo, Wuyang (Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, 201203, Shanghai, China)","Yang, Su (Fudan University)","Li, Suiyi (Fudan University); Chen, Yuxuan (Fudan University); Yang, Su (Fudan University); Luo, Wuyang (Fudan University)",25,19,,,,https://app.dimensions.ai/details/publication/pub.1119958880,46 Information and Computing Sciences,,,,,,,,,,,,
1586,pub.1119178849,10.48550/arxiv.1807.08555,,,Iterative Interaction Training for Segmentation Editing Networks,"Automatic segmentation has great potential to facilitate morphological
measurements while simultaneously increasing efficiency. Nevertheless often
users want to edit the segmentation to their own needs and will need different
tools for this. There has been methods developed to edit segmentations of
automatic methods based on the user input, primarily for binary segmentations.
Here however, we present an unique training strategy for convolutional neural
networks (CNNs) trained on top of an automatic method to enable interactive
segmentation editing that is not limited to binary segmentation. By utilizing a
robot-user during training, we closely mimic realistic use cases to achieve
optimal editing performance. In addition, we show that an increase of the
iterative interactions during the training process up to ten improves the
segmentation editing performance substantially. Furthermore, we compare our
segmentation editing CNN (interCNN) to state-of-the-art interactive
segmentation algorithms and show a superior or on par performance.",,,arXiv,,,2018-07-23,2018,,,,,,All OA, Green,Preprint,"Bredell, Gustav; Tanner, Christine; Konukoglu, Ender","Bredell, Gustav (); Tanner, Christine (); Konukoglu, Ender ()",,"Bredell, Gustav (); Tanner, Christine (); Konukoglu, Ender ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119178849,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
1586,pub.1119171222,10.48550/arxiv.1807.06742,,,3D Global Convolutional Adversarial Network\\ for Prostate MR Volume  Segmentation,"Advanced deep learning methods have been developed to conduct prostate MR
volume segmentation in either a 2D or 3D fully convolutional manner. However,
2D methods tend to have limited segmentation performance, since large amounts
of spatial information of prostate volumes are discarded during the
slice-by-slice segmentation process; and 3D methods also have room for
improvement, since they use isotropic kernels to perform 3D convolutions
whereas most prostate MR volumes have anisotropic spatial resolution. Besides,
the fully convolutional structural methods achieve good performance for
localization issues but neglect the per-voxel classification for segmentation
tasks. In this paper, we propose a 3D Global Convolutional Adversarial Network
(3D GCA-Net) to address efficient prostate MR volume segmentation. We first
design a 3D ResNet encoder to extract 3D features from prostate scans, and then
develop the decoder, which is composed of a multi-scale 3D global convolutional
block and a 3D boundary refinement block, to address the classification and
localization issues simultaneously for volumetric segmentation. Additionally,
we combine the encoder-decoder segmentation network with an adversarial network
in the training phrase to enforce the contiguity of long-range spatial
predictions. Throughout the proposed model, we use anisotropic convolutional
processing for better feature learning on prostate MR scans. We evaluated our
3D GCA-Net model on two public prostate MR datasets and achieved
state-of-the-art performances.",,,arXiv,,,2018-07-17,2018,,,,,,All OA, Green,Preprint,"Jia, Haozhe; Song, Yang; Zhang, Donghao; Huang, Heng; Feng, Dagan; Fulham, Michael; Xia, Yong; Cai, Weidong","Jia, Haozhe (); Song, Yang (); Zhang, Donghao (); Huang, Heng (); Feng, Dagan (); Fulham, Michael (); Xia, Yong (); Cai, Weidong ()",,"Jia, Haozhe (); Song, Yang (); Zhang, Donghao (); Huang, Heng (); Feng, Dagan (); Fulham, Michael (); Xia, Yong (); Cai, Weidong ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119171222,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1577,pub.1146252717,10.48550/arxiv.2203.06000,,,Polar Transformation Based Multiple Instance Learning Assisting Weakly  Supervised Image Segmentation With Loose Bounding Box Annotations,"This study investigates weakly supervised image segmentation using loose
bounding box supervision. It presents a multiple instance learning strategy
based on polar transformation to assist image segmentation when loose bounding
boxes are employed as supervision. In this strategy, weighted smooth maximum
approximation is introduced to incorporate the observation that pixels closer
to the origin of the polar transformation are more likely to belong to the
object in the bounding box. The proposed approach was evaluated on a public
medical dataset using Dice coefficient. The results demonstrate its superior
performance. The codes are available at
\url{https://github.com/wangjuan313/wsis-polartransform}.",,,arXiv,,,2022-03-02,2022,,,,,,All OA, Green,Preprint,"Wang, Juan; Xia, Bin","Wang, Juan (); Xia, Bin ()",,"Wang, Juan (); Xia, Bin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146252717,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1577,pub.1138249568,10.1109/aieee51419.2021.9435797,,,Investigation of MRI Prostate Localization using Different MRI Modality Scans,"According to the data of World Cancer Research Fund International prostate cancer is the second most common after lung cancer and the fifth most common cause of cancer death amongst men. Prostate cancer is also the fourth most frequent tumor between both genders worldwide. Biopsy is the only way to detect prostate cancer so far. Statistics show that it is able to detect only 70-80% of clinically significant cancer cases. Multi parametric magnetic resonance imaging technique comes to play to help in determining the location to perform biopsy on. The first step to automating the detection of the location is applying prostate segmentation on magnetic resonance images. The fact that there is lack of standardization of signal intensity to acquire those images burdens the problem of automated prostate segmentation. Authors review the results of Prostate MR Image Segmentation (PROMISE12) challenge, designed to evaluate and compare different prostate segmentation algorithms, and provide insights on automated prostate segmentation by applying best open source algorithm under different circumstances. Authors applied selected algorithm on two different datasets and showed how segmentation results can be improved by applying even the most primitive image stretching techniques. Authors also showed that algorithm is promising in segmenting unseen dataset.",,,,"2020 IEEE 8th Workshop on Advances in Information, Electronic and Electrical Engineering (AIEEE)",,2021-04-24,2021,,2021-04-24,0,,1-5,Closed,Proceeding,"Jucevičius, Justinas; Treigys, Povilas; Bernatavičienė, Jolita; Briedienė, Rūta; Naruševičiūtė, Ieva; Trakymas, Mantas","Jucevičius, Justinas (Institute of Data Science and Digital Technologies, Vilnius University, Vilnius, Lithuania); Treigys, Povilas (Institute of Data Science and Digital Technologies, Vilnius University, Vilnius, Lithuania); Bernatavičienė, Jolita (Institute of Data Science and Digital Technologies, Vilnius University, Vilnius, Lithuania); Briedienė, Rūta (Radiology Department, National Cancer Institute of Lithuania, Vilnius, Lithuania); Naruševičiūtė, Ieva (Radiology Department, National Cancer Institute of Lithuania, Vilnius, Lithuania); Trakymas, Mantas (Radiology Department, National Cancer Institute of Lithuania, Vilnius, Lithuania)","Jucevičius, Justinas (Vilnius University)","Jucevičius, Justinas (Vilnius University); Treigys, Povilas (Vilnius University); Bernatavičienė, Jolita (Vilnius University); Briedienė, Rūta (National Cancer Institute); Naruševičiūtė, Ieva (National Cancer Institute); Trakymas, Mantas (National Cancer Institute)",2,2,,1.45,,https://app.dimensions.ai/details/publication/pub.1138249568,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
1576,pub.1121611353,10.1007/978-3-030-32486-5_15,,,Deep Generative Model-Driven Multimodal Prostate Segmentation in Radiotherapy,"Deep learning has shown unprecedented success in a variety of applications, such as computer vision and medical image analysis. However, there is still potential to improve segmentation in multimodal images by embedding prior knowledge via learning-based shape modeling and registration to learn the modality invariant anatomical structure of organs. For example, in radiotherapy automatic prostate segmentation is essential in prostate cancer diagnosis, therapy, and post-therapy assessment from T2-weighted MR or CT images. In this paper, we present a fully automatic deep generative model-driven multimodal prostate segmentation method using convolutional neural network (DGMNet). The novelty of our method comes with its embedded generative neural network for learning-based shape modeling and its ability to adapt for different imaging modalities via learning-based registration. The proposed method includes a multi-task learning framework that combines a convolutional feature extraction and an embedded regression and classification based shape modeling. This enables the network to predict the deformable shape of an organ. We show that generative neural network-based shape modeling trained on a reliable contrast imaging modality (such as MRI) can be directly applied to low contrast imaging modality (such as CT) to achieve accurate prostate segmentation. The method was evaluated on MRI and CT datasets acquired from different clinical centers with large variations in contrast and scanning protocols. Experimental results reveal that our method can be used to automatically and accurately segment the prostate gland in different imaging modalities.",,,Lecture Notes in Computer Science,Artificial Intelligence in Radiation Therapy,,2019-10-10,2019,2019-10-10,2019,11850,,119-127,All OA, Green,Chapter,"Girum, Kibrom Berihu; Créhange, Gilles; Hussain, Raabid; Walker, Paul Michael; Lalande, Alain","Girum, Kibrom Berihu (ImViA, Université de Bourgogne Franche-Comté, Dijon, France); Créhange, Gilles (ImViA, Université de Bourgogne Franche-Comté, Dijon, France; Department of Radiation Oncology, CGFL, Dijon, France); Hussain, Raabid (ImViA, Université de Bourgogne Franche-Comté, Dijon, France); Walker, Paul Michael (ImViA, Université de Bourgogne Franche-Comté, Dijon, France; Depratment of Medical Imaging, University Hospital of Dijon, Dijon, France); Lalande, Alain (ImViA, Université de Bourgogne Franche-Comté, Dijon, France; Depratment of Medical Imaging, University Hospital of Dijon, Dijon, France)","Girum, Kibrom Berihu (Université Bourgogne Franche-Comté)","Girum, Kibrom Berihu (Université Bourgogne Franche-Comté); Créhange, Gilles (Université Bourgogne Franche-Comté; Centre Georges François Leclerc); Hussain, Raabid (Université Bourgogne Franche-Comté); Walker, Paul Michael (Université Bourgogne Franche-Comté; Centre Hospitalier Universitaire Dijon Bourgogne); Lalande, Alain (Université Bourgogne Franche-Comté; Centre Hospitalier Universitaire Dijon Bourgogne)",10,7,,3.83,http://arxiv.org/pdf/1910.10542,https://app.dimensions.ai/details/publication/pub.1121611353,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1560,pub.1119396732,10.48550/arxiv.1906.03347,,,When Unseen Domain Generalization is Unnecessary? Rethinking Data  Augmentation,"Recent advances in deep learning for medical image segmentation demonstrate
expert-level accuracy. However, in clinically realistic environments, such
methods have marginal performance due to differences in image domains,
including different imaging protocols, device vendors and patient populations.
Here we consider the problem of domain generalization, when a model is trained
once, and its performance generalizes to unseen domains. Intuitively, within a
specific medical imaging modality the domain differences are smaller relative
to natural images domain variability. We rethink data augmentation for medical
3D images and propose a deep stacked transformations (DST) approach for domain
generalization. Specifically, a series of n stacked transformations are applied
to each image in each mini-batch during network training to account for the
contribution of domain-specific shifts in medical images. We comprehensively
evaluate our method on three tasks: segmentation of whole prostate from 3D MRI,
left atrial from 3D MRI, and left ventricle from 3D ultrasound. We demonstrate
that when trained on a small source dataset, (i) on average, DST models on
unseen datasets degrade only by 11% (Dice score change), compared to the
conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation
method (degrading 25%); (ii) when evaluation on the same domain, DST is also
better albeit only marginally. (iii) When training on large-sized data, DST on
unseen domains reaches performance of state-of-the-art fully supervised models.
These findings establish a strong benchmark for the study of domain
generalization in medical imaging, and can be generalized to the design of
robust deep segmentation models for clinical deployment.",,,arXiv,,,2019-06-07,2019,,,,,,All OA, Green,Preprint,"Zhang, Ling; Wang, Xiaosong; Yang, Dong; Sanford, Thomas; Harmon, Stephanie; Turkbey, Baris; Roth, Holger; Myronenko, Andriy; Xu, Daguang; Xu, Ziyue","Zhang, Ling (); Wang, Xiaosong (); Yang, Dong (); Sanford, Thomas (); Harmon, Stephanie (); Turkbey, Baris (); Roth, Holger (); Myronenko, Andriy (); Xu, Daguang (); Xu, Ziyue ()",,"Zhang, Ling (); Wang, Xiaosong (); Yang, Dong (); Sanford, Thomas (); Harmon, Stephanie (); Turkbey, Baris (); Roth, Holger (); Myronenko, Andriy (); Xu, Daguang (); Xu, Ziyue ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119396732,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1557,pub.1147389903,10.1109/isbi52829.2022.9761448,,,Long-Range 3D Self-Attention for MRI Prostate Segmentation,"The problem of prostate segmentation from Magnetic Resonance Imaging (MRI) is an intense research area, due to the increased use of MRI in the diagnosis and treatment planning of prostate cancer. The lack of clear boundaries and huge variation of texture and shapes between patients makes the task very challenging, and the 3D nature of the data makes 2D segmentation algorithms suboptimal for the task. With this paper, we propose a novel architecture to fill the gap be-tween the most recent advances in 2D computer vision and 3D semantic segmentation. In particular, the designed model retrieves multi-scale 3D features with dilated convolutions and makes use of a self-attention transformer to gain a global field of view. The proposed Long-Range 3D Self-Attention block allows the convolutional neural network to build significant features by merging together contextual information collected at various scales. Experimental results show that the proposed method improves the state-of-the-art segmentation accuracy on MRI prostate segmentation.",,,,2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),,2022-03-31,2022,,2022-03-31,0,,1-5,Closed,Proceeding,"Pollastri, Federico; Cipriano, Marco; Bolelli, Federico; Grana, Costantino","Pollastri, Federico (Department of Engineering ""Enzo Ferrari"", University of Modena and Reggio Emilia, Modena, Italy); Cipriano, Marco (Department of Engineering ""Enzo Ferrari"", University of Modena and Reggio Emilia, Modena, Italy); Bolelli, Federico (Department of Engineering ""Enzo Ferrari"", University of Modena and Reggio Emilia, Modena, Italy); Grana, Costantino (Department of Engineering ""Enzo Ferrari"", University of Modena and Reggio Emilia, Modena, Italy)","Pollastri, Federico (University of Modena and Reggio Emilia)","Pollastri, Federico (University of Modena and Reggio Emilia); Cipriano, Marco (University of Modena and Reggio Emilia); Bolelli, Federico (University of Modena and Reggio Emilia); Grana, Costantino (University of Modena and Reggio Emilia)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1147389903,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,,
1554,pub.1119188508,10.48550/arxiv.1805.10665,,,Adversarial Deformation Regularization for Training Image Registration  Neural Networks,"We describe an adversarial learning approach to constrain convolutional
neural network training for image registration, replacing heuristic smoothness
measures of displacement fields often used in these tasks. Using
minimally-invasive prostate cancer intervention as an example application, we
demonstrate the feasibility of utilizing biomechanical simulations to
regularize a weakly-supervised anatomical-label-driven registration network for
aligning pre-procedural magnetic resonance (MR) and 3D intra-procedural
transrectal ultrasound (TRUS) images. A discriminator network is optimized to
distinguish the registration-predicted displacement fields from the motion data
simulated by finite element analysis. During training, the registration network
simultaneously aims to maximize similarity between anatomical labels that
drives image alignment and to minimize an adversarial generator loss that
measures divergence between the predicted- and simulated deformation. The
end-to-end trained network enables efficient and fully-automated registration
that only requires an MR and TRUS image pair as input, without anatomical
labels or simulated data during inference. 108 pairs of labelled MR and TRUS
images from 76 prostate cancer patients and 71,500 nonlinear finite-element
simulations from 143 different patients were used for this study. We show that,
with only gland segmentation as training labels, the proposed method can help
predict physically plausible deformation without any other smoothness penalty.
Based on cross-validation experiments using 834 pairs of independent validation
landmarks, the proposed adversarial-regularized registration achieved a target
registration error of 6.3 mm that is significantly lower than those from
several other regularization methods.",,,arXiv,,,2018-05-27,2018,,,,,,All OA, Green,Preprint,"Hu, Yipeng; Gibson, Eli; Ghavami, Nooshin; Bonmati, Ester; Moore, Caroline M.; Emberton, Mark; Vercauteren, Tom; Noble, J. Alison; Barratt, Dean C.","Hu, Yipeng (); Gibson, Eli (); Ghavami, Nooshin (); Bonmati, Ester (); Moore, Caroline M. (); Emberton, Mark (); Vercauteren, Tom (); Noble, J. Alison (); Barratt, Dean C. ()",,"Hu, Yipeng (); Gibson, Eli (); Ghavami, Nooshin (); Bonmati, Ester (); Moore, Caroline M. (); Emberton, Mark (); Vercauteren, Tom (); Noble, J. Alison (); Barratt, Dean C. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119188508,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,
1554,pub.1150894281,10.1109/icecet55527.2022.9872602,,,Segmentation Mask Resampling for MRI Prostate Localization Improvement,"This study aimed to examine the effect of segmentation mask resampling on segmentation performance and to confirm whether attempt to convert anisotropic data to isotropic can improve results of the current state of the art segmentation technique. A total of 50 prostate cancer cases gathered at 4 different institutions were examined. Magnetic resonance images of different resolutions and spacing were input into both 2D and 3D nnU-Net networks. Dice similarity coefficient was used as a metric to compare segmentation results between different models validated with the use of k-fold cross validation. Models trained included 2D and 3D models on original data and 3D models on resampled data. Image resampling was performed by applying third order spline interpolation. Two techniques were used for binary segmentation mask resampling: nearest neighbor interpolation followed by morphological opening and closing and calculation of average shape to be inserted between two adjacent slices. The mean dice similarity coefficients were 0.8736±0.0480 and 0.8680±0.0552 for 2D and 3D models with original data respectively. The use of both resampling techniques resulted in better segmentation results than no resampling with 0.8787±0.0299 and 0.8965±0.0165 for models with segmentation masks resampled with morphological operations and average shape calculation respectively. Thus, the study confirmed that converting anisotropic data to isotropic can improve segmentation results as well as reduce room for error.",,,,"2022 International Conference on Electrical, Computer and Energy Technologies (ICECET)",,2022-07-22,2022,,2022-07-22,0,,1-6,Closed,Proceeding,"Jucevičius, Justinas; Treigys, Povilas; Bernatavičienė, Jolita; Trakymas, Mantas; Naruševičiūtė, Ieva; Briedienė, Rūta","Jucevičius, Justinas (Institute of Data Science and Digital Technologies Vilnius University, Vilnius, Lithuania); Treigys, Povilas (Institute of Data Science and Digital Technologies Vilnius University, Vilnius, Lithuania); Bernatavičienė, Jolita (Institute of Data Science and Digital Technologies Vilnius University, Vilnius, Lithuania); Trakymas, Mantas (Radiology Department, National Cancer Institute of Lithuania, Vilnius, Lithuania); Naruševičiūtė, Ieva (Radiology Department, National Cancer Institute of Lithuania, Vilnius, Lithuania); Briedienė, Rūta (Radiology Department, National Cancer Institute of Lithuania, Vilnius, Lithuania)","Jucevičius, Justinas (Vilnius University)","Jucevičius, Justinas (Vilnius University); Treigys, Povilas (Vilnius University); Bernatavičienė, Jolita (Vilnius University); Trakymas, Mantas (National Cancer Institute); Naruševičiūtė, Ieva (National Cancer Institute); Briedienė, Rūta (National Cancer Institute)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150894281,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,,
1554,pub.1147399437,10.1109/isbi52829.2022.9761453,,,FEW-SHOT Image Segmentation for Cross-Institution Male Pelvic Organs Using Registration-Assisted Prototypical Learning,"The ability to adapt medical image segmentation networks for a novel class such as an unseen anatomical or pathological structure, when only a few labelled examples of this class are available from local healthcare providers, is sought-after. This potentially addresses two widely recognised limitations in deploying modern deep learning models to clinical practice, expertise-and-labour-intensive labelling and cross-institution generalisation. This work presents the first 3D few-shot interclass segmentation network for medical images, using a labelled multi-institution dataset from prostate cancer patients with eight regions of interest. We propose an image alignment module registering the predicted segmentation of both query and support data, in a standard prototypical learning algorithm, to a reference atlas space. The built-in registration mechanism can effectively utilise the prior knowledge of consistent anatomy between subjects, regardless whether they are from the same institution or not. Experimental results demonstrated that the proposed registration-assisted prototypical learning significantly improved segmentation accuracy (p-values<0.01) on query data from a holdout institution, with varying availability of support data from multiple institutions. We also report the additional benefits of the proposed 3D networks with 75% fewer parameters and an arguably simpler implementation, compared with existing 2D few-shot approaches that segment 2D slices of volumetric medical images.",This work is supported by the Wellcome/EPSRC Centre for Interventional &amp, Surgical Sciences [203145Z/16/Z] and the CRUK International Alliance for Cancer Early Detection (ACED) [C28070/A30912, C73666/A31378].,This work is supported by the Wellcome/EPSRC Centre for Interventional & Surgical Sciences [203145Z/16/Z] and the CRUK International Alliance for Cancer Early Detection (ACED) [C28070/A30912, C73666/A31378].,,2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),,2022-03-31,2022,,2022-03-31,0,,1-5,All OA, Green,Proceeding,"Li, Yiwen; Fu, Yunguan; Yang, Qianye; Min, Zhe; Yan, Wen; Huisman, Henkjan; Barratt, Dean; Prisacariu, Victor Adrian; Hu, Yipeng","Li, Yiwen (University of Oxford); Fu, Yunguan (University College London; InstaDeep); Yang, Qianye (University College London); Min, Zhe (University College London); Yan, Wen (University College London; City University of Hong Kong); Huisman, Henkjan (Radboud University Nijmegen Medical Centre); Barratt, Dean (University College London); Prisacariu, Victor Adrian (University of Oxford); Hu, Yipeng (University of Oxford; University College London)","Li, Yiwen (University of Oxford)","Li, Yiwen (University of Oxford); Fu, Yunguan (University College London); Yang, Qianye (University College London); Min, Zhe (University College London); Yan, Wen (University College London; City University of Hong Kong); Huisman, Henkjan (Radboud University Nijmegen Medical Centre); Barratt, Dean (University College London); Prisacariu, Victor Adrian (University of Oxford); Hu, Yipeng (University of Oxford; University College London)",1,1,,,http://arxiv.org/pdf/2201.06358,https://app.dimensions.ai/details/publication/pub.1147399437,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,
1554,pub.1139741648,10.23919/date51398.2021.9474153,,,Morphable Convolutional Neural Network for Biomedical Image Segmentation,"We propose a morphable convolution framework, which can be applied to irregularly shaped region of input feature map. This framework reduces the computational footprint of a regular CNN operation in the context of biomedical semantic image segmentation. The traditional CNN based approach has high accuracy, but suffers from high training and inference computation costs, compared to a conventional edge detection based approach. In this work, we combine the concept of morphable convolution with the edge detection algorithms resulting in a hierarchical framework, which first detects the edges and then generate a layer-wise annotation map. The annotation map guides the convolution operation to be run only on a small, useful fraction of pixels in the feature map. We evaluate our framework on three cell tracking datasets and the experimental results indicate that our framework saves ~30% and ~10% execution time on CPU and GPU, respectively, without loss of accuracy, compared to the baseline conventional CNN approaches.","This work is supported in part by NSF grants 1629915, 1629129, 1763681 and 2008398 as well as a grant from Intel.",,,"2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)",,2021-02-05,2021,,2021-02-05,0,,1522-1525,Closed,Proceeding,"Jiang, Huaipan; Sarma, Anup; Fan, Mengran; Ryoo, Jihyun; Arunachalam, Meenakshi; Naveen, Sharada; Kandemir, Mahmut T.","Jiang, Huaipan (The Pennsylvania State University, University Park, PA, 16802); Sarma, Anup (The Pennsylvania State University, University Park, PA, 16802); Fan, Mengran (The Pennsylvania State University, University Park, PA, 16802); Ryoo, Jihyun (The Pennsylvania State University, University Park, PA, 16802); Arunachalam, Meenakshi (Intel); Naveen, Sharada (Intel); Kandemir, Mahmut T. (The Pennsylvania State University, University Park, PA, 16802)",,"Jiang, Huaipan (Pennsylvania State University); Sarma, Anup (Pennsylvania State University); Fan, Mengran (Pennsylvania State University); Ryoo, Jihyun (Pennsylvania State University); Arunachalam, Meenakshi (Intel (United States)); Naveen, Sharada (Intel (United States)); Kandemir, Mahmut T. (Pennsylvania State University)",1,1,,0.77,,https://app.dimensions.ai/details/publication/pub.1139741648,40 Engineering, 46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
1553,pub.1107026723,10.1007/978-3-030-00937-3_58,,,Inter-site Variability in Prostate Segmentation Accuracy Using Deep Learning,"Deep-learning-based segmentation tools have yielded higher reported segmentation accuracies for many medical imaging applications. However, inter-site variability in image properties can challenge the translation of these tools to data from ‘unseen’ sites not included in the training data. This study quantifies the impact of inter-site variability on the accuracy of deep-learning-based segmentations of the prostate from magnetic resonance (MR) images, and evaluates two strategies for mitigating the reduced accuracy for data from unseen sites: training on multi-site data and training with limited additional data from the unseen site. Using 376 T2-weighted prostate MR images from six sites, we compare the segmentation accuracy (Dice score and boundary distance) of three deep-learning-based networks trained on data from a single site and on various configurations of data from multiple sites. We found that the segmentation accuracy of a single-site network was substantially worse on data from unseen sites than on data from the training site. Training on multi-site data yielded marginally improved accuracy and robustness. However, including as few as 8 subjects from the unseen site, e.g. during commissioning of a new clinical system, yielded substantial improvement (regaining 75% of the difference in Dice score).",This publication presents independent research supported by Cancer Research UK (Multidisciplinary C28070/A19985).,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2018,,2018-09-13,2018,2018-09-13,2018,11073,,506-514,All OA, Green,Chapter,"Gibson, Eli; Hu, Yipeng; Ghavami, Nooshin; Ahmed, Hashim U.; Moore, Caroline; Emberton, Mark; Huisman, Henkjan J.; Barratt, Dean C.","Gibson, Eli (University College London, London, UK); Hu, Yipeng (University College London, London, UK); Ghavami, Nooshin (University College London, London, UK); Ahmed, Hashim U. (Imperial College, London, UK); Moore, Caroline (University College London, London, UK); Emberton, Mark (University College London, London, UK); Huisman, Henkjan J. (Radboud University Medical Center, Nijmegen, The Netherlands); Barratt, Dean C. (University College London, London, UK)","Gibson, Eli (University College London)","Gibson, Eli (University College London); Hu, Yipeng (University College London); Ghavami, Nooshin (University College London); Ahmed, Hashim U. (Imperial College London); Moore, Caroline (University College London); Emberton, Mark (University College London); Huisman, Henkjan J. (Radboud University Nijmegen Medical Centre); Barratt, Dean C. (University College London)",42,25,,,https://discovery.ucl.ac.uk/10057753/1/Gibson_MICCAI2018.pdf,https://app.dimensions.ai/details/publication/pub.1107026723,46 Information and Computing Sciences,,,,,,,,,,,
1553,pub.1107018967,10.1007/978-3-030-00937-3_43,,,ASDNet: Attention Based Semi-supervised Deep Networks for Medical Image Segmentation,"Segmentation is a key step for various medical image analysis tasks. Recently, deep neural networks could provide promising solutions for automatic image segmentation. The network training usually involves a large scale of training data with corresponding ground truth label maps. However, it is very challenging to obtain the ground-truth label maps due to the requirement of expertise knowledge and also intensive labor work. To address such challenges, we propose a novel semi-supervised deep learning framework, called “Attention based Semi-supervised Deep Networks” (ASDNet), to fulfill the segmentation tasks in an end-to-end fashion. Specifically, we propose a fully convolutional confidence network to adversarially train the segmentation network. Based on the confidence map from the confidence network, we then propose a region-attention based semi-supervised learning strategy to include the unlabeled data for training. Besides, sample attention mechanism is also explored to improve the network training. Experimental results on real clinical datasets show that our ASDNet can achieve state-of-the-art segmentation accuracy. Further analysis also indicates that our proposed network components contribute most to the improvement of performance.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2018,,2018-09-13,2018,2018-09-13,2018,11073,,370-378,Closed,Chapter,"Nie, Dong; Gao, Yaozong; Wang, Li; Shen, Dinggang","Nie, Dong (Department of Computer Science, University of North Carolina at Chapel Hill, Chapel Hill, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Gao, Yaozong (Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China); Wang, Li (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA); Shen, Dinggang (Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, USA)","Shen, Dinggang (University of North Carolina at Chapel Hill)","Nie, Dong (University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill); Gao, Yaozong (); Wang, Li (University of North Carolina at Chapel Hill); Shen, Dinggang (University of North Carolina at Chapel Hill)",198,140,,62.49,,https://app.dimensions.ai/details/publication/pub.1107018967,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1543,pub.1144782663,10.48550/arxiv.2201.06358,,,Few-shot image segmentation for cross-institution male pelvic organs  using registration-assisted prototypical learning,"The ability to adapt medical image segmentation networks for a novel class
such as an unseen anatomical or pathological structure, when only a few
labelled examples of this class are available from local healthcare providers,
is sought-after. This potentially addresses two widely recognised limitations
in deploying modern deep learning models to clinical practice,
expertise-and-labour-intensive labelling and cross-institution generalisation.
This work presents the first 3D few-shot interclass segmentation network for
medical images, using a labelled multi-institution dataset from prostate cancer
patients with eight regions of interest. We propose an image alignment module
registering the predicted segmentation of both query and support data, in a
standard prototypical learning algorithm, to a reference atlas space. The
built-in registration mechanism can effectively utilise the prior knowledge of
consistent anatomy between subjects, regardless whether they are from the same
institution or not. Experimental results demonstrated that the proposed
registration-assisted prototypical learning significantly improved segmentation
accuracy (p-values<0.01) on query data from a holdout institution, with varying
availability of support data from multiple institutions. We also report the
additional benefits of the proposed 3D networks with 75% fewer parameters and
an arguably simpler implementation, compared with existing 2D few-shot
approaches that segment 2D slices of volumetric medical images.",,,arXiv,,,2022-01-17,2022,,,,,,All OA, Green,Preprint,"Li, Yiwen; Fu, Yunguan; Yang, Qianye; Min, Zhe; Yan, Wen; Huisman, Henkjan; Barratt, Dean; Prisacariu, Victor Adrian; Hu, Yipeng","Li, Yiwen (); Fu, Yunguan (); Yang, Qianye (); Min, Zhe (); Yan, Wen (); Huisman, Henkjan (); Barratt, Dean (); Prisacariu, Victor Adrian (); Hu, Yipeng ()",,"Li, Yiwen (); Fu, Yunguan (); Yang, Qianye (); Min, Zhe (); Yan, Wen (); Huisman, Henkjan (); Barratt, Dean (); Prisacariu, Victor Adrian (); Hu, Yipeng ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144782663,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
1543,pub.1138982344,10.48550/arxiv.2106.09662,,,Automatic Segmentation of the Prostate on 3D Trans-rectal Ultrasound  Images using Statistical Shape Models and Convolutional Neural Networks,"In this work we propose to segment the prostate on a challenging dataset of
trans-rectal ultrasound (TRUS) images using convolutional neural networks
(CNNs) and statistical shape models (SSMs). TRUS is commonly used for a number
of image-guided interventions on the prostate. Fast and accurate segmentation
on the organ in these images is crucial to planning and fusion with other
modalities such as magnetic resonance images (MRIs) . However, TRUS has limited
soft tissue contrast and signal to noise ratio which makes the task of
segmenting the prostate challenging and subject to inter-observer and
intra-observer variability. This is especially problematic at the base and apex
where the gland boundary is hard to define. In this paper, we aim to tackle
this problem by taking advantage of shape priors learnt on an MR dataset which
has higher soft tissue contrast allowing the prostate to be contoured more
accurately. We use this shape prior in combination with a prostate tissue
probability map computed by a CNN for segmentation.",,,arXiv,,,2021-06-17,2021,,,,,,All OA, Green,Preprint,"Samei, Golnoosh; Karimi, Davood; Kesch, Claudia; Salcudean, Septimiu","Samei, Golnoosh (); Karimi, Davood (); Kesch, Claudia (); Salcudean, Septimiu ()",,"Samei, Golnoosh (); Karimi, Davood (); Kesch, Claudia (); Salcudean, Septimiu ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1138982344,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,
1543,pub.1107026833,10.1007/978-3-030-00937-3_59,,,Deep Learning-Based Boundary Detection for Model-Based Segmentation with Application to MR Prostate Segmentation,"Model-based segmentation (MBS) has been successfully used for the fully automatic segmentation of anatomical structures in medical images with well defined gray values due to its ability to incorporate prior knowledge about the organ shape. However, the robust and accurate detection of boundary points required for the MBS is still a challenge for organs with inhomogeneous appearance such as the prostate and magnetic resonance (MR) images, where the image contrast can vary greatly due to the use of different acquisition protocols and scanners at different clinical sites. In this paper, we propose a novel boundary detection approach and apply it to the segmentation of the whole prostate in MR images. We formulate boundary detection as a regression task, where a convolutional neural network is trained to predict the distances between a surface mesh and the corresponding boundary points. We have evaluated our method on the Prostate MR Image Segmentation 2012 challenge data set with the results showing that the new boundary detection approach can detect boundaries more robustly with respect to contrast and appearance variations and more accurately than previously used features. With an average boundary distance of 1.71 mm and a Dice similarity coefficient of 90.5%, our method was able to segment the prostate more accurately on average than a second human observer and placed first out of 40 entries submitted to the challenge at the writing of this paper.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2018,,2018-09-13,2018,2018-09-13,2018,11073,,515-522,Closed,Chapter,"Brosch, Tom; Peters, Jochen; Groth, Alexandra; Stehle, Thomas; Weese, Jürgen","Brosch, Tom (Philips GmbH Innovative Technologies, Hamburg, Germany); Peters, Jochen (Philips GmbH Innovative Technologies, Hamburg, Germany); Groth, Alexandra (Philips GmbH Innovative Technologies, Hamburg, Germany); Stehle, Thomas (Philips GmbH Innovative Technologies, Hamburg, Germany); Weese, Jürgen (Philips GmbH Innovative Technologies, Hamburg, Germany)","Brosch, Tom (Philips (Germany))","Brosch, Tom (Philips (Germany)); Peters, Jochen (Philips (Germany)); Groth, Alexandra (Philips (Germany)); Stehle, Thomas (Philips (Germany)); Weese, Jürgen (Philips (Germany))",24,10,,,,https://app.dimensions.ai/details/publication/pub.1107026833,46 Information and Computing Sciences,,,,,,,,,,,,
1543,pub.1104292118,10.1109/isbi.2018.8363549,,,Automatic High Resolution Segmentation of the Prostate from Multi-Planar MRI,"Individualized and accurate segmentations of the prostate are essential for diagnosis as well as therapy planning in prostate cancer (PCa). Most of the previously proposed prostate segmentation approaches rely purely on axial MRI scans, which suffer from low out-of-plane resolution. We propose a method that makes use of sagittal and coronal MRI scans to improve the accuracy of segmentation. These scans are typically acquired as standard of care for PCa staging, but are generally ignored by the segmentation algorithms. Our method is based on a multi-stream 3D convolutional neural network for the automatic extraction of isotropic high resolution segmentations from MR images. We evaluated segmentation performance on an isotropic high resolution ground truth (n = 40 subjects). The results show that the use of multi-planar volumes for prostate segmentation leads to improved segmentation results not only for the whole prostate (92.1% Dice similarity coefficient), but also in apex and base regions.","This work has been funded by the EU and the federal state of Saxony-Anhalt, Germany under grant number FuE 74/16 and ZS/2016/04/78123 as part of the initiativ’Sachsen-Anhalt WISSENSCHAFT Schwerpunkte’, and grants from NIH (U24 CA180918 and P41 EB015898). The Titan Xp used for this research was donated by the NVIDIA Corporation. Data used in this research were obtained from The Cancer Imaging Archive (TCIA) sponsored by the SPIE, NCI/NIH, AAPM, and Radboud University. This work has been funded by the EU and the federal state of Saxony-Anhalt, Germany under grant number FuE 74/16 and ZS/2016/04/78123 as part of the initiative ‘Sachsen-Anhalt WISSENSCHAFT Schwerpunkte’, and grants from NIH (U24 CA180918 and P41 EB015898). The Titan Xp used for this research was donated by the NVIDIA Corporation. Data used in this research were obtained from The Cancer Imaging Archive (TCIA) sponsored by the SPIE, NCI/NIH, AAPM, and Radboud University.",,,2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018),,2018-04-01,2018,,2018-04-01,,,177-181,Closed,Proceeding,"Meyer, Anneke; Mehrtash, Alireza; Rak, Marko; Schindele, Daniel; Schostak, Martin; Tempany, Clare; Kapur, Tina; Abolmaesumi, Purang; Fedorov, Andriy; Hansen, Christian","Meyer, Anneke (Computer-Assisted Surgery Group, Faculty of Computer Science, University of Magdeburg, Germany); Mehrtash, Alireza (Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, USA; Robotics and Control Laboratory, University of British Columbia, Vancouver, Canada); Rak, Marko (Computer-Assisted Surgery Group, Faculty of Computer Science, University of Magdeburg, Germany); Schindele, Daniel (Clinic of Urology and Pediatric Urology, University Hospital Magdeburg, Germany); Schostak, Martin (Clinic of Urology and Pediatric Urology, University Hospital Magdeburg, Germany); Tempany, Clare (Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, USA); Kapur, Tina (Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, USA); Abolmaesumi, Purang (Robotics and Control Laboratory, University of British Columbia, Vancouver, Canada); Fedorov, Andriy (Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, USA); Hansen, Christian (Computer-Assisted Surgery Group, Faculty of Computer Science, University of Magdeburg, Germany; Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, USA)",,"Meyer, Anneke (Otto-von-Guericke University Magdeburg); Mehrtash, Alireza (Harvard University; Brigham and Women's Hospital; University of British Columbia); Rak, Marko (Otto-von-Guericke University Magdeburg); Schindele, Daniel (University Hospital Magdeburg); Schostak, Martin (University Hospital Magdeburg); Tempany, Clare (Harvard University; Brigham and Women's Hospital); Kapur, Tina (Harvard University; Brigham and Women's Hospital); Abolmaesumi, Purang (University of British Columbia); Fedorov, Andriy (Harvard University; Brigham and Women's Hospital); Hansen, Christian (Otto-von-Guericke University Magdeburg; Harvard University; Brigham and Women's Hospital)",24,10,,8.8,,https://app.dimensions.ai/details/publication/pub.1104292118,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
1543,pub.1024627994,10.1016/j.infrared.2014.07.002,,,Blurred trace infrared image segmentation based on template approach and immune factor,"This paper proposes an image segmentation method for blurred trace infrared images, exploring statistical properties of blurred infrared data to segment target regions. I consider the function of immune factors for blurred infrared image segmentation, combined to the template algorithm framework. First, all of the pixel antigens are divided into three classes by innate recognition factors: target antigens, background antigens and blurred antigens. Next, the innate presentation factors present template characteristics as new features for each antigen. Finally, target and background antigens are used for generating mature adaptive immune factors, and these mature factors will recognise each blurred antigen into two classes: a target pixel or a background pixel. Experimental results indicate that the superstring galaxy template algorithm can improve the target segmentation rate and reduce the segmentation error rate.",This research is supported by the National Natural Science Foundation of China (No. 61272358) and the international technological project between China and the Czech Republic (No. 39-10).,,Infrared Physics & Technology,,,2014-11,2014,,2014-11,67,,116-120,Closed,Article,"Yu, Xiao","Yu, Xiao (School of Electrical Engineering, Tianjin University of Technology, Tianjin 300384, China)",,"Yu, Xiao (Tianjin University of Technology)",15,6,,4.17,,https://app.dimensions.ai/details/publication/pub.1024627994,"51 Physical Sciences; 5102 Atomic, Molecular and Optical Physics; 5104 Condensed Matter Physics",,,,,,,,,,,,
1542,pub.1008680045,10.1007/978-3-319-24574-4_13,,,Statistical Power in Image Segmentation: Relating Sample Size to Reference Standard Quality,"Ideal reference standards for comparing segmentation algorithms balance trade-offs between the data set size, the costs of reference standard creation and the resulting accuracy. As reference standard quality impacts the likelihood of detecting significant improvements (i.e. the statistical power), we derived a sample size formula for segmentation accuracy comparison using an imperfect reference standard. We expressed this formula as a function of algorithm performance and reference standard quality (e.g. measured with a high quality reference standard on pilot data) to reveal the relationship between reference standard quality and statistical power, addressing key study design questions: (1) How many validation images are needed to compare segmentation algorithms? (2) How accurate should the reference standard be? The resulting formula predicted statistical power to within 2% of Monte Carlo simulations across a range of model parameters. A case study, using the PROMISE12 prostate segmentation data set, shows the practical use of the formula.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015,,2015-11-18,2015,2015-11-18,2015,9351,,105-113,Closed,Chapter,"Gibson, Eli; Huisman, Henkjan J.; Barratt, Dean C.","Gibson, Eli (Radboud University Medical Center, Nijmegen, The Netherlands; University College London, London, UK); Huisman, Henkjan J. (Radboud University Medical Center, Nijmegen, The Netherlands); Barratt, Dean C. (University College London, London, UK)","Gibson, Eli (Radboud University Nijmegen Medical Centre; University College London)","Gibson, Eli (Radboud University Nijmegen Medical Centre; University College London); Huisman, Henkjan J. (Radboud University Nijmegen Medical Centre); Barratt, Dean C. (University College London)",2,1,,0.92,,https://app.dimensions.ai/details/publication/pub.1008680045,46 Information and Computing Sciences, 4601 Applied Computing,,,,,,,,,,,
1527,pub.1119174316,10.48550/arxiv.1807.07464,,,Conditional Random Fields as Recurrent Neural Networks for 3D Medical  Imaging Segmentation,"The Conditional Random Field as a Recurrent Neural Network layer is a
recently proposed algorithm meant to be placed on top of an existing
Fully-Convolutional Neural Network to improve the quality of semantic
segmentation. In this paper, we test whether this algorithm, which was shown to
improve semantic segmentation for 2D RGB images, is able to improve
segmentation quality for 3D multi-modal medical images. We developed an
implementation of the algorithm which works for any number of spatial
dimensions, input/output image channels, and reference image channels. As far
as we know this is the first publicly available implementation of this sort. We
tested the algorithm with two distinct 3D medical imaging datasets, we
concluded that the performance differences observed were not statistically
significant. Finally, in the discussion section of the paper, we go into the
reasons as to why this technique transfers poorly from natural images to
medical images.",,,arXiv,,,2018-07-19,2018,,,,,,All OA, Green,Preprint,"Monteiro, Miguel; Figueiredo, Mário A. T.; Oliveira, Arlindo L.","Monteiro, Miguel (); Figueiredo, Mário A. T. (); Oliveira, Arlindo L. ()",,"Monteiro, Miguel (); Figueiredo, Mário A. T. (); Oliveira, Arlindo L. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119174316,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
1527,pub.1119119752,10.48550/arxiv.1806.07146,,,Automatic segmentation of prostate zones,"Convolutional networks have become state-of-the-art techniques for automatic
medical image analysis, with the U-net architecture being the most popular at
this moment. In this article we report the application of a 3D version of U-net
to the automatic segmentation of prostate peripheral and transition zones in 3D
MRI images. Our results are slightly better than recent studies that used 2D
U-net and handcrafted feature approaches.
  In addition, we test ideas for improving the 3D U-net setup, by 1) letting
the network segment surrounding tissues, making use of the fixed anatomy, and
2) adjusting the network architecture to reflect the anisotropy in the
dimensions of the MRI image volumes. While the latter adjustment gave a
marginal improvement, the former adjustment showed a significant deterioration
of the network performance. We were able to explain this deterioration by
inspecting feature map activations in all layers of the network. We show that
to segment more tissues the network replaces feature maps that were dedicated
to detecting prostate peripheral zones, by feature maps detecting the
surrounding tissues.",,,arXiv,,,2018-06-19,2018,,,,,,All OA, Green,Preprint,"Mooij, Germonda; Bagulho, Ines; Huisman, Henkjan","Mooij, Germonda (); Bagulho, Ines (); Huisman, Henkjan ()",,"Mooij, Germonda (); Bagulho, Ines (); Huisman, Henkjan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119119752,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
1527,pub.1118590908,10.48550/arxiv.1606.04797,,,V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image  Segmentation,"Convolutional Neural Networks (CNNs) have been recently employed to solve
problems from both the computer vision and medical image analysis fields.
Despite their popularity, most approaches are only able to process 2D images
while most medical data used in clinical practice consists of 3D volumes. In
this work we propose an approach to 3D image segmentation based on a
volumetric, fully convolutional, neural network. Our CNN is trained end-to-end
on MRI volumes depicting prostate, and learns to predict segmentation for the
whole volume at once. We introduce a novel objective function, that we optimise
during training, based on Dice coefficient. In this way we can deal with
situations where there is a strong imbalance between the number of foreground
and background voxels. To cope with the limited number of annotated volumes
available for training, we augment the data applying random non-linear
transformations and histogram matching. We show in our experimental evaluation
that our approach achieves good performances on challenging test data while
requiring only a fraction of the processing time needed by other previous
methods.",,,arXiv,,,2016-06-15,2016,,,,,,All OA, Green,Preprint,"Milletari, Fausto; Navab, Nassir; Ahmadi, Seyed-Ahmad","Milletari, Fausto (); Navab, Nassir (); Ahmadi, Seyed-Ahmad ()",,"Milletari, Fausto (); Navab, Nassir (); Ahmadi, Seyed-Ahmad ()",5,5,,1.35,,https://app.dimensions.ai/details/publication/pub.1118590908,"32 Biomedical and Clinical Sciences; 3202 Clinical Sciences; 46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4611 Machine Learning",,,,,,,,,,,
1527,pub.1105108866,10.1109/codit.2018.8394836,,,Automatic Prostate Segmentation on MR Images with Deeply Supervised Network,"Accurate and efficient segmentation of prostate image plays an important role in the diagnosis of prostate cancer. Since convolutional neural network demonstrates superior performance in computer vision applications, we present a multi-layer deeply supervised deconvolution network (DSDN) which completes end-to-end training to automatically segment magnetic resonance (MR) images. We put additional deeply supervised layers to supervise the performance of hidden layers. During training, the backpropagation process of gradient information in the additional deeply supervised layers accelerates the parameters update for hidden layers, which makes the trained model has strong capacity of features learning as well as passes the extracted features from shallow layers to higher layers effectively. A set of experiments using prostate magnetic resonance (MR) images is carried out to demonstrate that significant segmentation accuracy improvement has been achieved by our proposed method compared to other reported approaches.",,,"2018 5th International Conference on Control, Decision and Information Technologies (CoDIT)","2018 5th International Conference on Control, Decision and Information Technologies (CoDIT)",,2018-06-22,2018,2018-06-22,,,,309-314,Closed,Proceeding,"Ji, Dong; Yu, Jun; Kurihara, Toru; Xu, Liangfeng; Zhan, Shu","Ji, Dong (Sch. of Comput. &amp; Inf., Hefei Univ. of Technol., Hefei, China); Yu, Jun (Kochi Univ. of Technol., Kami, Japan); Kurihara, Toru (Kochi Univ. of Technol., Kami, Japan); Xu, Liangfeng (Sch. of Comput. &amp; Inf., Hefei Univ. of Technol., Hefei, China); Zhan, Shu (Sch. of Comput. &amp; Inf., Hefei Univ. of Technol., Hefei, China)",,"Ji, Dong (Hefei University of Technology); Yu, Jun (Kochi University of Technology); Kurihara, Toru (Kochi University of Technology); Xu, Liangfeng (Hefei University of Technology); Zhan, Shu (Hefei University of Technology)",3,2,,0.99,,https://app.dimensions.ai/details/publication/pub.1105108866,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,
1525,pub.1151033029,10.1007/978-3-031-16443-9_55,,,Atlas-Based Semantic Segmentation of Prostate Zones,"Abstract
Segmentation of the prostate into specific anatomical zones is important for radiological assessment of prostate cancer in magnetic resonance imaging (MRI). Of particular interest is segmenting the prostate into two regions of interest: the central gland (CG) and peripheral zone (PZ). In this paper, we propose to integrate an anatomical atlas of prostate zone shape into a deep learning semantic segmentation framework to segment the CG and PZ in T2-weighted MRI. Our approach incorporates anatomical information in the form of a probabilistic prostate zone atlas and utilizes a dynamically controlled hyperparameter to combine the atlas with the semantic segmentation result. In addition to providing significantly improved segmentation performance, this hyperparameter is capable of being dynamically adjusted during the inference stage to provide users with a mechanism to refine the segmentation. We validate our approach using an external test dataset and demonstrate Dice similarity coefficient values (mean±SD) of 0.91 ± 0.05 for the CG and 0.77 ± 0.16 for the PZ that significantly improves upon the baseline segmentation results without the atlas. All code is publicly available on GitHub: https://github.com/OnofreyLab/prostate_atlas_segm_miccai2022.",Research reported in this publication was supported by the National Cancer Institute (NCI) of the National Institutes of Health (NIH) under Award Number R42 CA224888. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH.,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13435,,570-579,Closed,Chapter,"Zhang, Jiazhen; Venkataraman, Rajesh; Staib, Lawrence H.; Onofrey, John A.","Zhang, Jiazhen (Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA); Venkataraman, Rajesh (Eigen Health, Grass Valley, CA, USA); Staib, Lawrence H. (Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Electrical Engineering, Yale University, New Haven, CT, USA; Department of Biomedical Engineering, Yale University, New Haven, CT, USA); Onofrey, John A. (Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Urology, Yale University, New Haven, CT, USA; Department of Biomedical Engineering, Yale University, New Haven, CT, USA)","Onofrey, John A. (Yale University; Yale University; Yale University)","Zhang, Jiazhen (Yale University); Venkataraman, Rajesh (); Staib, Lawrence H. (Yale University; Yale University; Yale University); Onofrey, John A. (Yale University; Yale University; Yale University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151033029,46 Information and Computing Sciences,,,,,,,,,,,,
1525,pub.1126838048,10.1109/spin48934.2020.9071339,,,Deep Dilated V-Net for 3D Volume Segmentation of Pancreas in CT images,"Pancreas segmentation is essential in the medical diagnosis of cancer, pancreatitis, and pancreatic surgeries. Computed tomography (CT) abdominal scans help in the detection of tumors, infections, and other injuries in the pancreas. The treatment of pancreatic tumors begins with surgery, followed by neoadjuvant therapy. However, it is challenging to detect boundaries, due to variable shape, small anatomical structures, and low contrast images of the pancreas in the abdominal CT scans. Recently, Convolutional Neural Networks (CNNs) based deep learning models show significant performance on medical imaging related tasks. However, most of the data available for the evaluation of diseases consists of 3D CT scan volumes. Hence, learning from volumetric data is essential in biomedical applications. V-Net achieves extraordinary performance in various medical datasets consists of 3D scan. In this paper, we have proposed a multi-rate Deep-Dilation Network (DDN) in V-Net for the segmentation of pancreas in CT-82 abdominal dataset. To overcome data-imbalance between bright and dark pixels, we propose a Weighted Fusion Loss (WFL) using Balanced Binary Cross-entropy (BBCE) loss and Smooth Dice Coefficient (SDC) loss. The proposed model attains a state-of-the-art performance for pancreas segmentation. The achieved dice score, sensitivity and precision are 83.31%, 87.70% and 97.07% respectively.",,,,2020 7th International Conference on Signal Processing and Integrated Networks (SPIN),,2020-02-28,2020,,2020-02-28,0,,591-596,Closed,Proceeding,"Giddwani, Bharat; Tekchandani, Hitesh; Verma, Shrish","Giddwani, Bharat (Electronics & Communication, National Institute of Technology, Raipur, Chhattisgarh, India); Tekchandani, Hitesh (Electronics & Communication, National Institute of Technology, Raipur, Chhattisgarh, India); Verma, Shrish (Electronics & Communication, National Institute of Technology, Raipur, Chhattisgarh, India)","Giddwani, Bharat (National Institute of Technology Raipur)","Giddwani, Bharat (National Institute of Technology Raipur); Tekchandani, Hitesh (National Institute of Technology Raipur); Verma, Shrish (National Institute of Technology Raipur)",11,8,,6.83,,https://app.dimensions.ai/details/publication/pub.1126838048,32 Biomedical and Clinical Sciences, 46 Information and Computing Sciences, 4611 Machine Learning, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
1525,pub.1111912453,10.1109/cisp-bmei.2018.8633233,,,Automated Segmentation Based on Residual U-Net Model for MR Prostate Images,"Computer-assisted prostate clinical diagnosis is significant for early detection and early treatment of prostate cancer. However, due to the small effective area of the prostate magnetic resonance (MR) images and similar gray values of other tissues around it, it is arduous to meet the clinical requirements by relying on the professional doctor to manually sketch the boundary, the challenges of prostate segmentation from MR images is arduous. In consideration of the superiority of convolution network in image processing field, we propose a U-Net model combined with residual connections to get more precise segmentation result on prostate MR images. First, we perform curvature-driven denoising on each prostate MR image and use histogram equalization to obtain potential boundary regions. Then, we enhance the MR image and perform network training. The results of experiments on the PROMISE12 dataset and the cooperative hospital datasets indicate that the proposed network model capable of more effective training, we got the 0.872 ± 0.053 in dice similarity coefficient (DSC), 1.45mm in average boundary distance (ABD) and 10.285mm in Harsdorf distance (HD), which illustrates the proposed method's effectiveness.",The authors greatly appreciate the financial supported by Natural Science Foundation of China No.81500078 and Shanghai Science and Technology Committee No.17411952300. This work is supported by Natural Science Foundation of China No.815000 and Shanghai Science and Technology Committee No.17411952300,,,"2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",,2018-10-13,2018,,2018-10-13,0,,1-6,Closed,Proceeding,"Xiangxiang, Qin; Yu, Zhu; Bingbing, Zheng","Xiangxiang, Qin (College of Information Science and Engineering, East China University of Science and Technology Shanghai, China); Yu, Zhu (College of Information Science and Engineering, East China University of Science and Technology Shanghai, China); Bingbing, Zheng (College of Information Science and Engineering, East China University of Science and Technology Shanghai, China)",,"Xiangxiang, Qin (Northeastern University); Yu, Zhu (Northeastern University); Bingbing, Zheng (Northeastern University)",2,1,,0.58,,https://app.dimensions.ai/details/publication/pub.1111912453,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
1522,pub.1119419852,10.48550/arxiv.1906.04306,,,Semantic-guided Encoder Feature Learning for Blurry Boundary Delineation,"Encoder-decoder architectures are widely adopted for medical image
segmentation tasks. With the lateral skip connection, the models can obtain and
fuse both semantic and resolution information in deep layers to achieve more
accurate segmentation performance. However, in many applications (e.g., blurry
boundary images), these models often cannot precisely locate complex boundaries
and segment tiny isolated parts. To solve this challenging problem, we firstly
analyze why simple skip connections are not enough to help accurately locate
indistinct boundaries and argue that it is due to the fuzzy information in the
skip connection provided in the encoder layers. Then we propose a
semantic-guided encoder feature learning strategy to learn both high resolution
and rich semantic encoder features so that we can more accurately locate the
blurry boundaries, which can also enhance the network by selectively learning
discriminative features. Besides, we further propose a soft contour constraint
mechanism to model the blurry boundary detection. Experimental results on real
clinical datasets show that our proposed method can achieve state-of-the-art
segmentation accuracy, especially for the blurry regions. Further analysis also
indicates that our proposed network components indeed contribute to the
improvement of performance. Experiments on additional datasets validate the
generalization ability of our proposed method.",,,arXiv,,,2019-06-10,2019,,,,,,All OA, Green,Preprint,"Nie, Dong; Shen, Dinggang","Nie, Dong (); Shen, Dinggang ()",,"Nie, Dong (); Shen, Dinggang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119419852,40 Engineering, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,
1522,pub.1146975998,10.48550/arxiv.2204.02450,,,Federated Cross Learning for Medical Image Segmentation,"Federated learning (FL) can collaboratively train deep learning models using
isolated patient data owned by different hospitals for various clinical
applications, including medical image segmentation. However, a major problem of
FL is its performance degradation when dealing with the data that are not
independently and identically distributed (non-iid), which is often the case in
medical images. In this paper, we first conduct a theoretical analysis on the
FL algorithm to reveal the problem of model aggregation during training on
non-iid data. With the insights gained through the analysis, we propose a
simple and yet effective method, federated cross learning (FedCross), to tackle
this challenging problem. Unlike the conventional FL methods that combine
multiple individually trained local models on a server node, our FedCross
sequentially trains the global model across different clients in a round-robin
manner, and thus the entire training procedure does not involve any model
aggregation steps. To further improve its performance to be comparable with the
centralized learning method, we combine the FedCross with an ensemble learning
mechanism to compose a federated cross ensemble learning (FedCrossEns) method.
Finally, we conduct extensive experiments using a set of public datasets. The
experimental results show that the proposed FedCross training strategy
outperforms the mainstream FL methods on non-iid data. In addition to improving
the segmentation performance, our FedCrossEns can further provide a
quantitative estimation of the model uncertainty, demonstrating the
effectiveness and clinical significance of our designs. Source code will be
made publicly available after paper publication.",,,arXiv,,,2022-04-05,2022,,,,,,All OA, Green,Preprint,"Xu, Xuanang; Chen, Tianyi; Deng, Han; Kuang, Tianshu; Barber, Joshua C.; Kim, Daeseung; Gateno, Jaime; Yan, Pingkun; Xia, James J.","Xu, Xuanang (); Chen, Tianyi (); Deng, Han (); Kuang, Tianshu (); Barber, Joshua C. (); Kim, Daeseung (); Gateno, Jaime (); Yan, Pingkun (); Xia, James J. ()",,"Xu, Xuanang (); Chen, Tianyi (); Deng, Han (); Kuang, Tianshu (); Barber, Joshua C. (); Kim, Daeseung (); Gateno, Jaime (); Yan, Pingkun (); Xia, James J. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1146975998,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
1522,pub.1141618957,10.48550/arxiv.2110.00934,,,Bounding Box Tightness Prior for Weakly Supervised Image Segmentation,"This paper presents a weakly supervised image segmentation method that adopts
tight bounding box annotations. It proposes generalized multiple instance
learning (MIL) and smooth maximum approximation to integrate the bounding box
tightness prior into the deep neural network in an end-to-end manner. In
generalized MIL, positive bags are defined by parallel crossing lines with a
set of different angles, and negative bags are defined as individual pixels
outside of any bounding boxes. Two variants of smooth maximum approximation,
i.e., $\alpha$-softmax function and $\alpha$-quasimax function, are exploited
to conquer the numeral instability introduced by maximum function of bag
prediction. The proposed approach was evaluated on two pubic medical datasets
using Dice coefficient. The results demonstrate that it outperforms the
state-of-the-art methods. The codes are available at
\url{https://github.com/wangjuan313/wsis-boundingbox}.",,,arXiv,,,2021-10-03,2021,,,,,,All OA, Green,Preprint,"Wang, Juan; Xia, Bin","Wang, Juan (); Xia, Bin ()",,"Wang, Juan (); Xia, Bin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141618957,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1522,pub.1131364147,10.1007/978-3-030-59710-8_38,,,MS-NAS: Multi-scale Neural Architecture Search for Medical Image Segmentation,"The recent breakthroughs of Neural Architecture Search (NAS) have motivated various applications in medical image segmentation. However, most existing work either simply rely on hyper-parameter tuning or stick to a fixed network backbone, thereby limiting the underlying search space to identify more efficient architecture. This paper presents a Multi-Scale NAS (MS-NAS) framework that is featured with multi-scale search space from network backbone to cell operation, and multi-scale fusion capability to fuse features with different sizes. To mitigate the computational overhead due to the larger search space, a partial channel connection scheme and a two-step decoding method are utilized to reduce computational overhead while maintaining optimization quality. Experimental results show that on various datasets for segmentation, MS-NAS outperforms the state-of-the-art methods and achieves 0.6–5.4% mIOU and 0.4–3.5% DSC improvements, while the computational resource consumption is reduced by 18.0–24.9%.",This work was supported by National Key Research and Development Program Program of China [No.2018YFE0126300] and Key Area Research and Development Program of Guangdong Province [No.2018B030338001].,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12261,,388-397,All OA, Green,Chapter,"Yan, Xingang; Jiang, Weiwen; Shi, Yiyu; Zhuo, Cheng","Yan, Xingang (ZheJiang University, Hangzhou, China); Jiang, Weiwen (University of Notre Dame, Notre Dame, USA); Shi, Yiyu (University of Notre Dame, Notre Dame, USA); Zhuo, Cheng (ZheJiang University, Hangzhou, China)","Zhuo, Cheng (Zhejiang University)","Yan, Xingang (Zhejiang University); Jiang, Weiwen (University of Notre Dame); Shi, Yiyu (University of Notre Dame); Zhuo, Cheng (Zhejiang University)",31,30,,15.98,http://arxiv.org/pdf/2007.06151,https://app.dimensions.ai/details/publication/pub.1131364147,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1522,pub.1107031515,10.1007/978-3-030-00928-1_87,,,Adversarial Deformation Regularization for Training Image Registration Neural Networks,"Abstract
We describe an adversarial learning approach to constrain convolutional neural network training for image registration, replacing heuristic smoothness measures of displacement fields often used in these tasks. Using minimally-invasive prostate cancer intervention as an example application, we demonstrate the feasibility of utilizing biomechanical simulations to regularize a weakly-supervised anatomical-label-driven registration network for aligning pre-procedural magnetic resonance (MR) and 3D intra-procedural transrectal ultrasound (TRUS) images. A discriminator network is optimized to distinguish the registration-predicted displacement fields from the motion data simulated by finite element analysis. During training, the registration network simultaneously aims to maximize similarity between anatomical labels that drives image alignment and to minimize an adversarial generator loss that measures divergence between the predicted- and simulated deformation. The end-to-end trained network enables efficient and fully-automated registration that only requires an MR and TRUS image pair as input, without anatomical labels or simulated data during inference. 108 pairs of labelled MR and TRUS images from 76 prostate cancer patients and 71,500 nonlinear finite-element simulations from 143 different patients were used for this study. We show that, with only gland segmentation as training labels, the proposed method can help predict physically plausible deformation without any other smoothness penalty. Based on cross-validation experiments using 834 pairs of independent validation landmarks, the proposed adversarial-regularized registration achieved a target registration error of 6.3 mm that is significantly lower than those from several other regularization methods.","This work received support from CRUK, the EPSRC and the Wellcome Trust (C28070/A19985, EP/M020533/1, NS/A000050/1, EP/N026993/1).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2018,,2018-09-26,2018,2018-09-26,2018,11070,,774-782,All OA, Green,Chapter,"Hu, Yipeng; Gibson, Eli; Ghavami, Nooshin; Bonmati, Ester; Moore, Caroline M.; Emberton, Mark; Vercauteren, Tom; Noble, J. Alison; Barratt, Dean C.","Hu, Yipeng (Centre for Medical Image Computing, University College London, London, UK; Institute of Biomedical Engineering, University of Oxford, Oxford, UK); Gibson, Eli (Centre for Medical Image Computing, University College London, London, UK); Ghavami, Nooshin (Centre for Medical Image Computing, University College London, London, UK); Bonmati, Ester (Centre for Medical Image Computing, University College London, London, UK); Moore, Caroline M. (Division of Surgery and Interventional Science, University College London, London, UK); Emberton, Mark (Division of Surgery and Interventional Science, University College London, London, UK); Vercauteren, Tom (Centre for Medical Image Computing, University College London, London, UK); Noble, J. Alison (Institute of Biomedical Engineering, University of Oxford, Oxford, UK); Barratt, Dean C. (Centre for Medical Image Computing, University College London, London, UK)","Hu, Yipeng (University College London; University of Oxford)","Hu, Yipeng (University College London; University of Oxford); Gibson, Eli (University College London); Ghavami, Nooshin (University College London); Bonmati, Ester (University College London); Moore, Caroline M. (University College London); Emberton, Mark (University College London); Vercauteren, Tom (University College London); Noble, J. Alison (University of Oxford); Barratt, Dean C. (University College London)",52,25,,16.41,http://arxiv.org/pdf/1805.10665,https://app.dimensions.ai/details/publication/pub.1107031515,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1522,pub.1092750471,10.1117/12.2285942,,,Volumetric multimodality neural network for brain tumor segmentation,"Brain lesion segmentation is one of the hardest tasks to be solved in computer vision with an emphasis on the medical field. We present a convolutional neural network that produces a semantic segmentation of brain tumors, capable of processing volumetric data along with information from multiple MRI modalities at the same time. This results in the ability to learn from small training datasets and highly imbalanced data. Our method is based on DeepMedic, the state of the art in brain lesion segmentation. We develop a new architecture with more convolutional layers, organized in three parallel pathways with different input resolution, and additional fully connected layers. We tested our method over the 2015 BraTS Challenge dataset, reaching an average dice coefficient of 84%, while the standard DeepMedic implementation reached 74%.",,,,13th International Conference on Medical Information Processing and Analysis,,2017-11-17,2017,,,10572,,105720e,Closed,Proceeding,"Castillo, Laura Silvana; Daza, Laura Alexandra; Rivera, Luis Carlos; Arbeláez, Pablo","Castillo, Laura Silvana (Univ. de los Andes (Colombia)); Daza, Laura Alexandra (Univ. de los Andes (Colombia)); Rivera, Luis Carlos (Univ. de los Andes (Colombia)); Arbeláez, Pablo (Univ. de los Andes (Colombia))",,"Castillo, Laura Silvana (Universidad de Los Andes); Daza, Laura Alexandra (Universidad de Los Andes); Rivera, Luis Carlos (Universidad de Los Andes); Arbeláez, Pablo (Universidad de Los Andes)",11,5,,2.96,,https://app.dimensions.ai/details/publication/pub.1092750471,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1521,pub.1130742891,10.23919/ccc50068.2020.9188748,,,Prostate Localization in 2D Sequence MR with Fusion of Center Position Prior and Sequence Correlation,"A prostate organ localization algorithm based on the state-of-art object detection framework Faster R-CNN is proposed in this paper for Magnetic Resonance (MR) slice sequence. Using the prior information of the central position of the organ in the image, ResNet-50 with spatial attention mechanism is introduced as the network's feature extraction module to enhance the sensitivity of the network to the spatial location features. In addition, inspired by the correlation between neighboring slice images in the position and morphological size of the organ, spatial curve fitting of key points of the object bounding boxes based on the sequence direction is applied to further improve the detection performance of the algorithm. Compared with the original Faster R-CNN framework, the algorithm we proposed has achieved better performance of prostate localization on the PROMISE12 dataset, which is mainly reflected in the area recall rate and localization success rate of the organs increased by 7.1% and 6.5%, respectively. It establishes a good foundation for subsequent medical image processing tasks such as organ segmentation and lesion detection.",This study is supported by the National Natural Science Foundation of China under Grant 61972282.,,,2020 39th Chinese Control Conference (CCC),,2020-07-29,2020,,2020-07-29,0,,6464-6469,Closed,Proceeding,"Lu, Zhiying; Zhao, Mingyue; Xiao, Yang; Pang, Yong","Lu, Zhiying (School of Electrical Engineering and Information Engineering, Tianjin University, Tianjin, 300072); Zhao, Mingyue (School of Electrical Engineering and Information Engineering, Tianjin University, Tianjin, 300072); Xiao, Yang (School of Electrical Engineering and Information Engineering, Tianjin University, Tianjin, 300072); Pang, Yong (Institute of Microelectronics of the Chinese Academy of Sciences, Beijing, 100029)","Lu, Zhiying (Tianjin University)","Lu, Zhiying (Tianjin University); Zhao, Mingyue (Tianjin University); Xiao, Yang (Tianjin University); Pang, Yong (Institute of Microelectronics)",1,1,,0.45,,https://app.dimensions.ai/details/publication/pub.1130742891,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,,,,,,,,,,
1521,pub.1129329011,10.48550/arxiv.2007.06151,,,MS-NAS: Multi-Scale Neural Architecture Search for Medical Image  Segmentation,"The recent breakthroughs of Neural Architecture Search (NAS) have motivated
various applications in medical image segmentation. However, most existing work
either simply rely on hyper-parameter tuning or stick to a fixed network
backbone, thereby limiting the underlying search space to identify more
efficient architecture. This paper presents a Multi-Scale NAS (MS-NAS)
framework that is featured with multi-scale search space from network backbone
to cell operation, and multi-scale fusion capability to fuse features with
different sizes. To mitigate the computational overhead due to the larger
search space, a partial channel connection scheme and a two-step decoding
method are utilized to reduce computational overhead while maintaining
optimization quality. Experimental results show that on various datasets for
segmentation, MS-NAS outperforms the state-of-the-art methods and achieves
0.6-5.4% mIOU and 0.4-3.5% DSC improvements, while the computational resource
consumption is reduced by 18.0-24.9%.",,,arXiv,,,2020-07-12,2020,,,,,,All OA, Green,Preprint,"Yan, Xingang; Jiang, Weiwen; Shi, Yiyu; Zhuo, Cheng","Yan, Xingang (); Jiang, Weiwen (); Shi, Yiyu (); Zhuo, Cheng ()",,"Yan, Xingang (); Jiang, Weiwen (); Shi, Yiyu (); Zhuo, Cheng ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129329011,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1520,pub.1151124915,10.1007/978-3-031-16852-9_3,,,Supervised Domain Adaptation Using Gradients Transfer for Improved Medical Image Analysis,"A well known problem in medical imaging is the performance degradation that occurs when using a model learned on source data, in a new site. Supervised Domain Adaptation (SDA) strategies that focus on this challenge, assume the availability of a limited number of annotated samples from the new site. A typical SDA approach is to pre-train the model on the source site and then fine-tune on the target site. Current research has thus mainly focused on which layers should be fine-tuned. Our approach is based on transferring also the gradients history of the pre-training phase to the fine-tuning phase. We present two schemes to transfer the gradients information to improve the generalization achieved during pre-training while fine-tuning the model. We show that our methods outperform the state-of-the-art with different levels of data scarcity from the target site, on multiple datasets and tasks.",,,Lecture Notes in Computer Science,Domain Adaptation and Representation Transfer,,2022-09-15,2022,2022-09-15,2022,13542,,23-32,Closed,Chapter,"Goodman, Shaya; Greenspan, Hayit; Goldberger, Jacob","Goodman, Shaya (Tel-Aviv University, Tel-Aviv, Israel); Greenspan, Hayit (Tel-Aviv University, Tel-Aviv, Israel); Goldberger, Jacob (Bar-Ilan University, Ramat-Gan, Israel)","Goldberger, Jacob (Bar-Ilan University)","Goodman, Shaya (Tel Aviv University); Greenspan, Hayit (Tel Aviv University); Goldberger, Jacob (Bar-Ilan University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1151124915,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1520,pub.1147899317,10.48550/arxiv.2205.06551,,,Contrastive Domain Disentanglement for Generalizable Medical Image  Segmentation,"Efficiently utilizing discriminative features is crucial for convolutional
neural networks to achieve remarkable performance in medical image segmentation
and is also important for model generalization across multiple domains, where
letting model recognize domain-specific and domain-invariant information among
multi-site datasets is a reasonable strategy for domain generalization.
Unfortunately, most of the recent disentangle networks are not directly
adaptable to unseen-domain datasets because of the limitations of offered data
distribution. To tackle this deficiency, we propose Contrastive Domain
Disentangle (CDD) network for generalizable medical image segmentation. We
first introduce a disentangle network to decompose medical images into an
anatomical representation factor and a modality representation factor. Then, a
style contrastive loss is proposed to encourage the modality representations
from the same domain to distribute as close as possible while different domains
are estranged from each other. Finally, we propose a domain augmentation
strategy that can randomly generate new domains for model generalization
training. Experimental results on multi-site fundus image datasets for optic
cup and disc segmentation show that the CDD has good model generalization. Our
proposed CDD outperforms several state-of-the-art methods in domain
generalizable segmentation.",,,arXiv,,,2022-05-13,2022,,,,,,All OA, Green,Preprint,"Gu, Ran; Lu, Jiangshan; Zhang, Jingyang; Lei, Wenhui; Zhang, Xiaofan; Wang, Guotai; Zhang, Shaoting","Gu, Ran (); Lu, Jiangshan (); Zhang, Jingyang (); Lei, Wenhui (); Zhang, Xiaofan (); Wang, Guotai (); Zhang, Shaoting ()",,"Gu, Ran (); Lu, Jiangshan (); Zhang, Jingyang (); Lei, Wenhui (); Zhang, Xiaofan (); Wang, Guotai (); Zhang, Shaoting ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1147899317,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
1511,pub.1093777472,10.1109/icsp.2016.7877790,,,Landmark Based Prostate MRI Segmentation via Improved Level Set Method,"Accurate and efficient segmentation of prostate is a significant but difficult task for numerous clinical applications like image-guided prostate intervention and detection of prostate cancer. In this paper, a novel landmark detection based prostate magnetic resonance imaging (MRI) segmentation method is proposed via a modified level set formulation with shape prior. Firstly, the medium slice of prostate MR data is segmented artificially to offer the prior information. Secondly, a prostate detection is implemented by the similarity estimation of texture presentation. Thirdly, we apply a texture constrain procedure to avoid the delineation of fake contour. Finally, the contour of prostate can be captured by the improved level set model with shape prior. A set of experiments using real prostate magnetic resonance images is carried out to demonstrate the significant improvements of our methods on both segmentation accuracy and noise sensitivity comparing to the state-of-the-art methods. The average dice coefficient is 0.91 with expert-defined prostate segmentation in 72 images of 18 different individual.",This work was supported by National Natural Science Foundation of China Grant No: 61371156 and Anhui Province Scientific and Technological Research Programs Grant No: 1401B042019. The authors would like to thank the anonymous reviewers for their helpful and constructive comments and suggestions. This work was supported by National Natural Science Foundation of China Grant No: 61371156 and Anhui Province Scientific and Technological Research Programs Grant No: 1401B042019. The authors would like to thank the anonymous reviewers for their helpful and constructive comments and suggestions.,,,2016 IEEE 13th International Conference on Signal Processing (ICSP),,2016-11-01,2016,,2016-11-01,,,29-34,Closed,Proceeding,"Yang, Xiong; Zhan, Shu; Xie, Dongdong","Yang, Xiong (School of Computer and Information, Hefei University of Technology, Hefei, China, 230009); Zhan, Shu (School of Computer and Information, Hefei University of Technology, Hefei, China, 230009); Xie, Dongdong (Second Affiliated Hospital of Urology, Anhui Medical University, Hefei, China, 230601)",,"Yang, Xiong (Hefei University of Technology); Zhan, Shu (Hefei University of Technology); Xie, Dongdong (Anhui Medical University)",6,0,,1.4,,https://app.dimensions.ai/details/publication/pub.1093777472,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
1510,pub.1151124918,10.1007/978-3-031-16852-9_6,,,Unsupervised Site Adaptation by Intra-site Variability Alignment,"A medical imaging network that was trained on a particular source domain usually suffers significant performance degradation when transferred to a different target domain. This is known as the domain-shift problem. In this study, we propose a general method for transfer knowledge from a source site with labeled data to a target site where only unlabeled data is available. We leverage the variability that is often present within each site, the intra-site variability, and propose an unsupervised site adaptation method that jointly aligns the intra-site data variability in the source and target sites while training the network on the labeled source site data. We applied our method to several medical MRI image segmentation tasks and show that it consistently outperforms state-of-the-art methods.",,,Lecture Notes in Computer Science,Domain Adaptation and Representation Transfer,,2022-09-15,2022,2022-09-15,2022,13542,,56-65,Closed,Chapter,"Goodman, Shaya; Kasten Serlin, Shira; Greenspan, Hayit; Goldberger, Jacob","Goodman, Shaya (Tel-Aviv University, Tel-Aviv, Israel); Kasten Serlin, Shira (Tel-Aviv University, Tel-Aviv, Israel); Greenspan, Hayit (Tel-Aviv University, Tel-Aviv, Israel); Goldberger, Jacob (Bar-Ilan University, Ramat-Gan, Israel)","Goldberger, Jacob (Bar-Ilan University)","Goodman, Shaya (Tel Aviv University); Kasten Serlin, Shira (Tel Aviv University); Greenspan, Hayit (Tel Aviv University); Goldberger, Jacob (Bar-Ilan University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1151124918,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1510,pub.1141326778,10.1007/978-3-030-87196-3_49,,,Bounding Box Tightness Prior for Weakly Supervised Image Segmentation,"This paper presents a weakly supervised image segmentation method that adopts tight bounding box annotations. It proposes generalized multiple instance learning (MIL) and smooth maximum approximation to integrate the bounding box tightness prior into the deep neural network in an end-to-end manner. In generalized MIL, positive bags are defined by parallel crossing lines with a set of different angles, and negative bags are defined as individual pixels outside of any bounding boxes. Two variants of smooth maximum approximation, i.e., α\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha $$\end{document}-softmax function and α\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha $$\end{document}-quasimax function, are exploited to conquer the numeral instability introduced by maximum function of bag prediction. The proposed approach was evaluated on two pubic medical datasets using Dice coefficient. The results demonstrate that it outperforms the state-of-the-art methods. The codes are available at https://github.com/wangjuan313/wsis-boundingbox.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12902,,526-536,All OA, Green,Chapter,"Wang, Juan; Xia, Bin","Wang, Juan (Delta Micro Technology, Inc., 92653, Laguna Hills, CA, USA); Xia, Bin (Shenzhen SiBright Co. Ltd., 518052, Shenzhen, Guangdong, China)","Wang, Juan ","Wang, Juan (); Xia, Bin ()",8,8,,6.55,http://arxiv.org/pdf/2110.00934,https://app.dimensions.ai/details/publication/pub.1141326778,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1510,pub.1093381804,10.1109/isbi.2016.7493479,,,Comprehensive Autoencoder for Prostate Recognition on MR Images,"Automatic recognition of anatomical structures is an essential prerequisite in computer aided diagnoses (CAD) such as tissue segmentation, physiological signal measurement and disease classification. However, insufficient color and speckle information in medical images pose challenges to the recognition of anatomical structures. Such challenges are evident with prostate recognition on magnetic resonance (MR) images and thus remain an open problem, although prostate cancer is an important problem that are attracting increasing interests in medical imaging. In this study, we propose an automatic approach for prostate recognition on MR images. Firstly, compared to existing works which integrate autoencoder with a specific type of classifier, we let autoencoder itself serve as a classifier and therefore lessening the impact from irregular and complex background found in prostate recognition. Secondly, an image energy minimization scheme with consideration of the coherence information from neighboring pixels is proposed to improve the recognition results with clear boundary appearance. We evaluate our method in comparison with three widely applied classifiers and the phase of atlas-based seeds-selection in prostate segmentation on a public prostate database. Our experiment results demonstrate significant superiority of our method in terms of both precision and F -measure.",,,,2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),,2016-04-01,2016,,2016-04-01,,,1190-1194,All OA, Green,Proceeding,"Yan, Ke; Li, Changyang; Wang, Xiuying; Yuan, Yuchen; Li, Ang; Kim, Jinman; Li, Biao; Feng, Dagan","Yan, Ke (School of Information Technologies, University of Sydney); Li, Changyang (School of Information Technologies, University of Sydney); Wang, Xiuying (School of Information Technologies, University of Sydney); Yuan, Yuchen (School of Information Technologies, University of Sydney); Li, Ang (School of Information Technologies, University of Sydney); Kim, Jinman (School of Information Technologies, University of Sydney); Li, Biao (Department of Nuclear Medicine, Ruijin Hospital, Shanghai Jiaotong University School of Medicine); Feng, Dagan (School of Information Technologies, University of Sydney)",,"Yan, Ke (The University of Sydney); Li, Changyang (The University of Sydney); Wang, Xiuying (The University of Sydney); Yuan, Yuchen (The University of Sydney); Li, Ang (The University of Sydney); Kim, Jinman (The University of Sydney); Li, Biao (); Feng, Dagan (The University of Sydney)",8,3,,2.19,https://ses.library.usyd.edu.au/bitstream/2123/21699/1/COMPREHENSIVE%20AUTOENCODER%20FOR%20PROSTATE%20RECOGNITION%20ON%20MR%20IMAGES.pdf,https://app.dimensions.ai/details/publication/pub.1093381804,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,
1464,pub.1154886846,10.1109/bigdata55660.2022.10020468,,,ACL-Net: Adaptive and Collaborative Learning Network for Multi-Site Prostate MRI Segmentation,"High-performance deep learning models require large amounts of data with high quality annotations for model training, while the labeling work usually takes a lot of time for the experts. Meanwhile, the inter-observer variability al-ways exist between annotations from different experts and the distribution shift between the data acquired from different medical institutions. To address these challenges, we propose an end-to-end domain adaptive collaborative learning network for multi-institutional prostate MRI segmentation. Specifically, we introduce an unpaired image translation module to match the image domains between different institutions, which can alleviate the heterogeneity between 1.5T and 3T prostate MR images during model training. Moreover, we design a self-taught strategy to transfer domain-aware knowledge to jointly learn generic and unique representations. Furthermore, we evaluate our approach in scenarios with limited or without annotations, experimental results show that our approach has better adaptation performance than traditional supervised learning approaches, and has the potential to extend to unsupervised domain adaptation scenario. We also evaluate our approach with prostate MRI segmentation benchmark datasets, experimental results show that our approach outperforms several state-of-the-art methods.",,,,2022 IEEE International Conference on Big Data (Big Data),,2022-12-20,2022,,2022-12-20,0,,3112-3117,Closed,Proceeding,"Ma, Zibo; Zhang, Bo; Zhang, Zheng; Wang, Wendong; Mi, Yue; Huang, Haiwen; Wu, Jingyun","Ma, Zibo (Beijing University of Posts and Telecommunications, Beijing, 100876, China; State Key Laboratory of Networking and Switching Technology, School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications); Zhang, Bo (Beijing University of Posts and Telecommunications, Beijing, 100876, China; School of Modern Post, Beijing University of Posts and Telecommunications); Zhang, Zheng (Beijing University of Posts and Telecommunications, Beijing, 100876, China; School of Modern Post, Beijing University of Posts and Telecommunications); Wang, Wendong (State Key Laboratory of Networking and Switching Technology, School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications, Beijing, 100876, China); Mi, Yue (Peking University First Hospital, Beijing, 100034, China; Department of Urology, Peking University First Hospital); Huang, Haiwen (Department of Urology, Peking University First Hospital; Peking University First Hospital, Beijing, 100034, China); Wu, Jingyun (Peking University First Hospital, Beijing, 100034, China; Department of Radiology, Peking University First Hospital)","Ma, Zibo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Zhang, Bo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications)","Ma, Zibo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Zhang, Bo (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Zhang, Zheng (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Wang, Wendong (Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications); Mi, Yue (Peking University First Hospital); Huang, Haiwen (Peking University First Hospital); Wu, Jingyun (Peking University First Hospital)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154886846,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1464,pub.1141326840,10.1007/978-3-030-87199-4_5,,,Group Shift Pointwise Convolution for Volumetric Medical Image Segmentation,"Recent studies have witnessed the effectiveness of 3D convolutions on segmenting volumetric medical images. Compared with the 2D counterparts, 3D convolutions can capture the spatial context in three dimensions. Nevertheless, models employing 3D convolutions introduce more trainable parameters and are more computationally complex, which may lead easily to model overfitting especially for medical applications with limited available training data. This paper aims to improve the effectiveness and efficiency of 3D convolutions by introducing a novel Group Shift Pointwise Convolution (GSP-Conv). GSP-Conv simplifies 3D convolutions into pointwise ones with 1×1×1\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 1\times 1$$\end{document} kernels, which dramatically reduces the number of model parameters and FLOPs (e.g. 27×\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$27\times $$\end{document} fewer than 3D convolutions with 3×3×3\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3\times 3\times 3$$\end{document} kernels). Naïve pointwise convolutions with limited receptive fields cannot make full use of the spatial image context. To address this problem, we propose a parameter-free operation, Group Shift (GS), which shifts the feature maps along different spatial directions in an elegant way. With GS, pointwise convolutions can access features from different spatial locations, and the limited receptive fields of pointwise convolutions can be compensated. We evaluate the proposed method on two datasets, PROMISE12 and BraTS18. Results show that our method, with substantially decreased model complexity, achieves comparable or even better performance than models employing 3D convolutions.","This research is partially supported by the National Key Research and Development Program of China (No. 2020YFC2004804 and 2016YFC0106200), the Scientific and Technical Innovation 2030-“New Generation Artificial Intelligence” Project (No. 2020AAA0104100 and 2020AAA0104105), the Shanghai Committee of Science and Technology, China (No. 20DZ1100800 and 21DZ1100100), Beijing Natural Science Foundation-Haidian Original Innovation Collaborative Fund (No. L192006), the funding from Institute of Medical Robotics of Shanghai Jiao Tong University, the 863 national research fund (No. 2015AA043203), the National Natural Science Foundation of China (No. 61871371 and 81830056), the Key-Area Research and Development Program of GuangDong Province (No. 2018B010109009), the Key Laboratory for Magnetic Resonance and Multimodality Imaging of Guangdong Province (2020B1212060051), the Basic Research Program of Shenzhen (No. JCYJ20180507182400762), and the Youth Innovation Promotion Association Program of Chinese Academy of Sciences (No. 2019351).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12903,,48-58,All OA, Green,Chapter,"He, Junjun; Ye, Jin; Li, Cheng; Song, Diping; Chen, Wanli; Wang, Shanshan; Gu, Lixu; Qiao, Yu","He, Junjun (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China; Shenzhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Shanghai AI Lab, Shanghai, China); Ye, Jin (Shenzhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Shanghai AI Lab, Shanghai, China); Li, Cheng (Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China); Song, Diping (Shenzhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Shanghai AI Lab, Shanghai, China); Chen, Wanli (The Chinese University of Hong Kong, Hong Kong, China); Wang, Shanshan (Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Pazhou Lab, Guangzhou, Guangdong, China); Gu, Lixu (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China); Qiao, Yu (Shenzhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Shanghai AI Lab, Shanghai, China)","Qiao, Yu (Shenzhen Institutes of Advanced Technology; Shanghai Artificial Intelligence Laboratory)","He, Junjun (Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shenzhen Institutes of Advanced Technology; Shanghai Artificial Intelligence Laboratory); Ye, Jin (Shenzhen Institutes of Advanced Technology; Shanghai Artificial Intelligence Laboratory); Li, Cheng (Shenzhen Institutes of Advanced Technology); Song, Diping (Shenzhen Institutes of Advanced Technology; Shanghai Artificial Intelligence Laboratory); Chen, Wanli (Chinese University of Hong Kong); Wang, Shanshan (Shenzhen Institutes of Advanced Technology; Peng Cheng Laboratory; Guangdong Laboratory of Artificial Intelligence and Digital Economy); Gu, Lixu (Shanghai Jiao Tong University; Shanghai Jiao Tong University); Qiao, Yu (Shenzhen Institutes of Advanced Technology; Shanghai Artificial Intelligence Laboratory)",0,0,,,http://arxiv.org/pdf/2109.12629,https://app.dimensions.ai/details/publication/pub.1141326840,46 Information and Computing Sciences,,,,,,,,,,,
1464,pub.1131873592,10.13104/imri.2020.24.3.123,,,Optimization of Multi-Atlas Segmentation with Joint Label Fusion Algorithm for Automatic Segmentation in Prostate MR Imaging,,,,Investigative Magnetic Resonance Imaging,,,2020,2020,,2020,24,3,123-131,Closed,Article,"Choi, Yoon Ho; Kim, Jae-Hun; Kim, Chan Kyo","Choi, Yoon Ho (Department of Health Sciences and Technology, SAIHST, Sungkyunkwan University, Seoul,); Kim, Jae-Hun (Department of Radiology and Center for Imaging Science, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul,); Kim, Chan Kyo (Department of Radiology and Center for Imaging Science, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul,; Department of Medical Device Management and Research, SAIHST, Sungkyunkwan University, Seoul,; Department of Digital Health, SAIHST, Sungkyunkwan University, Seoul,)",,"Choi, Yoon Ho (Sungkyunkwan University); Kim, Jae-Hun (Samsung Medical Center; Sungkyunkwan University); Kim, Chan Kyo (Samsung Medical Center; Sungkyunkwan University; Sungkyunkwan University; Sungkyunkwan University)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1131873592,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 51 Physical Sciences,,,,,,,,,,
1461,pub.1148654975,10.1007/978-3-031-08223-8_28,,,Brain Tumour Segmentation on 3D MRI Using Attention V-Net,"Brain tumour segmentation on 3D MRI imaging is one of the most critical deep learning applications. In this paper, for the segmentation of tumour sub-regions in brain MRI images, we study some popular architecture for medical imaging segmentation. We further, inspired by them, proposed an architecture that is an end-to-end trainable, fully convolutional neural network that uses attention block to learn localization of different features of the multiple sub-regions of a tumour. We also experiment with a combination of the weighted cross-entropy loss function and dice loss function on the model’s performance and the quality of the output segmented labels. The results of the evaluation of our model are received through BraTS’19 dataset challenge. The model can achieve a dice score of 0.80 for the whole tumour segmentation and dice scores of 0.639 and 0.536 for the other two sub-regions within the tumour on the validation dataset.",,,Communications in Computer and Information Science,Engineering Applications of Neural Networks,,2022-06-10,2022,2022-06-10,2022,1600,,336-348,Closed,Chapter,"Giri, Charul; Sharma, Jivitesh; Goodwin, Morten","Giri, Charul (Center for Artificial Intelligence Research (CAIR), University of Agder, Kristiansand, Norway); Sharma, Jivitesh (Center for Artificial Intelligence Research (CAIR), University of Agder, Kristiansand, Norway); Goodwin, Morten (Center for Artificial Intelligence Research (CAIR), University of Agder, Kristiansand, Norway)","Giri, Charul (University of Agder)","Giri, Charul (University of Agder); Sharma, Jivitesh (University of Agder); Goodwin, Morten (University of Agder)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1148654975,46 Information and Computing Sciences, 4611 Machine Learning, 51 Physical Sciences,,,,,,,,,,
1461,pub.1138783558,10.1007/978-981-16-1550-4_14,,,A Study on Using Deep Learning for Segmentation of Medical Image,"Segmentation of medical images using deep learning has provided state-of-the-art performances in this area of work. With the availability of large digital datasets and access to powerful GPUs, deep learning has transformed our world. We are now able to make computers mimic and replicate the functions of the human mind simply by providing enough data and computing the problem. Deep learning has a huge potential for medical image analysis and now it has been firmly established as a robust tool in image segmentation. This paper addresses the six popular methods that have employed deep-learning techniques for the segmentation of medical images which play a massive impact in the medical healthcare industry and in turn make a contributing role towards the concept of smart cities. A comparative study on these deep learning-based segmentation techniques will provide a researcher working in the field of medical imaging to explore further in this area for higher accuracy and better results.",,,Lecture Notes in Electrical Engineering,Emerging Technologies for Smart Cities,,2021-06-12,2021,2021-06-12,2021,765,,127-138,Closed,Chapter,"Boro, Lal Omega; Nandi, Gypsy","Boro, Lal Omega (School of Technology, Assam Don Bosco University, 781017, Guwahati, Assam, India); Nandi, Gypsy (School of Technology, Assam Don Bosco University, 781017, Guwahati, Assam, India)","Boro, Lal Omega (Assam Don Bosco University)","Boro, Lal Omega (Assam Don Bosco University); Nandi, Gypsy (Assam Don Bosco University)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1138783558,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,,
1461,pub.1129039446,10.48550/arxiv.2007.02035,,,Shape-aware Meta-learning for Generalizing Prostate MRI Segmentation to  Unseen Domains,"Model generalization capacity at domain shift (e.g., various imaging
protocols and scanners) is crucial for deep learning methods in real-world
clinical deployment. This paper tackles the challenging problem of domain
generalization, i.e., learning a model from multi-domain source data such that
it can directly generalize to an unseen target domain. We present a novel
shape-aware meta-learning scheme to improve the model generalization in
prostate MRI segmentation. Our learning scheme roots in the gradient-based
meta-learning, by explicitly simulating domain shift with virtual meta-train
and meta-test during training. Importantly, considering the deficiencies
encountered when applying a segmentation model to unseen domains (i.e.,
incomplete shape and ambiguous boundary of the prediction masks), we further
introduce two complementary loss objectives to enhance the meta-optimization,
by particularly encouraging the shape compactness and shape smoothness of the
segmentations under simulated domain shift. We evaluate our method on prostate
MRI data from six different institutions with distribution shifts acquired from
public datasets. Experimental results show that our approach outperforms many
state-of-the-art generalization methods consistently across all six settings of
unseen domains.",,,arXiv,,,2020-07-04,2020,,,,,,All OA, Green,Preprint,"Liu, Quande; Dou, Qi; Heng, Pheng-Ann","Liu, Quande (); Dou, Qi (); Heng, Pheng-Ann ()",,"Liu, Quande (); Dou, Qi (); Heng, Pheng-Ann ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129039446,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4611 Machine Learning",,,,,,,,,,,
1460,pub.1152026937,10.1109/cyber55403.2022.9907293,,,Design of prostate puncture system guided by 3D MRI and TRUS images,"The incidence rate of prostate cancer in the world is increasing year by year. At present, the mainstream diagnostic method is puncture biopsy, the doctor holds an ultrasonic probe for imaging and manual puncture. There are many uncertain factors, such as ultrasound imaging effect, doctor's clinical level and psychological state. This paper designs a robot system including control system, image acquisition and processing system, probe feeding mechanism and mechanical arm. An automatic segmentation algorithm for prostate 3D MRI (Magnetic Resonance Imaging) and TRUS (Trans rectal Ultrasonography) images is proposed, which takes into account both real time and accuracy. A novel convolution module that integrates residual connection, dense connection and deep separable convolution is proposed for multi-scale feature extraction and fusion. At the same time, deep supervision mechanism and multiple attention mechanisms are integrated to improve training efficiency. The experimental results show that the real time and accuracy of the segmentation method proposed in this paper are better than the existing methods. The image-guided puncture experiment also shows that the puncture accuracy of the system designed in this paper is less than 1.5mm, The system and network designed in this paper have important clinical significance and practical value for image-guided biopsy.",,Research supported by the National Natural Science Foundation of China under Grant 61873045 in part by the Fundamental Research Funds for the Central Universities under Grant DUT20YG10l.,,"2022 12th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",,2022-07-31,2022,,2022-07-31,0,,144-149,Closed,Proceeding,"Liu, Dong; Wang, Long; Du, Yu; Yang, Deyong; Wang, Xuefei; Cong, Ming","Liu, Dong (University of Technology, Dalian, 116024, China); Wang, Long (University of Technology, Dalian, 116024, China); Du, Yu (Dalian Jiaotong University, Dalian, 116025, China); Yang, Deyong (The First Affiliated Hospital of Dalian Medical University, Dalian, 116011, China); Wang, Xuefei (University of Technology, Dalian, 116024, China); Cong, Ming (University of Technology, Dalian, 116024, China)","Du, Yu (Dalian Jiaotong University)","Liu, Dong (); Wang, Long (); Du, Yu (Dalian Jiaotong University); Yang, Deyong (First Affiliated Hospital of Dalian Medical University); Wang, Xuefei (); Cong, Ming ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152026937,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
1460,pub.1151033053,10.1007/978-3-031-16452-1_14,,,Semi-supervised Medical Image Segmentation Using Cross-Model Pseudo-Supervision with Shape Awareness and Local Context Constraints,"In semi-supervised medical image segmentation, the limited amount of labeled data available for training is often insufficient to learn the variability and complexity of target regions. To overcome these challenges, we propose a novel framework based on cross-model pseudo-supervision that generates anatomically plausible predictions using shape awareness and local context constraints. Our framework consists of two parallel networks, a shape-aware network and a shape-agnostic network, which provide pseudo-labels to each other for using unlabeled data effectively. The shape-aware network implicitly captures information on the shape of target regions by adding the prediction of the other network as input. On the other hand, the shape-agnostic network leverages Monte-Carlo dropout uncertainty estimation to generate reliable pseudo-labels to the other network. The proposed framework also comprises a new loss function that enables the network to learn the local context of the segmentation, thus improving the overall segmentation accuracy. Experiments on two publicly-available datasets show that our method outperforms state-of-the-art approaches for semi-supervised segmentation and better preserves anatomical morphology compared to these approaches. Code is available at https://github.com/igip-liu/SLC-Net.","This work is supported by the National Key R &amp;D Plan on Strategic International Scientific and Technological Innovation Cooperation Special Project (No. 2021YFE0203800), the NSFC-Zhejiang Joint Fund of the Integration of Informatization and Industrialization (No. U1909210), the National Natural Science Foundation of China under Grant (No. 62172257, 61902217), the Natural Science Foundation of Shandong Province (ZR2019BF043).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13438,,140-150,Closed,Chapter,"Liu, Jinhua; Desrosiers, Christian; Zhou, Yuanfeng","Liu, Jinhua (School of Software, Shandong University, Jinan, China); Desrosiers, Christian (Software and IT Engineering Department, École de technologie supérieure, Montreal, Canada); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, China)","Zhou, Yuanfeng (Shandong University)","Liu, Jinhua (Shandong University); Desrosiers, Christian (École de Technologie Supérieure); Zhou, Yuanfeng (Shandong University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1151033053,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1460,pub.1141326811,10.1007/978-3-030-87199-4_23,,,Domain Composition and Attention for Unseen-Domain Generalizable Medical Image Segmentation,"Domain generalizable model is attracting increasing attention in medical image analysis since data is commonly acquired from different institutes with various imaging protocols and scanners. To tackle this challenging domain generalization problem, we propose a Domain Composition and Attention-based network (DCA-Net) to improve the ability of domain representation and generalization. First, we present a domain composition method that represents one certain domain by a linear combination of a set of basis representations (i.e., a representation bank). Second, a novel plug-and-play parallel domain preceptor is proposed to learn these basis representations and we introduce a divergence constraint function to encourage the basis representations are as divergent as possible. Then, a domain attention module is proposed to learn the linear combination coefficients of the basis representations. The result of liner combination is used to calibrate the feature maps of an input image, which enables the model to generalize to different and even unseen domains. We validate our method on public prostate MRI dataset acquired from six different institutions with apparent domain shift. Experimental results show that our proposed model can generalizes well on different and even unseen domains and it outperforms state-of-the-art methods on the multi-domain prostate segmentation task. Code is available at https://github.com/HiLab-git/DCA-Net.","This work was supported by the National Natural Science Foundations of China [61901084 and 81771921] funding, key research and development project of Sichuan province, China [No. 20ZDYF2817].",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12903,,241-250,All OA, Green,Chapter,"Gu, Ran; Zhang, Jingyang; Huang, Rui; Lei, Wenhui; Wang, Guotai; Zhang, Shaoting","Gu, Ran (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; SenseTime Research, Shanghai, China); Zhang, Jingyang (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; SenseTime Research, Shanghai, China); Huang, Rui (SenseTime Research, Shanghai, China); Lei, Wenhui (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China); Wang, Guotai (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China); Zhang, Shaoting (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; SenseTime Research, Shanghai, China)","Wang, Guotai (University of Electronic Science and Technology of China)","Gu, Ran (University of Electronic Science and Technology of China); Zhang, Jingyang (Shanghai Jiao Tong University); Huang, Rui (); Lei, Wenhui (University of Electronic Science and Technology of China); Wang, Guotai (University of Electronic Science and Technology of China); Zhang, Shaoting (University of Electronic Science and Technology of China)",6,6,,4.91,http://arxiv.org/pdf/2109.08852,https://app.dimensions.ai/details/publication/pub.1141326811,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1460,pub.1129582067,10.1109/iwssip48289.2020.9145218,,,Automatic Prostate Segmentation on 3D MRI Scans Using Convolutional Neural Networks with Residual Connections and Superpixels,"Automatic and reliable prostate segmentation is an essential prerequisite for assisting the diagnosis and treatment, such as guiding biopsy procedure and radiation therapy. Notwithstanding, automatic segmentation is challenging due to the lack of clear prostate boundaries owing to similar appearance of prostate and surrounding tissues and the wide variation in size and shape among different patients. Therefore, this paper proposes an automatic method for prostate segmentation on 3D MRI scans using a content-sensitive superpixels technique, deep convolutional neural networks with residual connections (ResCNN), and the particle swarm optimization (PSO) algorithm. The proposed method has been evaluated on the Prostate 3T and PROMISE12 databases, presenting a Dice similarity coefficient of 86.68 %, Jaccard index of 76.58%, relative volume difference of 3.92%, volumetric similarity of 96.69 %, sensitivity of 88.36 %, specificity of 93.43%, accuracy of 91.97% and an area under ROC curve of 90.90 %. Experimental results demonstrate the high performance-potential of the proposed method comparable to those previously published.",,,,"2020 International Conference on Systems, Signals and Image Processing (IWSSIP)",,2020-07-01,2020,,2020-07-01,0,,51-56,Closed,Proceeding,"da Silva, Giovanni Lucca França; França, João Vitor Ferreira; Diniz, Petterson Sousa; Silva, Aristófanes Corrêa; de Paiva, Anselmo Cardoso; de Cavalcanti, Elton Anderson Araújo","da Silva, Giovanni Lucca França (Federal University of Maranhão - UFMA, Applied Computing Group - NCA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); França, João Vitor Ferreira (Federal University of Maranhão - UFMA, Applied Computing Group - NCA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); Diniz, Petterson Sousa (Federal University of Maranhão - UFMA, Applied Computing Group - NCA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); Silva, Aristófanes Corrêa (Federal University of Maranhão - UFMA, Applied Computing Group - NCA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); de Paiva, Anselmo Cardoso (Federal University of Maranhão - UFMA, Applied Computing Group - NCA, Av. dos Portugueses, SN, Bacanga, 65085-580, São Luís, MA, Brazil); de Cavalcanti, Elton Anderson Araújo (University Hospital of the Federal University of Maranhão - HUUFMA, R. Barão de Itapary, 227, Centro, 65020-070, São Luís, MA, Brazil)",,"da Silva, Giovanni Lucca França (Federal University of Maranhão); França, João Vitor Ferreira (Federal University of Maranhão); Diniz, Petterson Sousa (Federal University of Maranhão); Silva, Aristófanes Corrêa (Federal University of Maranhão); de Paiva, Anselmo Cardoso (Federal University of Maranhão); de Cavalcanti, Elton Anderson Araújo (Hospital Universitário da Universidade Federal do Maranhão)",3,3,,1.53,,https://app.dimensions.ai/details/publication/pub.1129582067,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
1460,pub.1119394934,10.48550/arxiv.1903.12571,,,CNN-based Prostate Zonal Segmentation on T2-weighted MR Images: A  Cross-dataset Study,"Prostate cancer is the most common cancer among US men. However, prostate
imaging is still challenging despite the advances in multi-parametric Magnetic
Resonance Imaging (MRI), which provides both morphologic and functional
information pertaining to the pathological regions. Along with whole prostate
gland segmentation, distinguishing between the Central Gland (CG) and
Peripheral Zone (PZ) can guide towards differential diagnosis, since the
frequency and severity of tumors differ in these regions; however, their
boundary is often weak and fuzzy. This work presents a preliminary study on
Deep Learning to automatically delineate the CG and PZ, aiming at evaluating
the generalization ability of Convolutional Neural Networks (CNNs) on two
multi-centric MRI prostate datasets. Especially, we compared three CNN-based
architectures: SegNet, U-Net, and pix2pix. In such a context, the segmentation
performances achieved with/without pre-training were compared in 4-fold
cross-validation. In general, U-Net outperforms the other methods, especially
when training and testing are performed on multiple datasets.",,,arXiv,,,2019-03-29,2019,,,,,,All OA, Green,Preprint,"Rundo, Leonardo; Han, Changhee; Zhang, Jin; Hataya, Ryuichiro; Nagano, Yudai; Militello, Carmelo; Ferretti, Claudio; Nobile, Marco S.; Tangherloni, Andrea; Gilardi, Maria Carla; Vitabile, Salvatore; Nakayama, Hideki; Mauri, Giancarlo","Rundo, Leonardo (); Han, Changhee (); Zhang, Jin (); Hataya, Ryuichiro (); Nagano, Yudai (); Militello, Carmelo (); Ferretti, Claudio (); Nobile, Marco S. (); Tangherloni, Andrea (); Gilardi, Maria Carla (); Vitabile, Salvatore (); Nakayama, Hideki (); Mauri, Giancarlo ()",,"Rundo, Leonardo (); Han, Changhee (); Zhang, Jin (); Hataya, Ryuichiro (); Nagano, Yudai (); Militello, Carmelo (); Ferretti, Claudio (); Nobile, Marco S. (); Tangherloni, Andrea (); Gilardi, Maria Carla (); Vitabile, Salvatore (); Nakayama, Hideki (); Mauri, Giancarlo ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119394934,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
1460,pub.1117496860,10.24193/subbi.2019.1.03,,,Medical Image Analysis with Semantic Segmentation and Active Learning,"We address object detection using semantic segmentation and apply it for prostate detection in an MRI data-set. Our detection pipeline uses first a segmentation step followed by a classifier with a convolutional neural network (CNN). Since the segmentation provides a set of unbalanced data-sets – where a high accuracy is difficult to obtain – we leverage the prospect of improving detection accuracy using a Bayesian treatment of deep networks and the possibility of better exploiting the data using active learning. The resulting algorithm is both adaptive and data-efficient: by assuming that from a large pool of data only a few are segmented, the active learning module of the algorithm finds the image that improves most detection accuracy. We test our algorithm on a prostate medical image data-set and show that the active learning-based algorithm performs well in the prostate detection class. The resulting system is invariant to translations within the image and the results show improvements when using the pipeline that includes active learning and CNNs.",,,Studia Universitatis Babeș-Bolyai Informatica,,,2019-06-19,2019,2019-06-19,,64,1,26-38,All OA, Gold,Article,"Saidu, C.I.; Csato, L.","Saidu, C.I. (Computer Science Department, African University of Science and Technology, Airport Road, 10 km, Abuja, Nigeria); Csato, L. (Faculty of Mathematics and Computer Science, Babes-Bolyai University, Cluj, Romania)",,"Saidu, C.I. (African Institute of Science and Technology); Csato, L. (Babeș-Bolyai University)",2,2,,0.73,http://www.cs.ubbcluj.ro/~studia-i/journal/journal/article/download/34/34,https://app.dimensions.ai/details/publication/pub.1117496860,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
1460,pub.1093838265,10.1109/3dv.2016.79,,,V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation,"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.","We would like to acknowledge NVidia corporation, that donated a Tesla K40 GPU to our group enabling this research, Dr. Geert Litjens who dedicated some of his time to evaluate our results against the ground truth of the PROMISE 2012 dataset and Ms. Iro Laina for her support to this project. We would like to acknowledge NVidia corporation, that donated a Tesla K40 GPU to our group enabling this research, Dr. Geert Litjens who dedicated some of his time to evaluate our results against the ground truth of the PROMISE 2012 dataset and Ms. Iro Laina for her support to this project.",,,2016 Fourth International Conference on 3D Vision (3DV),,2016-10-01,2016,,2016-10-01,,,565-571,All OA, Green,Proceeding,"Milletari, Fausto; Navab, Nassir; Ahmadi, Seyed-Ahmad","Milletari, Fausto (Technical University of Munich, Boltzmannstr. 3, 85748, Munich); Navab, Nassir (Technical University of Munich, Boltzmannstr. 3, 85748, Munich); Ahmadi, Seyed-Ahmad (Ludwig-Maximilians-Universität München, Marchioninistrasse 15, 81377, Munich)",,"Milletari, Fausto (Technical University of Munich); Navab, Nassir (Technical University of Munich); Ahmadi, Seyed-Ahmad (Ludwig-Maximilians-Universität München)",5169,3052,,1391.12,http://arxiv.org/pdf/1606.04797,https://app.dimensions.ai/details/publication/pub.1093838265,"32 Biomedical and Clinical Sciences; 3202 Clinical Sciences; 46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4611 Machine Learning",,,,,,,,,,,
1460,pub.1150816281,10.48550/arxiv.2209.01300,,,Source-Free Unsupervised Domain Adaptation with Norm and Shape  Constraints for Medical Image Segmentation,"Unsupervised domain adaptation (UDA) is one of the key technologies to solve
a problem where it is hard to obtain ground truth labels needed for supervised
learning. In general, UDA assumes that all samples from source and target
domains are available during the training process. However, this is not a
realistic assumption under applications where data privacy issues are
concerned. To overcome this limitation, UDA without source data, referred to
source-free unsupervised domain adaptation (SFUDA) has been recently proposed.
Here, we propose a SFUDA method for medical image segmentation. In addition to
the entropy minimization method, which is commonly used in UDA, we introduce a
loss function for avoiding feature norms in the target domain small and a prior
to preserve shape constraints of the target organ. We conduct experiments using
datasets including multiple types of source-target domain combinations in order
to show the versatility and robustness of our method. We confirm that our
method outperforms the state-of-the-art in all datasets.",,,arXiv,,,2022-09-02,2022,,,,,,All OA, Green,Preprint,"Kondo, Satoshi","Kondo, Satoshi ()",,"Kondo, Satoshi ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150816281,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
1460,pub.1141326823,10.1007/978-3-030-87199-4_34,,,Federated Whole Prostate Segmentation in MRI with Personalized Neural Architectures,"Building robust deep learning-based models requires diverse training data, ideally from several sources. However, these datasets cannot be combined easily because of patient privacy concerns or regulatory hurdles, especially if medical data is involved. Federated learning (FL) is a way to train machine learning models without the need for centralized datasets. Each FL client trains on their local data while only sharing model parameters with a global server that aggregates the parameters from all clients. At the same time, each client’s data can exhibit differences and inconsistencies due to the local variation in the patient population, imaging equipment, and acquisition protocols. Hence, the federated learned models should be able to adapt to the local particularities of a client’s data. In this work, we combine FL with an AutoML technique based on local neural architecture search by training a “supernet”. Furthermore, we propose an adaptation scheme to allow for personalized model architectures at each FL client’s site. The proposed method is evaluated on four different datasets from 3D prostate MRI and shown to improve the local models’ performance after adaptation through selecting an optimal path through the AutoML supernet.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12903,,357-366,All OA, Green,Chapter,"Roth, Holger R.; Yang, Dong; Li, Wenqi; Myronenko, Andriy; Zhu, Wentao; Xu, Ziyue; Wang, Xiaosong; Xu, Daguang","Roth, Holger R. (NVIDIA, Bethesda, MD, USA); Yang, Dong (NVIDIA, Bethesda, MD, USA); Li, Wenqi (NVIDIA, Bethesda, MD, USA); Myronenko, Andriy (NVIDIA, Bethesda, MD, USA); Zhu, Wentao (NVIDIA, Bethesda, MD, USA); Xu, Ziyue (NVIDIA, Bethesda, MD, USA); Wang, Xiaosong (NVIDIA, Bethesda, MD, USA); Xu, Daguang (NVIDIA, Bethesda, MD, USA)","Roth, Holger R. (Nvidia (United States)); Xu, Daguang (Nvidia (United States))","Roth, Holger R. (Nvidia (United States)); Yang, Dong (Nvidia (United States)); Li, Wenqi (Nvidia (United States)); Myronenko, Andriy (Nvidia (United States)); Zhu, Wentao (Nvidia (United States)); Xu, Ziyue (Nvidia (United States)); Wang, Xiaosong (Nvidia (United States)); Xu, Daguang (Nvidia (United States))",10,10,,8.18,http://arxiv.org/pdf/2107.08111,https://app.dimensions.ai/details/publication/pub.1141326823,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,
1460,pub.1117944783,10.1109/isbi.2019.8759572,,,Towards Patient-Individual PI-Rads v2 Sector Map: Cnn for Automatic Segmentation of Prostatic Zones From T2-Weighted MRI,"Automatic segmentation of the prostate, its inner and surrounding structures is highly desired for various applications. Several works have been presented for segmentation of anatomical zones of the prostate that are limited to the transition and peripheral zone. Following the spatial division according to the PI-RADS v2 sector map, we present a multi-class segmentation method that additionally targets the anterior fibromuscular stroma and distal prostatic urethra to improve computer-aided detection methods and enable a more precise therapy planning. We propose a multi-class segmentation with an anisotropic convolutional neural network that generates a topologically correct division of the prostate into these four structures. We evaluated our method on a dataset of T2-weighted axial MRI scans (n=98 subjects) and obtained results in the range of inter-rater variability for the majority of the zones.","This work has been funded by the EU and the federal state of Saxony-Anhalt, Germany under grant number FuE 74/16 and ZS/2016/04/78123 as part of the initiative ’Sachsen-Anhalt WISSENSCHAFT Schwerpunkte’. This work has been funded by the EU and the federal state of Saxony-Anhalt, Germany under grant number FuE 74/16 and ZS/2016/04/78123 as part of the initiative ’Sachsen-Anhalt WISSENSCHAFT Schwerpunkte’. The authors would like to thank the NVIDIA Corporation for donating the Titan Xp used for this research. Data used in this research were obtained from The Cancer Imaging Archive (TCIA) sponsored by the SPIE, NCI/NIH, AAPM, and Radboud University.",,,2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),,2019-04-11,2019,,2019-04-11,0,,696-700,Closed,Proceeding,"Meyer, Anneke; Rakr, Marko; Schindele, Daniel; Blaschke, Simon; Schostak, Martin; Fedorov, Andriy; Hansen, Christian","Meyer, Anneke (Faculty of Computer Science & Research Campus STIMULATE, University of Magdeburg, Germany); Rakr, Marko (Faculty of Computer Science & Research Campus STIMULATE, University of Magdeburg, Germany); Schindele, Daniel (Clinic of Urology and Pediatric Urology, University Hospital Magdeburg, Germany); Blaschke, Simon (Clinic of Urology and Pediatric Urology, University Hospital Magdeburg, Germany); Schostak, Martin (Clinic of Urology and Pediatric Urology, University Hospital Magdeburg, Germany); Fedorov, Andriy (Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, USA); Hansen, Christian (Faculty of Computer Science & Research Campus STIMULATE, University of Magdeburg, Germany)","Meyer, Anneke (Otto-von-Guericke University Magdeburg)","Meyer, Anneke (Otto-von-Guericke University Magdeburg); Rakr, Marko (Otto-von-Guericke University Magdeburg); Schindele, Daniel (University Hospital Magdeburg); Blaschke, Simon (University Hospital Magdeburg); Schostak, Martin (University Hospital Magdeburg); Fedorov, Andriy (Brigham and Women's Hospital; Harvard University); Hansen, Christian (Otto-von-Guericke University Magdeburg)",23,16,,8.12,,https://app.dimensions.ai/details/publication/pub.1117944783,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
1459,pub.1151032985,10.1007/978-3-031-16443-9_15,,,MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation,"Convolutional neural networks (CNNs) have achieved remarkable segmentation accuracy on benchmark datasets where training and test sets are from the same domain, yet their performance can degrade significantly on unseen domains, which hinders the deployment of CNNs in many clinical scenarios. Most existing works improve model out-of-domain (OOD) robustness by collecting multi-domain datasets for training, which is expensive and may not always be feasible due to privacy and logistical issues. In this work, we focus on improving model robustness using a single-domain dataset only. We propose a novel data augmentation framework called MaxStyle, which maximizes the effectiveness of style augmentation for model OOD performance. It attaches an auxiliary style-augmented image decoder to a segmentation network for robust feature learning and data augmentation. Importantly, MaxStyle augments data with improved image style diversity and hardness, by expanding the style space with noise and searching for the worst-case style composition of latent features via adversarial training. With extensive experiments on multiple public cardiac and prostate MR datasets, we demonstrate that MaxStyle leads to significantly improved out-of-distribution robustness against unseen corruptions as well as common distribution shifts across multiple, different, unseen sites and unknown image sequences under both low- and high-training data settings. The code can be found at https://github.com/cherise215/MaxStyle.","This work was supported by two EPSRC Programme Grants (EP/P001009/1, EP/W01842X/1) and the UKRI Innovate UK Grant (No.104691).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13435,,151-161,All OA, Green,Chapter,"Chen, Chen; Li, Zeju; Ouyang, Cheng; Sinclair, Matthew; Bai, Wenjia; Rueckert, Daniel","Chen, Chen (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Li, Zeju (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Ouyang, Cheng (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Sinclair, Matthew (BioMedIA Group, Department of Computing, Imperial College London, London, UK; HeartFlow, Mountain View, USA); Bai, Wenjia (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Data Science Institute, Imperial College London, London, UK; Department of Brain Sciences, Imperial College London, London, UK); Rueckert, Daniel (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Klinikum rechts der Isar, Technical University of Munich, Munich, Germany)","Chen, Chen (Imperial College London)","Chen, Chen (Imperial College London); Li, Zeju (Imperial College London); Ouyang, Cheng (Imperial College London); Sinclair, Matthew (Imperial College London; HeartFlow (United States)); Bai, Wenjia (Imperial College London; Imperial College London; Imperial College London); Rueckert, Daniel (Imperial College London; Rechts der Isar Hospital; Technical University of Munich)",0,0,,,http://arxiv.org/pdf/2206.01737,https://app.dimensions.ai/details/publication/pub.1151032985,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1459,pub.1141451150,10.48550/arxiv.2109.12629,,,Group Shift Pointwise Convolution for Volumetric Medical Image  Segmentation,"Recent studies have witnessed the effectiveness of 3D convolutions on
segmenting volumetric medical images. Compared with the 2D counterparts, 3D
convolutions can capture the spatial context in three dimensions. Nevertheless,
models employing 3D convolutions introduce more trainable parameters and are
more computationally complex, which may lead easily to model overfitting
especially for medical applications with limited available training data. This
paper aims to improve the effectiveness and efficiency of 3D convolutions by
introducing a novel Group Shift Pointwise Convolution (GSP-Conv). GSP-Conv
simplifies 3D convolutions into pointwise ones with 1x1x1 kernels, which
dramatically reduces the number of model parameters and FLOPs (e.g. 27x fewer
than 3D convolutions with 3x3x3 kernels). Naïve pointwise convolutions with
limited receptive fields cannot make full use of the spatial image context. To
address this problem, we propose a parameter-free operation, Group Shift (GS),
which shifts the feature maps along with different spatial directions in an
elegant way. With GS, pointwise convolutions can access features from different
spatial locations, and the limited receptive fields of pointwise convolutions
can be compensated. We evaluate the proposed methods on two datasets, PROMISE12
and BraTS18. Results show that our method, with substantially decreased model
complexity, achieves comparable or even better performance than models
employing 3D convolutions.",,,arXiv,,,2021-09-26,2021,,,,,,All OA, Green,Preprint,"He, Junjun; Ye, Jin; Li, Cheng; Song, Diping; Chen, Wanli; Wang, Shanshan; Gu, Lixu; Qiao, Yu","He, Junjun (); Ye, Jin (); Li, Cheng (); Song, Diping (); Chen, Wanli (); Wang, Shanshan (); Gu, Lixu (); Qiao, Yu ()",,"He, Junjun (); Ye, Jin (); Li, Cheng (); Song, Diping (); Chen, Wanli (); Wang, Shanshan (); Gu, Lixu (); Qiao, Yu ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141451150,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
1459,pub.1141327126,10.1007/978-3-030-87722-4_5,,,Self-supervised Multimodal Generalized Zero Shot Learning for Gleason Grading,"Gleason grading from histopathology images is essential for accurate prostate cancer (PCa) diagnosis. Since such images are obtained after invasive tissue resection quick diagnosis is challenging under the existing paradigm. We propose a method to predict Gleason grades from magnetic resonance (MR) images which are non-interventional and easily acquired. We solve the problem in a generalized zero-shot learning (GZSL) setting since we may not access training images of every disease grade. Synthetic MRI feature vectors of unseen grades (classes) are generated by exploiting Gleason grades’ ordered nature through a conditional variational autoencoder (CVAE) incorporating self-supervised learning. Corresponding histopathology features are generated using cycle GANs, and combined with MR features to predict Gleason grades of test images. Experimental results show our method outperforms competing feature generating approaches for GZSL, and comes close to performance of fully supervised methods.",,,Lecture Notes in Computer Science,"Domain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health",,2021-09-21,2021,2021-09-21,2021,12968,,46-56,Closed,Chapter,"Mahapatra, Dwarikanath; Bozorgtabar, Behzad; Kuanar, Shiba; Ge, Zongyuan","Mahapatra, Dwarikanath (Inception Institute of Artificial Intelligence, Abu Dhabi, UAE); Bozorgtabar, Behzad (Signal Processing Laboratory 5, EPFL, Lausanne, Switzerland); Kuanar, Shiba (Yale University, New Haven, USA); Ge, Zongyuan (Monash University, Melbourne, Australia; Airdoc Research, Melbourne, Australia)","Mahapatra, Dwarikanath (Inception Institute of Artificial Intelligence)","Mahapatra, Dwarikanath (Inception Institute of Artificial Intelligence); Bozorgtabar, Behzad (École Polytechnique Fédérale de Lausanne); Kuanar, Shiba (Yale University); Ge, Zongyuan (Monash University)",6,6,,4.91,,https://app.dimensions.ai/details/publication/pub.1141327126,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1459,pub.1124286065,10.1007/978-3-030-39343-4_2,,,Using a Conditional Generative Adversarial Network (cGAN) for Prostate Segmentation,"Prostate cancer is the second most commonly diagnosed cancer among men and currently multi-parametric MRI is a promising imaging technique used for clinical workup of prostate cancer. Accurate detection and localisation of the prostate tissue boundary on various MRI scans can be helpful for obtaining a region of interest for Computer Aided Diagnosis systems. In this paper, we present a fully automated detection and segmentation pipeline using a conditional Generative Adversarial Network (cGAN). We investigated the robustness of the cGAN model against adding Gaussian noise or removing noise from the training data. Based on the detection and segmentation metrics, de-noising did not show a significant improvement. However, by including noisy images in the training data, the detection and segmentation performance was improved in each 3D modality, which resulted in comparable to state-of-the-art results.",The authors would like to acknowledge Dr. Alun Jones and Sandy Spence for their support and maintenance of the GPU and the computer systems used for this research.,,Communications in Computer and Information Science,Medical Image Understanding and Analysis,,2020-01-24,2020,2020-01-24,2020,1065,,15-25,Closed,Chapter,"Grall, Amelie; Hamidinekoo, Azam; Malcolm, Paul; Zwiggelaar, Reyer","Grall, Amelie (Department of Computer Science, Aberystwyth University, Aberystwyth, UK); Hamidinekoo, Azam (Department of Computer Science, Aberystwyth University, Aberystwyth, UK); Malcolm, Paul (Department of Radiology, Norwich & Norfolk University Hospital, Norwich, UK); Zwiggelaar, Reyer (Department of Computer Science, Aberystwyth University, Aberystwyth, UK)","Hamidinekoo, Azam (Aberystwyth University)","Grall, Amelie (Aberystwyth University); Hamidinekoo, Azam (Aberystwyth University); Malcolm, Paul (); Zwiggelaar, Reyer (Aberystwyth University)",6,3,,2.56,,https://app.dimensions.ai/details/publication/pub.1124286065,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
1450,pub.1150793511,10.1007/978-3-031-15937-4_65,,,TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation,"Medical image segmentation is one of the most fundamental tasks concerning medical information analysis. Various solutions have been proposed so far, including many deep learning-based techniques, such as U-Net, FC-DenseNet, etc. However, high-precision medical image segmentation remains a highly challenging task due to the existence of inherent magnification and distortion in medical images as well as the presence of lesions with similar density to normal tissues. In this paper, we propose TFCNs (Transformers for Fully Convolutional denseNets) to tackle the problem by introducing ResLinear-Transformer (RL-Transformer) and Convolutional Linear Attention Block (CLAB) to FC-DenseNet. TFCNs is not only able to utilize more latent information from the CT images for feature extraction, but also can capture and disseminate semantic features and filter non-semantic features more effectively through the CLAB module. Our experimental results show that TFCNs can achieve state-of-the-art performance with dice scores of 83.72% on the Synapse dataset. In addition, we evaluate the robustness of TFCNs for lesion area effects on the COVID-19 public datasets. The Python code will be made publicly available on https://github.com/HUANGLIZI/TFCNs.","This work was supported in part by the Natural Science Foundation of Fujian Province of China (No. 2020J01006), the National Natural Science Foundation of China (No. 61502402), and the Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University (No. VRLAB2022AC04).",,Lecture Notes in Computer Science,Artificial Neural Networks and Machine Learning – ICANN 2022,,2022-09-07,2022,2022-09-07,2022,13532,,781-792,All OA, Green,Chapter,"Li, Zihan; Li, Dihan; Xu, Cangbai; Wang, Weice; Hong, Qingqi; Li, Qingde; Tian, Jie","Li, Zihan (Xiamen University, 361005, Xiamen, China); Li, Dihan (Xiamen University, 361005, Xiamen, China); Xu, Cangbai (Xiamen University, 361005, Xiamen, China); Wang, Weice (Xiamen University, 361005, Xiamen, China); Hong, Qingqi (Xiamen University, 361005, Xiamen, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China); Li, Qingde (University of Hull, HU6 7RX, Hull, UK); Tian, Jie (Chinese Academy of Sciences, 100190, Beijing, China)","Hong, Qingqi (Xiamen University; Beihang University)","Li, Zihan (Xiamen University); Li, Dihan (Xiamen University); Xu, Cangbai (Xiamen University); Wang, Weice (Xiamen University); Hong, Qingqi (Xiamen University; Beihang University); Li, Qingde (University of Hull); Tian, Jie (Chinese Academy of Sciences)",3,3,,,http://arxiv.org/pdf/2207.03450,https://app.dimensions.ai/details/publication/pub.1150793511,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,3 Good Health and Well Being,,,,,,,,
1450,pub.1149334286,10.48550/arxiv.2207.03450,,,TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation,"Medical image segmentation is one of the most fundamental tasks concerning
medical information analysis. Various solutions have been proposed so far,
including many deep learning-based techniques, such as U-Net, FC-DenseNet, etc.
However, high-precision medical image segmentation remains a highly challenging
task due to the existence of inherent magnification and distortion in medical
images as well as the presence of lesions with similar density to normal
tissues. In this paper, we propose TFCNs (Transformers for Fully Convolutional
denseNets) to tackle the problem by introducing ResLinear-Transformer
(RL-Transformer) and Convolutional Linear Attention Block (CLAB) to
FC-DenseNet. TFCNs is not only able to utilize more latent information from the
CT images for feature extraction, but also can capture and disseminate semantic
features and filter non-semantic features more effectively through the CLAB
module. Our experimental results show that TFCNs can achieve state-of-the-art
performance with dice scores of 83.72\% on the Synapse dataset. In addition, we
evaluate the robustness of TFCNs for lesion area effects on the COVID-19 public
datasets. The Python code will be made publicly available on
https://github.com/HUANGLIZI/TFCNs.",,,arXiv,,,2022-07-07,2022,,,,,,All OA, Green,Preprint,"Li, Zihan; Li, Dihan; Xu, Cangbai; Wang, Weice; Hong, Qingqi; Li, Qingde; Tian, Jie","Li, Zihan (); Li, Dihan (); Xu, Cangbai (); Wang, Weice (); Hong, Qingqi (); Li, Qingde (); Tian, Jie ()",,"Li, Zihan (); Li, Dihan (); Xu, Cangbai (); Wang, Weice (); Hong, Qingqi (); Li, Qingde (); Tian, Jie ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149334286,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,3 Good Health and Well Being,,,,,,,,
1450,pub.1141301963,10.1007/978-3-030-87193-2_29,,,Context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation,"Despite their outstanding accuracy, semi-supervised segmentation methods based on deep neural networks can still yield predictions that are considered anatomically impossible by clinicians, for instance, containing holes or disconnected regions. To solve this problem, we present a Context-aware Virtual Adversarial Training (CaVAT) method for generating anatomically plausible segmentation. Unlike approaches focusing solely on accuracy, our method also considers complex topological constraints like connectivity which cannot be easily modeled in a differentiable loss function. We use adversarial training to generate examples violating the constraints, so the network can learn to avoid making such incorrect predictions on new examples, and employ the Reinforce algorithm to handle non-differentiable segmentation constraints. The proposed method offers a generic and efficient way to add any constraint on top of any segmentation network. Experiments on two clinically-relevant datasets show our method to produce segmentations that are both accurate and anatomically-plausible in terms of region connectivity.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12901,,304-314,All OA, Green,Chapter,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (Department of Software and IT Engineering, ETS, Montreal, Canada); Peng, Jizong (Department of Software and IT Engineering, ETS, Montreal, Canada); Pedersoli, Marco (Department of Software and IT Engineering, ETS, Montreal, Canada); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, China); Zhang, Caiming (School of Software, Shandong University, Jinan, China); Desrosiers, Christian (Department of Software and IT Engineering, ETS, Montreal, Canada)","Wang, Ping ","Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (Shandong University); Zhang, Caiming (Shandong University); Desrosiers, Christian ()",2,2,,1.64,http://arxiv.org/pdf/2107.05532,https://app.dimensions.ai/details/publication/pub.1141301963,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1450,pub.1141270750,10.48550/arxiv.2109.08852,,,Domain Composition and Attention for Unseen-Domain Generalizable Medical  Image Segmentation,"Domain generalizable model is attracting increasing attention in medical
image analysis since data is commonly acquired from different institutes with
various imaging protocols and scanners. To tackle this challenging domain
generalization problem, we propose a Domain Composition and Attention-based
network (DCA-Net) to improve the ability of domain representation and
generalization. First, we present a domain composition method that represents
one certain domain by a linear combination of a set of basis representations
(i.e., a representation bank). Second, a novel plug-and-play parallel domain
preceptor is proposed to learn these basis representations and we introduce a
divergence constraint function to encourage the basis representations to be as
divergent as possible. Then, a domain attention module is proposed to learn the
linear combination coefficients of the basis representations. The result of
linear combination is used to calibrate the feature maps of an input image,
which enables the model to generalize to different and even unseen domains. We
validate our method on public prostate MRI dataset acquired from six different
institutions with apparent domain shift. Experimental results show that our
proposed model can generalize well on different and even unseen domains and it
outperforms state-of-the-art methods on the multi-domain prostate segmentation
task.",,,arXiv,,,2021-09-18,2021,,,,,,All OA, Green,Preprint,"Gu, Ran; Zhang, Jingyang; Huang, Rui; Lei, Wenhui; Wang, Guotai; Zhang, Shaoting","Gu, Ran (); Zhang, Jingyang (); Huang, Rui (); Lei, Wenhui (); Wang, Guotai (); Zhang, Shaoting ()",,"Gu, Ran (); Zhang, Jingyang (); Huang, Rui (); Lei, Wenhui (); Wang, Guotai (); Zhang, Shaoting ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141270750,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1450,pub.1141301972,10.1007/978-3-030-87193-2_37,,,Comprehensive Importance-Based Selective Regularization for Continual Segmentation Across Multiple Sites,"In clinical practice, a desirable medical image segmentation model should be able to learn from sequential training data from multiple sites, as collecting these data together could be difficult due to the storage cost and privacy restriction. However, existing methods often suffer from catastrophic forgetting problem for previous sites when learning from images from a new site. In this paper, we propose a novel comprehensive importance-based selective regularization method for continual segmentation, aiming to mitigate model forgetting by maintaining both shape and reliable semantic knowledge for previous sites. Specifically, we define a comprehensive importance weight for each model parameter, which consists of shape-aware importance and uncertainty-guided semantics-aware importance, by measuring how a segmentation’s shape and reliable semantic information is sensitive to the parameter. When training model on a new site, we adopt a selective regularization scheme that penalizes changes of parameters with high comprehensive importance, avoiding the shape knowledge and reliable semantics related to previous sites being forgotten. We evaluate our method on prostate MRI data sequentially acquired from six institutes. Results show that our method outperforms many continual learning methods for relieving model forgetting issue. Code is available at https://github.com/jingyzhang/CISR.","This research is partially supported by the National Key research and development program (No. 2016YFC0106200), Beijing Natural Science Foundation-Haidian Original Innovation Collaborative Fund (No. L192006), and the funding from Institute of Medical Robotics of Shanghai Jiao Tong University as well as the 863 national research fund (No. 2015AA043203).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12901,,389-399,Closed,Chapter,"Zhang, Jingyang; Gu, Ran; Wang, Guotai; Gu, Lixu","Zhang, Jingyang (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China; SenseTime Research, Shanghai, China); Gu, Ran (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; SenseTime Research, Shanghai, China); Wang, Guotai (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China); Gu, Lixu (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China; SenseTime Research, Shanghai, China)","Gu, Lixu (Shanghai Jiao Tong University; Shanghai Jiao Tong University; )","Zhang, Jingyang (Shanghai Jiao Tong University; Shanghai Jiao Tong University); Gu, Ran (University of Electronic Science and Technology of China); Wang, Guotai (University of Electronic Science and Technology of China); Gu, Lixu (Shanghai Jiao Tong University; Shanghai Jiao Tong University)",10,10,,8.18,,https://app.dimensions.ai/details/publication/pub.1141301972,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1450,pub.1139802983,10.48550/arxiv.2107.08111,,,Federated Whole Prostate Segmentation in MRI with Personalized Neural  Architectures,"Building robust deep learning-based models requires diverse training data,
ideally from several sources. However, these datasets cannot be combined easily
because of patient privacy concerns or regulatory hurdles, especially if
medical data is involved. Federated learning (FL) is a way to train machine
learning models without the need for centralized datasets. Each FL client
trains on their local data while only sharing model parameters with a global
server that aggregates the parameters from all clients. At the same time, each
client's data can exhibit differences and inconsistencies due to the local
variation in the patient population, imaging equipment, and acquisition
protocols. Hence, the federated learned models should be able to adapt to the
local particularities of a client's data. In this work, we combine FL with an
AutoML technique based on local neural architecture search by training a
""supernet"". Furthermore, we propose an adaptation scheme to allow for
personalized model architectures at each FL client's site. The proposed method
is evaluated on four different datasets from 3D prostate MRI and shown to
improve the local models' performance after adaptation through selecting an
optimal path through the AutoML supernet.",,,arXiv,,,2021-07-16,2021,,,,,,All OA, Green,Preprint,"Roth, Holger R.; Yang, Dong; Li, Wenqi; Myronenko, Andriy; Zhu, Wentao; Xu, Ziyue; Wang, Xiaosong; Xu, Daguang","Roth, Holger R. (); Yang, Dong (); Li, Wenqi (); Myronenko, Andriy (); Zhu, Wentao (); Xu, Ziyue (); Wang, Xiaosong (); Xu, Daguang ()",,"Roth, Holger R. (); Yang, Dong (); Li, Wenqi (); Myronenko, Andriy (); Zhu, Wentao (); Xu, Ziyue (); Wang, Xiaosong (); Xu, Daguang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139802983,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,
1450,pub.1135111352,10.48550/arxiv.2102.01549,,,Medical Datasets Collections for Artificial Intelligence-based Medical  Image Analysis,"We collected 32 public datasets, of which 28 for medical imaging and 4 for
natural images, to conduct study. The images of these datasets are captured by
different cameras, thus vary from each other in modality, frame size and
capacity. For data accessibility, we also provide the websites of most datasets
and hope this will help the readers reach the datasets.",,,arXiv,,,2021-02-02,2021,,,,,,All OA, Green,Preprint,"Wen, Yang","Wen, Yang ()",,"Wen, Yang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1135111352,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1450,pub.1110303318,10.1109/icpr.2018.8545341,,,Automatic Prostate Segmentation on MR Images Using Enhanced Holistically-Nested Networks,"Magnetic resonance(MR) imaging has shown to be succeed in detecting and visualizing the prostate location. The accurate segmentation of the prostate gland from MR images is necessary for clinical applications. However, the segmentation of prostate is also a challenging task because of the shape of prostate varies significantly and the inhomogeneous intensity distributions in different scans. In this paper, we present an automatic deep learning method for prostate MR images segmentation using the enhanced holistically-nested framework. The network Holistically-Nested Networks(HNN) was first proposed as an image-to-image solution to extract object edges and boundaries visually. We modify HNN via putting additional skip connections from later stages to early stages in order to combine both low-level features and high-level features. The deeper framework exploits multi-level and multi-scale information for the image-to-image prediction in a holistic manner. Experimental evaluation demonstrates that significant segmentation accuracy has been achieved by our proposed enhanced holistically-nested networks compared to other deep learning approaches.",This work was supported by National Nature Science Foundation of China Grand No:61371156. The authors would like to thank the anonymous reviews for their helpful and constructive comments and suggestions regarding this manuscript. This work was supported by National Nature Science Foundation of China Grand No:61371156. The authors would like to thank the anonymous reviews for their helpful and constructive comments and suggestions regarding this manuscript.,,,2018 24th International Conference on Pattern Recognition (ICPR),,2018-08-20,2018,,2018-08-20,0,,3820-3825,Closed,Proceeding,"Jit, Dong; Qian, Jinzhao; Yu, Jun; Kurihara, Toru; Zhan, Shu","Jit, Dong (School of Computer and Information Hefei, University of Technology, Hefei, China, 230601); Qian, Jinzhao (Department of Automation, Tsinghua University, Beijing, China, 100084); Yu, Jun (School of Information, Kochi University of Technology, Kochi, Japan, 782-8502); Kurihara, Toru (School of Information, Kochi University of Technology, Kochi, Japan, 782-8502); Zhan, Shu (School of Computer and Information Hefei, University of Technology, Hefei, China, 230601)",,"Jit, Dong (Hefei University of Technology); Qian, Jinzhao (Tsinghua University); Yu, Jun (Kochi University of Technology); Kurihara, Toru (Kochi University of Technology); Zhan, Shu (Hefei University of Technology)",1,0,,0.34,,https://app.dimensions.ai/details/publication/pub.1110303318,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences, 51 Physical Sciences,,,,,,,,,
1409,pub.1131399173,10.1007/978-3-030-59713-9_46,,,Shape-Aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains,"Model generalization capacity at domain shift (e.g., various imaging protocols and scanners) is crucial for deep learning methods in real-world clinical deployment. This paper tackles the challenging problem of domain generalization, i.e., learning a model from multi-domain source data such that it can directly generalize to an unseen target domain. We present a novel shape-aware meta-learning scheme to improve the model generalization in prostate MRI segmentation. Our learning scheme roots in the gradient-based meta-learning, by explicitly simulating domain shift with virtual meta-train and meta-test during training. Importantly, considering the deficiencies encountered when applying a segmentation model to unseen domains (i.e., incomplete shape and ambiguous boundary of the prediction masks), we further introduce two complementary loss objectives to enhance the meta-optimization, by particularly encouraging the shape compactness and shape smoothness of the segmentations under simulated domain shift. We evaluate our method on prostate MRI data from six different institutions with distribution shifts acquired from public datasets. Experimental results show that our approach outperforms many state-of-the-art generalization methods consistently across all six settings of unseen domains (Code and dataset are available at https://github.com/liuquande/SAML).","This work was supported in parts by the following grants: Key-Area Research and Development Program of Guangdong Province, China (2020B010165004), Hong Kong Innovation and Technology Fund (Project No. ITS/426/17FP), Hong Kong RGC TRS Project T42-409/18-R, and National Natural Science Foundation of China with Project No. U1813204.",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12262,,475-485,All OA, Green,Chapter,"Liu, Quande; Dou, Qi; Heng, Pheng-Ann","Liu, Quande (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong SAR, China); Dou, Qi (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong SAR, China; T Stone Robotics Institute, The Chinese University of Hong Kong, Shatin, Hong Kong SAR, China); Heng, Pheng-Ann (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong SAR, China; Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China)","Dou, Qi (Chinese University of Hong Kong; Chinese University of Hong Kong)","Liu, Quande (Chinese University of Hong Kong); Dou, Qi (Chinese University of Hong Kong; Chinese University of Hong Kong); Heng, Pheng-Ann (Chinese University of Hong Kong; Shenzhen Institutes of Advanced Technology)",67,67,,34.53,http://arxiv.org/pdf/2007.02035,https://app.dimensions.ai/details/publication/pub.1131399173,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1409,pub.1121091045,10.1007/978-981-13-8950-4_25,,,CNN-Based Prostate Zonal Segmentation on T2-Weighted MR Images: A Cross-Dataset Study,"Prostate Rundo, Leonardo is the most common cancer among US men. However, Han, Changhee imaging is still challenging despite the advances in multi-parametric magnetic resonance imaging (MRI), which provides both morphologic and functional information pertaining to the pathological regions. Zhang, Jin with whole prostate gland segmentation, Hataya, Ryuichiro between the Nagano, Yudai gland (CG) and Militello, Carmelo zone (PZ) can Ferretti, Claudio toward differential diagnosis, Nobile, Marco S. the frequency Tangherloni, Andrea severity of tumors Gilardi, Maria Carla in these regions; however, Vitabile, Salvatore boundary is often Nakayama, Hideki and fuzzy. This work Mauri, Giancarlo a preliminary study on deep learning to automatically delineate the CG and PZ, aiming at evaluating the generalization ability of convolutional neural networks (CNNs) on two multi-centric MRI prostate datasets. Especially, we compared three CNN-based architectures: SegNet, U-Net, and pix2pix. In such a context, the segmentation performances achieved with/without pre-training were compared in 4-fold cross-validation. In general, U-Net outperforms the other methods, especially when training and testing are performed on multiple datasets.","This work was partially supported by the Graduate Program for Social ICT Global Creative Leaders of The University of Tokyo by JSPS. We thank the Cannizzaro Hospital, Catania, Italy, for providing one of the imaging datasets analyzed in this study.",,"Smart Innovation, Systems and Technologies",Neural Approaches to Dynamics of Signal Exchanges,,2019-09-19,2019,2019-09-19,2020,151,,269-280,All OA, Green,Chapter,"Rundo, Leonardo; Han, Changhee; Zhang, Jin; Hataya, Ryuichiro; Nagano, Yudai; Militello, Carmelo; Ferretti, Claudio; Nobile, Marco S.; Tangherloni, Andrea; Gilardi, Maria Carla; Vitabile, Salvatore; Nakayama, Hideki; Mauri, Giancarlo","Rundo, Leonardo (Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milan, Italy; Institute of Molecular Bioimaging and Physiology (IBFM), Italian National Research Council (CNR), Cefalù (PA), Italy); Han, Changhee (Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan); Zhang, Jin (Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan); Hataya, Ryuichiro (Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan); Nagano, Yudai (Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan); Militello, Carmelo (Institute of Molecular Bioimaging and Physiology (IBFM), Italian National Research Council (CNR), Cefalù (PA), Italy); Ferretti, Claudio (Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milan, Italy); Nobile, Marco S. (Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milan, Italy); Tangherloni, Andrea (Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milan, Italy); Gilardi, Maria Carla (Institute of Molecular Bioimaging and Physiology (IBFM), Italian National Research Council (CNR), Cefalù (PA), Italy); Vitabile, Salvatore (Department of Biopathology and Medical Biotechnologies, University of Palermo, Palermo, Italy); Nakayama, Hideki (Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan); Mauri, Giancarlo (Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milan, Italy)","Rundo, Leonardo (University of Milano-Bicocca; Institute of Molecular Bioimaging and Physiology)","Rundo, Leonardo (University of Milano-Bicocca; Institute of Molecular Bioimaging and Physiology); Han, Changhee (University of Tokyo); Zhang, Jin (University of Tokyo); Hataya, Ryuichiro (University of Tokyo); Nagano, Yudai (University of Tokyo); Militello, Carmelo (Institute of Molecular Bioimaging and Physiology); Ferretti, Claudio (University of Milano-Bicocca); Nobile, Marco S. (University of Milano-Bicocca); Tangherloni, Andrea (University of Milano-Bicocca); Gilardi, Maria Carla (Institute of Molecular Bioimaging and Physiology); Vitabile, Salvatore (University of Palermo); Nakayama, Hideki (University of Tokyo); Mauri, Giancarlo (University of Milano-Bicocca)",34,29,,12.01,http://arxiv.org/pdf/1903.12571,https://app.dimensions.ai/details/publication/pub.1121091045,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,
1409,pub.1104208998,10.20535/srit.2308-8893.2018.1.05,,,Medical image segmentation methods overview,"This article provides an overview of the modern medical image segmentation methods. The most popular methods such as multi-atlas based methods and deep learning approach are considered in more details. In addition, this article overviews different steps of the multi-atlas based methods (MAS) in detail and shows which modern algorithms and approaches used in different steps of MAS to achieve state of the art results in the medical image segmentation task and how it affects the accuracy of the algorithm. Also, there is a brief description of the modern deep learning algorithms which are used for the medical image segmentation. Such type of algorithm is used as an independent algorithm or as a part of the MAS. Finally, this article summarizes described algorithms and evaluate which approaches promise to improve state of the art result of the medical image segmentation in the future.",,,System research and information technologies,,,2018-03-20,2018,2018-03-20,,0,1,72-81,All OA, Gold,Article,"Chapaliuk, Bohdan V.; Zaychenko, Yuriy P.","Chapaliuk, Bohdan V. (The Department of the Mathematical Methods of System Analysis of the Educational and Scientific Complex ""Institute for Applied System Analysis"" of National Technical University of Ukraine ""Igor Sikorsky Kyiv Polytechnic Institute"", Kyiv); Zaychenko, Yuriy P. (The Department of the Mathematical Methods of System Analysis of the Educational and Scientific Complex ""Institute for Applied System Analysis"" of National Technical University of Ukraine ""Igor Sikorsky Kyiv Polytechnic Institute"", Kyiv)",,"Chapaliuk, Bohdan V. (National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”); Zaychenko, Yuriy P. (National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”)",0,0,,0.0,http://journal.iasa.kpi.ua/article/download/126679/123509,https://app.dimensions.ai/details/publication/pub.1104208998,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1407,pub.1151033009,10.1007/978-3-031-16443-9_37,,,Learning Towards Synchronous Network Memorizability and Generalizability for Continual Segmentation Across Multiple Sites,"In clinical practice, a segmentation network is often required to continually learn on a sequential data stream from multiple sites rather than a consolidated set, due to the storage cost and privacy restriction. However, during the continual learning process, existing methods are usually restricted in either network memorizability on previous sites or generalizability on unseen sites. This paper aims to tackle the challenging problem of Synchronous Memorizability and Generalizability (SMG) and to simultaneously improve performance on both previous and unseen sites, with a novel proposed SMG-learning framework. First, we propose a Synchronous Gradient Alignment (SGA) objective, which not only promotes the network memorizability by enforcing coordinated optimization for a small exemplar set from previous sites (called replay buffer), but also enhances the generalizability by facilitating site-invariance under simulated domain shift. Second, to simplify the optimization of SGA objective, we design a Dual-Meta algorithm that approximates the SGA objective as dual meta-objectives for optimization without expensive computation overhead. Third, for efficient rehearsal, we configure the replay buffer comprehensively considering additional inter-site diversity to reduce redundancy. Experiments on prostate MRI data sequentially acquired from six institutes demonstrate that our method can simultaneously achieve higher memorizability and generalizability over state-of-the-art methods. Code is available at https://github.com/jingyzhang/SMG-Learning.","This work was supported in part by National Natural Science Foundation of China (grant number 62131015), and Science and Technology Commission of Shanghai Municipality (STCSM) (grant number 21010502600).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13435,,380-390,All OA, Green,Chapter,"Zhang, Jingyang; Xue, Peng; Gu, Ran; Gu, Yuning; Liu, Mianxin; Pan, Yongsheng; Cui, Zhiming; Huang, Jiawei; Ma, Lei; Shen, Dinggang","Zhang, Jingyang (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Xue, Peng (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Gu, Ran (School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China); Gu, Yuning (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Liu, Mianxin (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Pan, Yongsheng (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Cui, Zhiming (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China); Huang, Jiawei (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Ma, Lei (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China); Shen, Dinggang (School of Biomedical Engineering, ShanghaiTech University, Shanghai, China; Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China)","Shen, Dinggang (ShanghaiTech University; )","Zhang, Jingyang (ShanghaiTech University); Xue, Peng (ShanghaiTech University); Gu, Ran (University of Electronic Science and Technology of China); Gu, Yuning (ShanghaiTech University); Liu, Mianxin (ShanghaiTech University); Pan, Yongsheng (ShanghaiTech University); Cui, Zhiming (ShanghaiTech University; University of Hong Kong); Huang, Jiawei (ShanghaiTech University); Ma, Lei (ShanghaiTech University); Shen, Dinggang (ShanghaiTech University)",0,0,,,http://arxiv.org/pdf/2206.06813,https://app.dimensions.ai/details/publication/pub.1151033009,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1405,pub.1141326771,10.1007/978-3-030-87196-3_42,,,Tripled-Uncertainty Guided Mean Teacher Model for Semi-supervised Medical Image Segmentation,"Due to the difficulty in accessing a large amount of labeled data, semi-supervised learning is becoming an attractive solution in medical image segmentation. To make use of unlabeled data, current popular semi-supervised methods (e.g., temporal ensembling, mean teacher) mainly impose data-level and model-level consistency on unlabeled data. In this paper, we argue that in addition to these strategies, we could further utilize auxiliary tasks and consider task-level consistency to better leverage unlabeled data for segmentation. Specifically, we introduce two auxiliary tasks, i.e., a foreground and background reconstruction task for capturing semantic information and a signed distance field (SDF) prediction task for imposing shape constraint, and explore the mutual promotion effect between the two auxiliary and the segmentation tasks based on mean teacher architecture. Moreover, to handle the potential bias of the teacher model caused by annotation scarcity, we develop a tripled-uncertainty guided framework to encourage the three tasks in the teacher model to generate more reliable pseudo labels. When calculating uncertainty, we propose an uncertainty weighted integration (UWI) strategy for yielding the segmentation predictions of the teacher. Extensive experiments on public 2017 ACDC dataset and PROMISE12 dataset have demostrated the effectiveness of our method. Code is available at https://github.com/DeepMedLab/Tri-U-MT.","This work is supported by National Natural Science Foundation of China (NFSC 62071314) and Sichuan Science and Technology Program (2021YFG0326, 2020YFG0079).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,,2021-09-21,2021,2021-09-21,2021,12902,,450-460,Closed,Chapter,"Wang, Kaiping; Zhan, Bo; Zu, Chen; Wu, Xi; Zhou, Jiliu; Zhou, Luping; Wang, Yan","Wang, Kaiping (School of Computer Science, Sichuan University, Chengdu, China); Zhan, Bo (School of Computer Science, Sichuan University, Chengdu, China); Zu, Chen (Department of Risk Controlling Research, JD.COM, Chengdu, China); Wu, Xi (School of Computer Science, Chengdu University of Information Technology, Chengdu, China); Zhou, Jiliu (School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, China); Zhou, Luping (School of Electrical and Information Engineering, University of Sydney, Sydney, Australia); Wang, Yan (School of Computer Science, Sichuan University, Chengdu, China)","Wang, Yan (Sichuan University)","Wang, Kaiping (Sichuan University); Zhan, Bo (Sichuan University); Zu, Chen (Jingdong (China)); Wu, Xi (Chengdu University of Information Technology); Zhou, Jiliu (Sichuan University; Chengdu University of Information Technology); Zhou, Luping (The University of Sydney); Wang, Yan (Sichuan University)",22,22,,18.01,,https://app.dimensions.ai/details/publication/pub.1141326771,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1405,pub.1129418683,10.48550/arxiv.2007.08238,,,U-Net Based Architecture for an Improved Multiresolution Segmentation in  Medical Images,"Purpose: Manual medical image segmentation is an exhausting and
time-consuming task along with high inter-observer variability. In this study,
our objective is to improve the multi-resolution image segmentation performance
of U-Net architecture. Approach: We have proposed a fully convolutional neural
network for image segmentation in a multi-resolution framework. We used U-Net
as the base architecture and modified that to improve its image segmentation
performance. In the proposed architecture (mrU-Net), the input image and its
down-sampled versions were used as the network inputs. We added more
convolution layers to extract features directly from the down-sampled images.
We trained and tested the network on four different medical datasets, including
skin lesion photos, lung computed tomography (CT) images (LUNA dataset), retina
images (DRIVE dataset), and prostate magnetic resonance (MR) images (PROMISE12
dataset). We compared the performance of mrU-Net to U-Net under similar
training and testing conditions. Results: Comparing the results to manual
segmentation labels, mrU-Net achieved average Dice similarity coefficients of
70.6%, 97.9%, 73.6%, and 77.9% for the skin lesion, LUNA, DRIVE, and PROMISE12
segmentation, respectively. For the skin lesion, LUNA, and DRIVE datasets,
mrU-Net outperformed U-Net with significantly higher accuracy and for the
PROMISE12 dataset, both networks achieved similar accuracy. Furthermore, using
mrU-Net led to a faster training rate on LUNA and DRIVE datasets when compared
to U-Net. Conclusions: The striking feature of the proposed architecture is its
higher capability in extracting image-derived features compared to U-Net.
mrU-Net illustrated a faster training rate and slightly more accurate image
segmentation compared to U-Net.",,,arXiv,,,2020-07-16,2020,,,,,,All OA, Green,Preprint,"Jahangard, Simindokht; Zangooei, Mohammad Hossein; Shahedi, Maysam","Jahangard, Simindokht (); Zangooei, Mohammad Hossein (); Shahedi, Maysam ()",,"Jahangard, Simindokht (); Zangooei, Mohammad Hossein (); Shahedi, Maysam ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129418683,46 Information and Computing Sciences, 4605 Data Management and Data Science, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
1405,pub.1121402371,10.1109/ijcnn.2019.8851908,,,Prostate Segmentation using 2D Bridged U-net,"In this paper, we focus on three problems in deep learning based medical image segmentation. Firstly, U-net, as a popular model for medical image segmentation, is difficult to train when convolutional layers increase even though a deeper network usually has a better generalization ability because of more learnable parameters. Secondly, the exponential ReLU (ELU), as an alternative of ReLU, is not much different from ReLU when the network of interest gets deep. Thirdly, the Dice loss, as one of the pervasive loss functions for medical image segmentation, is not effective when the prediction is close to ground truth and will cause oscillation during training. To address the aforementioned three problems, we propose and validate a deeper network that can fit medical image datasets that are usually small in the sample size. Meanwhile, we propose a new loss function to accelerate the learning process and a combination of different activation functions to improve the network performance. Our experimental results suggest that our network is comparable or superior to state-of-the-art methods.",,,,2019 International Joint Conference on Neural Networks (IJCNN),,2019-07-19,2019,,2019-07-19,0,,1-7,All OA, Green,Proceeding,"Chen, Wanli; Zhang, Yue; He, Junjun; Qiao, Yu; Chen, Yifan; Shi, Hongjian; Wu, Ed X.; Tang, Xiaoying","Chen, Wanli (Southern University of Science and Technology, Shenzhen, China); Zhang, Yue (Southern University of Science and Technology, Shenzhen, China; The University of Hong Kong, Hong Kong, China); He, Junjun (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China); Qiao, Yu (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China); Chen, Yifan (The University of Waikato, Hamilton, New Zealand); Shi, Hongjian (Southern University of Science and Technology, Shenzhen, China); Wu, Ed X. (The University of Hong Kong, Hong Kong, China); Tang, Xiaoying (Southern University of Science and Technology, Shenzhen, China)","Chen, Wanli (Southern University of Science and Technology)","Chen, Wanli (Southern University of Science and Technology); Zhang, Yue (Southern University of Science and Technology; University of Hong Kong); He, Junjun (Shenzhen Institutes of Advanced Technology); Qiao, Yu (Shenzhen Institutes of Advanced Technology); Chen, Yifan (University of Waikato); Shi, Hongjian (Southern University of Science and Technology); Wu, Ed X. (University of Hong Kong); Tang, Xiaoying (Southern University of Science and Technology)",25,17,,9.58,http://arxiv.org/pdf/1807.04459,https://app.dimensions.ai/details/publication/pub.1121402371,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1404,pub.1134221982,10.1109/rcar49640.2020.9303035,,,Self-Paced Learning for Automatic Prostate Segmentation on MR Images with Hierarchical Boundary Sensitive Network,"Accurate segmentation of Magnetic Resonance (MR) on prostate is an essential step for robotics surgery in prostate cancer treatment planning. This paper proposes a Hierarchical Boundary Sensitive Residual U-net (HBS-RUnet) model with self-paced learning strategy for prostate segmentation in MR image. Instead of regarding the segmentation task independently, our network consists of two branches: one segmentation branch detects the prostate region and the boundary branch finds prostate shape. The outputs of boundary branch are employed to refine the HBS-RUnet model by adding a boundary regularization, which helps to find desirable and spatially consistent prostate region. Moreover, a hierarchical dynamic self-paced learning strategy is proposed to measure the difficulty for each prostate image and gradually select the relatively simpler samples for model training. Such a simple-to-complex learning strategy could robustly learn image features and enable the robust prostate segmentation. We applied 66 cases from the PROSTATEx Challenge to evaluate the robustness and effectiveness of the proposed HBS-RUnet, and our fully automatic segmentation results demonstrate high consistency (DSC 87.1%) with the manual segmentation results by experienced physicians.","The authors thank the AAPM2017 PROSTATEx challenge for providing the dataset. This work is supported by Shenzhen-Hong Kong Innovation Circle Category D Project SGDX2019081623300177, 9240008, and Shenzhen basic technology research project JCYJ20170818160306270.",,,2020 IEEE International Conference on Real-time Computing and Robotics (RCAR),,2020-01-29,2020,,2020-01-29,0,,321-326,Closed,Proceeding,"Qin, Wenjian; Xiao, Zhibo; Xie, Yaoqin; Yuan, Yixuan","Qin, Wenjian (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China); Xiao, Zhibo (Department of Radiology, the First Affiliated Hospital, Chongqing Medical University, Chongqing, China); Xie, Yaoqin (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China); Yuan, Yixuan (Department of Electrical Engineering, City University of Hong Kong, Hong Kong, China)","Yuan, Yixuan (City University of Hong Kong)","Qin, Wenjian (Shenzhen Institutes of Advanced Technology); Xiao, Zhibo (Chongqing Medical University); Xie, Yaoqin (Shenzhen Institutes of Advanced Technology); Yuan, Yixuan (City University of Hong Kong)",2,2,,0.85,,https://app.dimensions.ai/details/publication/pub.1134221982,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
1403,pub.1137749297,10.48550/arxiv.2105.00859,,,Beyond pixel-wise supervision for segmentation: A few global shape  descriptors might be surprisingly good!,"Standard losses for training deep segmentation networks could be seen as
individual classifications of pixels, instead of supervising the global shape
of the predicted segmentations. While effective, they require exact knowledge
of the label of each pixel in an image.
  This study investigates how effective global geometric shape descriptors
could be, when used on their own as segmentation losses for training deep
networks. Not only interesting theoretically, there exist deeper motivations to
posing segmentation problems as a reconstruction of shape descriptors:
Annotations to obtain approximations of low-order shape moments could be much
less cumbersome than their full-mask counterparts, and anatomical priors could
be readily encoded into invariant shape descriptions, which might alleviate the
annotation burden. Also, and most importantly, we hypothesize that, given a
task, certain shape descriptions might be invariant across image acquisition
protocols/modalities and subject populations, which might open interesting
research avenues for generalization in medical image segmentation.
  We introduce and formulate a few shape descriptors in the context of deep
segmentation, and evaluate their potential as standalone losses on two
different challenging tasks. Inspired by recent works in constrained
optimization for deep networks, we propose a way to use those descriptors to
supervise segmentation, without any pixel-level label. Very surprisingly, as
little as 4 descriptors values per class can approach the performance of a
segmentation mask with 65k individual discrete labels. We also found that shape
descriptors can be a valid way to encode anatomical priors about the task,
enabling to leverage expert knowledge without additional annotations. Our
implementation is publicly available and can be easily extended to other tasks
and descriptors: https://github.com/hkervadec/shape_descriptors",,,arXiv,,,2021-05-03,2021,,,,,,All OA, Green,Preprint,"Kervadec, Hoel; Bahig, Houda; Letourneau-Guillon, Laurent; Dolz, Jose; Ayed, Ismail Ben","Kervadec, Hoel (); Bahig, Houda (); Letourneau-Guillon, Laurent (); Dolz, Jose (); Ayed, Ismail Ben ()",,"Kervadec, Hoel (); Bahig, Houda (); Letourneau-Guillon, Laurent (); Dolz, Jose (); Ayed, Ismail Ben ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1137749297,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
1403,pub.1119960148,10.48550/arxiv.1907.11587,,,Self-Adaptive 2D-3D Ensemble of Fully Convolutional Networks for Medical  Image Segmentation,"Segmentation is a critical step in medical image analysis. Fully
Convolutional Networks (FCNs) have emerged as powerful segmentation models
achieving state-of-the-art results in various medical image datasets. Network
architectures are usually designed manually for a specific segmentation task so
applying them to other medical datasets requires extensive experience and time.
Moreover, the segmentation requires handling large volumetric data that results
in big and complex architectures. Recently, methods that automatically design
neural networks for medical image segmentation have been presented; however,
most approaches either do not fully consider volumetric information or do not
optimize the size of the network. In this paper, we propose a novel
self-adaptive 2D-3D ensemble of FCNs for medical image segmentation that
incorporates volumetric information and optimizes both the model's performance
and size. The model is composed of an ensemble of a 2D FCN that extracts
intra-slice information, and a 3D FCN that exploits inter-slice information.
The architectures of the 2D and 3D FCNs are automatically adapted to a medical
image dataset using a multiobjective evolutionary based algorithm that
minimizes both the segmentation error and number of parameters in the network.
The proposed 2D-3D FCN ensemble was tested on the task of prostate segmentation
on the image dataset from the PROMISE12 Grand Challenge. The resulting network
is ranked in the top 10 submissions, surpassing the performance of other
automatically-designed architectures while being considerably smaller in size.",,,arXiv,,,2019-07-26,2019,,,,,,All OA, Green,Preprint,"Calisto, Maria G. Baldeon; Lai-Yuen, Susana K.","Calisto, Maria G. Baldeon (); Lai-Yuen, Susana K. ()",,"Calisto, Maria G. Baldeon (); Lai-Yuen, Susana K. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119960148,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
1402,pub.1151072754,10.1007/978-3-031-16449-1_66,,,Estimating Model Performance Under Domain Shifts with Class-Specific Confidence Scores,"Machine learning models are typically deployed in a test setting that differs from the training setting, potentially leading to decreased model performance because of domain shift. If we could estimate the performance that a pre-trained model would achieve on data from a specific deployment setting, for example a certain clinic, we could judge whether the model could safely be deployed or if its performance degrades unacceptably on the specific data. Existing approaches estimate this based on the confidence of predictions made on unlabeled test data from the deployment’s domain. We find existing methods struggle with data that present class imbalance, because the methods used to calibrate confidence do not account for bias induced by class imbalance, consequently failing to estimate class-wise accuracy. Here, we introduce class-wise calibration within the framework of performance estimation for imbalanced datasets. Specifically, we derive class-specific modifications of state-of-the-art confidence-based model evaluation methods including temperature scaling (TS), difference of confidences (DoC), and average thresholded confidence (ATC). We also extend the methods to estimate Dice similarity coefficient (DSC) in image segmentation. We conduct experiments on four tasks and find the proposed modifications consistently improve the estimation accuracy for imbalanced datasets. Our methods improve accuracy estimation by 18% in classification under natural domain shifts, and double the estimation accuracy on segmentation tasks, when compared with prior methods (Code is available at https://github.com/ZerojumpLine/ModelEvaluationUnderClassImbalance).","ZL is grateful for a China Scholarship Council (CSC) Imperial Scholarship. This project has received funding from the ERC under the EU’s Horizon 2020 research and innovation programme (grant No 757173) and the UKRI London Medical Imaging &amp; Artificial Intelligence Centre for Value Based Healthcare, and a EPSRC Programme Grant (EP/P001009/1).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-17,2022,2022-09-17,2022,13437,,693-703,All OA, Green,Chapter,"Li, Zeju; Kamnitsas, Konstantinos; Islam, Mobarakol; Chen, Chen; Glocker, Ben","Li, Zeju (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Kamnitsas, Konstantinos (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, UK; School of Computer Science, University of Birmingham, Birmingham, UK); Islam, Mobarakol (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Chen, Chen (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Glocker, Ben (BioMedIA Group, Department of Computing, Imperial College London, London, UK)","Li, Zeju (Imperial College London)","Li, Zeju (Imperial College London); Kamnitsas, Konstantinos (Imperial College London; University of Oxford; University of Birmingham); Islam, Mobarakol (Imperial College London); Chen, Chen (Imperial College London); Glocker, Ben (Imperial College London)",0,0,,,http://arxiv.org/pdf/2207.09957,https://app.dimensions.ai/details/publication/pub.1151072754,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1402,pub.1148688697,10.48550/arxiv.2206.06813,,,Learning towards Synchronous Network Memorizability and Generalizability  for Continual Segmentation across Multiple Sites,"In clinical practice, a segmentation network is often required to continually
learn on a sequential data stream from multiple sites rather than a
consolidated set, due to the storage cost and privacy restriction. However,
during the continual learning process, existing methods are usually restricted
in either network memorizability on previous sites or generalizability on
unseen sites. This paper aims to tackle the challenging problem of Synchronous
Memorizability and Generalizability (SMG) and to simultaneously improve
performance on both previous and unseen sites, with a novel proposed
SMG-learning framework. First, we propose a Synchronous Gradient Alignment
(SGA) objective, which not only promotes the network memorizability by
enforcing coordinated optimization for a small exemplar set from previous sites
(called replay buffer), but also enhances the generalizability by facilitating
site-invariance under simulated domain shift. Second, to simplify the
optimization of SGA objective, we design a Dual-Meta algorithm that
approximates the SGA objective as dual meta-objectives for optimization without
expensive computation overhead. Third, for efficient rehearsal, we configure
the replay buffer comprehensively considering additional inter-site diversity
to reduce redundancy. Experiments on prostate MRI data sequentially acquired
from six institutes demonstrate that our method can simultaneously achieve
higher memorizability and generalizability over state-of-the-art methods. Code
is available at https://github.com/jingyzhang/SMG-Learning.",,,arXiv,,,2022-06-14,2022,,,,,,All OA, Green,Preprint,"Zhang, Jingyang; Xue, Peng; Gu, Ran; Gu, Yuning; Liu, Mianxin; Pan, Yongsheng; Cui, Zhiming; Huang, Jiawei; Ma, Lei; Shen, Dinggang","Zhang, Jingyang (); Xue, Peng (); Gu, Ran (); Gu, Yuning (); Liu, Mianxin (); Pan, Yongsheng (); Cui, Zhiming (); Huang, Jiawei (); Ma, Lei (); Shen, Dinggang ()",,"Zhang, Jingyang (); Xue, Peng (); Gu, Ran (); Gu, Yuning (); Liu, Mianxin (); Pan, Yongsheng (); Cui, Zhiming (); Huang, Jiawei (); Ma, Lei (); Shen, Dinggang ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148688697,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1402,pub.1145013863,10.1007/s11760-021-02115-w,,,Multiscale transunet + + : dense hybrid U-Net with transformer for medical image segmentation,"Automatic medical image segmentation as assistance to doctors is important for diagnosis and treatment of various diseases. TransUNet that integrates the advantages of transformer and CNN has achieved success in medical image segmentation tasks. However, TransUNet simply combines feature maps between encoder and decoder via skip connections at the same resolution, which leads to be an unnecessarily restrictive fusion design. Moreover, the positional encoding and input tokens in standard transformer blocks of TransUNet have a fixed scale, which are not suitable for dense prediction. To alleviate the above problems, in this paper, we propose a novel architecture named multiscale TransUNet +  + (MS-TransUNet + +), which employs a multiscale and flexible feature fusion scheme between encoder and decoder at different levels. The novel skip connections densely bridge the extracted feature representations with different resolutions, and the hybrid CNN-Transformer encoder with long-range dependencies directly passes the high-level features to each stage of decoder. Besides, in order to obtain more effective feature representations, an efficient multi-scale visual transformer is introduced for feature encoder. More importantly, we employ a weighted loss function composed of focal, multiscale structure similarity and Jaccard index to penalize the training error of medical image segmentation, jointly realizing pixel-level, patch-level and map-level optimization. Extensive experimental results demonstrate that our proposed multiscale TransUNet +  + can achieve competitive performance for prostate MR and liver CT image segmentation.",This work was supported by the National Natural Science Foundation of China (No. 62041108), the Natural Science Foundation of Ningxia (No. 2020AAC03029), Innovation and Entrepreneurship Project for Returnees in Ningxia 2020.,This work was supported by the National Natural Science Foundation of China (No. 62041108), the Natural Science Foundation of Ningxia (No. 2020AAC03029), Innovation and Entrepreneurship Project for Returnees in Ningxia 2020.,"Signal, Image and Video Processing",,,2022-01-27,2022,2022-01-27,2022-09,16,6,1607-1614,Closed,Article,"Wang, Bo; Wang, ·Fan; Dong, Pengwei; Li, ·Chongyi","Wang, Bo (School of Physics and Electronic-Electrical Engineering, Ningxia University, 750021, Yinchuan, People’s Republic of China); Wang, ·Fan (School of Physics and Electronic-Electrical Engineering, Ningxia University, 750021, Yinchuan, People’s Republic of China); Dong, Pengwei (School of Physics and Electronic-Electrical Engineering, Ningxia University, 750021, Yinchuan, People’s Republic of China); Li, ·Chongyi (School of Computer Science and Engineering, Nanyang Technological University, 639798, Singapore, Singapore)","Wang, Bo (Ningxia University)","Wang, Bo (Ningxia University); Wang, ·Fan (Ningxia University); Dong, Pengwei (Ningxia University); Li, ·Chongyi (Nanyang Technological University)",5,5,,,,https://app.dimensions.ai/details/publication/pub.1145013863,40 Engineering, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,,,,,
1402,pub.1144654092,10.1109/bigdata52589.2021.9672031,,,DRA U-Net: An Attention based U-Net Framework for 2D Medical Image Segmentation,"Limited by the size of the dataset, deep learning models for medical image analysis are usually difficult to train well, and the complex deep learning model with large amount of trainable parameters can not achieve good results. At the same time, due to the lack of clear boundaries, especially in the root tips and roots, as well as the huge differences in shape and texture between images from different patients, an overly simple model cannot accurately segment organs. In order to improve the accuracy of organ segmentation for prostate region detection, in this paper we propose an attention based U-Net framework, which includes an attention mechanism and residual feature extraction network. In addition, we also design an improved loss function to improve the training effect for organ segmentation. We conduct several batches of experiments with the prostate dataset PROMISE12 and the pneumothorax dataset SIIM, the experimental results show that significant segmentation accuracy improvement has been achieved by our proposed method compared to other reported approaches.","This work was supported in part by the Beijing Natural Science Foundation-Haidian Original Innovation Joint Fund Project (No.L182034), the National Natural Science Foundation of China (No.61802022 and No.61802027), and the Fundamental Research Funds for the Central Universities (No.2019XD-A12 and No.2020RC07). This work was supported in part by the Beijing Natural Science Foundation-Haidian Original Innovation Joint Fund Project (No.L182034), the National Natural Science Foundation of China (No.61802022 and No.61802027), and the Fundamental Research Funds for the Central Universities (No.2019XD-A12 and No.2020RC07).",,,2021 IEEE International Conference on Big Data (Big Data),,2021-12-18,2021,,2021-12-18,0,,3936-3942,Closed,Proceeding,"Zhang, Xian; Feng, Ziyuan; Zhong, Tianchi; Shen, Sicheng; Zhang, Ruolin; Zhou, Lijie; Zhang, Bo; Wang, Wendong","Zhang, Xian (Beijing Univ. of Posts and Telecomm., Beijing, China); Feng, Ziyuan (Beijing Univ. of Posts and Telecomm., Beijing, China); Zhong, Tianchi (Beijing Univ. of Posts and Telecomm., Beijing, China); Shen, Sicheng (Beijing Univ. of Posts and Telecomm., Beijing, China); Zhang, Ruolin (Beijing Univ. of Posts and Telecomm., Beijing, China); Zhou, Lijie (Beijing Univ. of Posts and Telecomm., Beijing, China); Zhang, Bo (Beijing Univ. of Posts and Telecomm., Beijing, China); Wang, Wendong (Beijing Univ. of Posts and Telecomm., Beijing, China)",,"Zhang, Xian (Beijing University of Posts and Telecommunications); Feng, Ziyuan (Beijing University of Posts and Telecommunications); Zhong, Tianchi (Beijing University of Posts and Telecommunications); Shen, Sicheng (Beijing University of Posts and Telecommunications); Zhang, Ruolin (Beijing University of Posts and Telecommunications); Zhou, Lijie (Beijing University of Posts and Telecommunications); Zhang, Bo (Beijing University of Posts and Telecommunications); Wang, Wendong (Beijing University of Posts and Telecommunications)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144654092,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1402,pub.1119160073,10.48550/arxiv.1807.04459,,,Prostate Segmentation using 2D Bridged U-net,"In this paper, we focus on three problems in deep learning based medical
image segmentation. Firstly, U-net, as a popular model for medical image
segmentation, is difficult to train when convolutional layers increase even
though a deeper network usually has a better generalization ability because of
more learnable parameters. Secondly, the exponential ReLU (ELU), as an
alternative of ReLU, is not much different from ReLU when the network of
interest gets deep. Thirdly, the Dice loss, as one of the pervasive loss
functions for medical image segmentation, is not effective when the prediction
is close to ground truth and will cause oscillation during training. To address
the aforementioned three problems, we propose and validate a deeper network
that can fit medical image datasets that are usually small in the sample size.
Meanwhile, we propose a new loss function to accelerate the learning process
and a combination of different activation functions to improve the network
performance. Our experimental results suggest that our network is comparable or
superior to state-of-the-art methods.",,,arXiv,,,2018-07-12,2018,,,,,,All OA, Green,Preprint,"Chen, Wanli; Zhang, Yue; He, Junjun; Qiao, Yu; Chen, Yifan; Shi, Hongjian; Tang, Xiaoying","Chen, Wanli (); Zhang, Yue (); He, Junjun (); Qiao, Yu (); Chen, Yifan (); Shi, Hongjian (); Tang, Xiaoying ()",,"Chen, Wanli (); Zhang, Yue (); He, Junjun (); Qiao, Yu (); Chen, Yifan (); Shi, Hongjian (); Tang, Xiaoying ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119160073,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1394,pub.1148488878,10.48550/arxiv.2206.01737,,,MaxStyle: Adversarial Style Composition for Robust Medical Image  Segmentation,"Convolutional neural networks (CNNs) have achieved remarkable segmentation
accuracy on benchmark datasets where training and test sets are from the same
domain, yet their performance can degrade significantly on unseen domains,
which hinders the deployment of CNNs in many clinical scenarios. Most existing
works improve model out-of-domain (OOD) robustness by collecting multi-domain
datasets for training, which is expensive and may not always be feasible due to
privacy and logistical issues. In this work, we focus on improving model
robustness using a single-domain dataset only. We propose a novel data
augmentation framework called MaxStyle, which maximizes the effectiveness of
style augmentation for model OOD performance. It attaches an auxiliary
style-augmented image decoder to a segmentation network for robust feature
learning and data augmentation. Importantly, MaxStyle augments data with
improved image style diversity and hardness, by expanding the style space with
noise and searching for the worst-case style composition of latent features via
adversarial training. With extensive experiments on multiple public cardiac and
prostate MR datasets, we demonstrate that MaxStyle leads to significantly
improved out-of-distribution robustness against unseen corruptions as well as
common distribution shifts across multiple, different, unseen sites and unknown
image sequences under both low- and high-training data settings. The code can
be found at https://github.com/cherise215/MaxStyle.",,,arXiv,,,2022-06-02,2022,,,,,,All OA, Green,Preprint,"Chen, Chen; Li, Zeju; Ouyang, Cheng; Sinclair, Matt; Bai, Wenjia; Rueckert, Daniel","Chen, Chen (); Li, Zeju (); Ouyang, Cheng (); Sinclair, Matt (); Bai, Wenjia (); Rueckert, Daniel ()",,"Chen, Chen (); Li, Zeju (); Ouyang, Cheng (); Sinclair, Matt (); Bai, Wenjia (); Rueckert, Daniel ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148488878,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1357,pub.1148451315,10.48550/arxiv.2206.01369,,,Incremental Learning Meets Transfer Learning: Application to Multi-site  Prostate MRI Segmentation,"Many medical datasets have recently been created for medical image
segmentation tasks, and it is natural to question whether we can use them to
sequentially train a single model that (1) performs better on all these
datasets, and (2) generalizes well and transfers better to the unknown target
site domain. Prior works have achieved this goal by jointly training one model
on multi-site datasets, which achieve competitive performance on average but
such methods rely on the assumption about the availability of all training
data, thus limiting its effectiveness in practical deployment. In this paper,
we propose a novel multi-site segmentation framework called
incremental-transfer learning (ITL), which learns a model from multi-site
datasets in an end-to-end sequential fashion. Specifically, ""incremental""
refers to training sequentially constructed datasets, and ""transfer"" is
achieved by leveraging useful information from the linear combination of
embedding features on each dataset. In addition, we introduce our ITL
framework, where we train the network including a site-agnostic encoder with
pre-trained weights and at most two segmentation decoder heads. We also design
a novel site-level incremental loss in order to generalize well on the target
domain. Second, we show for the first time that leveraging our ITL training
scheme is able to alleviate challenging catastrophic forgetting problems in
incremental learning. We conduct experiments using five challenging benchmark
datasets to validate the effectiveness of our incremental-transfer learning
approach. Our approach makes minimal assumptions on computation resources and
domain-specific expertise, and hence constitutes a strong starting point in
multi-site medical image segmentation.",,,arXiv,,,2022-06-02,2022,,,,,,All OA, Green,Preprint,"You, Chenyu; Xiang, Jinlin; Su, Kun; Zhang, Xiaoran; Dong, Siyuan; Onofrey, John; Staib, Lawrence; Duncan, James S.","You, Chenyu (); Xiang, Jinlin (); Su, Kun (); Zhang, Xiaoran (); Dong, Siyuan (); Onofrey, John (); Staib, Lawrence (); Duncan, James S. ()",,"You, Chenyu (); Xiang, Jinlin (); Su, Kun (); Zhang, Xiaoran (); Dong, Siyuan (); Onofrey, John (); Staib, Lawrence (); Duncan, James S. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148451315,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1355,pub.1139752487,10.1142/s0219467822500310,,,Integration of Dynamic Multi-Atlas and Deep Learning Techniques to Improve Segmentation of the Prostate in MR Images,"Accurate delineation of the prostate in MR images is an essential step for treatment planning and volume estimation of the organ. Prostate segmentation is a challenging task due to its variable size and shape. Moreover, neighboring tissues have a low-contrast with the prostate. We propose a robust and precise automatic algorithm to define the prostate’s boundaries in MR images in this paper. First, we find the prostate’s ROI by a deep neural network and decrease the input image’s size. Next, a dynamic multi-atlas-based approach obtains the initial segmentation of the prostate. A watershed algorithm improves the initial segmentation at the next stage. Finally, an SSM algorithm keeps the result in the domain of allowable prostate shapes. The quantitative evaluation of 74 prostate volumes demonstrated that the proposed method yields a mean Dice coefficient of [Formula: see text]. In comparison with recent researches, our algorithm is robust against shape and size variations.",,,International Journal of Image and Graphics,,,2021-07-14,2021,2021-07-14,2022-07,22,4,2250031,Closed,Article,"Moradi, Hamid; Foruzan, Amir Hossein","Moradi, Hamid (Department of Biomedical Engineering, Engineering Faculty, Shahed University, Tehran, Iran); Foruzan, Amir Hossein (Department of Biomedical Engineering, Engineering Faculty, Shahed University, Tehran, Iran)","Foruzan, Amir Hossein (Shahed University)","Moradi, Hamid (Shahed University); Foruzan, Amir Hossein (Shahed University)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139752487,"46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,,
1355,pub.1124938282,10.48550/arxiv.2002.06330,,,Development of Conditional Random Field Insert for UNet-based Zonal  Prostate Segmentation on T2-Weighted MRI,"Purpose: A conventional 2D UNet convolutional neural network (CNN)
architecture may result in ill-defined boundaries in segmentation output.
Several studies imposed stronger constraints on each level of UNet to improve
the performance of 2D UNet, such as SegNet. In this study, we investigated 2D
SegNet and a proposed conditional random field insert (CRFI) for zonal prostate
segmentation from clinical T2-weighted MRI data.
  Methods: We introduced a new methodology that combines SegNet and CRFI to
improve the accuracy and robustness of the segmentation. CRFI has feedback
connections that encourage the data consistency at multiple levels of the
feature pyramid. On the encoder side of the SegNet, the CRFI combines the input
feature maps and convolution block output based on their spatial local
similarity, like a trainable bilateral filter. For all networks, 725 2D images
(i.e., 29 MRI cases) were used in training; while, 174 2D images (i.e., 6
cases) were used in testing.
  Results: The SegNet with CRFI achieved the relatively high Dice coefficients
(0.76, 0.84, and 0.89) for the peripheral zone, central zone, and whole gland,
respectively. Compared with UNet, the SegNet+CRFIs segmentation has generally
higher Dice score and showed the robustness in determining the boundaries of
anatomical structures compared with the SegNet or UNet segmentation. The SegNet
with a CRFI at the end showed the CRFI can correct the segmentation errors from
SegNet output, generating smooth and consistent segmentation for the prostate.
  Conclusion: UNet based deep neural networks demonstrated in this study can
perform zonal prostate segmentation, achieving high Dice coefficients compared
with those in the literature. The proposed CRFI method can reduce the fuzzy
boundaries that affected the segmentation performance of baseline UNet and
SegNet models.",,,arXiv,,,2020-02-15,2020,,,,,,All OA, Green,Preprint,"Cao, Peng; Noworolski, Susan M.; Starobinets, Olga; Korn, Natalie; Kramer, Sage P.; Westphalen, Antonio C.; Leynes, Andrew P.; Pedoia, Valentina; Larson, Peder","Cao, Peng (); Noworolski, Susan M. (); Starobinets, Olga (); Korn, Natalie (); Kramer, Sage P. (); Westphalen, Antonio C. (); Leynes, Andrew P. (); Pedoia, Valentina (); Larson, Peder ()",,"Cao, Peng (); Noworolski, Susan M. (); Starobinets, Olga (); Korn, Natalie (); Kramer, Sage P. (); Westphalen, Antonio C. (); Leynes, Andrew P. (); Pedoia, Valentina (); Larson, Peder ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1124938282,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
1353,pub.1069362027,10.2298/vsp150901231d,,,A quantitative analysis of two-dimensional manually segmented transrectal ultrasound axial images in planning high dose rate brachytherapy for prostate cancer,"Background/Aim. Prostate delineation, pre-planning and catheter implantation procedures, in high-dose rate brachytherapy (HDR-BT), are commonly based on the prostate manually segmented transrectal ultrasound (TRUS) images. The aim of this study was to quantitatively analyze the consistency of prostate capsule delineation, done by a single therapist, prior to each HDR-BT fraction and the changes in the shape of the prostate capsule during HDR-BT, using two dimensional (2D) TRUS axial image. Methods. A group of 16 patients were treated at the Medical System Belgrade Brachytherapy Department with definitive HDRBT. The total applied median dose of 52 Gy was divided into four individual fractions, each fraction being delivered 2? 3 weeks apart. Real time prostate axial visualization and the manual segmentation prior to each fraction were performed using B-K Medical ultrasound. Quantitative analyses, analysis of an area and shape were applied on 2D-TRUS axial images of the prostate. Area analyses were used to calculate the average value of the cross-sectional area of the prostate image. The parameters of the prostate shape, the fractal dimension and the circularity ratio of the prostate capsule contour were estimated at the maximum axial cross section of the prostate image. Results. The sample group consisted of four phases, each phase being performed prior to the first, second, third and fourth HDR-BT fraction, respectively. Statistical analysis showed that during HDR-BT fractions there were no significant differences in the average value of area, as well as in the maximum shape of prostate capsule. Conclusions. Quantitative analysis of TRUS axial prostate segmented images shows a successful capsule delineation in the series of manually segmented TRUS images, and the prostate maximum shape remaining unchanged during HDR-BT fractions.
nema",,,Vojnosanitetski Pregled,,,2017,2017,,2017,74,5,420-427,All OA, Gold,Article,"Dabic-Stankovic, Kata; Rajkovic, Katarina; Acimovic, Miodrag; Milosevic, Nebojsa; Stankovic, Jovan","Dabic-Stankovic, Kata (Institute for Oncology and Radiology of Serbia, Belgrade + General Hospital “Medical System Belgrade”, Belgrade); Rajkovic, Katarina (Faculty of Medicine, Institute of Biophysics, Belgrade); Acimovic, Miodrag (General Hospital “Medical System Belgrade”, Belgrade + Clinical Center of Serbia, Clinic of Urology, Belgrade); Milosevic, Nebojsa (Faculty of Medicine, Belgrade); Stankovic, Jovan (General Hospital “Medical System Belgrade”, Belgrade + High Medical School, Belgrade)",,"Dabic-Stankovic, Kata (Institut za onkologiju i radiologiju Srbije); Rajkovic, Katarina (); Acimovic, Miodrag (Klinički centar Srbije); Milosevic, Nebojsa (University of Belgrade); Stankovic, Jovan ()",1,0,,0.25,http://www.doiserbia.nb.rs/ft.aspx?id=0042-84501600231D,https://app.dimensions.ai/details/publication/pub.1069362027,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
1351,pub.1091380853,10.1007/978-3-319-56904-8_3,,,Fully Automatic Multispectral MR Image Segmentation of Prostate Gland Based on the Fuzzy C-Means Clustering Algorithm,"Prostate imaging is a very critical issue in the clinical practice, especially for diagnosis, therapy, and staging of prostate cancer. Magnetic Resonance Imaging (MRI) can provide both morphologic and complementary functional information of tumor region. Manual detection and segmentation of prostate gland and carcinoma on multispectral MRI data is not easily practicable in the clinical routine because of the long times required by experienced radiologists to analyze several types of imaging data. In this paper, a fully automatic image segmentation method, exploiting an unsupervised Fuzzy C-Means (FCM) clustering technique for multispectral T1-weighted and T2-weighted MRI data processing, is proposed. This approach enables prostate segmentation and automatic gland volume calculation. Segmentation trials have been performed on a dataset composed of 7 patients affected by prostate cancer, using both area-based and distance-based metrics for its evaluation. The achieved experimental results are encouraging, showing good segmentation accuracy.",,,"Smart Innovation, Systems and Technologies",Multidisciplinary Approaches to Neural Computing,,2017-08-30,2017,2017-08-30,2018,69,,23-37,All OA, Green,Chapter,"Rundo, Leonardo; Militello, Carmelo; Russo, Giorgio; D’Urso, Davide; Valastro, Lucia Maria; Garufi, Antonio; Mauri, Giancarlo; Vitabile, Salvatore; Gilardi, Maria Carla","Rundo, Leonardo (Dipartimento di Informatica, Sistemistica e Comunicazione (DISCo), Università degli Studi di Milano-Bicocca, Milan, Italy; Istituto di Bioimmagini e Fisiologia Molecolare - Consiglio Nazionale delle Ricerche (IBFM-CNR), Cefalù (PA), Italy); Militello, Carmelo (Istituto di Bioimmagini e Fisiologia Molecolare - Consiglio Nazionale delle Ricerche (IBFM-CNR), Cefalù (PA), Italy); Russo, Giorgio (Istituto di Bioimmagini e Fisiologia Molecolare - Consiglio Nazionale delle Ricerche (IBFM-CNR), Cefalù (PA), Italy; Azienda Ospedaliera per l’Emergenza Cannizzaro, Catania, Italy; Laboratori Nazionali del Sud (LNS), Istituto Nazionale di Fisica Nucleare (INFN), Catania, Italy); D’Urso, Davide (Istituto di Bioimmagini e Fisiologia Molecolare - Consiglio Nazionale delle Ricerche (IBFM-CNR), Cefalù (PA), Italy; Università degli Studi di Catania, Catania, Italy); Valastro, Lucia Maria (Azienda Ospedaliera per l’Emergenza Cannizzaro, Catania, Italy; Laboratori Nazionali del Sud (LNS), Istituto Nazionale di Fisica Nucleare (INFN), Catania, Italy); Garufi, Antonio (Azienda Ospedaliera per l’Emergenza Cannizzaro, Catania, Italy); Mauri, Giancarlo (Dipartimento di Informatica, Sistemistica e Comunicazione (DISCo), Università degli Studi di Milano-Bicocca, Milan, Italy); Vitabile, Salvatore (Dipartimento di Biopatologia e Biotecnologie Mediche (DIBIMED), Università degli Studi di Palermo, Palermo, Italy); Gilardi, Maria Carla (Istituto di Bioimmagini e Fisiologia Molecolare - Consiglio Nazionale delle Ricerche (IBFM-CNR), Cefalù (PA), Italy)","Rundo, Leonardo (University of Milano-Bicocca; Institute of Molecular Bioimaging and Physiology)","Rundo, Leonardo (University of Milano-Bicocca; Institute of Molecular Bioimaging and Physiology); Militello, Carmelo (Institute of Molecular Bioimaging and Physiology); Russo, Giorgio (Institute of Molecular Bioimaging and Physiology; Laboratori Nazionali del Sud); D’Urso, Davide (Institute of Molecular Bioimaging and Physiology; University of Catania); Valastro, Lucia Maria (Laboratori Nazionali del Sud); Garufi, Antonio (); Mauri, Giancarlo (University of Milano-Bicocca); Vitabile, Salvatore (University of Palermo); Gilardi, Maria Carla (Institute of Molecular Bioimaging and Physiology)",9,3,,2.37,https://media.springer.com/full/springer-instructions-for-authors-assets/pdf/SN_BPF_EN.pdf,https://app.dimensions.ai/details/publication/pub.1091380853,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,,,,,,
1343,pub.1128553271,10.1007/s00521-020-05083-3,,,Attention deep residual networks for MR image analysis,"Prostate diseases often occur in men. For further clinical treatment and diagnosis, we need to do accurate segmentation on prostate. There are already many methods that concentrate on solving the problem of automatic prostate MR image segmentation. However, the design of some hyperparameters of these methods is migrated from the models that are used for nature images which do not consider the difference between medical image and nature image. Besides, there is trend that researchers are likely to use deeper and more complicated networks to achieve high accuracy. The improvement is limited with surging parameters, computations, training time, and inference time. In this paper, we propose an efficient attention residual U-Net to segment the prostate MR image. We analyze the property of prostate MR image and fine-tune the architecture of U-Net. To accelerate the convergence of our method, residual connection and channel attention are added to our network. A set of experiments suggest our method can achieve a similar accuracy of state of the art with less parameters, less computations, shorter training time, and shorter inference time.",,,Neural Computing and Applications,,,2020-06-16,2020,2020-06-16,,,,1-10,Closed,Article,"Mei, Mengqing; He, Fazhi; Xue, Shan","Mei, Mengqing (School of Computer Science, Wuhan University, Bayi Road, Wuhan, Hubei, China); He, Fazhi (School of Computer Science, Wuhan University, Bayi Road, Wuhan, Hubei, China); Xue, Shan (Department of Computing, Macquarie University, Sydney, Australia)","He, Fazhi (Wuhan University)","Mei, Mengqing (Wuhan University); He, Fazhi (Wuhan University); Xue, Shan (Macquarie University)",1,1,,0.52,,https://app.dimensions.ai/details/publication/pub.1128553271,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1309,pub.1144596444,10.21203/rs.3.rs-1225229/v1,,,Explainable AI for CNN-based Prostate Tumor Segmentation in Multi-parametric MRI Correlated to Whole Mount Histopathology,"Automatic prostate tumor segmentation is often unable to identify the lesion even if in multi-parametric MRI data is used as input, and the segmentation output is difficult to verify due to the lack of clinically established ground truth images. In this work we use an explainable deep learning model to interpret the predictions of a convolutional neural network (CNN) for prostate tumor segmentation. The CNN uses a U-Net architecture which was trained on multi-parametric MRI data from 122 patients to automatically segment the prostate gland and prostate tumor lesions. In addition, co-registered ground truth data from whole mount histopathology images were available in 15 patients that were used as a test set during CNN testing. To be able to interpret the segmentation results of the CNN, heat maps were generated using the Gradient Weighted Class Activation Map (Grad-CAM) method. With the CNN a mean Dice Sorensen Coefficient for the prostate gland and the tumor lesions of 0.62 and 0.31 with the radiologist drawn ground truth and 0.32 with wholemount histology ground truth for tumor lesions could be achieved. Dice Sorensen Coefficient between CNN predictions and manual segmentations from MRI and histology data were not significantly different. In the prostate the Grad-CAM heat maps could differentiate between tumor and healthy prostate tissue, which indicates that the image information in the tumor was essential for the CNN segmentation.",,,Research Square,,,2022-01-11,2022,2022-01-11,,,,,All OA, Green,Preprint,"Gunashekar, Deepa Darshini; Bielak, Lars; Hägele, Leonard; Berlin, Arnie; Oerther, Benedict; Benndorf, Matthias; Grosu, Anca; Zamboglou, Constantinos; Bock, Michael","Gunashekar, Deepa Darshini (Universitätsklinikum Freiburg: Universitatsklinikum Freiburg); Bielak, Lars (University Medical Center Freiburg: Universitatsklinikum Freiburg); Hägele, Leonard (University Medical Center Freiburg: Universitatsklinikum Freiburg); Berlin, Arnie (Mathworks Inc); Oerther, Benedict (University Medical Center Freiburg: Universitatsklinikum Freiburg); Benndorf, Matthias (University Medical Center Freiburg: Universitatsklinikum Freiburg); Grosu, Anca (University Medical Center Freiburg: Universitatsklinikum Freiburg); Zamboglou, Constantinos (University Medical Center Freiburg: Universitatsklinikum Freiburg); Bock, Michael (University of Freiburg Hospital: Universitatsklinikum Freiburg)",,"Gunashekar, Deepa Darshini (); Bielak, Lars (); Hägele, Leonard (); Berlin, Arnie (); Oerther, Benedict (); Benndorf, Matthias (); Grosu, Anca (); Zamboglou, Constantinos (); Bock, Michael ()",0,0,,,https://ro-journal.biomedcentral.com/track/pdf/10.1186/s13014-022-02035-0,https://app.dimensions.ai/details/publication/pub.1144596444,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
1307,pub.1154178222,10.1109/bibm55620.2022.9995034,,,Invariant Content Synergistic Learning for Domain Generalization on Medical Image Segmentation,"Although deep convolution neural networks (DC-NNs) can achieve remarkable success on medical image segmentation, their performance might significantly deteriorate when confronting testing data with the new distribution. Recent studies suggest that one major cause of this issue is the strong inductive bias of DCNNs, which towards image styles (e.g., superficial texture) that are sensitive to change, instead of the invariant content (e.g., object shapes). Inspired by this, we propose a novel method, named Invariant Content Synergistic Learning (ICSL), to improve the generalization ability of DCNNs on unseen data by controlling the inductive bias. Specifically, ICSL first mixes the style of training instances to perturb the training distribution, so that more diverse domains or styles would be made available for training DCNNs. Then, based on the perturbed distribution, we carefully design a dual-branches invariant content synergistic learning strategy to prevent style-biased predictions and maintain the invariant content. Extensive experimental results demonstrate the superior performance of the proposed method over state-of-the-art domain generalization methods on two typical medical segmentation tasks.","This work is supported by the National Natural Science Foundation of China (NSFC Grant No.62073260 and No.62106198), the Natural Science Foundation of Shaanxi Province of China (2021JQ-461), and the project of Xi’an Science and Technology Bureau (21RGZN0019).","This work is supported by the National Natural Science Foundation of China (NSFC Grant No.62073260 and No.62106198), the Natural Science Foundation of Shaanxi Province of China (2021JQ-461), and the project of Xi’an Science and Technology Bureau (21RGZN0019).",,2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),,2022-12-08,2022,,2022-12-08,0,,450-456,All OA, Green,Proceeding,"Kang, Yuxin; Li, Hansheng; Zhao, Xuan; Shi, Xiaoshuang; Liu, Feihong; Yan, Qingguo; Guo, Ying; Cui, Lei; Feng, Jun; Yang, Lin","Kang, Yuxin (School of Information Science and Technology, Northwest University, Xi’an, 710127, Shaanxi, China); Li, Hansheng (School of Information Science and Technology, Northwest University, Xi’an, 710127, Shaanxi, China); Zhao, Xuan (School of Information Science and Technology, Northwest University, Xi’an, 710127, Shaanxi, China); Shi, Xiaoshuang (School of Information Science and Technology, Northwest University, Xi’an, 710127, Shaanxi, China); Liu, Feihong (School of Information Science and Technology, Northwest University, Xi’an, 710127, Shaanxi, China); Yan, Qingguo (School of Medicine, Northwest University, Xi’an, 710069, Shaanxi, China); Guo, Ying (School of Medicine, Northwest University, Xi’an, 710069, Shaanxi, China; Department of Pathology, Xi’an Daxing Hospital, Xi’an, 710082, Shaanxi, China); Cui, Lei (School of Information Science and Technology, Northwest University, Xi’an, 710127, Shaanxi, China); Feng, Jun (School of Information Science and Technology, Northwest University, Xi’an, 710127, Shaanxi, China); Yang, Lin (School of Information Science and Technology, Northwest University, Xi’an, 710127, Shaanxi, China)","Cui, Lei (Northwest University)","Kang, Yuxin (Northwest University); Li, Hansheng (Northwest University); Zhao, Xuan (Northwest University); Shi, Xiaoshuang (Northwest University); Liu, Feihong (Northwest University); Yan, Qingguo (Northwest University); Guo, Ying (Northwest University); Cui, Lei (Northwest University); Feng, Jun (Northwest University); Yang, Lin (Northwest University)",0,0,,,http://arxiv.org/pdf/2205.02845,https://app.dimensions.ai/details/publication/pub.1154178222,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1307,pub.1151694517,10.1007/978-3-031-18523-6_1,,,Incremental Learning Meets Transfer Learning: Application to Multi-site Prostate MRI Segmentation,"Many medical datasets have recently been created for medical image segmentation tasks, and it is natural to question whether we can use them to sequentially train a single model that (1) performs better on all these datasets, and (2) generalizes well and transfers better to the unknown target site domain. Prior works have achieved this goal by jointly training one model on multi-site datasets, which achieve competitive performance on average but such methods rely on the assumption about the availability of all training data, thus limiting its effectiveness in practical deployment. In this paper, we propose a novel multi-site segmentation framework called incremental-transfer learning (ITL), which learns a model from multi-site datasets in an end-to-end sequential fashion. Specifically, “incremental” refers to training sequentially constructed datasets, and “transfer” is achieved by leveraging useful information from the linear combination of embedding features on each dataset. In addition, we introduce our ITL framework, where we train the network including a site-agnostic encoder with pretrained weights and at most two segmentation decoder heads. We also design a novel site-level incremental loss in order to generalize well on the target domain. Second, we show for the first time that leveraging our ITL training scheme is able to alleviate challenging catastrophic forgetting problems in incremental learning. We conduct experiments using five challenging benchmark datasets to validate the effectiveness of our incremental-transfer learning approach. Our approach makes minimal assumptions on computation resources and domain-specific expertise, and hence constitutes a strong starting point in multi-site medical image segmentation.",,,Lecture Notes in Computer Science,"Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health",,2022-10-07,2022,2022-10-07,2022,13573,,3-16,All OA, Green,Chapter,"You, Chenyu; Xiang, Jinlin; Su, Kun; Zhang, Xiaoran; Dong, Siyuan; Onofrey, John; Staib, Lawrence; Duncan, James S.","You, Chenyu (Electrical Engineering, Yale University, New Haven, CT, USA); Xiang, Jinlin (Electrical and Computer Engineering, The University of Washington, WA, USA); Su, Kun (Electrical and Computer Engineering, The University of Washington, WA, USA); Zhang, Xiaoran (Biomedical Engineering, Yale University, New Haven, CT, USA); Dong, Siyuan (Electrical Engineering, Yale University, New Haven, CT, USA); Onofrey, John (Radiology and Biomedical Imaging, Yale School of Medicine, New Haven, CT, USA); Staib, Lawrence (Electrical Engineering, Yale University, New Haven, CT, USA; Biomedical Engineering, Yale University, New Haven, CT, USA; Radiology and Biomedical Imaging, Yale School of Medicine, New Haven, CT, USA); Duncan, James S. (Electrical Engineering, Yale University, New Haven, CT, USA; Biomedical Engineering, Yale University, New Haven, CT, USA; Radiology and Biomedical Imaging, Yale School of Medicine, New Haven, CT, USA)","You, Chenyu (Yale University)","You, Chenyu (Yale University); Xiang, Jinlin (University of Washington); Su, Kun (University of Washington); Zhang, Xiaoran (Yale University); Dong, Siyuan (Yale University); Onofrey, John (Yale University); Staib, Lawrence (Yale University; Yale University; Yale University); Duncan, James S. (Yale University; Yale University; Yale University)",6,6,,,http://arxiv.org/pdf/2206.01369,https://app.dimensions.ai/details/publication/pub.1151694517,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1305,pub.1149212898,10.1201/9781003240037-3,,,Swarm Intelligence and Machine Learning Algorithms for Cancer Diagnosis,,,,,Swarm Intelligence and Machine Learning,,2022-07-05,2022,2022-07-05,,,,34-50,Closed,Chapter,"Sharma, Pankaj; Jain, Vinay; Tailang, Mukul","Sharma, Pankaj (); Jain, Vinay (); Tailang, Mukul ()",,"Sharma, Pankaj (); Jain, Vinay (); Tailang, Mukul ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149212898,,,,,,,,,,,,,
1304,pub.1119366478,10.48550/arxiv.1901.09462,,,A deep learning-based method for prostate segmentation in T2-weighted  magnetic resonance imaging,"We propose a novel automatic method for accurate segmentation of the prostate
in T2-weighted magnetic resonance imaging (MRI). Our method is based on
convolutional neural networks (CNNs). Because of the large variability in the
shape, size, and appearance of the prostate and the scarcity of annotated
training data, we suggest training two separate CNNs. A global CNN will
determine a prostate bounding box, which is then resampled and sent to a local
CNN for accurate delineation of the prostate boundary. This way, the local CNN
can effectively learn to segment the fine details that distinguish the prostate
from the surrounding tissue using the small amount of available training data.
To fully exploit the training data, we synthesize additional data by deforming
the training images and segmentations using a learned shape model. We apply the
proposed method on the PROMISE12 challenge dataset and achieve state of the art
results. Our proposed method generates accurate, smooth, and artifact-free
segmentations. On the test images, we achieve an average Dice score of 90.6
with a small standard deviation of 2.2, which is superior to all previous
methods. Our two-step segmentation approach and data augmentation strategy may
be highly effective in segmentation of other organs from small amounts of
annotated medical images.",,,arXiv,,,2019-01-27,2019,,,,,,All OA, Green,Preprint,"Karimi, Davood; Samei, Golnoosh; Shao, Yanan; Salcudean, Septimiu","Karimi, Davood (); Samei, Golnoosh (); Shao, Yanan (); Salcudean, Septimiu ()",,"Karimi, Davood (); Samei, Golnoosh (); Shao, Yanan (); Salcudean, Septimiu ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119366478,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,
1302,pub.1124678335,10.23919/fruct48121.2019.8981537,,,Database Acquisition for the Lung Cancer Computer Aided Diagnostic Systems,"Most of the used computer aided diagnostic (CAD) systems based on applying the deep learning algorithms are similar from the point of view of data processing stages. The main typical stages are the training data acquisition, pre-processing, segmentation and classification. Homogeneity of a training dataset structure and its completeness are very important for minimizing inaccuracies in the development of the CAD systems. The main difficulties in the medical training data acquisition are concerned with their heterogeneity and incompleteness. Another problem is a lack of a sufficient large amount of data for training deep neural networks which are a basis of the CAD systems. In order to overcome these problems in the lung cancer CAD systems, a new methodology of the dataset acquisition is proposed by using as an example the database called LIRA which has been applied to training the intellectual lung cancer CAD system called by Dr. AIzimov. One of the important peculiarities of the dataset LIRA is the morphological confirmation of diseases. Another peculiarity is taking into account and including “atypical” cases from the point of view of radiographic features. The database development is carried out in the interdisciplinary collaboration of radiologists and data scientists developing the CAD system.","The reported study was funded by RFBR, project number 19-29-01004. The reported study was funded by RFBR, project number 19-29-01004. The results of the work were obtained using computational resources of Peter the Great Saint-Petersburg Polytechnic University Supercomputing Center ( www.spbstu.ru ) which is registered as a center of collective usage ( http://ckprf.ru/ckp/500675/ ).",,,2019 25th Conference of Open Innovations Association (FRUCT),,2019-11-08,2019,,2019-11-08,0,,220-227,Closed,Proceeding,"Meldo, Anna; Utkin, Lev; Lukashin, Aleksey; Muliukha, Vladimir; Zaborovsky, Vladimir","Meldo, Anna (Clinical Research Center of Specialized Types of Medical Care (Oncological), St.Petersburg, Russia); Utkin, Lev (Peter the Great St.Petersburg Polytechnic University (SPbPU), St.Petersburg, Russia); Lukashin, Aleksey (Peter the Great St.Petersburg Polytechnic University (SPbPU), St.Petersburg, Russia); Muliukha, Vladimir (Peter the Great St.Petersburg Polytechnic University (SPbPU), St.Petersburg, Russia); Zaborovsky, Vladimir (Peter the Great St.Petersburg Polytechnic University (SPbPU), St.Petersburg, Russia)","Meldo, Anna ","Meldo, Anna (); Utkin, Lev (Peter the Great St. Petersburg Polytechnic University); Lukashin, Aleksey (Peter the Great St. Petersburg Polytechnic University); Muliukha, Vladimir (Peter the Great St. Petersburg Polytechnic University); Zaborovsky, Vladimir (Peter the Great St. Petersburg Polytechnic University)",1,0,,0.38,,https://app.dimensions.ai/details/publication/pub.1124678335,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1302,pub.1129069870,10.1007/978-3-030-52791-4_22,,,Radiomics: A New Biomedical Workflow to Create a Predictive Model,"Abstract‘Radiomics’ is utilized to improve the prediction of patient overall survival and/or outcome. Target segmentation, feature extraction, feature selection, and classification model are the fundamental blocks of a radiomics workflow. Nevertheless, these blocks can be affected by several issues, i.e. high inter- and intra-observer variability. To overcome these issues obtaining reproducible results, we propose a novel radiomics workflow to identify a relevant prognostic model concerning a real clinical problem. In the specific, we propose an operator-independent segmentation system with the consequent automatic extraction of radiomics features, and a novel feature selection approach to create a relevant predictive model in 46 patients with prostate lesion underwent magnetic resonance imaging.In the specific, using an operator-independent method of target segmentation based on an active contour, ad-hoc automated high-throughput analysis tool capable of calculating a total of 290 radiomics features for each imaging sequence, a novel statistical system for feature reduction and selection, and the discriminant analysis as a method for feature classification, we propose a performant and replicable radiomics workflow for the diagnosis of prostate cancer.The proposed workflow revealed three and five relevant features on T2-weighted and apparent diffusion coefficient (ADC) maps images, respectively, that were significantly correlated with the histopathological results. In the specific, good performance in lesion discrimination was obtained using the combination of the selected features (accuracy 76.76% and 75.20%, for T2-weighted and ADC maps images, respectively) in an operator-independent and automatic way.",,,Communications in Computer and Information Science,Medical Image Understanding and Analysis,,2020-07-08,2020,2020-07-08,2020,1248,,280-293,Closed,Chapter,"Comelli, Albert; Stefano, Alessandro; Coronnello, Claudia; Russo, Giorgio; Vernuccio, Federica; Cannella, Roberto; Salvaggio, Giuseppe; Lagalla, Roberto; Barone, Stefano","Comelli, Albert (Ri.MED Foundation, Palermo, Italy); Stefano, Alessandro (Institute of Molecular Bioimaging and Physiology, National Research Council (IBFM-CNR), Cefalù, Italy); Coronnello, Claudia (Ri.MED Foundation, Palermo, Italy); Russo, Giorgio (Institute of Molecular Bioimaging and Physiology, National Research Council (IBFM-CNR), Cefalù, Italy); Vernuccio, Federica (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, Palermo, Italy); Cannella, Roberto (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, Palermo, Italy); Salvaggio, Giuseppe (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, Palermo, Italy); Lagalla, Roberto (Dipartimento di Biomedicina, Neuroscienze e Diagnostica avanzata (BIND), University of Palermo, Palermo, Italy); Barone, Stefano (Dipartimento di Scienze Agronomiche, Alimentari e Forestali (SAAF), University of Palermo, Palermo, Italy)","Comelli, Albert (Ri.MED)","Comelli, Albert (Ri.MED); Stefano, Alessandro (); Coronnello, Claudia (Ri.MED); Russo, Giorgio (); Vernuccio, Federica (University of Palermo); Cannella, Roberto (University of Palermo); Salvaggio, Giuseppe (University of Palermo); Lagalla, Roberto (University of Palermo); Barone, Stefano (University of Palermo)",26,23,,9.68,,https://app.dimensions.ai/details/publication/pub.1129069870,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,3 Good Health and Well Being,,,,,,,,,
1295,pub.1149637704,10.48550/arxiv.2207.09957,,,Estimating Model Performance under Domain Shifts with Class-Specific  Confidence Scores,"Machine learning models are typically deployed in a test setting that differs
from the training setting, potentially leading to decreased model performance
because of domain shift. If we could estimate the performance that a
pre-trained model would achieve on data from a specific deployment setting, for
example a certain clinic, we could judge whether the model could safely be
deployed or if its performance degrades unacceptably on the specific data.
Existing approaches estimate this based on the confidence of predictions made
on unlabeled test data from the deployment's domain. We find existing methods
struggle with data that present class imbalance, because the methods used to
calibrate confidence do not account for bias induced by class imbalance,
consequently failing to estimate class-wise accuracy. Here, we introduce
class-wise calibration within the framework of performance estimation for
imbalanced datasets. Specifically, we derive class-specific modifications of
state-of-the-art confidence-based model evaluation methods including
temperature scaling (TS), difference of confidences (DoC), and average
thresholded confidence (ATC). We also extend the methods to estimate Dice
similarity coefficient (DSC) in image segmentation. We conduct experiments on
four tasks and find the proposed modifications consistently improve the
estimation accuracy for imbalanced datasets. Our methods improve accuracy
estimation by 18\% in classification under natural domain shifts, and double
the estimation accuracy on segmentation tasks, when compared with prior
methods.",,,arXiv,,,2022-07-20,2022,,,,,,All OA, Green,Preprint,"Li, Zeju; Kamnitsas, Konstantinos; Islam, Mobarakol; Chen, Chen; Glocker, Ben","Li, Zeju (); Kamnitsas, Konstantinos (); Islam, Mobarakol (); Chen, Chen (); Glocker, Ben ()",,"Li, Zeju (); Kamnitsas, Konstantinos (); Islam, Mobarakol (); Chen, Chen (); Glocker, Ben ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149637704,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1295,pub.1139701802,10.48550/arxiv.2107.05532,,,Context-aware virtual adversarial training for anatomically-plausible  segmentation,"Despite their outstanding accuracy, semi-supervised segmentation methods
based on deep neural networks can still yield predictions that are considered
anatomically impossible by clinicians, for instance, containing holes or
disconnected regions. To solve this problem, we present a Context-aware Virtual
Adversarial Training (CaVAT) method for generating anatomically plausible
segmentation. Unlike approaches focusing solely on accuracy, our method also
considers complex topological constraints like connectivity which cannot be
easily modeled in a differentiable loss function. We use adversarial training
to generate examples violating the constraints, so the network can learn to
avoid making such incorrect predictions on new examples, and employ the
Reinforce algorithm to handle non-differentiable segmentation constraints. The
proposed method offers a generic and efficient way to add any constraint on top
of any segmentation network. Experiments on two clinically-relevant datasets
show our method to produce segmentations that are both accurate and
anatomically-plausible in terms of region connectivity.",,,arXiv,,,2021-07-12,2021,,,,,,All OA, Green,Preprint,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (); Zhang, Caiming (); Desrosiers, Christian ()",,"Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (); Zhang, Caiming (); Desrosiers, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139701802,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1295,pub.1126708369,10.48550/arxiv.2004.06816,,,Bounding boxes for weakly supervised segmentation: Global constraints  get close to full supervision,"We propose a novel weakly supervised learning segmentation based on several
global constraints derived from box annotations. Particularly, we leverage a
classical tightness prior to a deep learning setting via imposing a set of
constraints on the network outputs. Such a powerful topological prior prevents
solutions from excessive shrinking by enforcing any horizontal or vertical line
within the bounding box to contain, at least, one pixel of the foreground
region. Furthermore, we integrate our deep tightness prior with a global
background emptiness constraint, guiding training with information outside the
bounding box. We demonstrate experimentally that such a global constraint is
much more powerful than standard cross-entropy for the background class. Our
optimization problem is challenging as it takes the form of a large set of
inequality constraints on the outputs of deep networks. We solve it with
sequence of unconstrained losses based on a recent powerful extension of the
log-barrier method, which is well-known in the context of interior-point
methods. This accommodates standard stochastic gradient descent (SGD) for
training deep networks, while avoiding computationally expensive and unstable
Lagrangian dual steps and projections. Extensive experiments over two different
public data sets and applications (prostate and brain lesions) demonstrate that
the synergy between our global tightness and emptiness priors yield very
competitive performances, approaching full supervision and outperforming
significantly DeepCut. Furthermore, our approach removes the need for
computationally expensive proposal generation. Our code is shared anonymously.",,,arXiv,,,2020-04-14,2020,,,,,,All OA, Green,Preprint,"Kervadec, Hoel; Dolz, Jose; Wang, Shanshan; Granger, Eric; Ayed, Ismail Ben","Kervadec, Hoel (); Dolz, Jose (); Wang, Shanshan (); Granger, Eric (); Ayed, Ismail Ben ()",,"Kervadec, Hoel (); Dolz, Jose (); Wang, Shanshan (); Granger, Eric (); Ayed, Ismail Ben ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1126708369,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1295,pub.1011074374,10.1080/21681163.2014.960535,,,SmartPaint: a tool for interactive segmentation of medical volume images,"We present SmartPaint, a general-purpose method and software for interactive segmentation of medical volume images. SmartPaint uses a novel paint-brush interaction paradigm, where the user segments objects in the image by ‘sweeping’ over them with the mouse cursor. The key feature of SmartPaint is that the painting tools adapt to the image content, selectively sticking to objects of interest while avoiding other structures. This behaviour is achieved by modulating the effect of the tools by both the Euclidean distance and the range distance (difference in image intensity values) from the mouse cursor. We evaluate SmartPaint on three publicly available medical image datasets, covering different image modalities and segmentation targets. The results show that, with a limited user effort, SmartPaint can produce segmentations whose accuracy is comparable to both the state-of-the-art automatic segmentation methods and manual delineations produced by expert users. The SmartPaint software is freely available, and can be downloaded from the authors' web page (http://www.cb.uu.se/~filip/SmartPaint/).","The authors wish to thank Håkan Ahlström, Ewert Bengtsson and Charlotte Ebeling Barbier, Uppsala University, for valuable input during the preparation of this manuscript.",,Computer Methods in Biomechanics and Biomedical Engineering Imaging & Visualization,,,2014-09-23,2014,2014-09-23,2017-01-02,5,1,36-44,Closed,Article,"Malmberg, Filip; Nordenskjöld, Richard; Strand, Robin; Kullberg, Joel","Malmberg, Filip (Department of Information Technology, Uppsala University, Uppsala, Sweden); Nordenskjöld, Richard (Department of Radiology, Oncology and Radiation Sciences, Uppsala University, Uppsala, Sweden); Strand, Robin (Department of Information Technology, Uppsala University, Uppsala, Sweden; Department of Radiology, Oncology and Radiation Sciences, Uppsala University, Uppsala, Sweden); Kullberg, Joel (Department of Radiology, Oncology and Radiation Sciences, Uppsala University, Uppsala, Sweden)","Malmberg, Filip (Uppsala University)","Malmberg, Filip (Uppsala University); Nordenskjöld, Richard (Uppsala University); Strand, Robin (Uppsala University; Uppsala University); Kullberg, Joel (Uppsala University)",28,8,,,,https://app.dimensions.ai/details/publication/pub.1011074374,46 Information and Computing Sciences,,,,,,,,,,,,
1262,pub.1144552736,10.1007/s13369-021-06502-w,,,Prostate Segmentation via Dynamic Fusion Model,"Nowadays, many different methods are used in diagnosing prostate cancer. Among these methods, MRI-based imaging methods provide more precise information than other methods by obtaining the prostate's image from different angles (axial, sagittal, coronal). However, manually segmenting these images is very time-consuming and laborious. Besides, another challenge is the inhomogeneous and inconsistent appearance around the prostate borders, which is essential for cancer diagnosis. Nowadays, scientists are working intensively on deep learning-based techniques to identify prostate boundaries more efficiently and with high accuracy. In this study, a dynamic fusion architecture is proposed. For the fusion model, the Unet + Resnet3D and Unet + Resnet2D models were fused. Evaluation experiments were performed on the MICCAI 2012 Prostate Segmentation Challenge Dataset (PROMISE12) and the NCI-ISBI 2013(NCI_ISBI-13) Prostate Segmentation Challenge Dataset. Comparative analyzes show that the advantages and robustness of our method are superior to state-of-the-art approaches.",,,Arabian Journal for Science and Engineering,,,2022-01-10,2022,2022-01-10,2022-08,47,8,10211-10224,Closed,Article,"Ocal, Hakan; Barisci, Necaattin","Ocal, Hakan (Computer Engineering, School of Natural and Applied Sciences, Gazi University, Ankara, Turkey); Barisci, Necaattin (Computer Engineering, School of Natural and Applied Sciences, Gazi University, Ankara, Turkey)","Ocal, Hakan (Gazi University)","Ocal, Hakan (Gazi University); Barisci, Necaattin (Gazi University)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1144552736,40 Engineering, 4004 Chemical Engineering, 4005 Civil Engineering, 4016 Materials Engineering,,,,,,,,,
1259,pub.1140770007,10.1002/asmb.2642,,,Hybrid descriptive‐inferential method for key feature selection in prostate cancer radiomics,"In healthcare industry 4.0, a big role is played by radiomics. Radiomics concerns the extraction and analysis of quantitative information not visible to the naked eye, even by expert operators, from biomedical images. Radiomics involves the management of digital images as data matrices, with the aim of extracting a number of morphological and predictive variables, named features, using automatic or semi‐automatic methods. Multidisciplinary methods as machine learning and deep learning are fully involved in this field. However, the large number of features requires efficient and effective core methods for their selection, in order to avoid bias or misinterpretations problems. In this work, the authors propose a novel method for feature selection in radiomics. The proposed method is based on an original combination of descriptive and inferential statistics. Its validity is illustrated through a case study on prostate cancer analysis, conducted at the university hospital of Palermo, Italy.","The authors wish to thank the chief editor of the ASMBI journal, the guest editors of this special issue for inviting this contribution, and especially the two anonymous referees for their careful and critical analysis of a first version of the article. An anonymous associate editor provided extremely important recommendations for a final revision of the article. All these authoritative critics and recommendations enormously contributed to further reflections, rework, and improvement of the article.",,Applied Stochastic Models in Business and Industry,,,2021-08-31,2021,2021-08-31,2021-09,37,5,961-972,Closed,Article,"Barone, Stefano; Cannella, Roberto; Comelli, Albert; Pellegrino, Arianna; Salvaggio, Giuseppe; Stefano, Alessandro; Vernuccio, Federica","Barone, Stefano (Dipartimento di Scienze Agrarie, Alimentari e Forestali, Università degli Studi di Palermo, Palermo, Italy); Cannella, Roberto (Dipartimento di Biomedicina, Neuroscienze e Diagnostica Avanzata, Università degli Studi di Palermo, Palermo, Italy); Comelli, Albert (Fondazione Ri.MED, Palermo, Italy; Istituto di Bioimmagini e Fisiologia Molecolare, Consiglio Nazionale delle Ricerche (IBFM‐CNR), Cefalù, Italy); Pellegrino, Arianna (Dipartimento di Ingegneria Meccanica e Aerospaziale, Politecnico di Torino, Turin, Italy); Salvaggio, Giuseppe (Dipartimento di Biomedicina, Neuroscienze e Diagnostica Avanzata, Università degli Studi di Palermo, Palermo, Italy); Stefano, Alessandro (Istituto di Bioimmagini e Fisiologia Molecolare, Consiglio Nazionale delle Ricerche (IBFM‐CNR), Cefalù, Italy); Vernuccio, Federica (Dipartimento di Biomedicina, Neuroscienze e Diagnostica Avanzata, Università degli Studi di Palermo, Palermo, Italy)","Barone, Stefano (University of Palermo)","Barone, Stefano (University of Palermo); Cannella, Roberto (University of Palermo); Comelli, Albert (Ri.MED); Pellegrino, Arianna (Polytechnic University of Turin); Salvaggio, Giuseppe (University of Palermo); Stefano, Alessandro (); Vernuccio, Federica (University of Palermo)",20,20,,24.77,,https://app.dimensions.ai/details/publication/pub.1140770007,"35 Commerce, Management, Tourism and Services; 3502 Banking, Finance and Investment; 49 Mathematical Sciences; 4901 Applied Mathematics; 4905 Statistics",3 Good Health and Well Being,,,,,,,,,,,
1257,pub.1149469790,10.48550/arxiv.2207.06168,,,MRF-UNets: Searching UNet with Markov Random Fields,"UNet [27] is widely used in semantic segmentation due to its simplicity and
effectiveness. However, its manually-designed architecture is applied to a
large number of problem settings, either with no architecture optimizations, or
with manual tuning, which is time consuming and can be sub-optimal. In this
work, firstly, we propose Markov Random Field Neural Architecture Search
(MRF-NAS) that extends and improves the recent Adaptive and Optimal Network
Width Search (AOWS) method [4] with (i) a more general MRF framework (ii)
diverse M-best loopy inference (iii) differentiable parameter learning. This
provides the necessary NAS framework to efficiently explore network
architectures that induce loopy inference graphs, including loops that arise
from skip connections. With UNet as the backbone, we find an architecture,
MRF-UNet, that shows several interesting characteristics. Secondly, through the
lens of these characteristics, we identify the sub-optimality of the original
UNet architecture and further improve our results with MRF-UNetV2. Experiments
show that our MRF-UNets significantly outperform several benchmarks on three
aerial image datasets and two medical image datasets while maintaining low
computational costs. The code is available at:
https://github.com/zifuwanggg/MRF-UNets.",,,arXiv,,,2022-07-13,2022,,,,,,All OA, Green,Preprint,"Wang, Zifu; Blaschko, Matthew B.","Wang, Zifu (); Blaschko, Matthew B. ()",,"Wang, Zifu (); Blaschko, Matthew B. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149469790,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1250,pub.1122373771,10.1109/access.2019.2951762,,,Isolated Pulmonary Nodules Characteristics Detection Based on CT Images,"Pulmonary nodules are the main pathological changes of the lung. Malignant pulmonary nodules will be transformed into lung cancer, which is a serious threat to human health and life. Therefore, the detection of pulmonary nodules is of great significance to save lives. However, in the face of a large number of lung CT image sequences, doctors need to spend a lot of time and energy, and in the detection process will inevitably produce the problem of false detection and missed detection. Therefore, it is very necessary for computer-aided doctors to detect pulmonary nodules. It is difficult to segment pulmonary nodules accurately and recognize the characteristics of pulmonary nodules in CT images. A complete set of semi-automatic lung nodule extraction and feature identification system is established, which is in line with the doctor’s diagnosis process. A segmentation algorithm of pulmonary nodules based on regional statistical information is proposed to extract pulmonary nodules accurately. This is the first time that dynamic time warping algorithm is applied in the field of image processing, focusing on the lung nodule boundary. On this basis, the recursive graph visualization model is established to realize the visualization of boundary similarity. Finally, in order to accurately identify the characteristics of pulmonary nodules, a video similarity distance discrimination system is introduced to quantify the similarity between the nodules to be examined and the pulmonary nodules in the database. The experimental results show that the algorithm can accurately identify the normal shape, lobulated shape and lobulated shape of pulmonary nodules. The average processing speed is 0.58s/nodule. To some extent, it can reduce the misdiagnosis caused by experience and fatigue.","This work was supported by in part by the National Natural Science Foundation of China under Grant 61873145, the Natural Science Foundation of Shandong Province for Excellent Young Scholars under Grant ZR2017JL029, the Fostering Project of Dominant Discipline and Talent Team of Shandong Province Higher Education Institutions, and the National Natural Science Foundation of China under Grant 61561040.",,IEEE Access,,,2019-01-01,2019,2019-11-06,2019-01-01,7,,165597-165606,All OA, Gold,Article,"Qiu, Shi; Guo, Qiang; Zhou, Dongmei; Jin, Yi; Zhou, Tao; He, Zhen’an","Qiu, Shi (Key Laboratory of Spectral Imaging Technology CAS, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, 710119, China); Guo, Qiang (School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, 250014, China); Zhou, Dongmei (School of Information Science and Technology, Chengdu University of Technology, Chengdu, 610059, China); Jin, Yi (School of Computer and Information Technology, Beijing Jiaotong University, Beijing, 100044, China); Zhou, Tao (School of Computer Science and Engineering, North Minzu University, Yinchuan, 750021, China); He, Zhen’an (Shaanxi Institute of Medical Device Quality Supervision and Inspection, Xi’an, 712046, China)","Qiu, Shi (Xian Institute of Optics and Precision Mechanics)","Qiu, Shi (Xian Institute of Optics and Precision Mechanics); Guo, Qiang (Shandong University of Finance and Economics); Zhou, Dongmei (Chengdu University of Technology); Jin, Yi (Beijing Jiaotong University); Zhou, Tao (North Minzu University); He, Zhen’an ()",5,2,,,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08892468.pdf,https://app.dimensions.ai/details/publication/pub.1122373771,46 Information and Computing Sciences,,,,,,,,,,,
1221,pub.1151380745,10.1109/cvpr52688.2022.01138,,,C-CAM: Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image,"Recently, many excellent weakly supervised semantic segmentation (WSSS) works are proposed based on class activation mapping (CAM). However, there are few works that consider the characteristics of medical images. In this paper, we find that there are mainly two challenges of medical images in WSSS: i) the boundary of object foreground and background is not clear; ii) the co-occurrence phenomenon is very severe in training stage. We thus propose a Causal CAM (C-CAM) method to overcome the above challenges. Our method is motivated by two cause-effect chains including category-causality chain and anatomy-causality chain. The category-causality chain represents the image content (cause) affects the category (effect). The anatomy-causality chain represents the anatomical structure (cause) affects the organ segmentation (effect). Extensive experiments were conducted on three public medical image data sets. Our C-CAM generates the best pseudo masks with the DSC of 77.26%, 80.34% and 78.15% on ProMRI, ACDC and CHAOS compared with other CAM-like methods. The pseudo masks of C-CAM are further used to improve the segmentation performance for organ segmentation tasks. Our C-CAM achieves DSC of 83.83% on ProMRI and DSC of 87.54% on ACDC, which outperforms state-of-the-art WSSS methods. Our code is available athttps://github.com/Tian-lab/C-CAM.","This work was supported in part by NSFC under grant Nos. 62173269 and 61876148, Natural Science Basic Research Plan in Shaanxi Province of China under Grant No. 2022JM-324, Key Research and Development Program of Shaanxi Province of China under Grant No. 2020GXLH-Y-008, Social Science Foundation of Shaanxi Province of China under Grant No. 2021K014.","This work was supported in part by NSFC under grant Nos. 62173269 and 61876148, Natural Science Basic Research Plan in Shaanxi Province of China under Grant No. 2022JM-324, Key Research and Development Program of Shaanxi Province of China under Grant No. 2020GXLH-Y-008, Social Science Foundation of Shaanxi Province of China under Grant No. 2021K014.",,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2022-06-24,2022,,2022-06-24,0,,11666-11675,Closed,Proceeding,"Chen, Zhang; Tian, Zhiqiang; Zhu, Jihua; Li, Ce; Du, Shaoyi","Chen, Zhang (Xi'an Jiaotong University, Xi'an, China); Tian, Zhiqiang (Xi'an Jiaotong University, Xi'an, China); Zhu, Jihua (Xi'an Jiaotong University, Xi'an, China); Li, Ce (Lanzhou University of Technology, Xi'an, China); Du, Shaoyi (Xi'an Jiaotong University, Xi'an, China)","Tian, Zhiqiang (Xi'an Jiaotong University); Du, Shaoyi (Xi'an Jiaotong University)","Chen, Zhang (Xi'an Jiaotong University); Tian, Zhiqiang (Xi'an Jiaotong University); Zhu, Jihua (Xi'an Jiaotong University); Li, Ce (Lanzhou University of Technology); Du, Shaoyi (Xi'an Jiaotong University)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1151380745,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,,
1220,pub.1141745751,10.2478/jaiscr-2022-0003,,,Mixup (Sample Pairing) Can Improve the Performance of Deep Segmentation Networks,"Abstract Researchers address the generalization problem of deep image processing networks mainly through extensive use of data augmentation techniques such as random flips, rotations, and deformations. A data augmentation technique called mixup, which constructs virtual training samples from convex combinations of inputs, was recently proposed for deep classification networks. The algorithm contributed to increased performance on classification in a variety of datasets, but so far has not been evaluated for image segmentation tasks. In this paper, we tested whether the mixup algorithm can improve the generalization performance of deep segmentation networks for medical image data. We trained a standard U-net architecture to segment the prostate in 100 T2-weighted 3D magnetic resonance images from prostate cancer patients, and compared the results with and without mixup in terms of Dice similarity coefficient and mean surface distance from a reference segmentation made by an experienced radiologist. Our results suggest that mixup offers a statistically significant boost in performance compared to non-mixup training, leading to up to 1.9% increase in Dice and a 10.9% decrease in surface distance. The mixup algorithm may thus offer an important aid for medical image segmentation applications, which are typically limited by severe data scarcity.",,,Journal of Artificial Intelligence and Soft Computing Research,,,2021-10-08,2021,2021-10-08,2022-01-01,12,1,29-39,All OA, Gold,Article,"Isaksson, Lars J.; Summers, Paul; Raimondi, Sara; Gandini, Sara; Bhalerao, Abhir; Marvaso, Giulia; Petralia, Giuseppe; Pepa, Matteo; Jereczek-Fossa, Barbara A.","Isaksson, Lars J. (Division of Radiotherapy, European Institute of Oncology IRCCS, via Ripamonti 435, Milan, Italy); Summers, Paul (Division of Radiology, European Institute of Oncology IRCCS, via Ripamonti 435, Milan, Italy); Raimondi, Sara (Department of Experimental Oncology, European Institute of Oncology IRCCS, via Ripamonti 435, Milan, Italy); Gandini, Sara (Department of Experimental Oncology, European Institute of Oncology IRCCS, via Ripamonti 435, Milan, Italy); Bhalerao, Abhir (Department of Computer Science, University of Warwick, Coventry CV4 7AL, Warwick, United Kingdom); Marvaso, Giulia (Division of Radiotherapy, European Institute of Oncology IRCCS, via Ripamonti 435, Milan, Italy; Department of Oncology and Hemato-oncology, University of Milan, via Festa del Perdono 7, Milan, Italy); Petralia, Giuseppe (Division of Radiology, European Institute of Oncology IRCCS, via Ripamonti 435, Milan, Italy; Department of Oncology and Hemato-oncology, University of Milan, via Festa del Perdono 7, Milan, Italy); Pepa, Matteo (Division of Radiotherapy, European Institute of Oncology IRCCS, via Ripamonti 435, Milan, Italy); Jereczek-Fossa, Barbara A. (Division of Radiotherapy, European Institute of Oncology IRCCS, via Ripamonti 435, Milan, Italy; Department of Oncology and Hemato-oncology, University of Milan, via Festa del Perdono 7, Milan, Italy)",,"Isaksson, Lars J. (); Summers, Paul (); Raimondi, Sara (European Institute of Oncology); Gandini, Sara (European Institute of Oncology); Bhalerao, Abhir (University of Warwick); Marvaso, Giulia (University of Milan); Petralia, Giuseppe (University of Milan); Pepa, Matteo (); Jereczek-Fossa, Barbara A. (University of Milan)",8,8,,,https://doi.org/10.2478/jaiscr-2022-0003,https://app.dimensions.ai/details/publication/pub.1141745751,46 Information and Computing Sciences,,,,,,,,,,,
1220,pub.1135773781,10.1007/978-3-030-69525-5_39,,,Bidirectional Pyramid Networks for Semantic Segmentation,"Semantic segmentation is a fundamental problem in computer vision that has attracted a lot of attention. Recent efforts have been devoted to network architecture innovations for efficient semantic segmentation that can run in real-time for autonomous driving and other applications. Information flow between scales is crucial because accurate segmentation needs both large context and fine detail. However, most existing approaches still rely on pretrained backbone models (e.g. ResNet on ImageNet). In this work, we propose to open up the backbone and design a simple yet effective multiscale network architecture, Bidirectional Pyramid Network (BPNet). BPNet takes the shape of a pyramid: information flows from bottom (high-resolution, small receptive field) to top (low-resolution, large receptive field), and from top to bottom, in a systematic manner, at every step of the processing. More importantly, fusion needs to be efficient; this is done through an add-and-multiply module with learned weights. We also apply a unary-pairwise attention mechanism to balance position sensitivity and context aggregation. Auxiliary loss is applied at multiple steps of the pyramid bottom. The resulting network achieves high accuracy with efficiency, without the need of pretraining. On the standard Cityscapes dataset, we achieve test mIoU 76.3 with 5.1M parameters and 36 fps (on Nvidia 2080 Ti), competitive with the state of the time real-time models. Meanwhile, our design is general and can be used to build heavier networks: a ResNet-101 equivalent version of BPNet achieves mIoU 81.9 on Cityscapes, competitive with the best published results. We further demonstrate the flexibility of BPNet on a prostate MRI segmentation task, achieving the state of the art with a 45×\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document} speed-up.",,,Lecture Notes in Computer Science,Computer Vision – ACCV 2020,,2021-02-27,2021,2021-02-27,2021,12622,,654-671,Closed,Chapter,"Nie, Dong; Xue, Jia; Ren, Xiaofeng","Nie, Dong (Amap, Alibaba Group, Beijing, China); Xue, Jia (Rutgers University, New Brunswick, USA); Ren, Xiaofeng (Amap, Alibaba Group, Beijing, China)","Nie, Dong (Alibaba Group (China))","Nie, Dong (Alibaba Group (China)); Xue, Jia (Rutgers, The State University of New Jersey); Ren, Xiaofeng (Alibaba Group (China))",11,11,,9.0,,https://app.dimensions.ai/details/publication/pub.1135773781,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1218,pub.1146728904,10.1007/s00521-022-07188-3,,,A novel prostate segmentation method: triple fusion model with hybrid loss,"Early and rapid diagnosis of prostate cancer, the horsehead disease among men, has become increasingly important. Nowadays, many methods are used in the early diagnosis of prostate cancer. Compared to other imaging methods, magnetic resonance imaging (MRI) based on prostate gland imaging is preferred because angular imaging (axial, sagittal, and coronal) provides precise information. But diagnosing the disease from these MR images is time-consuming. For example, imaging differences between MR devices for prostate segmentation and inhomogeneous and inconsistent prostate appearance are significant challenges. Because of these segmentation difficulties, manual segmentation of prostate images is challenging. In recent years, computer-aided intelligent architectures (deep learning-based architecture) have been used to overcome the manual segmentation of prostate images. These architectures can now perform manual prostate segmentation in seconds that used to take days thanks to their end-to-end automatic deep convolutional neural networks (DCNN). Inspired by the studies mentioned above, this study proposes a novel DCNN approach for prostate segmentation by combining ResUnet 2D with residual blocks and Edge Attention Vnet 3D architectures. In addition, the weighted foal Twersky loss function, which was proposed for the first time, significantly increased the architecture's performance. Evaluation experiments were performed on the MICCAI 2012 Prostate Segmentation Challenge Dataset (PROMISE12) and the NCI-ISBI 2013(NCI_ISBI-13) Prostate Segmentation Challenge Dataset. As a result of the tests performed, Dice scores of 91.92 and 91.15% in the whole prostate volume were obtained in the Promise 12 and NCI_ISBI 13 datasets, respectively. Comparative analyses show that the advantages and robustness of our method are superior to the state-of-the-art approaches.",,,Neural Computing and Applications,,,2022-03-30,2022,2022-03-30,2022-08,34,16,13559-13574,Closed,Article,"Ocal, Hakan; Barisci, Necaattin","Ocal, Hakan (Computer Engineering/School of Natural and Applied Sciences, Gazi University, Ankara, Turkey); Barisci, Necaattin (Computer Engineering/Technology Faculty, Gazi University, Ankara, Turkey)","Ocal, Hakan (Gazi University)","Ocal, Hakan (Gazi University); Barisci, Necaattin (Gazi University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1146728904,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,,
1218,pub.1140407225,10.1007/s11042-021-11334-9,,,Unet based Xception Model for Prostate Cancer Segmentation from MRI Images,"One of the most prevalent forms of tumor found in males all over the world is prostate cancer. The main risk factors are age and family history. Magnetic Resonance Imaging (MRI) is highly recommended for detecting and localizing prostate cancer. It is very important for precise segmentation of the prostate region in MRI scans to improve the treatment possibilities and the chance of patient survival with prostate cancer. Manually segmenting the prostate region is a daunting task and often time-consuming because of the variation in shapes of prostates among patients, poor delineation of the boundary, and the use of different MRI modes. In this paper, we propose an automatic segmentation model for the prostate regions in MRI scans based on Unet and Xception net. To boost  the performance of model, local residual connections are added in the decoder stage of the Unet. The empirical results are compared to different Unet based models with different preprocessing methods to assess the effectiveness of the proposed model. The experimentations are performed to support the fact that the proposed model performs better than other methods taken under study.",The datasets used in this work formed part of the task of promise12.,,Multimedia Tools and Applications,,,2021-08-13,2021,2021-08-13,2022-11,81,26,37333-37349,Closed,Article,"Chahal, Ekam Singh; Patel, Aarya; Gupta, Ayush; Purwar, Archana; G, Dhanalekshmi","Chahal, Ekam Singh (Department of Computer Science and Engineering/ Information Technology, Jaypee Institute of Information Technology, Noida, India); Patel, Aarya (Department of Computer Science and Engineering/ Information Technology, Jaypee Institute of Information Technology, Noida, India); Gupta, Ayush (Department of Computer Science and Engineering/ Information Technology, Jaypee Institute of Information Technology, Noida, India); Purwar, Archana (Department of Computer Science and Engineering/ Information Technology, Jaypee Institute of Information Technology, Noida, India); G, Dhanalekshmi (Department of Computer Science and Engineering/ Information Technology, Jaypee Institute of Information Technology, Noida, India)","Purwar, Archana (Jaypee Institute of Information Technology)","Chahal, Ekam Singh (Jaypee Institute of Information Technology); Patel, Aarya (Jaypee Institute of Information Technology); Gupta, Ayush (Jaypee Institute of Information Technology); Purwar, Archana (Jaypee Institute of Information Technology); G, Dhanalekshmi (Jaypee Institute of Information Technology)",6,6,,4.31,,https://app.dimensions.ai/details/publication/pub.1140407225,"40 Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation",,,,,,,,,,,,
1218,pub.1118959395,10.48550/arxiv.1801.10517,,,Densely Dilated Spatial Pooling Convolutional Network using benign loss  functions for imbalanced volumetric prostate segmentation,"The high incidence rate of prostate disease poses a requirement in early
detection for diagnosis. As one of the main imaging methods used for prostate
cancer detection, Magnetic Resonance Imaging (MRI) has wide range of appearance
and imbalance problems, making automated prostate segmentation fundamental but
challenging. Here we propose a novel Densely Dilated Spatial Pooling
Convolutional Network (DDSP ConNet) in encoder-decoder structure. It employs
dense structure to combine dilated convolution and global pooling, thus
supplies coarse segmentation results from encoder and decoder subnet and
preserves more contextual information. To obtain richer hierarchical feature
maps, residual long connection is furtherly adopted to fuse contexture
features. Meanwhile, we adopt DSC loss and Jaccard loss functions to train our
DDSP ConNet. We surprisingly found and proved that, in contrast to re-weighted
cross entropy, DSC loss and Jaccard loss have a lot of benign properties in
theory, including symmetry, continuity and differentiability about the
parameters of network. Extensive experiments on the MICCAI PROMISE12 challenge
dataset have been done to corroborate the effectiveness of our DDSP ConNet with
DSC loss and Jaccard loss. Totally, our method achieves a score of 85.78 in the
test dataset, outperforming most of other competitors.",,,arXiv,,,2018-01-31,2018,,,,,,All OA, Green,Preprint,"Liu, Qiuhua; Fu, Min; Gong, Xinqi; Jiang, Hao","Liu, Qiuhua (); Fu, Min (); Gong, Xinqi (); Jiang, Hao ()",,"Liu, Qiuhua (); Fu, Min (); Gong, Xinqi (); Jiang, Hao ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1118959395,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
1214,pub.1119130384,10.48550/arxiv.1804.10481,,,Interactive Medical Image Segmentation via Point-Based Interaction and  Sequential Patch Learning,"Due to low tissue contrast, irregular object appearance, and unpredictable
location variation, segmenting the objects from different medical imaging
modalities (e.g., CT, MR) is considered as an important yet challenging task.
In this paper, we present a novel method for interactive medical image
segmentation with the following merits. (1) Our design is fundamentally
different from previous pure patch-based and image-based segmentation methods.
We observe that during delineation, the physician repeatedly check the
inside-outside intensity changing to determine the boundary, which indicates
that comparison in an inside-outside manner is extremely important. Thus, we
innovatively model our segmentation task as learning the representation of the
bi-directional sequential patches, starting from (or ending in) the given
central point of the object. This can be realized by our proposed ConvRNN
network embedded with a gated memory propagation unit. (2) Unlike previous
interactive methods (requiring bounding box or seed points), we only ask the
physician to merely click on the rough central point of the object before
segmentation, which could simultaneously enhance the performance and reduce the
segmentation time. (3) We utilize our method in a multi-level framework for
better performance. We systematically evaluate our method in three different
segmentation tasks including CT kidney tumor, MR prostate, and PROMISE12
challenge, showing promising results compared with state-of-the-art methods.
The code is available here:
\href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.",,,arXiv,,,2018-04-27,2018,,,,,,All OA, Green,Preprint,"Sun, Jinquan; Shi, Yinghuan; Gao, Yang; Wang, Lei; Zhou, Luping; Yang, Wanqi; Shen, Dinggang","Sun, Jinquan (); Shi, Yinghuan (); Gao, Yang (); Wang, Lei (); Zhou, Luping (); Yang, Wanqi (); Shen, Dinggang ()",,"Sun, Jinquan (); Shi, Yinghuan (); Gao, Yang (); Wang, Lei (); Zhou, Luping (); Yang, Wanqi (); Shen, Dinggang ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119130384,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
1208,pub.1139278046,10.1134/s1054661821020024,,,Dilated Volumetric Network: an Enhanced Fully Convolutional Network for Volumetric Prostate Segmentation from Magnetic Resonance Imaging,"Early detection of prostate cancer is crucial for its successful treatment. However, it is not always an easy task because of the various image capturing configurations, like acquisition protocols, magnetic field strength, presence/absence of endorectal coil, and resolution. The major bottleneck in the process is the delineation of the prostate boundary for its localization, which is required for the detection of abnormalities and performing radiotherapy accurately. Phenomenal development in Artificial Intelligence and Deep Learning has been contributing significantly to medical diagnostics using Computer Vision and the self-learning capabilities of Deep Learning has been explored to present a viable solution to automate this repetitive task of prostate segmentation. The previous approaches of 2D segmentation do not capture volumetric information and are very time consuming too. Hence, we have developed a Deep Learning based automated solution called DV-Net (Dilated Volumetric Network) for volumetric segmentation of prostate cancer. The proposed method considers the full prostate volume in 3D and requires minimal post-processing, which makes it less dependent on the type of input. We also focus on increasing the receptive field of the network and use deep supervision for better segmentation accuracy. Owing to all these features, DV-Net has shown to outperform the accuracy of the baseline V-Net model on the Prostate MR Image Segmentation (PROMISE) data set.",,,Pattern Recognition and Image Analysis,,,2021-04,2021,2021-06-30,2021-04,31,2,228-239,Closed,Article,"Aman Agarwal; Mishra, Aditya; Basavarajaiah, Madhushree; Sharma, Priyanka; Tanwar, Sudeep","Aman Agarwal (Department of Computer Science and Engineering, Institute of Technology, Nirma University, 382481, Ahmedabad, Gujarat, India); Mishra, Aditya (Department of Computer Science and Engineering, Institute of Technology, Nirma University, 382481, Ahmedabad, Gujarat, India); Basavarajaiah, Madhushree (Department of Computer Science and Engineering, Institute of Technology, Nirma University, 382481, Ahmedabad, Gujarat, India); Sharma, Priyanka (Department of Computer Science and Engineering, Institute of Technology, Nirma University, 382481, Ahmedabad, Gujarat, India); Tanwar, Sudeep (Department of Computer Science and Engineering, Institute of Technology, Nirma University, 382481, Ahmedabad, Gujarat, India)","Aman Agarwal (Nirma University); Mishra, Aditya (Nirma University); Basavarajaiah, Madhushree (Nirma University); Sharma, Priyanka (Nirma University); Tanwar, Sudeep (Nirma University)","Aman Agarwal (Nirma University); Mishra, Aditya (Nirma University); Basavarajaiah, Madhushree (Nirma University); Sharma, Priyanka (Nirma University); Tanwar, Sudeep (Nirma University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1139278046,40 Engineering, 49 Mathematical Sciences,,,,,,,,,,,
1182,pub.1105427299,10.1016/j.bbe.2018.06.009,,,Computer-aided diagnosis of clinically significant prostate cancer from MRI images using sparse autoencoder and random forest classifier,"A novel method to diagnose clinically significant prostate cancer (PCa) using Multi-parametric Magnetic Resonance Imaging (mpMRI) biomarkers in a highly imbalanced dataset is investigated in this paper. Transaxial T2 Weighted (T2W), Apparent Diffusion Coefficient (ADC) and high B-Value (BVAL) Diffusion-Weighted (DW) images obtained from PROSTATEx 2016 challenge dataset publicly available in TCIA (The Cancer Imaging Archive) is used for this study. High-level features are extracted using a single layer Sparse Autoencoder (SAE). Synthetic Minority Oversampling Technique (SMOTE), Weka Resample algorithm and Adaptive Synthetic (ADASYN) sampling approach are explored to solve the class-imbalance problem. The performance of various classifiers are also investigated and it was found that the data augmented using ADASYN followed by classification using random forest classifier achieved the best performance. It achieved an area under ROC curve of 0.979. It also reached a Cohen's kappa score of 0.873, an accuracy of 93.65% and F-Measure of 0.94 in distinguishing clinically significant PCa from indolent PCa.","We thank American Association of Physicists in Medicine (AAPM), SPIE (The International Society for Optics and Photonics) and the National Cancer Institute (NCI) and TCIA (The Cancer Imaging Archive) and Radboud University for providing the dataset. We are grateful to Dr. Anil Prahladan, Assistant Professor of Radiology, Regional Cancer Centre, Thiruvananthapuram, Kerala, India for suggesting Computer Aided Prostate Cancer Detection as our research area. Bejoy Abraham is thankful to University of Kerala for providing him University Junior Research Fellowship.",,Journal of Applied Biomedicine,,,2018,2018,,2018,38,3,733-744,Closed,Article,"Abraham, Bejoy; Nair, Madhu S.","Abraham, Bejoy (Department of Computer Science, University of Kerala, Kariavattom, Thiruvananthapuram 695581, Kerala, India); Nair, Madhu S. (Department of Computer Science, Cochin University of Science and Technology, Kochi 682022, Kerala, India)","Abraham, Bejoy (University of Kerala)","Abraham, Bejoy (University of Kerala); Nair, Madhu S. (Cochin University of Science and Technology)",23,9,,3.74,,https://app.dimensions.ai/details/publication/pub.1105427299,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,,
1178,pub.1149106611,10.48550/arxiv.2206.14467,,,Single-domain Generalization in Medical Image Segmentation via Test-time  Adaptation from Shape Dictionary,"Domain generalization typically requires data from multiple source domains
for model learning. However, such strong assumption may not always hold in
practice, especially in medical field where the data sharing is highly
concerned and sometimes prohibitive due to privacy issue. This paper studies
the important yet challenging single domain generalization problem, in which a
model is learned under the worst-case scenario with only one source domain to
directly generalize to different unseen target domains. We present a novel
approach to address this problem in medical image segmentation, which extracts
and integrates the semantic shape prior information of segmentation that are
invariant across domains and can be well-captured even from single domain data
to facilitate segmentation under distribution shifts. Besides, a test-time
adaptation strategy with dual-consistency regularization is further devised to
promote dynamic incorporation of these shape priors under each unseen domain to
improve model generalizability. Extensive experiments on two medical image
segmentation tasks demonstrate the consistent improvements of our method across
various unseen domains, as well as its superiority over state-of-the-art
approaches in addressing domain generalization under the worst-case scenario.",,,arXiv,,,2022-06-29,2022,,,,,,All OA, Green,Preprint,"Liu, Quande; Chen, Cheng; Dou, Qi; Heng, Pheng-Ann","Liu, Quande (); Chen, Cheng (); Dou, Qi (); Heng, Pheng-Ann ()",,"Liu, Quande (); Chen, Cheng (); Dou, Qi (); Heng, Pheng-Ann ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149106611,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1178,pub.1099643186,10.1109/access.2017.2781278,,,Automatic Magnetic Resonance Image Prostate Segmentation Based on Adaptive Feature Learning Probability Boosting Tree Initialization and CNN-ASM Refinement,"This paper proposes a method based on the active shape model (ASM) to segment the prostate in magnetic resonance (MR) images. Due to the great variability in appearance among different boundaries of the prostate and among subjects, the traditional ASM is weak in MR image prostate segmentation. To address these limitations, we investigated a novel ASM-based method by incorporating deep feature learning into our previous liver segmentation framework. First, an adaptive feature learning probability boosting tree (AFL-PBT) based on both simple handcrafted features and deep learned features was developed for prostate pre-segmentation and for further shape model initialization. The proposed AFL-PBT classifier also provided a boundary searching band, which made the ASM less sensitive to model initialization. Then, the convolutional neutral network (CNN) deep learning method was used to train a boundary model, which separated voxels into three types: near, inside, and outside the boundary. A three-level ASM based on the CNN boundary model was employed for the final segmentation refinement. On MICCAI PROMISE12 test data sets, the proposed method yielded a mean Dice score of 84% with a standard deviation of 4%. The experimental results demonstrated that the proposed method outperformed other ASM-based prostate MRI segmentation methods and achieved a level of accuracy comparable to that of state-of-the-art methods.",,,IEEE Access,,,2018-02-14,2018,2018-02-14,,6,,2005-2015,All OA, Gold,Article,"He, Baochun; Xiao, Deqiang; Hu, Qingmao; Jia, Fucang","He, Baochun (Research Laboratory for Medical Imaging and Digital Surgery, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China); Xiao, Deqiang (Research Laboratory for Medical Imaging and Digital Surgery, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China); Hu, Qingmao (Research Laboratory for Medical Imaging and Digital Surgery, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China); Jia, Fucang (Research Laboratory for Medical Imaging and Digital Surgery, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China)",,"He, Baochun (Shenzhen Institutes of Advanced Technology); Xiao, Deqiang (Shenzhen Institutes of Advanced Technology); Hu, Qingmao (Shenzhen Institutes of Advanced Technology); Jia, Fucang (Shenzhen Institutes of Advanced Technology)",19,7,,6.19,https://doi.org/10.1109/access.2017.2781278,https://app.dimensions.ai/details/publication/pub.1099643186,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1175,pub.1130690712,10.1016/j.bbe.2020.07.011,,,Prostate lesion segmentation in MR images using radiomics based deeply supervised U-Net,"Prostate lesion detection in an axial T2 weighted (T2W) MR images is a very challenging task due to heterogeneous and inconsistent pixel representation surrounding the prostate boundary. In this paper, a radiomics based deeply supervised U-Net is proposed for both prostate gland and prostate lesion segmentation. The proposed pipeline is trained and validated on 1174 and 2071 T2W MR images of 40 patients and tested on 250 and 415 T2W MR images of 10 patients for prostate capsule segmentation and prostate lesion segmentation, respectively. Effective segmentation of prostate lesions in various stages of prostate cancer (namely T1, T2, T3, and T4) is achieved using the proposed framework. The mean Dice Similarity Coefficient (DSC) for actual prostate capsule segmentation and prostate lesion segmentation is 0.8958 and 0.9176, respectively. The proposed framework is also tested on Promise12 public dataset for performance analysis in segmenting prostate gland. The segmentation results using proposed architecture are promising compared to state-of-the-art techniques. It also improves the accuracy of the prostate cancer diagnosis.",,,Journal of Applied Biomedicine,,,2020-10,2020,,2020-10,40,4,1421-1435,Closed,Article,"Hambarde, Praful; Talbar, Sanjay; Mahajan, Abhishek; Chavan, Satishkumar; Thakur, Meenakshi; Sable, Nilesh","Hambarde, Praful (SGGS Institute of Engineering and Technology, Nanded - 431606, Maharashtra, India); Talbar, Sanjay (SGGS Institute of Engineering and Technology, Nanded - 431606, Maharashtra, India); Mahajan, Abhishek (Tata Memorial Hospital, Parel, Mumbai, Maharashtra 400012, India); Chavan, Satishkumar (Don Bosco Institute of Technology, Kurla (W), Mumbai - 400070, Maharashtra, India); Thakur, Meenakshi (Tata Memorial Hospital, Parel, Mumbai, Maharashtra 400012, India); Sable, Nilesh (Tata Memorial Hospital, Parel, Mumbai, Maharashtra 400012, India)","Sable, Nilesh (Tata Memorial Hospital)","Hambarde, Praful (); Talbar, Sanjay (); Mahajan, Abhishek (Tata Memorial Hospital); Chavan, Satishkumar (University of Mumbai); Thakur, Meenakshi (Tata Memorial Hospital); Sable, Nilesh (Tata Memorial Hospital)",28,28,,6.83,,https://app.dimensions.ai/details/publication/pub.1130690712,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,,
1175,pub.1128506834,10.1145/3341105.3373856,,,EvoU-Net,"Developing a Deep Convolutional Neural Network (DCNN) for image segmentation is challenging research topic which needs extensive experiments to find an appropriate network structure and a precise set of hyper-parameters. The limited number of available labelled images and the required computational infrastructure make this task even more challenging. Evolutionary strategy is an optimisation technique that is applicable to alleviate the above difficulties. This paper proposes an evolutionary-based method to find a precise and small network for medical image segmentation. To the best of our knowledge, EvoU-Net is the first evolutionary method to develop an U-Net-based deep network topology with for medical image segmentation. In the proposed model, a Genetic Algorithm (GA) is applied to design an optimal network structure, along with its parameters, for MRI image segmentation. EvoU-Net outperformed U-Net and AdaResU-Net while using less than 10% and 50% of trainable parameters respectively, for segmentation of a publicly available prostate MRI dataset.",,,,Proceedings of the 35th Annual ACM Symposium on Applied Computing,,2020-03-30,2020,2020-03-30,2020-03-30,,,181-189,Closed,Proceeding,"Hassanzadeh, Tahereh; Essam, Daryl; Sarker, Ruhul","Hassanzadeh, Tahereh (University of New South Wales, Canberra, Australia); Essam, Daryl (University of New South Wales, Canberra, Australia); Sarker, Ruhul (University of New South Wales, Canberra, Australia)",,"Hassanzadeh, Tahereh (UNSW Sydney); Essam, Daryl (UNSW Sydney); Sarker, Ruhul (UNSW Sydney)",7,7,,3.25,,https://app.dimensions.ai/details/publication/pub.1128506834,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4605 Data Management and Data Science,,,,,,,,,,
1175,pub.1121899749,10.1109/icecct.2019.8869533,,,On Computer-Aided Diagnosis of Prostate Cancer from MRI using Machine Intelligence Techniques,"Prostate cancer is one of the major cause of cancer death among men. Mortality rate due to prostate cancer can be reduced significantly by early diagnosis and treatment planning. Magnetic Resonance Imaging (MRI) is a major modality for early detection of prostate cancer. Recent studies have proven that MRI can be used for grading of prostate cancer also. In this paper, a survey on computer-aided detection and grading of prostate cancer is done exploring the state-of-the-art techniques. The paper examines various stages of computer-aided diagnosis of prostate cancer including detection of cancer, estimation of clinical significance of cancer and grading of cancer. The survey compares the performance of different techniques available in the lliteraure and provides directions for further research.",,,,"2019 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)",,2019-02-22,2019,,2019-02-22,0,,1-8,Closed,Proceeding,"Abraham, Bejoy; Nair, Madhu S.","Abraham, Bejoy (Department of Computer Science, University of Kerala, Kariavattom, Thiruvananthapuram, 695581, Kerala, India; Department of Computer Science and Engineering, College of Engineering Perumon, Perinad, Kollam, 691601, Kerala, India); Nair, Madhu S. (Department of Computer Science, Cochin University of Science and Technology, Kochi, 682022, Kerala, India)","Abraham, Bejoy (University of Kerala; )","Abraham, Bejoy (University of Kerala); Nair, Madhu S. (Cochin University of Science and Technology)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1121899749,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,3 Good Health and Well Being,,,,,,,,,
1170,pub.1136816488,10.48550/arxiv.2103.15858,,,CateNorm: Categorical Normalization for Robust Medical Image  Segmentation,"Batch normalization (BN) uniformly shifts and scales the activations based on
the statistics of a batch of images. However, the intensity distribution of the
background pixels often dominates the BN statistics because the background
accounts for a large proportion of the entire image. This paper focuses on
enhancing BN with the intensity distribution of foreground pixels, the one that
really matters for image segmentation. We propose a new normalization strategy,
named categorical normalization (CateNorm), to normalize the activations
according to categorical statistics. The categorical statistics are obtained by
dynamically modulating specific regions in an image that belong to the
foreground. CateNorm demonstrates both precise and robust segmentation results
across five public datasets obtained from different domains, covering complex
and variable data distributions. It is attributable to the ability of CateNorm
to capture domain-invariant information from multiple domains (institutions) of
medical data. Code is available at https://github.com/lambert-x/CateNorm.",,,arXiv,,,2021-03-29,2021,,,,,,All OA, Green,Preprint,"Xiao, Junfei; Yu, Lequan; Zhou, Zongwei; Bai, Yutong; Xing, Lei; Yuille, Alan; Zhou, Yuyin","Xiao, Junfei (); Yu, Lequan (); Zhou, Zongwei (); Bai, Yutong (); Xing, Lei (); Yuille, Alan (); Zhou, Yuyin ()",,"Xiao, Junfei (); Yu, Lequan (); Zhou, Zongwei (); Bai, Yutong (); Xing, Lei (); Yuille, Alan (); Zhou, Yuyin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136816488,46 Information and Computing Sciences, 49 Mathematical Sciences, 4905 Statistics,,,,,,,,,
1170,pub.1123148724,10.1101/856542,,,Measurement of Prostate Volume with MRI (A Guide for the Perplexed): Biproximate Method with Analysis of Precision and Accuracy,"Abstract
                
                  Purpose
                  To review the anatomic basis of prostate boundary selection on T2-weighted magnetic resonance imaging (MRI). To introduce an alternative 3D ellipsoid measuring technique that maximizes precision, report the intra- and inter-observer reliability, and to advocate it’s use for research involving multiple observers.
                
                
                  Methods
                  A demonstration of prostate boundary anatomy using gross pathology and MRI examples provides background for selection of key boundary marks when measuring prostate volume. An alternative ellipsoid volume method is illustrated. An IRB approved retrospective study of 140 patients with elevated serum prostate specific antigen levels and/or abnormal digital rectal examinations was done with T2-weighted MRI applying the new (Biproximate) technique. Measurements were made by 2 examiners, correlated with each other for interobserver precision and with an expert observer for accuracy. Correlation statistics, linear regression analysis, and tests of means were applied using p≤0.05 as the threshold for significance.
                
                
                  Results
                  Inter-observer correlation (precision) was 0.95 between observers. Correlation between these observers and the expert (accuracy) was 0.94 and 0.97 respectively. Intra-observer correlation for the expert was 0.98. Means for inter-rater reliability and accuracy were all the same (p=0.001).
                
                
                  Conclusions
                  Anatomic foundations for the boundaries of the prostate are reviewed. Precision and accuracy of total prostate volume using an alternative method is reported and found to be reproducible.",,,bioRxiv,,,2019-12-04,2019,2019-12-04,,,,856542,All OA, Green,Preprint,"Wasserman, Neil F.; Niendorf, Eric; Spilseth, Benjamin","Wasserman, Neil F. (Department of Radiology University of Minnesota, Mayo Mail Code 292, 420 Delaware Street S.E. Minneapolis, MN 55455, (612) 626-3343); Niendorf, Eric (Department of Radiology University of Minnesota, Mayo Mail Code 292, 420 Delaware Street S.E. Minneapolis, MN 55455, (612) 626-3343); Spilseth, Benjamin (Department of Radiology University of Minnesota, Mayo Mail Code 292, 420 Delaware Street S.E. Minneapolis, MN 55455, (612) 626-3343)","Wasserman, Neil F. (University of Minnesota)","Wasserman, Neil F. (University of Minnesota); Niendorf, Eric (University of Minnesota); Spilseth, Benjamin (University of Minnesota)",0,0,,0.0,https://www.nature.com/articles/s41598-019-57046-x.pdf,https://app.dimensions.ai/details/publication/pub.1123148724,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
1145,pub.1152337565,10.48550/arxiv.2210.15949,,,IB-U-Nets: Improving medical image segmentation tasks with 3D Inductive  Biased kernels,"Despite the success of convolutional neural networks for 3D medical-image
segmentation, the architectures currently used are still not robust enough to
the protocols of different scanners, and the variety of image properties they
produce. Moreover, access to large-scale datasets with annotated regions of
interest is scarce, and obtaining good results is thus difficult. To overcome
these challenges, we introduce IB-U-Nets, a novel architecture with inductive
bias, inspired by the visual processing in vertebrates. With the 3D U-Net as
the base, we add two 3D residual components to the second encoder blocks. They
provide an inductive bias, helping U-Nets to segment anatomical structures from
3D images with increased robustness and accuracy. We compared IB-U-Nets with
state-of-the-art 3D U-Nets on multiple modalities and organs, such as the
prostate and spleen, using the same training and testing pipeline, including
data processing, augmentation and cross-validation. Our results demonstrate the
superior robustness and accuracy of IB-U-Nets, especially on small datasets, as
is typically the case in medical-image analysis. IB-U-Nets source code and
models are publicly available.",,,arXiv,,,2022-10-28,2022,,,,,,All OA, Green,Preprint,"Bhandary, Shrajan; Babaiee, Zahra; Kostyszyn, Dejan; Fechter, Tobias; Zamboglou, Constantinos; Grosu, Anca-Ligia; Grosu, Radu","Bhandary, Shrajan (); Babaiee, Zahra (); Kostyszyn, Dejan (); Fechter, Tobias (); Zamboglou, Constantinos (); Grosu, Anca-Ligia (); Grosu, Radu ()",,"Bhandary, Shrajan (); Babaiee, Zahra (); Kostyszyn, Dejan (); Fechter, Tobias (); Zamboglou, Constantinos (); Grosu, Anca-Ligia (); Grosu, Radu ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152337565,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1145,pub.1140052522,10.48550/arxiv.2107.13741,,,Self-Paced Contrastive Learning for Semi-supervised Medical Image  Segmentation with Meta-labels,"Pre-training a recognition model with contrastive learning on a large dataset
of unlabeled data has shown great potential to boost the performance of a
downstream task, e.g., image classification. However, in domains such as
medical imaging, collecting unlabeled data can be challenging and expensive. In
this work, we propose to adapt contrastive learning to work with meta-label
annotations, for improving the model's performance in medical image
segmentation even when no additional unlabeled data is available. Meta-labels
such as the location of a 2D slice in a 3D MRI scan or the type of device used,
often come for free during the acquisition process. We use the meta-labels for
pre-training the image encoder as well as to regularize a semi-supervised
training, in which a reduced set of annotated data is used for training.
Finally, to fully exploit the weak annotations, a self-paced learning approach
is used to help the learning and discriminate useful labels from noise. Results
on three different medical image segmentation datasets show that our approach:
i) highly boosts the performance of a model trained on a few scans, ii)
outperforms previous contrastive and semi-supervised approaches, and iii)
reaches close to the performance of a model trained on the full data.",,,arXiv,,,2021-07-29,2021,,,,,,All OA, Green,Preprint,"Peng, Jizong; Wang, Ping; Desrosiers, Chrisitian; Pedersoli, Marco","Peng, Jizong (); Wang, Ping (); Desrosiers, Chrisitian (); Pedersoli, Marco ()",,"Peng, Jizong (); Wang, Ping (); Desrosiers, Chrisitian (); Pedersoli, Marco ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140052522,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1145,pub.1139635590,10.1109/access.2021.3096665,,,ProSegNet: A New Network of Prostate Segmentation Based on MR Images,"Prostate cancer is the most common cancer in men after lung cancer. Generally, the segmentation of the prostate is the preprocessing work for the diagnosis of prostate cancer. Aiming at the variety of prostate and the similarity of visual characteristics between prostates and their surroundings, this paper proposes a new prostate segmentation network based on MR images, denoted as ProSegNet. ProSegNet consists of two parts: encoder and decoder. To improve the feature extraction capability of the encoder, we use dense blocks as the feature extraction unit, and at the same time introduce a cross-stage partial (CSP) structure to reduce the amount of calculation. In the design of the decoder structure, we integrated the spatial attention mechanism and the channel attention mechanism to enable it to focus on the important features while ignoring the invalid features. In addition, to segment the prostate more accurately, we add a prostate contour segmentation branch to the output of the segmentation network to learn the contour features of the prostate. Finally, to alleviate the problem of small intensity difference between the prostate and surrounding tissues, we designed a truncated intensity stretching image enhancement method. The performance of ProSegNet has been experimentally verified in the Promise12 and ProstateX datasets. On the Promise12 dataset, the dice similarity coefficient (DSC) and hausdorff (Haus) distance are 0.908 and 9.87 respectively. On the ProstateX dataset, the DSC and Haus reach the results of 0.892 and 10.45, respectively. Experimental results show that the ProSegNet can obtain a competitive performance.",This work was supported by the Natural Science Foundation of Zhejiang Province under Grant LGF19F020004.,,IEEE Access,,,2021-01-01,2021,2021-07-12,2021-01-01,9,,106293-106302,All OA, Gold,Article,"Qian, Yuejing","Qian, Yuejing (Zhejiang Industry and Trade Vocational College, Wenzhou, Zhejiang, 325000, China)","Qian, Yuejing ","Qian, Yuejing ()",1,1,,,https://ieeexplore.ieee.org/ielx7/6287639/9312710/09481240.pdf,https://app.dimensions.ai/details/publication/pub.1139635590,46 Information and Computing Sciences,,,,,,,,,,,
1143,pub.1149622997,10.21203/rs.3.rs-1833303/v1,,,Leveraging Image Complexity in Macro-Level Neural Network Design for Medical Image Segmentation,"Recent progress in encoder-decoder neural network architecture design has led to significant performance improvements in a wide range of medical image segmentation tasks. However, state-of-the-art networks for a given task may be too computationally demanding to run on affordable hardware, and thus users often resort to practical workarounds by modifying various macro-level design aspects. Two common examples are downsampling of the input images and reducing the network depth or size to meet computer memory constraints. In this paper, we investigate the effects of these changes on segmentation performance and show that image complexity can be used as a guideline in choosing what is best for a given dataset. We consider four statistical measures to quantify image complexity and evaluate their suitability on ten different public datasets. For the purpose of our illustrative experiments, we use DeepLabV3+ (deep large-size), M2U-Net (deep lightweight), U-Net (shallow large-size), and U-Net Lite (shallow lightweight). Our results suggest that median frequency is the best complexity measure when deciding on an acceptable input down-sampling factor and using a deep versus shallow, large-size versus lightweight network. For high-complexity datasets, a lightweight network running on the original images may yield better segmentation results than a large-size network running on downsampled images, whereas the opposite may be the case for low-complexity images.",,,Research Square,,,2022-07-20,2022,2022-07-20,,,,,All OA, Green,Preprint,"Khan, Tariq M.; Naqvi, Syed S.; Meijering, Erik","Khan, Tariq M. (University of New South Wales); Naqvi, Syed S. (COMSATS University Islamabad); Meijering, Erik (University of New South Wales)",,"Khan, Tariq M. (UNSW Sydney); Naqvi, Syed S. (COMSATS University Islamabad); Meijering, Erik (UNSW Sydney)",1,1,,,https://www.researchsquare.com/article/rs-1833303/latest.pdf,https://app.dimensions.ai/details/publication/pub.1149622997,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1143,pub.1137543646,10.48550/arxiv.2104.13082,,,Weakly Supervised Volumetric Segmentation via Self-taught Shape  Denoising Model,"Weakly supervised segmentation is an important problem in medical image
analysis due to the high cost of pixelwise annotation. Prior methods, while
often focusing on weak labels of 2D images, exploit few structural cues of
volumetric medical images. To address this, we propose a novel
weakly-supervised segmentation strategy capable of better capturing 3D shape
prior in both model prediction and learning. Our main idea is to extract a
self-taught shape representation by leveraging weak labels, and then integrate
this representation into segmentation prediction for shape refinement. To this
end, we design a deep network consisting of a segmentation module and a shape
denoising module, which are trained by an iterative learning strategy.
Moreover, we introduce a weak annotation scheme with a hybrid label design for
volumetric images, which improves model learning without increasing the overall
annotation cost. The empirical experiments show that our approach outperforms
existing SOTA strategies on three organ segmentation benchmarks with
distinctive shape properties. Notably, we can achieve strong performance with
even 10\% labeled slices, which is significantly superior to other methods.",,,arXiv,,,2021-04-27,2021,,,,,,All OA, Green,Preprint,"He, Qian; Li, Shuailin; He, Xuming","He, Qian (); Li, Shuailin (); He, Xuming ()",,"He, Qian (); Li, Shuailin (); He, Xuming ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1137543646,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1143,pub.1124648256,10.48550/arxiv.2002.01568,,,DVNet: A Memory-Efficient Three-Dimensional CNN for Large-Scale  Neurovascular Reconstruction,"Maps of brain microarchitecture are important for understanding neurological
function and behavior, including alterations caused by chronic conditions such
as neurodegenerative disease. Techniques such as knife-edge scanning microscopy
(KESM) provide the potential for whole organ imaging at sub-cellular
resolution. However, multi-terabyte data sizes make manual annotation
impractical and automatic segmentation challenging. Densely packed cells
combined with interconnected microvascular networks are a challenge for current
segmentation algorithms. The massive size of high-throughput microscopy data
necessitates fast and largely unsupervised algorithms. In this paper, we
investigate a fully-convolutional, deep, and densely-connected encoder-decoder
for pixel-wise semantic segmentation. The excessive memory complexity often
encountered with deep and dense networks is mitigated using skip connections,
resulting in fewer parameters and enabling a significant performance increase
over prior architectures. The proposed network provides superior performance
for semantic segmentation problems applied to open-source benchmarks. We
finally demonstrate our network for cellular and microvascular segmentation,
enabling quantitative metrics for organ-scale neurovascular analysis.",,,arXiv,,,2020-02-04,2020,,,,,,All OA, Green,Preprint,"Saadatifard, Leila; Mobiny, Aryan; Govyadinov, Pavel; Nguyen, Hien; Mayerich, David","Saadatifard, Leila (); Mobiny, Aryan (); Govyadinov, Pavel (); Nguyen, Hien (); Mayerich, David ()",,"Saadatifard, Leila (); Mobiny, Aryan (); Govyadinov, Pavel (); Nguyen, Hien (); Mayerich, David ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1124648256,40 Engineering, 46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,
1143,pub.1124229426,10.1016/j.jmse.2020.01.002,,,Computer-aided diagnosis and decision-making system for medical data analysis: A case study on prostate MR images,"Prostate cancer is the most common cancer in males and a major cause of cancer-related death. Magnetic resonance (MR) imaging is recently emerging as a powerful tool for prostate cancer diagnosis. To clinically diagnose prostate cancer, doctors need to segment the prostate area in the MR image. However, manual segmentation is time consuming and influenced by the physician’s experience. Computer-aided diagnosis and decision-making systems have shown great effectiveness in assisting doctors for this purpose. At the same time, deep learning based on Generative Adversarial Networks can be applied to the segmentation of prostate MR images. In this paper, we propose a new computer-aided diagnosis and decision-making system based on a deep learning model to automatically segment the prostate region from prostate MR images. Additionally, receptive field block (RFB) was integrated into the model to enhance the discriminability and robustness of the extracted multi-scale features. We also introduced dense upsampling convolution instead of the traditional bilinear interpolation to capture and recover fine-detailed information. Adversarial training was used to train the model, and the segmentation results were experimentally tested. The results showed that adversarial training and RFB are indeed effective, and the proposed method is superior to other methods on various evaluation metrics.",This work was supported by National Nature Science Foundation of China Grand No: 61371156. The authors would like to thank the anonymous reviews for their helpful and constructive comments and suggestions regarding this manuscript.,,Journal of Management Science and Engineering,,,2019-12,2019,,2019-12,4,4,266-278,All OA, Gold,Article,"Chen, Ailian; Zhu, Leilei; Zang, Huaijuan; Ding, Zhenglong; Zhan, Shu","Chen, Ailian (School of Computer and Information, Hefei University of Technology, Hefei, 230601, China); Zhu, Leilei (Anhui Institute of Information Technology, Wuhu, 241000, China); Zang, Huaijuan (School of Computer and Information, Hefei University of Technology, Hefei, 230601, China); Ding, Zhenglong (Anhui Institute of Information Technology, Wuhu, 241000, China); Zhan, Shu (School of Computer and Information, Hefei University of Technology, Hefei, 230601, China)","Zhan, Shu (Hefei University of Technology)","Chen, Ailian (Hefei University of Technology); Zhu, Leilei (Anhui Institute of Information Technology); Zang, Huaijuan (Hefei University of Technology); Ding, Zhenglong (Anhui Institute of Information Technology); Zhan, Shu (Hefei University of Technology)",9,7,,4.12,https://doi.org/10.1016/j.jmse.2020.01.002,https://app.dimensions.ai/details/publication/pub.1124229426,"35 Commerce, Management, Tourism and Services; 3507 Strategy, Management and Organisational Behaviour; 40 Engineering; 4010 Engineering Practice and Education",,,,,,,,,,,
1143,pub.1123505199,10.1016/j.dsp.2019.102649,,,Automatic segmentation of 3D prostate MR images with iterative localization refinement,"Accurate segmentation of the prostate gland from Magnetic Resonance (MR) images is still a challenging problem due to large variability and heterogeneity in the prostate appearance. To overcome this problem, we present a coarse-to-fine prostate segmentation approach with iterative localization refinement. Specifically, we first propose a resolution-aware 3D U-shaped network to balance the difference between the in-plane resolution and the through-plane distance. Then a case-wise loss function is introduced to alleviate the data imbalance problem and individual differences of the prostate MR images. In the inference stage, we extract a shrunk prostate region and improve the segmentation results in an iterative manner. Evaluation experiments are carried out on the MICCAI 2012 Prostate Segmentation Challenge Dataset (PROMISE12) and the NCI-ISBI 2013 Prostate Segmentation Challenge Dataset. Comparison results demonstrate that our method achieves significant improvements over the state-of-the-art approaches, and outperforms more than 290 submissions on the website of PROMISE12.","This work is supported in part by the Key Program of Zhejiang Provincial Natural Science Foundation of China (LZ14F020003), Open Project of Zhejiang Provincial Key Laboratory of Information Processing, Communication and Networking. The authors are grateful for the anonymous reviewers who made constructive comments.",,Digital Signal Processing,,,2020-03,2020,,2020-03,98,,102649,Closed,Article,"Zhou, Wenhui; Tao, Xing; Wei, Zhan; Lin, Lili","Zhou, Wenhui (Hangzhou Dianzi University, Hangzhou, China; Zhejiang Provincial Key Laboratory of Information Processing, Communication and Networking, Zhejiang, China); Tao, Xing (Hangzhou Dianzi University, Hangzhou, China); Wei, Zhan (Hangzhou Dianzi University, Hangzhou, China); Lin, Lili (Zhejiang Gongshang University, Hangzhou, China)","Lin, Lili (Zhejiang Gongshang University)","Zhou, Wenhui (Hangzhou Dianzi University); Tao, Xing (Hangzhou Dianzi University); Wei, Zhan (Hangzhou Dianzi University); Lin, Lili (Zhejiang Gongshang University)",12,12,,,,https://app.dimensions.ai/details/publication/pub.1123505199,46 Information and Computing Sciences,,,,,,,,,,,,
1143,pub.1092749893,10.1117/12.2283487,,,Open-source software platform for medical image segmentation applications,"Segmenting 2D and 3D images is a crucial and challenging problem in medical image analysis. Although several image segmentation algorithms have been proposed for different applications, no universal method currently exists. Moreover, their use is usually limited when detection of complex and multiple adjacent objects of interest is needed. In addition, the continually increasing volumes of medical imaging scans require more efficient segmentation software design and highly usable applications. In this context, we present an extension of our previous segmentation framework which allows the combination of existing explicit deformable models in an efficient and transparent way, handling simultaneously different segmentation strategies and interacting with a graphic user interface (GUI). We present the object-oriented design and the general architecture which consist of two layers: the GUI at the top layer, and the processing core filters at the bottom layer. We apply the framework for segmenting different real-case medical image scenarios on public available datasets including bladder and prostate segmentation from 2D MRI, and heart segmentation in 3D CT. Our experiments on these concrete problems show that this framework facilitates complex and multi-object segmentation goals while providing a fast prototyping open-source segmentation tool.",,,,13th International Conference on Medical Information Processing and Analysis,,2017-11-17,2017,,,10572,,105721j,Closed,Proceeding,"Namías, R.; D'Amato, J. P.; del Fresno, M.","Namías, R. (CIFASIS, UNR-CONICET/UAM (France) (Argentina); Consejo Nacional de Investigaciones Científicas y Técnicas (Argentina)); D'Amato, J. P. (Instituto Pladema, Univ. Nacional del Centro (Argentina); Consejo Nacional de Investigaciones Científicas y Técnicas (Argentina)); del Fresno, M. (Instituto Pladema, Univ. Nacional del Centro (Argentina); Comisión de Investigaciones Científicas de la Provincia de Buenos Aires (Argentina))",,"Namías, R. (Centro Internacional Franco-Argentino de Ciencias de la Información y de Sistemas; National Scientific and Technical Research Council); D'Amato, J. P. (National Scientific and Technical Research Council); del Fresno, M. (Comisión de Investigaciones Científicas)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1092749893,46 Information and Computing Sciences, 4612 Software Engineering,,,,,,,,,,,
1142,pub.1130672749,10.48550/arxiv.2009.01924,,,"Introduction to Medical Image Registration with DeepReg, Between Old and  New","This document outlines a tutorial to get started with medical image
registration using the open-source package DeepReg. The basic concepts of
medical image registration are discussed, linking classical methods to newer
methods using deep learning. Two iterative, classical algorithms using
optimisation and one learning-based algorithm using deep learning are coded
step-by-step using DeepReg utilities, all with real, open-accessible, medical
data.",,,arXiv,,,2020-08-29,2020,,,,,,All OA, Green,Preprint,"Brown, N. Montana; Fu, Y.; Saeed, S. U.; Casamitjana, A.; Baum, Z. M. C.; Delaunay, R.; Yang, Q.; Grimwood, A.; Min, Z.; Bonmati, E.; Vercauteren, T.; Clarkson, M. J.; Hu, Y.","Brown, N. Montana (); Fu, Y. (); Saeed, S. U. (); Casamitjana, A. (); Baum, Z. M. C. (); Delaunay, R. (); Yang, Q. (); Grimwood, A. (); Min, Z. (); Bonmati, E. (); Vercauteren, T. (); Clarkson, M. J. (); Hu, Y. ()",,"Brown, N. Montana (); Fu, Y. (); Saeed, S. U. (); Casamitjana, A. (); Baum, Z. M. C. (); Delaunay, R. (); Yang, Q. (); Grimwood, A. (); Min, Z. (); Bonmati, E. (); Vercauteren, T. (); Clarkson, M. J. (); Hu, Y. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1130672749,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences, 51 Physical Sciences,,,,,,,,
1142,pub.1123513186,10.48550/arxiv.1912.08775,,,MRI Pulse Sequence Integration for Deep-Learning Based Brain Metastasis  Segmentation,"Magnetic resonance (MR) imaging is an essential diagnostic tool in clinical
medicine. Recently, a variety of deep learning methods have been applied to
segmentation tasks in medical images, with promising results for computer-aided
diagnosis. For MR images, effectively integrating different pulse sequences is
important to optimize performance. However, the best way to integrate different
pulse sequences remains unclear. In this study, we evaluate multiple
architectural features and characterize their effects in the task of metastasis
segmentation. Specifically, we consider (1) different pulse sequence
integration schemas, (2) different modes of weight sharing for parallel network
branches, and (3) a new approach for enabling robustness to missing pulse
sequences. We find that levels of integration and modes of weight sharing that
favor low variance work best in our regime of small data (n = 100). By adding
an input-level dropout layer, we could preserve the overall performance of
these networks while allowing for inference on inputs with missing pulse
sequence. We illustrate not only the generalizability of the network but also
the utility of this robustness when applying the trained model to data from a
different center, which does not use the same pulse sequences. Finally, we
apply network visualization methods to better understand which input features
are most important for network performance. Together, these results provide a
framework for building networks with enhanced robustness to missing data while
maintaining comparable performance in medical imaging applications.",,,arXiv,,,2019-12-18,2019,,,,,,All OA, Green,Preprint,"Yi, Darvin; Grøvik, Endre; Iv, Michael; Tong, Elizabeth; Emblem, Kyrre Eeg; Nilsen, Line Brennhaug; Saxhaug, Cathrine; Latysheva, Anna; Jacobsen, Kari Dolven; Helland, Åslaug; Zaharchuk, Greg; Rubin, Daniel","Yi, Darvin (); Grøvik, Endre (); Iv, Michael (); Tong, Elizabeth (); Emblem, Kyrre Eeg (); Nilsen, Line Brennhaug (); Saxhaug, Cathrine (); Latysheva, Anna (); Jacobsen, Kari Dolven (); Helland, Åslaug (); Zaharchuk, Greg (); Rubin, Daniel ()",,"Yi, Darvin (); Grøvik, Endre (); Iv, Michael (); Tong, Elizabeth (); Emblem, Kyrre Eeg (); Nilsen, Line Brennhaug (); Saxhaug, Cathrine (); Latysheva, Anna (); Jacobsen, Kari Dolven (); Helland, Åslaug (); Zaharchuk, Greg (); Rubin, Daniel ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1123513186,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1142,pub.1121306602,10.48550/arxiv.1909.11932,,,Adaptive Class Weight based Dual Focal Loss for Improved Semantic  Segmentation,"In this paper, we propose a Dual Focal Loss (DFL) function, as a replacement
for the standard cross entropy (CE) function to achieve a better treatment of
the unbalanced classes in a dataset. Our DFL method is an improvement on the
recently reported Focal Loss (FL) cross-entropy function, which proposes a
scaling method that puts more weight on the examples that are difficult to
classify over those that are easy. However, the scaling parameter of FL is
empirically set, which is problem-dependent. In addition, like other CE
variants, FL only focuses on the loss of true classes. Therefore, no loss
feedback is gained from the false classes. Although focusing only on true
examples increases probability on true classes and correspondingly reduces
probability on false classes due to the nature of the softmax function, it does
not achieve the best convergence due to avoidance of the loss on false classes.
Our DFL method improves on the simple FL in two ways. Firstly, it takes the
idea of FL to focus more on difficult examples than the easy ones, but
evaluates loss on both true and negative classes with equal importance.
Secondly, the scaling parameter of DFL has been made learnable so that it can
tune itself by backpropagation rather than being dependent on manual tuning. In
this way, our proposed DFL method offers an auto-tunable loss function that can
reduce the class imbalance effect as well as put more focus on both true
difficult examples and negative easy examples.",,,arXiv,,,2019-09-26,2019,,,,,,All OA, Green,Preprint,"Hossain, Md Sazzad; Paplinski, Andrew P; Betts, John M","Hossain, Md Sazzad (); Paplinski, Andrew P (); Betts, John M ()",,"Hossain, Md Sazzad (); Paplinski, Andrew P (); Betts, John M ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1121306602,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1142,pub.1119334301,10.48550/arxiv.1811.02654,,,A Volumetric Convolutional Neural Network for Brain Tumor Segmentation,"Brain cancer can be very fatal, but chances of survival increase through
early detection and treatment. Doctors use Magnetic Resonance Imaging (MRI) to
detect and locate tumors in the brain, and very carefully analyze scans to
segment brain tumors. Manual segmentation is time consuming and tiring for
doctors, and it can be difficult for them to notice extremely small
abnormalities. Automated segmentations performed by computers offer quicker
diagnoses, the ability to notice small details, and more accurate
segmentations. Advances in deep learning and computer hardware have allowed for
high-performing automated segmentation approaches. However, several problems
persist in practice: increased training time, class imbalance, and low
performance. In this paper, I propose applying V-Net, a volumetric, fully
convolutional neural network, to segment brain tumors in MRI scans from the
BraTS Challenges. With this approach, I achieve a whole tumor dice score of
0.89 and train the network in a short time while addressing class imbalance
with the use of a dice loss layer. Then, I propose applying an existing
technique to improve automated segmentation performance in practice.",,,arXiv,,,2018-10-26,2018,,,,,,All OA, Green,Preprint,"Sherman, Ryan","Sherman, Ryan ()",,"Sherman, Ryan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119334301,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,
1142,pub.1144119077,10.48550/arxiv.2112.11065,,,Leveraging Image Complexity in Macro-Level Neural Network Design for  Medical Image Segmentation,"Recent progress in encoder-decoder neural network architecture design has led
to significant performance improvements in a wide range of medical image
segmentation tasks. However, state-of-the-art networks for a given task may be
too computationally demanding to run on affordable hardware, and thus users
often resort to practical workarounds by modifying various macro-level design
aspects. Two common examples are downsampling of the input images and reducing
the network depth to meet computer memory constraints. In this paper we
investigate the effects of these changes on segmentation performance and show
that image complexity can be used as a guideline in choosing what is best for a
given dataset. We consider four statistical measures to quantify image
complexity and evaluate their suitability on ten different public datasets. For
the purpose of our experiments we also propose two new encoder-decoder
architectures representing shallow and deep networks that are more memory
efficient than currently popular networks. Our results suggest that median
frequency is the best complexity measure in deciding about an acceptable input
downsampling factor and network depth. For high-complexity datasets, a shallow
network running on the original images may yield better segmentation results
than a deep network running on downsampled images, whereas the opposite may be
the case for low-complexity images.",,,arXiv,,,2021-12-21,2021,,,,,,All OA, Green,Preprint,"Khan, Tariq M.; Naqvi, Syed S.; Meijering, Erik","Khan, Tariq M. (); Naqvi, Syed S. (); Meijering, Erik ()",,"Khan, Tariq M. (); Naqvi, Syed S. (); Meijering, Erik ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144119077,40 Engineering, 46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,
1142,pub.1120398564,10.48550/arxiv.1908.05770,,,Discretely-constrained deep network for weakly supervised segmentation,"An efficient strategy for weakly-supervised segmentation is to impose
constraints or regularization priors on target regions. Recent efforts have
focused on incorporating such constraints in the training of convolutional
neural networks (CNN), however this has so far been done within a continuous
optimization framework. Yet, various segmentation constraints and
regularization can be modeled and optimized more efficiently in a discrete
formulation. This paper proposes a method, based on the alternating direction
method of multipliers (ADMM) algorithm, to train a CNN with discrete
constraints and regularization priors. This method is applied to the
segmentation of medical images with weak annotations, where both size
constraints and boundary length regularization are enforced. Experiments on a
benchmark cardiac segmentation dataset show our method to yield a performance
near to full supervision.",,,arXiv,,,2019-08-15,2019,,,,,,All OA, Green,Preprint,"Peng, Jizong; Kervadec, Hoel; Dolz, Jose; Ayed, Ismail Ben; Pedersoli, Marco; Desrosiers, Christian","Peng, Jizong (); Kervadec, Hoel (); Dolz, Jose (); Ayed, Ismail Ben (); Pedersoli, Marco (); Desrosiers, Christian ()",,"Peng, Jizong (); Kervadec, Hoel (); Dolz, Jose (); Ayed, Ismail Ben (); Pedersoli, Marco (); Desrosiers, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1120398564,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
1142,pub.1119405953,10.48550/arxiv.1907.01743,,,Deep Attentive Features for Prostate Segmentation in 3D Transrectal  Ultrasound,"Automatic prostate segmentation in transrectal ultrasound (TRUS) images is of
essential importance for image-guided prostate interventions and treatment
planning. However, developing such automatic solutions remains very challenging
due to the missing/ambiguous boundary and inhomogeneous intensity distribution
of the prostate in TRUS, as well as the large variability in prostate shapes.
This paper develops a novel 3D deep neural network equipped with attention
modules for better prostate segmentation in TRUS by fully exploiting the
complementary information encoded in different layers of the convolutional
neural network (CNN). Our attention module utilizes the attention mechanism to
selectively leverage the multilevel features integrated from different layers
to refine the features at each individual layer, suppressing the non-prostate
noise at shallow layers of the CNN and increasing more prostate details into
features at deep layers. Experimental results on challenging 3D TRUS volumes
show that our method attains satisfactory segmentation performance. The
proposed attention mechanism is a general strategy to aggregate multi-level
deep features and has the potential to be used for other medical image
segmentation tasks. The code is publicly available at
https://github.com/wulalago/DAF3D.",,,arXiv,,,2019-07-03,2019,,,,,,All OA, Green,Preprint,"Wang, Yi; Dou, Haoran; Hu, Xiaowei; Zhu, Lei; Yang, Xin; Xu, Ming; Qin, Jing; Heng, Pheng-Ann; Wang, Tianfu; Ni, Dong","Wang, Yi (); Dou, Haoran (); Hu, Xiaowei (); Zhu, Lei (); Yang, Xin (); Xu, Ming (); Qin, Jing (); Heng, Pheng-Ann (); Wang, Tianfu (); Ni, Dong ()",,"Wang, Yi (); Dou, Haoran (); Hu, Xiaowei (); Zhu, Lei (); Yang, Xin (); Xu, Ming (); Qin, Jing (); Heng, Pheng-Ann (); Wang, Tianfu (); Ni, Dong ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119405953,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
1142,pub.1113240104,10.1109/access.2019.2908991,,,NAS-Unet: Neural Architecture Search for Medical Image Segmentation,"Neural architecture search (NAS) has significant progress in improving the accuracy of image classification. Recently, some works attempt to extend NAS to image segmentation which shows preliminary feasibility. However, all of them focus on searching architecture for semantic segmentation in natural scenes. In this paper, we design three types of primitive operation set on search space to automatically find two cell architecture DownSC and UpSC for semantic image segmentation especially medical image segmentation. Inspired by the U-net architecture and its variants successfully applied to various medical image segmentation, we propose NAS-Unet which is stacked by the same number of DownSC and UpSC on a U-like backbone network. The architectures of DownSC and UpSC updated simultaneously by a differential architecture strategy during the search stage. We demonstrate the good segmentation results of the proposed method on Promise12, Chaos, and ultrasound nerve datasets, which collected by magnetic resonance imaging, computed tomography, and ultrasound, respectively. Without any pretraining, our architecture searched on PASCAL VOC2012, attains better performances and much fewer parameters (about 0.8M) than U-net and one of its variants when evaluated on the above three types of medical image datasets.","This work was supported in part by the National Natural Science Foundation of China under Grant 61772575, and the National Key R&amp;D Program of China under Grant 2017YFB1402101.",,IEEE Access,,,2019-01-01,2019,2019-04-04,2019-01-01,7,,44247-44257,All OA, Gold,Article,"Weng, Yu; Zhou, Tianbao; Li, Yujie; Qiu, Xiaoyu","Weng, Yu (College of Information Engineering, Minzu University of China, Beijing, 100081, China); Zhou, Tianbao (College of Information Engineering, Minzu University of China, Beijing, 100081, China); Li, Yujie (School of Information Engineering, Yangzhou University, Yangzhou, 225009, China); Qiu, Xiaoyu (Library of Shandong University of Traditional Chinese Medicine, Jinan, 250355, China)","Weng, Yu (Minzu University of China)","Weng, Yu (Minzu University of China); Zhou, Tianbao (Minzu University of China); Li, Yujie (Yangzhou University); Qiu, Xiaoyu (Shandong University of Traditional Chinese Medicine)",224,176,,80.77,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08681706.pdf,https://app.dimensions.ai/details/publication/pub.1113240104,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
1138,pub.1149503125,10.1016/b978-0-323-99031-8.00011-9,,,Chapter 4 Computer-aided knee joint MR image segmentation—An overview,"Osteoarthritis (OA) is one of the significant medical problems among the old populace. X-ray is the most well-known innovation to watch and assess the advancement of OA courses. Nonetheless, the extraordinary work cost of magnetic resonance imaging (MRI) investigation makes the procedure wasteful and costly. Personal computer-supported knee MRI division is at present a functioning exploration field because it can ease specialists and radiologists from the tedious and repetitive activity and improve the finding execution which has massive potential for both center and logical research. In this paper, we arranged the current techniques by their standards and examine the momentum look into the status and point out the future research pattern from top to bottom.",,,,Computational Intelligence in Healthcare Applications,,2022,2022,,2022,,,55-70,Closed,Chapter,"Singh, Punit Kumar; Singh, Sudhakar","Singh, Punit Kumar (Department of BioEngineering, Integral University Lucknow, Lucknow, Uttar Pradesh, India); Singh, Sudhakar (Department of Biomedical Engineering, Lovely Professional University, Phagwara, Punjab, India)",,"Singh, Punit Kumar (University of Lucknow); Singh, Sudhakar (Lovely Professional University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149503125,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences,,,,,,,,,,
1138,pub.1143667545,10.48550/arxiv.2112.02723,,,Automated volumetric and statistical shape assessment of cam-type  morphology of the femoral head-neck region from 3D magnetic resonance images,"Femoroacetabular impingement (FAI) cam morphology is routinely assessed using
two-dimensional alpha angles which do not provide specific data on cam size
characteristics. The purpose of this study is to implement a novel, automated
three-dimensional (3D) pipeline, CamMorph, for segmentation and measurement of
cam volume, surface area and height from magnetic resonance (MR) images in
patients with FAI. The CamMorph pipeline involves two processes: i) proximal
femur segmentation using an approach integrating 3D U-net with focused shape
modelling (FSM); ii) use of patient-specific anatomical information from 3D FSM
to simulate healthy femoral bone models and pathological region constraints to
identify cam bone mass. Agreement between manual and automated segmentation of
the proximal femur was evaluated with the Dice similarity index (DSI) and
surface distance measures. Independent t-tests or Mann-Whitney U rank tests
were used to compare the femoral head volume, cam volume, surface area and
height data between female and male patients with FAI. There was a mean DSI
value of 0.964 between manual and automated segmentation of proximal femur
volume. Compared to female FAI patients, male patients had a significantly
larger mean femoral head volume (66.12cm3 v 46.02cm3, p<0.001). Compared to
female FAI patients, male patients had a significantly larger mean cam volume
(1136.87mm3 v 337.86mm3, p<0.001), surface area (657.36mm2 v 306.93mm2 ,
p<0.001), maximum-height (3.89mm v 2.23mm, p<0.001) and average-height (1.94mm
v 1.00mm, p<0.001). Automated analyses of 3D MR images from patients with FAI
using the CamMorph pipeline showed that, in comparison with female patients,
male patients had significantly greater cam volume, surface area and height.",,,arXiv,,,2021-12-05,2021,,,,,,All OA, Green,Preprint,"Bugeja, Jessica M.; Xia, Ying; Chandra, Shekhar S.; Murphy, Nicholas J.; Eyles, Jillian; Spiers, Libby; Crozier, Stuart; Hunter, David J.; Fripp, Jurgen; Engstrom, Craig","Bugeja, Jessica M. (); Xia, Ying (); Chandra, Shekhar S. (); Murphy, Nicholas J. (); Eyles, Jillian (); Spiers, Libby (); Crozier, Stuart (); Hunter, David J. (); Fripp, Jurgen (); Engstrom, Craig ()",,"Bugeja, Jessica M. (); Xia, Ying (); Chandra, Shekhar S. (); Murphy, Nicholas J. (); Eyles, Jillian (); Spiers, Libby (); Crozier, Stuart (); Hunter, David J. (); Fripp, Jurgen (); Engstrom, Craig ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143667545,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,
1138,pub.1092168751,10.1016/j.neucom.2017.09.084,,,Atlas registration and ensemble deep convolutional neural network-based prostate segmentation using magnetic resonance imaging,"Automatic segmentation of prostate in magnetic resonance (MR) images has been more and more applied to the diagnosis of prostate disease and various clinical applications. However, due to the inhomogeneous and varying anatomical appearance around prostate boundary, the segmentation of prostate MR images faces great challenges. Since deep learning shows superior performance in computer vision, we propose a coarse-to-fine segmentation strategy by using deep neural networks to tackle the segmentation problem of the endorectal coil prostate images and non-endorectal coil prostate images separately. First, we present a registration-based coarse segmentation to the pre-processed prostate MR images to get the potential boundary region. Second, we train deep neural networks as pixel-based classifier to predict whether the pixel in the potential boundary region is prostate pixel or not. To improve the discriminability of the algorithm, we further introduce ensemble learning for fine segmentation. Finally, a boundary refinement is used to eliminate the outlier and smooth the boundary. The proposed method has been extensively evaluated on the PROMIS12 challenge dataset and PROSTATEx17 challenge dataset. Experimental results show superior segmentation performance (0.910 ± 0.036 in dice ratio, 1.583 ± 0.441 in average boundary distance and 4.579 ± 1.791 in Hausdorff distance), which demonstrates the effectiveness of the proposed algorithm.","This work was supported in part by the National Natural Science Foundation of China under Grants 61471297 and 61771397, and in part by the Australian Research Council under Grants DP160103675, LP160101162 and DP170104304. We appreciate the efforts devoted by the organizers of the 2012 Prostate MR Image Segmentation Challenge to collect and share the data for comparing interactive and (semi)-automatic segmentation algorithms for MRI of the prostate.",,Neurocomputing,,,2018-01,2018,,2018-01,275,,1358-1369,Closed,Article,"Jia, Haozhe; Xia, Yong; Song, Yang; Cai, Weidong; Fulham, Michael; Feng, David Dagan","Jia, Haozhe (Shaanxi Key Lab of Speech & Image Information Processing (SAIIP), School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an 710072, PR China; Centre for Multidisciplinary Convergence Computing (CMCC), School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an 710072, PR China; Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, NSW 2006, Australia); Xia, Yong (Shaanxi Key Lab of Speech & Image Information Processing (SAIIP), School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an 710072, PR China; Centre for Multidisciplinary Convergence Computing (CMCC), School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an 710072, PR China); Song, Yang (Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, NSW 2006, Australia); Cai, Weidong (Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, NSW 2006, Australia); Fulham, Michael (Centre for Multidisciplinary Convergence Computing (CMCC), School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an 710072, PR China; Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, NSW 2006, Australia; Department of PET and Nuclear Medicine, Royal Prince Alfred Hospital, NSW 2050, Australia; Sydney Medical School, University of Sydney, NSW 2006, Australia); Feng, David Dagan (Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, NSW 2006, Australia)","Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University)","Jia, Haozhe (Northwestern Polytechnical University; Northwestern Polytechnical University; The University of Sydney); Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University); Song, Yang (The University of Sydney); Cai, Weidong (The University of Sydney); Fulham, Michael (Northwestern Polytechnical University; The University of Sydney; Royal Prince Alfred Hospital; The University of Sydney); Feng, David Dagan (The University of Sydney)",61,21,,,,https://app.dimensions.ai/details/publication/pub.1092168751,46 Information and Computing Sciences,,,,,,,,,,,,
1138,pub.1083693820,10.1016/j.neucom.2016.12.071,,,Hierarchical prostate MRI segmentation via level set clustering with shape prior,"Efficient and accurate segmentation of prostate is of great interest in image-guided prostate interventions and diagnosis of prostate cancer. In this paper, a novel hierarchical level set clustering approach is proposed to segment prostate from MR image, which makes full use of statistics information of manual segmentation result and incorporates shape prior into the segmentation task. The medium slice of prostate MR data, which is segmented artificially, is used to offer prior information and guide the segmentation of other slices. The Bhattacharyya coefficient between manual segmentation result of medium slice and local block region of pending slice is calculated to estimate the likelihood of local prostate region in pending slice. An adaptive blurring process is implemented before the optimization of level set function to restrain the redundancy texture information and retain the edge information in the meantime. We can capture the contour of prostate with a level set evolution embedded shape prior which is derived from the segmented result of medium slice. A comparative performance evaluation is carried out over a large set of experiments using real prostate magnetic resonance images and synthetic magnetic resonance data to demonstrate the validity of our method, showing significant improvements on both segmentation accuracy and noise sensitivity comparing to the state-of-the-art approaches.",This work was supported by National Natural Science Foundation of China Grant No: 61371156 and Anhui Province Scientific and Technological Research Programs Grant No: 1401B042019. The authors would like to thank the anonymous reviewers for their helpful and constructive comments and suggestions.,,Neurocomputing,,,2017-09,2017,,2017-09,257,,154-163,Closed,Article,"Yang, Xiong; Zhan, Shu; Xie, Dongdong; Zhao, Hong; Kurihara, Toru","Yang, Xiong (School of Computer and Information, Hefei University of Technology, Hefei 230009, China); Zhan, Shu (School of Computer and Information, Hefei University of Technology, Hefei 230009, China); Xie, Dongdong (Second Affiliated Hospital, Anhui Medical University, Hefei 230601, China); Zhao, Hong (Second Affiliated Hospital, Anhui Medical University, Hefei 230601, China); Kurihara, Toru (School of Information, Kochi University of Technology, Kochi 7828502, Japan)","Zhan, Shu (Hefei University of Technology)","Yang, Xiong (Hefei University of Technology); Zhan, Shu (Hefei University of Technology); Xie, Dongdong (Second Hospital of Anhui Medical University); Zhao, Hong (Second Hospital of Anhui Medical University); Kurihara, Toru (Kochi University of Technology)",22,5,,,,https://app.dimensions.ai/details/publication/pub.1083693820,46 Information and Computing Sciences,,,,,,,,,,,,
1133,pub.1147387476,10.21203/rs.3.rs-1582100/v1,,,Lifelong nnUNet: a framework for standardized medical continual learning,"As the acceptability of Deep Learning solutions grows, both medical practitioners and regulatory bodies are exploring ways to safely introduce image segmentation solutions in clinical practice. One frontier to overcome when translating promising research into the clinical open-world is the shift from static to lifelong learning. Continual learning, the practice of training models sequentially with shifting population dynamics, is seeing growing interest but is still at its infancy in healthcare. We present Lifelong nnUNet, a standardized framework for medical image segmentation that places state-of-the art continual segmentation at the hands of researchers and clinicians. Build on top of the popular nnUNet and equipped with all necessary modules for training and testing continual models, we ensure broad applicability and lower the barrier for evaluating new solutions in a sequential fashion. We introduce benchmark results on three medical segmentation use cases with open-source datasets, and provide the tools to quickly test five continual learning methods on further applications.",,,Research Square,,,2022-04-26,2022,2022-04-26,,,,,All OA, Green,Preprint,"Gonzalez, Camila; Ranem, Amin; Santos, Daniel Pinto dos; Othman, Ahmed; Mukhopadhyay, Anirban","Gonzalez, Camila (Darmstadt University of Technology); Ranem, Amin (Darmstadt University of Technology); Santos, Daniel Pinto dos (University of Cologne); Othman, Ahmed (University Medical Center Mainz); Mukhopadhyay, Anirban (Darmstadt University of Technology)",,"Gonzalez, Camila (TU Darmstadt); Ranem, Amin (TU Darmstadt); Santos, Daniel Pinto dos (University of Cologne); Othman, Ahmed (University Medical Center of the Johannes Gutenberg University Mainz); Mukhopadhyay, Anirban (TU Darmstadt)",1,1,,,https://www.researchsquare.com/article/rs-1582100/latest.pdf,https://app.dimensions.ai/details/publication/pub.1147387476,46 Information and Computing Sciences, 4611 Machine Learning,4 Quality Education,,,,,,,,,
1133,pub.1142604457,10.48550/arxiv.2111.07646,,,Multimodal Generalized Zero Shot Learning for Gleason Grading using  Self-Supervised Learning,"Gleason grading from histopathology images is essential for accurate prostate
cancer (PCa) diagnosis. Since such images are obtained after invasive tissue
resection quick diagnosis is challenging under the existing paradigm. We
propose a method to predict Gleason grades from magnetic resonance (MR) images
which are non-interventional and easily acquired. We solve the problem in a
generalized zero-shot learning (GZSL) setting since we may not access training
images of every disease grade. Synthetic MRI feature vectors of unseen grades
(classes) are generated by exploiting Gleason grades' ordered nature through a
conditional variational autoencoder (CVAE) incorporating self-supervised
learning. Corresponding histopathology features are generated using cycle GANs,
and combined with MR features to predict Gleason grades of test images.
Experimental results show our method outperforms competing feature generating
approaches for GZSL, and comes close to performance of fully supervised
methods.",,,arXiv,,,2021-11-15,2021,,,,,,All OA, Green,Preprint,"Mahapatra, Dwarikanath","Mahapatra, Dwarikanath ()",,"Mahapatra, Dwarikanath ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142604457,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1133,pub.1142385794,10.1109/cvpr46437.2021.00107,,,FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space,"Federated learning allows distributed medical institutions to collaboratively learn a shared prediction model with privacy protection. While at clinical deployment, the models trained in federated learning can still suffer from performance drop when applied to completely unseen hospitals outside the federation. In this paper, we point out and solve a novel problem setting of federated domain generalization (FedDG), which aims to learn a federated model from multiple distributed source domains such that it can directly generalize to unseen target domains. We present a novel approach, named as Episodic Learning in Continuous Frequency Space (ELCFS), for this problem by enabling each client to exploit multi-source data distributions under the challenging constraint of data decentralization. Our approach transmits the distribution information across clients in a privacy-protecting way through an effective continuous frequency space interpolation mechanism. With the transferred multi-source distributions, we further carefully design a boundary-oriented episodic learning paradigm to expose the local learning to domain distribution shifts and particularly meet the challenges of model generalization in medical image segmentation scenario. The effectiveness of our method is demonstrated with superior performance over state-of-the-arts and in-depth ablation experiments on two medical image segmentation tasks. The code is available at https://github.com/liuquande/FedDG-ELCFS.","This work was supported by Key-Area Research and Development Program of Guangdong Province, China (2020B010165004); National Natural Science Foundation of China with Project No. U1813204; Hong Kong Innovation and Technology Fund (Project No. ITS/311/18FP and GHP/110/19SZ). This work was supported by Key-Area Research and Development Program of Guangdong Province, China (2020B010165004); National Natural Science Foundation of China with Project No. U1813204; Hong Kong Innovation and Technology Fund (Project No. ITS/311/18FP and GHP/110/19SZ).",,,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,2021-06-25,2021,,2021-06-25,0,,1013-1023,All OA, Green,Proceeding,"Liu, Quande; Chen, Cheng; Qin, Jing; Dou, Qi; Heng, Pheng-Ann","Liu, Quande (Department of Computer Science and Engineering, The Chinese University of Hong Kong); Chen, Cheng (Department of Computer Science and Engineering, The Chinese University of Hong Kong); Qin, Jing (School of Nursing, The Hong Kong Polytechnic University); Dou, Qi (Department of Computer Science and Engineering, The Chinese University of Hong Kong); Heng, Pheng-Ann (Department of Computer Science and Engineering, The Chinese University of Hong Kong)","Dou, Qi (Chinese University of Hong Kong)","Liu, Quande (Chinese University of Hong Kong); Chen, Cheng (Chinese University of Hong Kong); Qin, Jing (Hong Kong Polytechnic University); Dou, Qi (Chinese University of Hong Kong); Heng, Pheng-Ann (Chinese University of Hong Kong)",72,72,,58.93,http://arxiv.org/pdf/2103.06030,https://app.dimensions.ai/details/publication/pub.1142385794,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1133,pub.1131140718,10.48550/arxiv.2009.11120,,,Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from  Multi-Planar MRI,"Background and Objective: Accurate and reliable segmentation of the prostate
gland in MR images can support the clinical assessment of prostate cancer, as
well as the planning and monitoring of focal and loco-regional therapeutic
interventions. Despite the availability of multi-planar MR scans due to
standardized protocols, the majority of segmentation approaches presented in
the literature consider the axial scans only. Methods: We propose an
anisotropic 3D multi-stream CNN architecture, which processes additional scan
directions to produce a higher-resolution isotropic prostate segmentation. We
investigate two variants of our architecture, which work on two (dual-plane)
and three (triple-plane) image orientations, respectively. We compare them with
the standard baseline (single-plane) used in literature, i.e., plain axial
segmentation. To realize a fair comparison, we employ a hyperparameter
optimization strategy to select optimal configurations for the individual
approaches. Results: Training and evaluation on two datasets spanning multiple
sites obtain statistical significant improvement over the plain axial
segmentation ($p<0.05$ on the Dice similarity coefficient). The improvement can
be observed especially at the base ($0.898$ single-plane vs. $0.906$
triple-plane) and apex ($0.888$ single-plane vs. $0.901$ dual-plane).
Conclusion: This study indicates that models employing two or three scan
directions are superior to plain axial segmentation. The knowledge of precise
boundaries of the prostate is crucial for the conservation of risk structures.
Thus, the proposed models have the potential to improve the outcome of prostate
cancer diagnosis and therapies.",,,arXiv,,,2020-09-23,2020,,,,,,All OA, Green,Preprint,"Meyer, Anneke; Chlebus, Grzegorz; Rak, Marko; Schindele, Daniel; Schostak, Martin; van Ginneken, Bram; Schenk, Andrea; Meine, Hans; Hahn, Horst K.; Schreiber, Andreas; Hansen, Christian","Meyer, Anneke (); Chlebus, Grzegorz (); Rak, Marko (); Schindele, Daniel (); Schostak, Martin (); van Ginneken, Bram (); Schenk, Andrea (); Meine, Hans (); Hahn, Horst K. (); Schreiber, Andreas (); Hansen, Christian ()",,"Meyer, Anneke (); Chlebus, Grzegorz (); Rak, Marko (); Schindele, Daniel (); Schostak, Martin (); van Ginneken, Bram (); Schenk, Andrea (); Meine, Hans (); Hahn, Horst K. (); Schreiber, Andreas (); Hansen, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1131140718,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
1133,pub.1119391858,10.48550/arxiv.1906.04714,,,Deep Neural Networks for Surface Segmentation Meet Conditional Random  Fields,"Automated surface segmentation is important and challenging in many medical
image analysis applications. Recent deep learning based methods have been
developed for various object segmentation tasks. Most of them are a
classification based approach (e.g., U-net), which predicts the probability of
being target object or background for each voxel. One problem of those methods
is lacking of topology guarantee for segmented objects, and usually post
processing is needed to infer the boundary surface of the object. In this
paper, a novel model based on 3-D convolutional neural networks (CNNs) and
Conditional Random Fields (CRFs) is proposed to tackle the surface segmentation
problem with end-to-end training. To the best of our knowledge, this is the
first study to apply a 3-D neural network with a CRFs model for direct surface
segmentation. Experiments carried out on NCI-ISBI 2013 MR prostate dataset and
Medical Segmentation Decathlon Spleen dataset demonstrated promising
segmentation results.",,,arXiv,,,2019-06-11,2019,,,,,,All OA, Green,Preprint,"Zhou, Leixin; Zhong, Zisha; Shah, Abhay; Qiu, Bensheng; Buatti, John; Wu, Xiaodong","Zhou, Leixin (); Zhong, Zisha (); Shah, Abhay (); Qiu, Bensheng (); Buatti, John (); Wu, Xiaodong ()",,"Zhou, Leixin (); Zhong, Zisha (); Shah, Abhay (); Qiu, Bensheng (); Buatti, John (); Wu, Xiaodong ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119391858,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1133,pub.1141302568,10.1007/s11063-021-10643-2,,,An Improved U-Net for Human Sperm Head Segmentation,"Sperm morphology analysis is an important step in the clinical diagnosis of male infertility, which means that the shape of sperm head is an important indicator in sperm morphology analysis. Therefore the accurate and efficient segmentation of human sperm head is essential for accurate and objective analysis of sperm morphology. In this paper, we have proposed an efficient deep learning algorithm for fully automatic segmentation of human sperm head based on the U-Net network structure. First of all, we performed sperm cell image collection and built a new dataset that is suitable for segmentation of human sperm heads in deep learning algorithms. Our dataset consists of 1207 sperm cell images from more than 20 male infertility patients. Then we improved the U-Net architecture by integrating the dilated convolution into it and replaced the long skip layer in the original network with the block we designed, which finally formed our final deep convolutional neural network. We use our dataset to train our proposed network so that we can segment the sperm head. Our algorithm is one of the few methods for segmentation of sperm head using deep learning algorithms. And compared with previous methods, our model not only achieve good results in unstained and low-resolution images containing only individual sperm cells, but also shows excellent performance in complex images containing multiple sperm cells. Our experimental results have confirmed that the HDC (Hybrid Dilated Convolution) module and our designed Block have a noticeable improvement on segmentation results. Meanwhile, we have achieved a high Dice coefficient of 95.14%\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\%$$\end{document}. The segmentation results we tested on the prostate dataset prove that our model has good generalization ability and robustness. It is worth noting that our algorithm processed the images showing the original true morphology of the sperm cells and achieved high segmentation accuracy. It’s is very important for doctors to diagnose whether the sperm morphology is abnormal in the clinic.","This work was supported by National Natural Science Foundation of China Grant No. 61371156, and Anhui Province Key Scientific and Technological Research Programs Grant No. 201904d07020018. The authors would like to thank the anonymous reviews for their helpful and constructive comments and suggestions regarding this manuscript. And all of our authors sincerely thank the physicians and staffs of the Reproductive Center of the First Affiliated Hospital of the University of Science and Technology of China for providing valuable sperm data, and conducting and assisting us in completing the data annotation.",,Neural Processing Letters,,,2021-09-22,2021,2021-09-22,2022-02,54,1,537-557,Closed,Article,"Lv, Qixian; Yuan, Xinrong; Qian, Jinzhao; Li, Xinke; Zhang, Haiyan; Zhan, Shu","Lv, Qixian (Key Laboratory of Knowledge Engineering with Big Data of Ministry of Education, Hefei University of Technology, 230601, Hefei, Anhui, People’s Republic of China; School of Computer and Information, Hefei University of Technology, 230601, Hefei, Anhui, People’s Republic of China); Yuan, Xinrong (XiangYa School of medicine, Central South University, 410013, Changsha, Hunan, People’s Republic of China); Qian, Jinzhao (Department of Automation, Tsinghua University, 100084, Beijing, People’s Republic of China); Li, Xinke (School of Computer and Information, Hefei University of Technology, 230601, Hefei, Anhui, People’s Republic of China); Zhang, Haiyan (Key Laboratory of Knowledge Engineering with Big Data of Ministry of Education, Hefei University of Technology, 230601, Hefei, Anhui, People’s Republic of China; School of Computer and Information, Hefei University of Technology, 230601, Hefei, Anhui, People’s Republic of China); Zhan, Shu (Key Laboratory of Knowledge Engineering with Big Data of Ministry of Education, Hefei University of Technology, 230601, Hefei, Anhui, People’s Republic of China; School of Computer and Information, Hefei University of Technology, 230601, Hefei, Anhui, People’s Republic of China)","Zhan, Shu (Hefei University of Technology; Hefei University of Technology)","Lv, Qixian (Hefei University of Technology; Hefei University of Technology); Yuan, Xinrong (Central South University); Qian, Jinzhao (Tsinghua University); Li, Xinke (Hefei University of Technology); Zhang, Haiyan (Hefei University of Technology; Hefei University of Technology); Zhan, Shu (Hefei University of Technology; Hefei University of Technology)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141302568,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1133,pub.1141111933,10.48550/arxiv.2109.05742,,,Domain Generalization for Medical Image Segmentation via Hierarchical  Consistency Regularization,"Modern deep neural networks struggle to transfer knowledge and generalize
across diverse domains when deployed to real-world applications. Currently,
domain generalization (DG) is introduced to learn a universal representation
from multiple domains to improve the network generalization ability on unseen
domains. However, previous DG methods only focus on the data-level consistency
scheme without considering the synergistic regularization among different
consistency schemes. In this paper, we present a novel Hierarchical Consistency
framework for Domain Generalization (HCDG) by integrating Extrinsic Consistency
and Intrinsic Consistency synergistically. Particularly, for the Extrinsic
Consistency, we leverage the knowledge across multiple source domains to
enforce data-level consistency. To better enhance such consistency, we design a
novel Amplitude Gaussian-mixing strategy into Fourier-based data augmentation
called DomainUp. For the Intrinsic Consistency, we perform task-level
consistency for the same instance under the dual-task scenario. We evaluate the
proposed HCDG framework on two medical image segmentation tasks, i.e., optic
cup/disc segmentation on fundus images and prostate MRI segmentation. Extensive
experimental results manifest the effectiveness and versatility of our HCDG
framework.",,,arXiv,,,2021-09-13,2021,,,,,,All OA, Green,Preprint,"Yang, Yijun; Wang, Shujun; Zhu, Lei; Heng, Pheng-Ann; Yu, Lequan","Yang, Yijun (); Wang, Shujun (); Zhu, Lei (); Heng, Pheng-Ann (); Yu, Lequan ()",,"Yang, Yijun (); Wang, Shujun (); Zhu, Lei (); Heng, Pheng-Ann (); Yu, Lequan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141111933,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
1077,pub.1132685852,10.48550/arxiv.2011.07795,,,Deep learning in magnetic resonance prostate segmentation: A review and  a new perspective,"Prostate radiotherapy is a well established curative oncology modality, which
in future will use Magnetic Resonance Imaging (MRI)-based radiotherapy for
daily adaptive radiotherapy target definition. However the time needed to
delineate the prostate from MRI data accurately is a time consuming process.
Deep learning has been identified as a potential new technology for the
delivery of precision radiotherapy in prostate cancer, where accurate prostate
segmentation helps in cancer detection and therapy. However, the trained models
can be limited in their application to clinical setting due to different
acquisition protocols, limited publicly available datasets, where the size of
the datasets are relatively small. Therefore, to explore the field of prostate
segmentation and to discover a generalisable solution, we review the
state-of-the-art deep learning algorithms in MR prostate segmentation; provide
insights to the field by discussing their limitations and strengths; and
propose an optimised 2D U-Net for MR prostate segmentation. We evaluate the
performance on four publicly available datasets using Dice Similarity
Coefficient (DSC) as performance metric. Our experiments include within dataset
evaluation and cross-dataset evaluation. The best result is achieved by
composite evaluation (DSC of 0.9427 on Decathlon test set) and the poorest
result is achieved by cross-dataset evaluation (DSC of 0.5892, Prostate X
training set, Promise 12 testing set). We outline the challenges and provide
recommendations for future work. Our research provides a new perspective to MR
prostate segmentation and more importantly, we provide standardised experiment
settings for researchers to evaluate their algorithms. Our code is available at
https://github.com/AIEMMU/MRI\_Prostate.",,,arXiv,,,2020-11-16,2020,,,,,,All OA, Green,Preprint,"Gillespie, David; Kendrick, Connah; Boon, Ian; Boon, Cheng; Rattay, Tim; Yap, Moi Hoon","Gillespie, David (); Kendrick, Connah (); Boon, Ian (); Boon, Cheng (); Rattay, Tim (); Yap, Moi Hoon ()",,"Gillespie, David (); Kendrick, Connah (); Boon, Ian (); Boon, Cheng (); Rattay, Tim (); Yap, Moi Hoon ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1132685852,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,
1076,pub.1149895190,10.48550/arxiv.2207.14472,,,Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image  Segmentation,"Automatic tumor or lesion segmentation is a crucial step in medical image
analysis for computer-aided diagnosis. Although the existing methods based on
Convolutional Neural Networks (CNNs) have achieved the state-of-the-art
performance, many challenges still remain in medical tumor segmentation. This
is because, although the human visual system can detect symmetries in 2D images
effectively, regular CNNs can only exploit translation invariance, overlooking
further inherent symmetries existing in medical images such as rotations and
reflections. To solve this problem, we propose a novel group equivariant
segmentation framework by encoding those inherent symmetries for learning more
precise representations. First, kernel-based equivariant operations are devised
on each orientation, which allows it to effectively address the gaps of
learning symmetries in existing approaches. Then, to keep segmentation networks
globally equivariant, we design distinctive group layers with layer-wise
symmetry constraints. Finally, based on our novel framework, extensive
experiments conducted on real-world clinical data demonstrate that a Group
Equivariant Res-UNet (named GER-UNet) outperforms its regular CNN-based
counterpart and the state-of-the-art segmentation methods in the tasks of
hepatic tumor segmentation, COVID-19 lung infection segmentation and retinal
vessel detection. More importantly, the newly built GER-UNet also shows
potential in reducing the sample complexity and the redundancy of filters,
upgrading current segmentation CNNs and delineating organs on other medical
imaging modalities.",,,arXiv,,,2022-07-29,2022,,,,,,All OA, Green,Preprint,"Pang, Shuchao; Du, Anan; Orgun, Mehmet A.; Wang, Yan; Sheng, Quan Z.; Wang, Shoujin; Huang, Xiaoshui; Yu, Zhenmei","Pang, Shuchao (); Du, Anan (); Orgun, Mehmet A. (); Wang, Yan (); Sheng, Quan Z. (); Wang, Shoujin (); Huang, Xiaoshui (); Yu, Zhenmei ()",,"Pang, Shuchao (); Du, Anan (); Orgun, Mehmet A. (); Wang, Yan (); Sheng, Quan Z. (); Wang, Shoujin (); Huang, Xiaoshui (); Yu, Zhenmei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149895190,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
1076,pub.1135085893,10.1109/jsen.2021.3056131,,,SK-Unet: An Improved U-Net Model With Selective Kernel for the Segmentation of LGE Cardiac MR Images,"In the clinical environment, myocardial infarction (MI) as one common cardiovascular disease is mainly evaluated using late gadolinium enhancement (LGE) cardiac magnetic resonance images (CMRIs). Accurate segmentation of the ventricles and the myocardium is a prerequisite for quantitative assessment of cardiac functions and disease progression. Performing the task using LGE images is, however, rather challenging due to heterogeneous image intensity distribution and lack of clear boundaries between adjacent organs and tissues. In this paper we propose a deep neural network method for automatic segmentation of the left ventricle (LV), right ventricle (RV), and left ventricular myocardium (LVM) from LGE CMRIs, which also leverages complementary information from cine and T2-weighted CMRIs if available. In the proposed method, termed SK-Unet, we augment the original U-Net model by adding a squeeze-and-excitation residual (SE-Res) module in the encoder and a selective kernel (SK) module in the decoder. The SE-Res module applies an attention mechanism to enhance informative feature extraction and suppress redundant ones. The SK module offers the ability to adaptively learn task-relevant multi-scale spatial features. We tested our method by participating in the MICCAI 2019 MS-CMRSeg challenge and achieved a mean dice score of 0.922 for LV segmentation, 0.827 for LVM, and 0.874 for RV. The results placed our method at the 1st place in the competition, and our accuracy of 0.827 also greatly surpasses the measured inter-observer agreement of 0.757 for manual segmentation of LVM in LGE CMRIs. The code accompanying our method is made available online at https://github.com/Xiyue-Wang/1st-in-MS-CMRSeg-2019.",This work was supported in part by the National Natural Science Foundation of China under Grant 61571314 and in part by the Innovative Youth Projects of Ocean Remote Sensing Engineering Technology Research Center of State Oceanic Administration of China under Grant 2015001.,,IEEE Sensors Journal,,,2021-01-15,2021,2021-02-01,2021-01-15,21,10,11643-11653,Closed,Article,"Wang, Xiyue; Yang, Sen; Fang, Yuqi; Wei, Yunpeng; Wang, Minghui; Zhang, Jing; Han, Xiao","Wang, Xiyue (College of Computer Science, Sichuan University, Chengdu, 610065, China); Yang, Sen (College of Biomedical Engineering, Sichuan University, Chengdu, 610065, China; Tencent AI Lab, Shenzhen, 518057, China); Fang, Yuqi (Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong); Wei, Yunpeng (Department of Cardiology, Fuwai Hospital, National Center for Cardiovascular Disease, Chinese Academy of Medical Sciences, Beijing, 100730, China; Peking Union Medical College, Beijing, 100730, China); Wang, Minghui (College of Computer Science, Sichuan University, Chengdu, 610065, China); Zhang, Jing (College of Computer Science, Sichuan University, Chengdu, 610065, China); Han, Xiao (Tencent AI Lab, Shenzhen, 518057, China)","Zhang, Jing (Sichuan University)","Wang, Xiyue (Sichuan University); Yang, Sen (Sichuan University; Tencent (China)); Fang, Yuqi (Chinese University of Hong Kong); Wei, Yunpeng (Fuwai Hospital; Chinese Academy of Medical Sciences & Peking Union Medical College; Chinese Academy of Medical Sciences & Peking Union Medical College); Wang, Minghui (Sichuan University); Zhang, Jing (Sichuan University); Han, Xiao (Tencent (China))",7,6,,,,https://app.dimensions.ai/details/publication/pub.1135085893,40 Engineering,,,,,,,,,,,,
1075,pub.1149605238,10.1109/tim.2022.3192292,,,3-D Prostate MR and TRUS Images Detection and Segmentation for Puncture Biopsy,"Prostate cancer is one of the male cancers with high mortality rate in the world. The mainstream diagnostic method is puncture biopsy. Accurate detection and segmentation of the prostate in magnetic resonance (MR) and transrectal ultrasonography (TRUS) images is an important prerequisite for image-guided puncture biopsy. Manual or semiautomatic detection and segmentation methods and traditional automatic detection and segmentation methods are difficult to achieve real-time and accurate detection and segmentation under the conditions of poor imaging effect or less labeled data. In this article, a real-time and accurate automatic detection and segmentation algorithm dense residual convolutional (DRC) U-Net for 3-D MR and TRUS images of prostate is proposed for prostate puncture biopsy. A novel convolution module that integrates residual connections, dense connections, and deep separable convolutions is proposed for multiscale feature extraction and fusion. At the same time, deep supervision mechanism and a variety of attention mechanisms are integrated in the network to improve training efficiency and ensure segmentation effect. The experimental results on the public MR dataset Promise12 and the private TRUS dataset show that the real-time performance and accuracy of this method are better than the existing 3-D medical image detection and segmentation methods, such as 3-D U-Net and V-Net, and also have some advantages compared with the most advanced 3-D medical image detection and segmentation methods, which proves that the proposed network structure has important clinical significance and practical value for image-guided puncture biopsy.",,This work was supported by the Fundamental Research Funds for the Central Universities under Grant DUT22YG222.,IEEE Transactions on Instrumentation and Measurement,,,2022-01-01,2022,2022-07-19,2022-01-01,71,,1-13,Closed,Article,"Liu, Dong; Wang, Long; Du, Yu; Cong, Ming; Li, Yongyao","Liu, Dong (School of Mechanical Engineering, Dalian University of Technology, Dalian, 116024, China); Wang, Long (School of Mechanical Engineering, Dalian University of Technology, Dalian, 116024, China); Du, Yu (School of Mechanical Engineering, Dalian Jiaotong University, Dalian, 116025, China); Cong, Ming (School of Mechanical Engineering, Dalian University of Technology, Dalian, 116024, China); Li, Yongyao (School of Mechanical Engineering, Dalian University of Technology, Dalian, 116024, China)","Du, Yu (Dalian Jiaotong University)","Liu, Dong (Dalian University of Technology); Wang, Long (Dalian University of Technology); Du, Yu (Dalian Jiaotong University); Cong, Ming (Dalian University of Technology); Li, Yongyao (Dalian University of Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149605238,"40 Engineering; 4006 Communications Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",,,,,,,,,,,,
1075,pub.1143055508,10.48550/arxiv.2111.12525,,,Causality-inspired Single-source Domain Generalization for Medical Image  Segmentation,"Deep learning models usually suffer from domain shift issues, where models
trained on one source domain do not generalize well to other unseen domains. In
this work, we investigate the single-source domain generalization problem:
training a deep network that is robust to unseen domains, under the condition
that training data is only available from one source domain, which is common in
medical imaging applications. We tackle this problem in the context of
cross-domain medical image segmentation. Under this scenario, domain shifts are
mainly caused by different acquisition processes. We propose a simple
causality-inspired data augmentation approach to expose a segmentation model to
synthesized domain-shifted training examples. Specifically, 1) to make the deep
model robust to discrepancies in image intensities and textures, we employ a
family of randomly-weighted shallow networks. They augment training images
using diverse appearance transformations. 2) Further we show that spurious
correlations among objects in an image are detrimental to domain robustness.
These correlations might be taken by the network as domain-specific clues for
making predictions, and they may break on unseen domains. We remove these
spurious correlations via causal intervention. This is achieved by resampling
the appearances of potentially correlated objects independently. The proposed
approach is validated on three cross-domain segmentation tasks: cross-modality
(CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI
segmentation, and cross-center prostate MRI segmentation. The proposed approach
yields consistent performance gains compared with competitive methods when
tested on unseen domains.",,,arXiv,,,2021-11-24,2021,,,,,,All OA, Green,Preprint,"Ouyang, Cheng; Chen, Chen; Li, Surui; Li, Zeju; Qin, Chen; Bai, Wenjia; Rueckert, Daniel","Ouyang, Cheng (); Chen, Chen (); Li, Surui (); Li, Zeju (); Qin, Chen (); Bai, Wenjia (); Rueckert, Daniel ()",,"Ouyang, Cheng (); Chen, Chen (); Li, Surui (); Li, Zeju (); Qin, Chen (); Bai, Wenjia (); Rueckert, Daniel ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143055508,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1075,pub.1119412079,10.48550/arxiv.1904.04205,,,Constrained Deep Networks: Lagrangian Optimization via Log-Barrier  Extensions,"This study investigates imposing hard inequality constraints on the outputs
of convolutional neural networks (CNN) during training. Several recent works
showed that the theoretical and practical advantages of Lagrangian optimization
over simple penalties do not materialize in practice when dealing with modern
CNNs involving millions of parameters. Therefore, constrained CNNs are
typically handled with penalties. We propose *log-barrier extensions*, which
approximate Lagrangian optimization of constrained-CNN problems with a sequence
of unconstrained losses. Unlike standard interior-point and log-barrier
methods, our formulation does not need an initial feasible solution. The
proposed extension yields an upper bound on the duality gap -- generalizing the
result of standard log-barriers -- and yielding sub-optimality certificates for
feasible solutions. While sub-optimality is not guaranteed for non-convex
problems, this result shows that log-barrier extensions are a principled way to
approximate Lagrangian optimization for constrained CNNs via implicit dual
variables. We report weakly supervised image segmentation experiments, with
various constraints, showing that our formulation outperforms substantially the
existing constrained-CNN methods, in terms of accuracy, constraint satisfaction
and training stability, more so when dealing with a large number of
constraints.",,,arXiv,,,2019-04-08,2019,,,,,,All OA, Green,Preprint,"Kervadec, Hoel; Dolz, Jose; Yuan, Jing; Desrosiers, Christian; Granger, Eric; Ayed, Ismail Ben","Kervadec, Hoel (); Dolz, Jose (); Yuan, Jing (); Desrosiers, Christian (); Granger, Eric (); Ayed, Ismail Ben ()",,"Kervadec, Hoel (); Dolz, Jose (); Yuan, Jing (); Desrosiers, Christian (); Granger, Eric (); Ayed, Ismail Ben ()",1,1,,0.59,,https://app.dimensions.ai/details/publication/pub.1119412079,49 Mathematical Sciences, 4901 Applied Mathematics, 4903 Numerical and Computational Mathematics,,,,,,,,,
1071,pub.1136305385,10.48550/arxiv.2103.06030,,,FedDG: Federated Domain Generalization on Medical Image Segmentation via  Episodic Learning in Continuous Frequency Space,"Federated learning allows distributed medical institutions to collaboratively
learn a shared prediction model with privacy protection. While at clinical
deployment, the models trained in federated learning can still suffer from
performance drop when applied to completely unseen hospitals outside the
federation. In this paper, we point out and solve a novel problem setting of
federated domain generalization (FedDG), which aims to learn a federated model
from multiple distributed source domains such that it can directly generalize
to unseen target domains. We present a novel approach, named as Episodic
Learning in Continuous Frequency Space (ELCFS), for this problem by enabling
each client to exploit multi-source data distributions under the challenging
constraint of data decentralization. Our approach transmits the distribution
information across clients in a privacy-protecting way through an effective
continuous frequency space interpolation mechanism. With the transferred
multi-source distributions, we further carefully design a boundary-oriented
episodic learning paradigm to expose the local learning to domain distribution
shifts and particularly meet the challenges of model generalization in medical
image segmentation scenario. The effectiveness of our method is demonstrated
with superior performance over state-of-the-arts and in-depth ablation
experiments on two medical image segmentation tasks. The code is available at
""https://github.com/liuquande/FedDG-ELCFS"".",,,arXiv,,,2021-03-10,2021,,,,,,All OA, Green,Preprint,"Liu, Quande; Chen, Cheng; Qin, Jing; Dou, Qi; Heng, Pheng-Ann","Liu, Quande (); Chen, Cheng (); Qin, Jing (); Dou, Qi (); Heng, Pheng-Ann ()",,"Liu, Quande (); Chen, Cheng (); Qin, Jing (); Dou, Qi (); Heng, Pheng-Ann ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136305385,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1071,pub.1101640766,10.1142/s2424905x18420035,,,Tele-Operated MRI-Guided Needle Insertion for Prostate Interventions,"Prostate cancer is one of the leading causes of death in men. Prostate interventions using magnetic resonance imaging (MRI) benefits from high tissue contrast if compared to other imaging modalities. The Minimally Invasive Robotics In An MRI environment (MIRIAM) robot is an MRI-compatible system able to steer different types of needles towards a point of interest using MRI guidance. However, clinicians can be reluctant to give the robot total control of the intervention. This work integrates a haptic device in the MIRIAM system to allow input from the clinician during the insertion. A shared control architecture is achieved by letting the clinician control the insertion depth via the haptic device, while the robotic system controls the needle orientation. The clinician receives haptic feedback based on the insertion depth and tissue characteristics. Four control laws relating the motion of the master robot (haptic device) to the motion of the slave robot (MIRIAM robot) are presented and evaluated. Quantitative and qualitative results from 20 human subjects demonstrate that the squared-velocity control law is the most suitable option for our application. Additionally, a pre-operative target localization algorithm is presented in order to provide the robot with the target location. The target localization and reconstruction algorithm are validated in phantom and patient images with an average dice similarity coefficient (DSC) of 0.78. The complete system is validated through experiments by inserting a needle towards a target within the MRI scanner. Four human subjects perform the experiment achieving an average targeting error of 3.4[Formula: see text]mm.",,,Journal of Medical Robotics Research,,,2019-03-19,2019,2019-03-19,2019-03,4,1,1842003,All OA, Green,Article,"Moreira, Pedro; Kuil, Leanne; Dias, Pedro; Borra, Ronald; Misra, Sarthak","Moreira, Pedro (Surgical Robotics Laboratory, Department of Biomechanical Engineering, University of Twente, The Netherlands); Kuil, Leanne (Surgical Robotics Laboratory, Department of Biomechanical Engineering, University of Twente, The Netherlands); Dias, Pedro (Department of Biomechanical Engineering, Universidade Nova de Lisboa, Portugal); Borra, Ronald (Faculty of Medical Sciences, Department of Nuclear Medicine and Molecular Imaging, University of Groningen and University Medical Center Groningen, The Netherlands; Medical Imaging Centre of Southwest Finland, Department of Diagnostic Radiology, Turku University Hospital, Turku, Finland); Misra, Sarthak (Surgical Robotics Laboratory, Department of Biomechanical Engineering, University of Twente, The Netherlands; Surgical Robotics Laboratory, Department of Biomedical Engineering, University of Groningen and University Medical Center Groningen, The Netherlands)",,"Moreira, Pedro (University of Twente); Kuil, Leanne (University of Twente); Dias, Pedro (Universidade Nova de Lisboa); Borra, Ronald (University Medical Center Groningen; Turku University Hospital); Misra, Sarthak (University of Twente; University Medical Center Groningen)",6,5,,2.22,https://ris.utwente.nl/ws/files/65755631/s2424905x18420035.pdf,https://app.dimensions.ai/details/publication/pub.1101640766,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 40 Engineering, 46 Information and Computing Sciences, 4608 Human-Centred Computing,,,,,,
1071,pub.1149658317,10.1109/tim.2022.3192862,,,An Automatic Prostate Surgical Region Reconstruction Method Based on Multilevel Learning,"Benign prostatic hyperplasia (BPH) is one of the main diseases affecting the health of middle-aged and elderly men. The accurate measurement and reconstruction of the surgical region (SR) from medical images is a vital and challenging step before the surgery. In this article, an automatic prostate SR reconstruction method based on multilevel learning is proposed. This method divides the reconstruction problem into two levels: prostate segmentation learning and reconstruction parameter learning. It can not only segment the prostate accurately, but also fuse various surgical constraints flexibly and doctors’ clinical experience. Compared with traditional methods, the proposed method has better reconstruction accuracy and flexibility. The proposed method was comprehensively validated on multiple datasets. It achieved better accuracy and robustness than current baselines on the magnetic resonance (MR) images of 20 patients from public and clinical datasets. Moreover, the clinical patients’ postoperative MR images were collected, and a preoperative–postoperative comparative study was carried out, which further proved the effectiveness of this method from a clinical perspective. Furthermore, this method has the potential to promote the development of BPH robotic surgery navigation and autonomy as well as improve the safety and efficiency of BPH surgery.",,"This work was supported by the National Natural Science Foundation of China under Grant 92048204, Grant 52188102, and Grant 91948301.",IEEE Transactions on Instrumentation and Measurement,,,2022-01-01,2022,2022-07-21,2022-01-01,71,,1-12,Closed,Article,"Chen, Teru; Zhao, Xingwei; Ling, Qing; Gong, Zeyu; Tao, Bo; Yin, Zhouping","Chen, Teru (State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Zhao, Xingwei (State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Ling, Qing (Department of Urology, Tongji Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, 430074, China); Gong, Zeyu (State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Tao, Bo (State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China); Yin, Zhouping (State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China)","Ling, Qing (Tongji Hospital; Huazhong University of Science and Technology)","Chen, Teru (Huazhong University of Science and Technology); Zhao, Xingwei (Huazhong University of Science and Technology); Ling, Qing (Tongji Hospital; Huazhong University of Science and Technology); Gong, Zeyu (Huazhong University of Science and Technology); Tao, Bo (Huazhong University of Science and Technology); Yin, Zhouping (Huazhong University of Science and Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149658317,"40 Engineering; 4006 Communications Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",,,,,,,,,,,,
1071,pub.1149421584,10.21203/rs.3.rs-1850296/v1,,,Comparison of automated segmentation techniques for magnetic resonance images of the prostate,"Background: The contouring of anatomical regions is a crucial step in the medical workflow and is both time-consuming and prone to intra- and inter-observer variability. This study compares different strategies for automatic segmentation of the prostate in T2-weighted MRIs.Methods: This study included 100 patients diagnosed with prostate adenocarcinoma who had undergone multi-parametric MRI and prostatectomy. From the T2-weighted MR images, ground truth segmentation masks were established by consensus from two expert radiologists. The prostate was then automatically contoured with six different methods: (1) an ad-hoc multi atlas-based algorithm, (2) a commercial package named SyngoVia, and four deep learning models: (3) a U-net deep learning architecture trained from scratch, (4) a U-net with a pre-trained encoder, (5) a GAN extension of the network in point 4, and (6) a segmentation-adapted modification of Google Brain’s EfficientDet architecture. The resulting segmentations were compared and scored against the ground truth masks with one 50/50 and one 70/30 train/test data split.Results: The best performing method was the adapted EfficientDet (model 6), achieving a mean Dice coefficient of 0.914, a mean absolute volume difference of 5.9%, a mean surface distance (MSD) of 1.93 pixels, and a mean 95th percentile Hausdorff distance of 3.77 pixels. The deep learning-based models were more reliable in terms of worst-case performance (0.854 minimum Dice and 4.02 maximum MSD), and no significant relationship was found between segmentation performance and clinical variables.Conclusion: Deep learning-based segmentation techniques can consistently achieve Dice coefficients of 0.9 or above with as few as 50 training patients, regardless of architectural archetype. Our atlas-based method and commercial software performed significantly worse (0.855-0.887 Dice).",,,Research Square,,,2022-07-12,2022,2022-07-12,,,,,All OA, Green,Preprint,"Isaksson, Lars Johannes; Pepa, Matteo; Summers, Paul; Zaffaroni, Mattia; Vincini, Maria Giulia; Corrao, Giulia; Mazzola, Giovanni Carlo; Rotondi, Marco; Presti, Giuliana Lo; Raimondi, Sara; Mistretta, Sara; Luzzago, Stefano; Cattani, Federica; Musi, Gennaro; De Cobelli, Ottavio; Cremonesi, Marta; Orecchia, Roberto; Marvaso, Giulia; Petralia, Giuseppe; Jereczek-Fossa, Barbara Alicja","Isaksson, Lars Johannes (European Institute of Oncology); Pepa, Matteo (European Institute of Oncology); Summers, Paul (European Institute of Oncology); Zaffaroni, Mattia (European Institute of Oncology); Vincini, Maria Giulia (European Institute of Oncology); Corrao, Giulia (European Institute of Oncology); Mazzola, Giovanni Carlo (European Institute of Oncology); Rotondi, Marco (European Institute of Oncology); Presti, Giuliana Lo (European Institute of Oncology); Raimondi, Sara (European Institute of Oncology); Mistretta, Sara (European Institute of Oncology); Luzzago, Stefano (European Institute of Oncology); Cattani, Federica (European Institute of Oncology); Musi, Gennaro (European Institute of Oncology); De Cobelli, Ottavio (European Institute of Oncology); Cremonesi, Marta (European Institute of Oncology); Orecchia, Roberto (European Institute of Oncology); Marvaso, Giulia (European Institute of Oncology); Petralia, Giuseppe (European Institute of Oncology); Jereczek-Fossa, Barbara Alicja (European Institute of Oncology)",,"Isaksson, Lars Johannes (European Institute of Oncology); Pepa, Matteo (European Institute of Oncology); Summers, Paul (European Institute of Oncology); Zaffaroni, Mattia (European Institute of Oncology); Vincini, Maria Giulia (European Institute of Oncology); Corrao, Giulia (European Institute of Oncology); Mazzola, Giovanni Carlo (European Institute of Oncology); Rotondi, Marco (European Institute of Oncology); Presti, Giuliana Lo (European Institute of Oncology); Raimondi, Sara (European Institute of Oncology); Mistretta, Sara (European Institute of Oncology); Luzzago, Stefano (European Institute of Oncology); Cattani, Federica (European Institute of Oncology); Musi, Gennaro (European Institute of Oncology); De Cobelli, Ottavio (European Institute of Oncology); Cremonesi, Marta (European Institute of Oncology); Orecchia, Roberto (European Institute of Oncology); Marvaso, Giulia (European Institute of Oncology); Petralia, Giuseppe (European Institute of Oncology); Jereczek-Fossa, Barbara Alicja (European Institute of Oncology)",2,2,,,https://www.researchsquare.com/article/rs-1850296/latest.pdf,https://app.dimensions.ai/details/publication/pub.1149421584,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,
1071,pub.1147253873,10.48550/arxiv.2204.08467,,,IOP-FL: Inside-Outside Personalization for Federated Medical Image  Segmentation,"Federated learning (FL) allows multiple medical institutions to
collaboratively learn a global model without centralizing all clients data. It
is difficult, if possible at all, for such a global model to commonly achieve
optimal performance for each individual client, due to the heterogeneity of
medical data from various scanners and patient demographics. This problem
becomes even more significant when deploying the global model to unseen clients
outside the FL with new distributions not presented during federated training.
To optimize the prediction accuracy of each individual client for critical
medical tasks, we propose a novel unified framework for both Inside and Outside
model Personalization in FL (IOP-FL). Our inside personalization is achieved by
a lightweight gradient-based approach that exploits the local adapted model for
each client, by accumulating both the global gradients for common knowledge and
local gradients for client-specific optimization. Moreover, and importantly,
the obtained local personalized models and the global model can form a diverse
and informative routing space to personalize a new model for outside FL
clients. Hence, we design a new test-time routing scheme inspired by the
consistency loss with a shape constraint to dynamically incorporate the models,
given the distribution information conveyed by the test data. Our extensive
experimental results on two medical image segmentation tasks present
significant improvements over SOTA methods on both inside and outside
personalization, demonstrating the great potential of our IOP-FL scheme for
clinical practice. Code will be released at https://github.com/med-air/IOP-FL.",,,arXiv,,,2022-04-16,2022,,,,,,All OA, Green,Preprint,"Jiang, Meirui; Yang, Hongzheng; Cheng, Chen; Dou, Qi","Jiang, Meirui (); Yang, Hongzheng (); Cheng, Chen (); Dou, Qi ()",,"Jiang, Meirui (); Yang, Hongzheng (); Cheng, Chen (); Dou, Qi ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1147253873,40 Engineering, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,
1071,pub.1134724456,10.3390/app11020844,,,Robust Resolution-Enhanced Prostate Segmentation in Magnetic Resonance and Ultrasound Images through Convolutional Neural Networks,"Prostate segmentations are required for an ever-increasing number of medical applications, such as image-based lesion detection, fusion-guided biopsy and focal therapies. However, obtaining accurate segmentations is laborious, requires expertise and, even then, the inter-observer variability remains high. In this paper, a robust, accurate and generalizable model for Magnetic Resonance (MR) and three-dimensional (3D) Ultrasound (US) prostate image segmentation is proposed. It uses a densenet-resnet-based Convolutional Neural Network (CNN) combined with techniques such as deep supervision, checkpoint ensembling and Neural Resolution Enhancement. The MR prostate segmentation model was trained with five challenging and heterogeneous MR prostate datasets (and two US datasets), with segmentations from many different experts with varying segmentation criteria. The model achieves a consistently strong performance in all datasets independently (mean Dice Similarity Coefficient -DSC- above 0.91 for all datasets except for one), outperforming the inter-expert variability significantly in MR (mean DSC of 0.9099 vs. 0.8794). When evaluated on the publicly available Promise12 challenge dataset, it attains a similar performance to the best entries. In summary, the model has the potential of having a significant impact on current prostate procedures, undercutting, and even eliminating, the need of manual segmentations through improvements in terms of robustness, generalizability and output resolution.",,"This work has been partially supported by a doctoral grant of the Spanish Ministry of Innovation and Science, with reference FPU17/01993.",Applied Sciences,,,2021-01-18,2021,2021-01-18,,11,2,844,All OA, Gold,Article,"Pellicer-Valero, Oscar J.; Gonzalez-Perez, Victor; Ramón-Borja, Juan Luis Casanova; García, Isabel Martín; Benito, María Barrios; Gómez, Paula Pelechano; Rubio-Briones, José; Rupérez, María José; Martín-Guerrero, José D.","Pellicer-Valero, Oscar J. (Intelligent Data Analysis Laboratory, Department of Electronic Engineering, ETSE (Engineering School), Universitat de València (UV), Av. Universitat, sn, 46100 Bujassot, Valencia, Spain;, jose.d.martin@uv.es); Gonzalez-Perez, Victor (Department of Radiodiagnosis, Fundación Instituto Valenciano de Oncología (FIVO), Beltrán Báguena, 8, 46009 Valencia, Spain;, vgonzalez@fivo.org, (V.G.-P.);, mismaga99@gmail.com, (I.M.G.);, mar7esc@gmail.com, (M.B.B.);, ppelechano@hotmail.com, (P.P.G.)); Ramón-Borja, Juan Luis Casanova (Department of Medical Physics, Fundación Instituto Valenciano de Oncología (FIVO), Beltrán Báguena, 8, 46009 Valencia, Spain;, jcasanova@fivo.org, (J.L.C.R.-B.);, jrubio@fivo.org, (J.R.-B.)); García, Isabel Martín (Department of Radiodiagnosis, Fundación Instituto Valenciano de Oncología (FIVO), Beltrán Báguena, 8, 46009 Valencia, Spain;, vgonzalez@fivo.org, (V.G.-P.);, mismaga99@gmail.com, (I.M.G.);, mar7esc@gmail.com, (M.B.B.);, ppelechano@hotmail.com, (P.P.G.)); Benito, María Barrios (Department of Radiodiagnosis, Fundación Instituto Valenciano de Oncología (FIVO), Beltrán Báguena, 8, 46009 Valencia, Spain;, vgonzalez@fivo.org, (V.G.-P.);, mismaga99@gmail.com, (I.M.G.);, mar7esc@gmail.com, (M.B.B.);, ppelechano@hotmail.com, (P.P.G.)); Gómez, Paula Pelechano (Department of Radiodiagnosis, Fundación Instituto Valenciano de Oncología (FIVO), Beltrán Báguena, 8, 46009 Valencia, Spain;, vgonzalez@fivo.org, (V.G.-P.);, mismaga99@gmail.com, (I.M.G.);, mar7esc@gmail.com, (M.B.B.);, ppelechano@hotmail.com, (P.P.G.)); Rubio-Briones, José (Department of Medical Physics, Fundación Instituto Valenciano de Oncología (FIVO), Beltrán Báguena, 8, 46009 Valencia, Spain;, jcasanova@fivo.org, (J.L.C.R.-B.);, jrubio@fivo.org, (J.R.-B.)); Rupérez, María José (Centro de Investigación en Ingeniería Mecánica (CIIM), Universitat Politècnica de València (UPV), Camino de Vera, sn, 46022 Valencia, Spain;, mjrupere@upvnet.upv.es); Martín-Guerrero, José D. (Intelligent Data Analysis Laboratory, Department of Electronic Engineering, ETSE (Engineering School), Universitat de València (UV), Av. Universitat, sn, 46100 Bujassot, Valencia, Spain;, jose.d.martin@uv.es)","Pellicer-Valero, Oscar J. (University of Valencia)","Pellicer-Valero, Oscar J. (University of Valencia); Gonzalez-Perez, Victor (Fundación Instituto Valenciano de Oncología); Ramón-Borja, Juan Luis Casanova (Fundación Instituto Valenciano de Oncología); García, Isabel Martín (Fundación Instituto Valenciano de Oncología); Benito, María Barrios (Fundación Instituto Valenciano de Oncología); Gómez, Paula Pelechano (Fundación Instituto Valenciano de Oncología); Rubio-Briones, José (Fundación Instituto Valenciano de Oncología); Rupérez, María José (Universitat Politècnica de València); Martín-Guerrero, José D. (University of Valencia)",3,3,,2.63,https://www.mdpi.com/2076-3417/11/2/844/pdf?version=1611035136,https://app.dimensions.ai/details/publication/pub.1134724456,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences, 51 Physical Sciences,,,,,,,,
1071,pub.1121232730,10.1109/access.2019.2943485,,,Feature-Based Deformable Registration Using Minimal Spanning Tree for Prostate MR Segmentation,"Automatic and accurate segmentation of the prostate is still a challenging task due to intensity inhomogeneity and complicated deformation of MR images. To tackle these problems with multi-atlas segmentation, in this paper, we propose a new metric for image registration and new descriptor for label fusion. First, to reduce the amount of edges in entropic graph, a modified $\alpha $ -mutual information ( $\alpha $ -MI) based on fast minimal spanning tree (MST) is implemented for deformable registration. Second, localized $\alpha $ -MI allowing for the spatial information is proposed with the stochastic gradient optimization, and the feature space is encoded by a sparse auto-encoder. Finally, a multi-scale descriptor utilizing local self-similarity is integrated into the patch-based label fusion to obtain final segmentation. Experiments were performed on two subsets of totally 46 T2-weighted prostate MR images from 46 patients. Compared to $\alpha $ -MI based on ${k}$ -nearest neighbor graph, the registration time of $\alpha $ -MI based on fast MST can be reduced by almost half. The median Dice overlap of registration using localized $\alpha $ -MI on one subset is shown to improve significantly from 0.725 to 0.764 ( $p=1.14\times 10^{-5}$ ), compared to using $\alpha $ -MI without the spatial information. The median Dice overlap of prostate segmentation using the proposed method on 20 testing images of another subset is 0.871, and the median Hausdorff distance is 8.013 mm, which demonstrate a comparable accuracy to state-of-the-art methods.","This work was supported in part by the National Natural Science Foundation of China under Grant 61002046 and Grant 81871332, the Research Grants Council of the Hong Kong Special Administrative Region of China under Grant CUHK14204117, the Innovation and Technology Commission under Grant GHP-025-17SZ and Grant GHP-028-14SZ, the Fundamental Research Funds for the Central Universities, the South-Central University for Nationalities under Grant CZY19024, and the Scheme Double First Class Program from the Ministry of Education, the Ministry of Finance, and the National Development and Reform Commission, China.",,IEEE Access,,,2019-01-01,2019,2019-09-24,2019-01-01,7,,138645-138656,All OA, Gold,Article,"Lu, Xuesong; Zha, Yunfei; Qiao, Yuchuan; Wang, Defeng","Lu, Xuesong (College of Biomedical Engineering, South-Central University for Nationalities, Wuhan, 430074, China); Zha, Yunfei (Department of Radiology, Renmin Hospital of Wuhan University, Wuhan, 430060, China); Qiao, Yuchuan (Laboratory of Neuro Imaging, Keck School of Medicine of USC, Los Angeles, CA, 90033, USA); Wang, Defeng (School of Instrumentation and Optoelectronic Engineering, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beijing, 100083, China; Research Centre for Medical Image Computing, Department of Imaging and Interventional Radiology, The Chinese University of Hong Kong, Hong Kong)","Lu, Xuesong (South Central University for Nationalities)","Lu, Xuesong (South Central University for Nationalities); Zha, Yunfei (Renmin Hospital of Wuhan University); Qiao, Yuchuan (University of Southern California); Wang, Defeng (Beihang University; Beihang University; Chinese University of Hong Kong)",2,1,,,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08847432.pdf,https://app.dimensions.ai/details/publication/pub.1121232730,40 Engineering, 46 Information and Computing Sciences,,,,,,,,,,
1067,pub.1141306350,10.1109/access.2021.3114733,,,ProCDet: A New Method for Prostate Cancer Detection Based on MR Images,"Prostate cancer is a malignant tumor that occurs in the male prostate. Prostate cancer lesions have the characteristics of small size and blurry outline, which is a challenge to design a robust prostate cancer detection method. At present, clinical diagnosis of prostate cancer is mainly based on magnetic resonance (MR) imaging. However, it is difficult to obtain prostate cancer data, and the data with true values is also very limited, which further increases the difficulty of prostate cancer detection methods based on MR images. To solve these problems, this paper designs a new method of prostate cancer detection based on MR images, which is recorded as ProCDet. The method consists of three modules: registration of prostate MR images, segmentation of prostate, and segmentation of prostate cancer lesions. First, the registration between different sequences of MR images is performed to find the spatial relationship between the different sequences. Then, the designed prostate segmentation network based on the attention mechanism is used to segment the prostate to remove the interference of background information. Finally, a 3D prostate cancer lesion segmentation network based on Focal Tversky Loss is applied to determine the specific location of prostate cancer. Moreover, in order to take full advantage of unlabeled prostate data, this paper designs a self-supervised learning method to improve the accuracy of prostate cancer detection. The proposed ProCDet has been experimentally verified on the ProstateX dataset. When the average number of false-positive lesions per patient is 0.6275, the true-positive rate is 91.82%. Experimental results show that the ProCDet can obtain competitive detection performance.","This work was supported by the Natural Science Foundation of Zhejiang Province under Grant LGF19F020004, and in part by Wenzhou Science and Technology Project under Grant G20190022.",,IEEE Access,,,2021-01-01,2021,2021-09-22,2021-01-01,9,,143495-143505,All OA, Gold,Article,"Qian, Yuejing; Zhang, Zengyou; Wang, Bo","Qian, Yuejing (Zhejiang Industry and Trade Vocational College, Wenzhou, Zhejiang, 313103, China); Zhang, Zengyou (Zhejiang Industry and Trade Vocational College, Wenzhou, Zhejiang, 313103, China); Wang, Bo (Zhejiang College of Security Technology, Wenzhou, Zhejiang, 325000, China)","Wang, Bo ","Qian, Yuejing (); Zhang, Zengyou (); Wang, Bo ()",1,1,,,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09546769.pdf,https://app.dimensions.ai/details/publication/pub.1141306350,46 Information and Computing Sciences,,,,,,,,,,,
1066,pub.1130866476,10.1007/s11042-020-09810-9,,,Brain tumor detection based on hybrid deep neural network in MRI by adaptive squirrel search optimization,"In the medical field, image segmentation is a paramount and challenging task. The head and vertebral column make up the central nervous system (CNS), which control all the paramount functions. These include thinking, speaking, and gestures. The uncontrolled growth in the CNS can affect a person’s thinking of communication or movement. The tumor is known as the uncontrolled growth of cells in brain. The tumor can be recognized by MRI image. Brain tumor detection is mostly affected with inaccurate classification. This proposed work designed a novel classification and segmentation algorithm for the brain tumor detection. The proposed system uses the Adaptive fuzzy deep neural network with frog leap optimization to detect normality and abnormality of the image. Accurate classification is achieved with error minimization strategy through our proposed method. Then, the abnormal image is segmented using adaptive flying squirrel algorithm and the size of the tumor is detected, which is used to find out the severity of the tumor. The proposed work is implemented in the MATLAB simulation platform. The proposed work Accuracy, sensitivity, specificity, false positive rate and false negative rate are 99.6%, 99.9%, 99.8%, 0.0043 and 0.543, respectively. The detection accuracy is better in our proposed system than the existing teaching and learning based algorithm, social group algorithm and deep neural network.",,,Multimedia Tools and Applications,,,2020-09-15,2020,2020-09-15,2021-01,80,2,2621-2645,Closed,Article,"Deb, Daizy; Roy, Sudipta","Deb, Daizy (Department of Computer Science and Engineering, Triguna Sen School of Technology, Assam University, 788011, Silchar, India); Roy, Sudipta (Department of Computer Science & Engineering, Triguna Sen School of Technology, Assam University (A Central University), 788011, Silchar, India)","Deb, Daizy (Assam University)","Deb, Daizy (Assam University); Roy, Sudipta (Assam University)",28,28,,14.43,,https://app.dimensions.ai/details/publication/pub.1130866476,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
1066,pub.1129749777,10.48550/arxiv.2007.14556,,,Accurate Lung Nodules Segmentation with Detailed Representation Transfer  and Soft Mask Supervision,"Accurate lung lesion segmentation from Computed Tomography (CT) images is
crucial to the analysis and diagnosis of lung diseases such as COVID-19 and
lung cancer. However, the smallness and variety of lung nodules and the lack of
high-quality labeling make the accurate lung nodule segmentation difficult. To
address these issues, we first introduce a novel segmentation mask named Soft
Mask which has richer and more accurate edge details description and better
visualization and develop a universal automatic Soft Mask annotation pipeline
to deal with different datasets correspondingly. Then, a novel Network with
detailed representation transfer and Soft Mask supervision (DSNet) is proposed
to process the input low-resolution images of lung nodules into high-quality
segmentation results. Our DSNet contains a special Detail Representation
Transfer Module (DRTM) for reconstructing the detailed representation to
alleviate the small size of lung nodules images, and an adversarial training
framework with Soft Mask for further improving the accuracy of segmentation.
Extensive experiments validate that our DSNet outperforms other
state-of-the-art methods for accurate lung nodule segmentation and has strong
generalization ability in other accurate medical segmentation tasks with
competitive results. Besides, we provide a new challenging lung nodules
segmentation dataset for further studies.",,,arXiv,,,2020-07-28,2020,,,,,,All OA, Green,Preprint,"Wang, Changwei; Xu, Rongtao; Xu, Shibiao; Meng, Weiliang; Xiao, Jun; Zhang, Xiaopeng","Wang, Changwei (); Xu, Rongtao (); Xu, Shibiao (); Meng, Weiliang (); Xiao, Jun (); Zhang, Xiaopeng ()",,"Wang, Changwei (); Xu, Rongtao (); Xu, Shibiao (); Meng, Weiliang (); Xiao, Jun (); Zhang, Xiaopeng ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129749777,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1017,pub.1118981923,10.48550/arxiv.1802.04894,,,Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A  Survey,"Osteoarthritis (OA) is one of the major health issues among the elderly
population. MRI is the most popular technology to observe and evaluate the
progress of OA course. However, the extreme labor cost of MRI analysis makes
the process inefficient and expensive. Also, due to human error and subjective
nature, the inter- and intra-observer variability is rather high.
Computer-aided knee MRI segmentation is currently an active research field
because it can alleviate doctors and radiologists from the time consuming and
tedious job, and improve the diagnosis performance which has immense potential
for both clinic and scientific research. In the past decades, researchers have
investigated automatic/semi-automatic knee MRI segmentation methods
extensively. However, to the best of our knowledge, there is no comprehensive
survey paper in this field yet. In this survey paper, we classify the existing
methods by their principles and discuss the current research status and point
out the future research trend in-depth.",,,arXiv,,,2018-02-13,2018,,,,,,All OA, Green,Preprint,"Zhang, Boyu; Zhang, Yingtao; Cheng, H. D.; Xian, Min; Gai, Shan; Cheng, Olivia; Huang, Kuan","Zhang, Boyu (); Zhang, Yingtao (); Cheng, H. D. (); Xian, Min (); Gai, Shan (); Cheng, Olivia (); Huang, Kuan ()",,"Zhang, Boyu (); Zhang, Yingtao (); Cheng, H. D. (); Xian, Min (); Gai, Shan (); Cheng, Olivia (); Huang, Kuan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1118981923,40 Engineering, 4003 Biomedical Engineering, 46 Information and Computing Sciences,,,,,,,,,
1015,pub.1150987953,10.48550/arxiv.2209.05160,,,Prototypical few-shot segmentation for cross-institution male pelvic  structures with spatial registration,"The prowess that makes few-shot learning desirable in medical image analysis
is the efficient use of the support image data, which are labelled to classify
or segment new classes, a task that otherwise requires substantially more
training images and expert annotations. This work describes a fully 3D
prototypical few-shot segmentation algorithm, such that the trained networks
can be effectively adapted to clinically interesting structures that are absent
in training, using only a few labelled images from a different institute.
First, to compensate for the widely recognised spatial variability between
institutions in episodic adaptation of novel classes, a novel spatial
registration mechanism is integrated into prototypical learning, consisting of
a segmentation head and an spatial alignment module. Second, to assist the
training with observed imperfect alignment, support mask conditioning module is
proposed to further utilise the annotation available from the support images.
Extensive experiments are presented in an application of segmenting eight
anatomical structures important for interventional planning, using a data set
of 589 pelvic T2-weighted MR images, acquired at seven institutes. The results
demonstrate the efficacy in each of the 3D formulation, the spatial
registration, and the support mask conditioning, all of which made positive
contributions independently or collectively. Compared with the previously
proposed 2D alternatives, the few-shot segmentation performance was improved
with statistical significance, regardless whether the support data come from
the same or different institutes.",,,arXiv,,,2022-09-12,2022,,,,,,All OA, Green,Preprint,"Li, Yiwen; Fu, Yunguan; Gayo, Iani; Yang, Qianye; Min, Zhe; Saeed, Shaheer; Yan, Wen; Wang, Yipei; Noble, J. Alison; Emberton, Mark; Clarkson, Matthew J.; Huisman, Henkjan; Barratt, Dean; Prisacariu, Victor Adrian; Hu, Yipeng","Li, Yiwen (); Fu, Yunguan (); Gayo, Iani (); Yang, Qianye (); Min, Zhe (); Saeed, Shaheer (); Yan, Wen (); Wang, Yipei (); Noble, J. Alison (); Emberton, Mark (); Clarkson, Matthew J. (); Huisman, Henkjan (); Barratt, Dean (); Prisacariu, Victor Adrian (); Hu, Yipeng ()",,"Li, Yiwen (); Fu, Yunguan (); Gayo, Iani (); Yang, Qianye (); Min, Zhe (); Saeed, Shaheer (); Yan, Wen (); Wang, Yipei (); Noble, J. Alison (); Emberton, Mark (); Clarkson, Matthew J. (); Huisman, Henkjan (); Barratt, Dean (); Prisacariu, Victor Adrian (); Hu, Yipeng ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150987953,46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
1015,pub.1130510832,10.1016/j.neucom.2020.07.116,,,3D multi-scale discriminative network with multi-directional edge loss for prostate zonal segmentation in bi-parametric MR images,"Accurate and reliable segmentation of the prostate, its inner and surrounding structures in magnetic resonance (MR) images, is of essential importance for image-guided prostate interventions and treatments. Further clinical demand is the automated segmentation of central gland (CG) and peripheral zone (PZ). Although T2-weighted (T2W) images can show prostate tissues more clearly, the lack of contrast between PZ and other surrounding tissues in T2W images increases the difficulty of PZ segmentation. In this context, it is necessary to consider using multi-parametric MR images to obtain complementary information for prostate zonal segmentation. In this paper, we propose a 3D multi-scale discriminative network with pyramid attention module (PAM) and residual refinement block (RRB) for automated and accurate segmentation of CG and PZ using bi-parametric (T2W and apparent diffusion coefficient) MR images. One of the major difficulties in prostate MR image segmentation is the ambiguous edge between the prostate and other surrounding anatomical structures, which is reflected in some specific directions. Therefore, we design a multi-directional edge loss to help the network focus on the multi-directional edge information of foreground areas. For the Prostate Multi-parametric MRI (PROMM) dataset, our proposed model achieved Dice similarity coefficient of 0.908 at CG and 0.785 at PZ. The average boundary distance obtained by our model is 1.397 mm at CG and 3.891 mm at PZ. For the NCI-ISBI dataset, our method greatly improves the Dice similarity coefficient at PZ, reaching 0.806 and achieved the Dice similarity coefficient of 0.901 at CG. The experimental results on two different MR prostate datasets demonstrate that our model is more sensitive to object boundaries and outperforms other state-of-the-art methods. The visualization of feature map activations in PAM shows that the proposed model can capture multi-scale discriminative features effectively.",This work was supported by the Science and Technology Commission of Shanghai Municipality (No. 17411952300). We greatly appreciate the organization for sharing the NCI-ISBI 2013 Automated Segmentation of Prostate Structures Challenge dataset analyzed in this study.,,Neurocomputing,,,2020-12,2020,,2020-12,418,,148-161,Closed,Article,"Qin, Xiangxiang; Zhu, Yu; Wang, Wei; Gui, Shaojun; Zheng, Bingbing; Wang, Peijun","Qin, Xiangxiang (School of Information Science and Technology, East China University of Science and Technology, Shanghai 200237, PR China); Zhu, Yu (School of Information Science and Technology, East China University of Science and Technology, Shanghai 200237, PR China); Wang, Wei (Department of Radiology, Tongji Hospital, Tongji University School of Medicine, Shanghai 200065, PR China); Gui, Shaojun (School of Information Science and Technology, East China University of Science and Technology, Shanghai 200237, PR China); Zheng, Bingbing (School of Information Science and Technology, East China University of Science and Technology, Shanghai 200237, PR China); Wang, Peijun (Department of Radiology, Tongji Hospital, Tongji University School of Medicine, Shanghai 200065, PR China)","Zhu, Yu (East China University of Science and Technology); Wang, Peijun (Tongji University)","Qin, Xiangxiang (East China University of Science and Technology); Zhu, Yu (East China University of Science and Technology); Wang, Wei (Tongji University); Gui, Shaojun (East China University of Science and Technology); Zheng, Bingbing (East China University of Science and Technology); Wang, Peijun (Tongji University)",7,7,,,,https://app.dimensions.ai/details/publication/pub.1130510832,40 Engineering, 46 Information and Computing Sciences, 52 Psychology,,,,,,,,,,
1011,pub.1111990221,10.1201/9780429434334-14,,,Automated Prostate Image Recognition and Segmentation,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,243-258,Closed,Chapter,"Yan, Ke; Wang, Xiuying; Kim, Jinman; Li, Changyang; Feng, Dagan; Khadra, Mohamed","Yan, Ke (); Wang, Xiuying (); Kim, Jinman (); Li, Changyang (); Feng, Dagan (); Khadra, Mohamed ()",,"Yan, Ke (); Wang, Xiuying (); Kim, Jinman (); Li, Changyang (); Feng, Dagan (); Khadra, Mohamed ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1111990221,,,,,,,,,,,,,
1008,pub.1151183568,10.48550/arxiv.2209.09641,,,Calibrating Segmentation Networks with Margin-based Label Smoothing,"Despite the undeniable progress in visual recognition tasks fueled by deep
neural networks, there exists recent evidence showing that these models are
poorly calibrated, resulting in over-confident predictions. The standard
practices of minimizing the cross entropy loss during training promote the
predicted softmax probabilities to match the one-hot label assignments.
Nevertheless, this yields a pre-softmax activation of the correct class that is
significantly larger than the remaining activations, which exacerbates the
miscalibration problem. Recent observations from the classification literature
suggest that loss functions that embed implicit or explicit maximization of the
entropy of predictions yield state-of-the-art calibration performances. Despite
these findings, the impact of these losses in the relevant task of calibrating
medical image segmentation networks remains unexplored. In this work, we
provide a unifying constrained-optimization perspective of current
state-of-the-art calibration losses. Specifically, these losses could be viewed
as approximations of a linear penalty (or a Lagrangian term) imposing equality
constraints on logit distances. This points to an important limitation of such
underlying equality constraints, whose ensuing gradients constantly push
towards a non-informative solution, which might prevent from reaching the best
compromise between the discriminative performance and calibration of the model
during gradient-based optimization. Following our observations, we propose a
simple and flexible generalization based on inequality constraints, which
imposes a controllable margin on logit distances. Comprehensive experiments on
a variety of public medical image segmentation benchmarks demonstrate that our
method sets novel state-of-the-art results on these tasks in terms of network
calibration, whereas the discriminative performance is also improved.",,,arXiv,,,2022-09-09,2022,,,,,,All OA, Green,Preprint,"Murugesan, Balamurali; Liu, Bingyuan; Galdran, Adrian; Ayed, Ismail Ben; Dolz, Jose","Murugesan, Balamurali (); Liu, Bingyuan (); Galdran, Adrian (); Ayed, Ismail Ben (); Dolz, Jose ()",,"Murugesan, Balamurali (); Liu, Bingyuan (); Galdran, Adrian (); Ayed, Ismail Ben (); Dolz, Jose ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151183568,46 Information and Computing Sciences, 4611 Machine Learning,10 Reduced Inequalities,,,,,,,,,
1008,pub.1145203833,10.1016/j.patcog.2022.108556,,,Learning multi-scale synergic discriminative features for prostate image segmentation,"Although deep convolutional neural networks (DCNNs) have been proposed for prostate MR image segmentation, the effectiveness of these methods is often limited by inadequate semantic discrimination and spatial context modeling. To address these issues, we propose a Multi-scale Synergic Discriminative Network (MSD-Net), which includes a shared encoder, a segmentation decoder, and a boundary detection decoder. We further design the cascaded pyramid convolutional block and residual refinement block, and incorporate them and the channel attention block into MSD-Net to exploit the multi-scale spatial contextual information and semantically consistent features of the gland. We also fuse the features from two decoders to boost the segmentation performance, and introduce the synergic multi-task loss to impose the consistence constraint on the joint segmentation and boundary detection. We evaluated MSD-Net against several prostate segmentation methods on three public datasets and achieved an improved accuracy. Our results indicate that the proposed MSD-Net outperforms existing methods with setting the new state-of-the-art for prostate segmentation in magnetic resonance images.","H. Jia and Y. Xia were supported in part by the National Natural Science Foundation of China under Grants 62171377, in part by the Natural Science Foundation of Ningbo City, China, under Grant 2021J052, in part by the Science and Technology Innovation Committee of Shenzhen Municipality, China, under Grants JCYJ20180306171334997, in part by the CAAI-Huawei MindSpore Open Fund under Grants CAAIXSJLJJ-2020-005B, and in part by the Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University, under Grant CX202042. W. Cai, H. Huang, and their employers received no financial support for the research, authorship, and/or publication of this article. The authors would like to appreciate the efforts devoted to collect and share the PROMISE12, NCI-ISBI13, and 12CVB datasets for comparing interactive and (semi)-automatic segmentation algorithms for MRI of the prostate.",,Pattern Recognition,,,2022-06,2022,,2022-06,126,,108556,Closed,Article,"Jia, Haozhe; Cai, Weidong; Huang, Heng; Xia, Yong","Jia, Haozhe (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an 710072, China; Research Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen 518057, China; Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA 15261, USA); Cai, Weidong (School of Computer Science, University of Sydney, Sydney, NSW 2006, Australia); Huang, Heng (Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA 15261, USA; JD Finance America Corporation, Mountain View, California, CA 94043, USA); Xia, Yong (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an 710072, China; Research Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen 518057, China)","Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University)","Jia, Haozhe (Northwestern Polytechnical University; Northwestern Polytechnical University; University of Pittsburgh); Cai, Weidong (The University of Sydney); Huang, Heng (University of Pittsburgh); Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1145203833,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,,
1008,pub.1144118799,10.48550/arxiv.2112.10775,,,HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on  Heterogeneous Medical Images,"Multiple medical institutions collaboratively training a model using
federated learning (FL) has become a promising solution for maximizing the
potential of data-driven models, yet the non-independent and identically
distributed (non-iid) data in medical images is still an outstanding challenge
in real-world practice. The feature heterogeneity caused by diverse scanners or
protocols introduces a drift in the learning process, in both local (client)
and global (server) optimizations, which harms the convergence as well as model
performance. Many previous works have attempted to address the non-iid issue by
tackling the drift locally or globally, but how to jointly solve the two
essentially coupled drifts is still unclear. In this work, we concentrate on
handling both local and global drifts and introduce a new harmonizing framework
called HarmoFL. First, we propose to mitigate the local update drift by
normalizing amplitudes of images transformed into the frequency domain to mimic
a unified imaging setting, in order to generate a harmonized feature space
across local clients. Second, based on harmonized features, we design a client
weight perturbation guiding each local model to reach a flat optimum, where a
neighborhood area of the local optimal solution has a uniformly low loss.
Without any extra communication cost, the perturbation assists the global model
to optimize towards a converged optimal solution by aggregating several local
flat optima. We have theoretically analyzed the proposed method and empirically
conducted extensive experiments on three medical image classification and
segmentation tasks, showing that HarmoFL outperforms a set of recent
state-of-the-art methods with promising convergence behavior. Code is available
at https://github.com/med-air/HarmoFL.",,,arXiv,,,2021-12-20,2021,,,,,,All OA, Green,Preprint,"Jiang, Meirui; Wang, Zirui; Dou, Qi","Jiang, Meirui (); Wang, Zirui (); Dou, Qi ()",,"Jiang, Meirui (); Wang, Zirui (); Dou, Qi ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144118799,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1008,pub.1139018739,10.1016/j.mlwa.2021.100078,,,Model-based segmentation using neural network-based boundary detectors: Application to prostate and heart segmentation in MR images,"Model-based segmentation (MBS) is a variant of active surfaces and active shape models that has successfully been used to segment anatomical structures such as the heart or the brain. We propose to integrate neural networks (NNs) into MBS for boundary detection. We formulate boundary detection as a regression task and use a NN to predict the distances between a surface mesh and the corresponding boundary points. The proposed approach has been applied to two tasks — prostate segmentation in MR images and the segmentation of the left and right ventricle in MR images. For the first task, data from the Prostate MR Image Segmentation 2012 (PROMISE12) challenge has been used. For the second task, a diverse database with cardiac MR images from six clinical sites has been used. We compare the results to the popular U-net approaches using the nnU-net implementation that is among the top performing segmentation algorithms in various challenges. In cross-validation experiments, the mean Dice scores are very similar and no statistically significant difference is observed. On the PROMISE12 test set, nnU-net Dice scores are significantly better. This is achieved by using an ensemble of 2D and 3D U-nets to generate the final segmentation, a concept that may also be adapted to NN-based boundary detection in the future. While the U-net provides a voxel labeling, our approach provides a 3D surface mesh with pre-defined mesh topology, establishes correspondences with respect to the reference mesh, avoids isolated falsely segmented regions and ensures proper connectivity of different regions.",,,Machine Learning with Applications,,,2021-12,2021,,2021-12,6,,100078,All OA, Gold,Article,"Brosch, Tom; Peters, Jochen; Groth, Alexandra; Weber, Frank Michael; Weese, Jürgen","Brosch, Tom (Philips GmbH Innovative Technologies, Röntgenstrasse 24, 22335 Hamburg, Germany); Peters, Jochen (Philips GmbH Innovative Technologies, Röntgenstrasse 24, 22335 Hamburg, Germany); Groth, Alexandra (Philips GmbH Innovative Technologies, Röntgenstrasse 24, 22335 Hamburg, Germany); Weber, Frank Michael (Philips GmbH Innovative Technologies, Röntgenstrasse 24, 22335 Hamburg, Germany); Weese, Jürgen (Philips GmbH Innovative Technologies, Röntgenstrasse 24, 22335 Hamburg, Germany)","Weese, Jürgen (Philips (Germany))","Brosch, Tom (Philips (Germany)); Peters, Jochen (Philips (Germany)); Groth, Alexandra (Philips (Germany)); Weber, Frank Michael (Philips (Germany)); Weese, Jürgen (Philips (Germany))",4,4,,3.27,https://doi.org/10.1016/j.mlwa.2021.100078,https://app.dimensions.ai/details/publication/pub.1139018739,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1008,pub.1133377973,10.48550/arxiv.2012.04885,,,Annotation-efficient deep learning for automatic medical image  segmentation,"Automatic medical image segmentation plays a critical role in scientific
research and medical care. Existing high-performance deep learning methods
typically rely on large training datasets with high-quality manual annotations,
which are difficult to obtain in many clinical applications. Here, we introduce
Annotation-effIcient Deep lEarning (AIDE), an open-source framework to handle
imperfect training datasets. Methodological analyses and empirical evaluations
are conducted, and we demonstrate that AIDE surpasses conventional
fully-supervised models by presenting better performance on open datasets
possessing scarce or noisy annotations. We further test AIDE in a real-life
case study for breast tumor segmentation. Three datasets containing 11,852
breast images from three medical centers are employed, and AIDE, utilizing 10%
training annotations, consistently produces segmentation maps comparable to
those generated by fully-supervised counterparts or provided by independent
radiologists. The 10-fold enhanced efficiency in utilizing expert labels has
the potential to promote a wide range of biomedical applications.",,,arXiv,,,2020-12-09,2020,,,,,,All OA, Green,Preprint,"Wang, Shanshan; Li, Cheng; Wang, Rongpin; Liu, Zaiyi; Wang, Meiyun; Tan, Hongna; Wu, Yaping; Liu, Xinfeng; Sun, Hui; Yang, Rui; Liu, Xin; Chen, Jie; Zhou, Huihui; Ayed, Ismail Ben; Zheng, Hairong","Wang, Shanshan (); Li, Cheng (); Wang, Rongpin (); Liu, Zaiyi (); Wang, Meiyun (); Tan, Hongna (); Wu, Yaping (); Liu, Xinfeng (); Sun, Hui (); Yang, Rui (); Liu, Xin (); Chen, Jie (); Zhou, Huihui (); Ayed, Ismail Ben (); Zheng, Hairong ()",,"Wang, Shanshan (); Li, Cheng (); Wang, Rongpin (); Liu, Zaiyi (); Wang, Meiyun (); Tan, Hongna (); Wu, Yaping (); Liu, Xinfeng (); Sun, Hui (); Yang, Rui (); Liu, Xin (); Chen, Jie (); Zhou, Huihui (); Ayed, Ismail Ben (); Zheng, Hairong ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1133377973,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1008,pub.1124801595,10.48550/arxiv.2002.03366,,,MS-Net: Multi-Site Network for Improving Prostate Segmentation with  Heterogeneous MRI Data,"Automated prostate segmentation in MRI is highly demanded for
computer-assisted diagnosis. Recently, a variety of deep learning methods have
achieved remarkable progress in this task, usually relying on large amounts of
training data. Due to the nature of scarcity for medical images, it is
important to effectively aggregate data from multiple sites for robust model
training, to alleviate the insufficiency of single-site samples. However, the
prostate MRIs from different sites present heterogeneity due to the differences
in scanners and imaging protocols, raising challenges for effective ways of
aggregating multi-site data for network training. In this paper, we propose a
novel multi-site network (MS-Net) for improving prostate segmentation by
learning robust representations, leveraging multiple sources of data. To
compensate for the inter-site heterogeneity of different MRI datasets, we
develop Domain-Specific Batch Normalization layers in the network backbone,
enabling the network to estimate statistics and perform feature normalization
for each site separately. Considering the difficulty of capturing the shared
knowledge from multiple datasets, a novel learning paradigm, i.e.,
Multi-site-guided Knowledge Transfer, is proposed to enhance the kernels to
extract more generic representations from multi-site data. Extensive
experiments on three heterogeneous prostate MRI datasets demonstrate that our
MS-Net improves the performance across all datasets consistently, and
outperforms state-of-the-art methods for multi-site learning.",,,arXiv,,,2020-02-09,2020,,,,,,All OA, Green,Preprint,"Liu, Quande; Dou, Qi; Yu, Lequan; Heng, Pheng Ann","Liu, Quande (); Dou, Qi (); Yu, Lequan (); Heng, Pheng Ann ()",,"Liu, Quande (); Dou, Qi (); Yu, Lequan (); Heng, Pheng Ann ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1124801595,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1007,pub.1148282795,10.48550/arxiv.2205.13723,,,DLTTA: Dynamic Learning Rate for Test-time Adaptation on Cross-domain  Medical Images,"Test-time adaptation (TTA) has increasingly been an important topic to
efficiently tackle the cross-domain distribution shift at test time for medical
images from different institutions. Previous TTA methods have a common
limitation of using a fixed learning rate for all the test samples. Such a
practice would be sub-optimal for TTA, because test data may arrive
sequentially therefore the scale of distribution shift would change frequently.
To address this problem, we propose a novel dynamic learning rate adjustment
method for test-time adaptation, called DLTTA, which dynamically modulates the
amount of weights update for each test image to account for the differences in
their distribution shift. Specifically, our DLTTA is equipped with a memory
bank based estimation scheme to effectively measure the discrepancy of a given
test sample. Based on this estimated discrepancy, a dynamic learning rate
adjustment strategy is then developed to achieve a suitable degree of
adaptation for each test sample. The effectiveness and general applicability of
our DLTTA is extensively demonstrated on three tasks including retinal optical
coherence tomography (OCT) segmentation, histopathological image
classification, and prostate 3D MRI segmentation. Our method achieves effective
and fast test-time adaptation with consistent performance improvement over
current state-of-the-art test-time adaptation methods. Code is available at:
https://github.com/med-air/DLTTA.",,,arXiv,,,2022-05-26,2022,,,,,,All OA, Green,Preprint,"Yang, Hongzheng; Chen, Cheng; Jiang, Meirui; Liu, Quande; Cao, Jianfeng; Heng, Pheng Ann; Dou, Qi","Yang, Hongzheng (); Chen, Cheng (); Jiang, Meirui (); Liu, Quande (); Cao, Jianfeng (); Heng, Pheng Ann (); Dou, Qi ()",,"Yang, Hongzheng (); Chen, Cheng (); Jiang, Meirui (); Liu, Quande (); Cao, Jianfeng (); Heng, Pheng Ann (); Dou, Qi ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148282795,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
1007,pub.1112736616,10.1109/access.2019.2903284,,,Convolutional Neural Networks for Prostate Magnetic Resonance Image Segmentation,"One of the most accurate and non-invasive prostate imaging methods is magnetic resonance imaging (MRI). Segmentation is needed to find the boundary of the prostate, either automatically or semi-automatically. Recently, fully convolutional neural networks (FCNN) are being used for this purpose. In this paper, to improve the FCNN performance for prostate MRI segmentation, we analyze various structures of shortcut connections together with the size of a deep network and suggest eight different FCNNs-based deep 2D network structures for automatic MRI prostate segmentation. Our evaluations on the PROMISE12 dataset with ten-fold cross-validation indicate improved and competitive results. We analyze the results in detail, considering MRI slices, MRI volumes, test folds, and also the impact on prostate segmentation of using an EndoRectal Coil to capture the prostate MRI. Our best 2D network outperforms the state-of-the-art 3D FCNN-based methods for prostate MRI segmentation on publicly available data, without any further post-processing.",,,IEEE Access,,,2019-04-01,2019,2019-04-01,,7,,36748-36760,All OA, Gold,Article,"Hassanzadeh, Tahereh; Hamey, Leonard G. C.; Ho-Shon, Kevin","Hassanzadeh, Tahereh (Department of Computing, Macquarie University, Sydney, NSW, Australia); Hamey, Leonard G. C. (Department of Computing, Macquarie University, Sydney, NSW, Australia); Ho-Shon, Kevin (Faculty of Medicine and Health Sciences, Macquarie University, Sydney, NSW, Australia)",,"Hassanzadeh, Tahereh (Macquarie University); Hamey, Leonard G. C. (Macquarie University); Ho-Shon, Kevin (Macquarie University)",24,15,,,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08666973.pdf,https://app.dimensions.ai/details/publication/pub.1112736616,46 Information and Computing Sciences,,,,,,,,,,,
964,pub.1145363879,10.48550/arxiv.2202.02371,,,Boundary-aware Information Maximization for Self-supervised Medical  Image Segmentation,"Unsupervised pre-training has been proven as an effective approach to boost
various downstream tasks given limited labeled data. Among various methods,
contrastive learning learns a discriminative representation by constructing
positive and negative pairs. However, it is not trivial to build reasonable
pairs for a segmentation task in an unsupervised way. In this work, we propose
a novel unsupervised pre-training framework that avoids the drawback of
contrastive learning. Our framework consists of two principles: unsupervised
over-segmentation as a pre-train task using mutual information maximization and
boundary-aware preserving learning. Experimental results on two benchmark
medical segmentation datasets reveal our method's effectiveness in improving
segmentation performance when few annotated images are available.",,,arXiv,,,2022-02-04,2022,,,,,,All OA, Green,Preprint,"Peng, Jizong; Wang, Ping; Pedersoli, Marco; Desrosiers, Christian","Peng, Jizong (); Wang, Ping (); Pedersoli, Marco (); Desrosiers, Christian ()",,"Peng, Jizong (); Wang, Ping (); Pedersoli, Marco (); Desrosiers, Christian ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1145363879,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
963,pub.1140038307,10.1016/j.neucom.2021.07.055,,,Dual Focal Loss to address class imbalance in semantic segmentation,"A common problem in pixelwise classification or semantic segmentation is class imbalance, which tends to reduce the classification accuracy of minority-class regions. An effective way to address this is to tune the loss function, particularly when Cross Entropy (CE), is used for classification. Although several CE variants have been reported in previous studies to address this problem, for example, Weighted Cross Entropy (WCE), Dual Cross Entropy (DCE), and Focal Loss (FL), each has their own limitations, such as introducing a vanishing gradient, penalizing negative classes inversely, or a sub-optimal loss weighting between classes. This limits their ability to improve classification accuracy or reduces their ease of use. Focal Loss has proven to be effective at balancing loss by increasing the loss on hard-to-classify classes. However, it tends to produce a vanishing gradient during backpropagation. To address these limitations, a Dual Focal Loss (DFL) function is proposed to improve the classification accuracy of the unbalanced classes in a dataset. The proposed loss function modifies the loss scaling method of FL to be effective against a vanishing gradient. In addition, inspired by DCE, a regularization term has also been added to DFL to constrain the negative class labels to further reduce the vanishing gradient effect and increase the loss on hard-to-classify classes. Experimental results show that DFL has better training performance, and provides greater accuracy compared to CE, WCE, FL and DCE in every test run conducted over a variety of different network models and datasets.","The TRUS dataset was provided by The Alfred Hospital, Melbourne, with relevant ethics approval of both Monash University and The Alfred Hospital. Computation was performed on the Massive™ HPC at Monash University. No funding was received for this research.",,Neurocomputing,,,2021-10,2021,,2021-10,462,,69-87,Closed,Article,"Hossain, Sazzad; Betts, John M.; Paplinski, Andrew P.","Hossain, Sazzad (Faculty of Information Technology, Monash University, Melbourne, Australia); Betts, John M. (Faculty of Information Technology, Monash University, Melbourne, Australia); Paplinski, Andrew P. (Faculty of Information Technology, Monash University, Melbourne, Australia)","Hossain, Sazzad (Monash University)","Hossain, Sazzad (Monash University); Betts, John M. (Monash University); Paplinski, Andrew P. (Monash University)",7,7,,5.73,,https://app.dimensions.ai/details/publication/pub.1140038307,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
958,pub.1150069389,10.48550/arxiv.2208.03217,,,Distance-based detection of out-of-distribution silent failures for  Covid-19 lung lesion segmentation,"Automatic segmentation of ground glass opacities and consolidations in chest
computer tomography (CT) scans can potentially ease the burden of radiologists
during times of high resource utilisation. However, deep learning models are
not trusted in the clinical routine due to failing silently on
out-of-distribution (OOD) data. We propose a lightweight OOD detection method
that leverages the Mahalanobis distance in the feature space and seamlessly
integrates into state-of-the-art segmentation pipelines. The simple approach
can even augment pre-trained models with clinically relevant uncertainty
quantification. We validate our method across four chest CT distribution shifts
and two magnetic resonance imaging applications, namely segmentation of the
hippocampus and the prostate. Our results show that the proposed method
effectively detects far- and near-OOD samples across all explored scenarios.",,,arXiv,,,2022-08-05,2022,,,,,,All OA, Green,Preprint,"Gonzalez, Camila; Gotkowski, Karol; Fuchs, Moritz; Bucher, Andreas; Dadras, Armin; Fischbach, Ricarda; Kaltenborn, Isabel; Mukhopadhyay, Anirban","Gonzalez, Camila (); Gotkowski, Karol (); Fuchs, Moritz (); Bucher, Andreas (); Dadras, Armin (); Fischbach, Ricarda (); Kaltenborn, Isabel (); Mukhopadhyay, Anirban ()",,"Gonzalez, Camila (); Gotkowski, Karol (); Fuchs, Moritz (); Bucher, Andreas (); Dadras, Armin (); Fischbach, Ricarda (); Kaltenborn, Isabel (); Mukhopadhyay, Anirban ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150069389,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 46 Information and Computing Sciences,,,,,,,,,
958,pub.1140244707,10.48550/arxiv.2108.02226,,,Terabyte-scale supervised 3D training and benchmarking dataset of the  mouse kidney,"The performance of machine learning algorithms used for the segmentation of
3D biomedical images lags behind that of the algorithms employed in the
classification of 2D photos. This may be explained by the comparative lack of
high-volume, high-quality training datasets, which require state-of-the art
imaging facilities, domain experts for annotation and large computational and
personal resources to create. The HR-Kidney dataset presented in this work
bridges this gap by providing 1.7 TB of artefact-corrected synchrotron
radiation-based X-ray phase-contrast microtomography images of whole mouse
kidneys and validated segmentations of 33 729 glomeruli, which represents a 1-2
orders of magnitude increase over currently available biomedical datasets. The
dataset further contains the underlying raw data, classical segmentations of
renal vasculature and uriniferous tubules, as well as true 3D manual
annotations. By removing limits currently imposed by small training datasets,
the provided data open up the possibility for disruptions in machine learning
for biomedical image analysis.",,,arXiv,,,2021-08-04,2021,,,,,,All OA, Green,Preprint,"Kuo, Willy; Rossinelli, Diego; Schulz, Georg; Wenger, Roland H.; Hieber, Simone; Müller, Bert; Kurtcuoglu, Vartan","Kuo, Willy (); Rossinelli, Diego (); Schulz, Georg (); Wenger, Roland H. (); Hieber, Simone (); Müller, Bert (); Kurtcuoglu, Vartan ()",,"Kuo, Willy (); Rossinelli, Diego (); Schulz, Georg (); Wenger, Roland H. (); Hieber, Simone (); Müller, Bert (); Kurtcuoglu, Vartan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140244707,31 Biological Sciences, 3102 Bioinformatics and Computational Biology, 51 Physical Sciences,,,,,,,,,
958,pub.1126576510,10.3390/app10072601,,,Optimisation of 2D U-Net Model Components for Automatic Prostate Segmentation on MRI,"In this paper, we develop an optimised state-of-the-art 2D U-Net model by studying the effects of the individual deep learning model components in performing prostate segmentation. We found that for upsampling, the combination of interpolation and convolution is better than the use of transposed convolution. For combining feature maps in each convolution block, it is only beneficial if a skip connection with concatenation is used. With respect to pooling, average pooling is better than strided-convolution, max, RMS or L2 pooling. Introducing a batch normalisation layer before the activation layer gives further performance improvement. The optimisation is based on a private dataset as it has a fixed 2D resolution and voxel size for every image which mitigates the need of a resizing operation in the data preparation process. Non-enhancing data preprocessing was applied and five-fold cross-validation was used to evaluate the fully automatic segmentation approach. We show it outperforms the traditional methods that were previously applied on the private dataset, as well as outperforming other comparable state-of-the-art 2D models on the public dataset PROMISE12.",I. Astono has been awarded a 2016 University of Newcastle Scholarship provided by UNIPRS and UNRSC 50:50.,This research received no external funding.,Applied Sciences,,,2020-04-09,2020,2020-04-09,,10,7,2601,All OA, Gold,Article,"Astono, Indriani P.; Welsh, James S.; Chalup, Stephan; Greer, Peter","Astono, Indriani P. (School of Electrical Engineering and Computing, The University of Newcastle, Callaghan, NSW 2308, Australia;, james.welsh@newcastle.edu.au, (J.S.W.);, stephan.chalup@newcastle.edu.au, (S.C.)); Welsh, James S. (School of Electrical Engineering and Computing, The University of Newcastle, Callaghan, NSW 2308, Australia;, james.welsh@newcastle.edu.au, (J.S.W.);, stephan.chalup@newcastle.edu.au, (S.C.)); Chalup, Stephan (School of Electrical Engineering and Computing, The University of Newcastle, Callaghan, NSW 2308, Australia;, james.welsh@newcastle.edu.au, (J.S.W.);, stephan.chalup@newcastle.edu.au, (S.C.)); Greer, Peter (School of Mathematical and Physical Sciences, The University of Newcastle, Callaghan, NSW 2308, Australia;, peter.greer@newcastle.edu.au)","Astono, Indriani P. (University of Newcastle Australia; )","Astono, Indriani P. (University of Newcastle Australia); Welsh, James S. (University of Newcastle Australia); Chalup, Stephan (University of Newcastle Australia); Greer, Peter (University of Newcastle Australia)",8,7,,2.98,https://www.mdpi.com/2076-3417/10/7/2601/pdf?version=1586834569,https://app.dimensions.ai/details/publication/pub.1126576510,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 46 Information and Computing Sciences,,,,,,,,,
957,pub.1132271368,10.48550/arxiv.2011.00325,,,Self-paced and self-consistent co-training for semi-supervised image  segmentation,"Deep co-training has recently been proposed as an effective approach for
image segmentation when annotated data is scarce. In this paper, we improve
existing approaches for semi-supervised segmentation with a self-paced and
self-consistent co-training method. To help distillate information from
unlabeled images, we first design a self-paced learning strategy for
co-training that lets jointly-trained neural networks focus on
easier-to-segment regions first, and then gradually consider harder ones.This
is achieved via an end-to-end differentiable loss inthe form of a generalized
Jensen Shannon Divergence(JSD). Moreover, to encourage predictions from
different networks to be both consistent and confident, we enhance this
generalized JSD loss with an uncertainty regularizer based on entropy. The
robustness of individual models is further improved using a self-ensembling
loss that enforces their prediction to be consistent across different training
iterations. We demonstrate the potential of our method on three challenging
image segmentation problems with different image modalities, using small
fraction of labeled data. Results show clear advantages in terms of performance
compared to the standard co-training baselines and recently proposed
state-of-the-art approaches for semi-supervised segmentation",,,arXiv,,,2020-10-31,2020,,,,,,All OA, Green,Preprint,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (); Zhang, Caiming (); Desrosiers, Christian ()",,"Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (); Zhang, Caiming (); Desrosiers, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1132271368,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
914,pub.1132825145,10.1109/access.2020.3039496,,,An Evolutionary DenseRes Deep Convolutional Neural Network for Medical Image Segmentation,"The performance of a Convolutional Neural Network (CNN) highly depends on its architecture and corresponding parameters. Manually designing a CNN is a time–consuming process in regards to the various layers that it can have, and the variety of parameters that must be set up. Increasing the complexity of the network structure by employing various types of connections makes designing a network even more challenging. Evolutionary computation as an optimisation technique can be applied to arrange the CNN layers and/or initiate its parameters automatically or semi–automatically. Dense network and Residual network are two popular network structures that were introduced to facilitate the training of deep networks. In this paper, leveraging the potentials of Dense and Residual blocks, and using the capability of evolutionary computation, we propose an automatic evolutionary model to detect an optimum and accurate network structure and its parameters for medical image segmentation. The proposed evolutionary DenseRes model is employed for segmentation of six publicly available MRI and CT medical datasets. The proposed model obtained high accuracy while employing networks with minimal parameters for the segmentation of medical images and outperformed manual and automatic designed networks, including U–Net, Residual U–Net, Dense U–Net, Non–Bypass Dense, NAS U–Net, AdaresU–Net, and EvoU–Net.","This work was supported by the Australian Government through the National Computational Infrastructure (NCI), which is undertaken with the assistance of resources and services.",,IEEE Access,,,2020-01-01,2020,2020-11-20,2020-01-01,8,,212298-212314,All OA, Gold,Article,"Hassanzadeh, Tahereh; Essam, Daryl; Sarker, Ruhul","Hassanzadeh, Tahereh (School of Engineering and Information Technology (SEIT), University of New South Wales, Canberra, ACT, 2612, Australia); Essam, Daryl (School of Engineering and Information Technology (SEIT), University of New South Wales, Canberra, ACT, 2612, Australia); Sarker, Ruhul (School of Engineering and Information Technology (SEIT), University of New South Wales, Canberra, ACT, 2612, Australia)","Hassanzadeh, Tahereh (UNSW Sydney)","Hassanzadeh, Tahereh (UNSW Sydney); Essam, Daryl (UNSW Sydney); Sarker, Ruhul (UNSW Sydney)",13,13,,6.24,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09265246.pdf,https://app.dimensions.ai/details/publication/pub.1132825145,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,
909,pub.1131181984,10.3390/app10196678,,,CDA-Net for Automatic Prostate Segmentation in MR Images,"Automatic and accurate prostate segmentation is an essential prerequisite for assisting diagnosis and treatment, such as guiding biopsy procedures and radiation therapy. Therefore, this paper proposes a cascaded dual attention network (CDA-Net) for automatic prostate segmentation in MRI scans. The network includes two stages of RAS-FasterRCNN and RAU-Net. Firstly, RAS-FasterRCNN uses improved FasterRCNN and sequence correlation processing to extract regions of interest (ROI) of organs. This ROI extraction serves as a hard attention mechanism to focus the segmentation of the subsequent network on a certain area. Secondly, the addition of residual convolution block and self-attention mechanism in RAU-Net enables the network to gradually focus on the area where the organ exists while making full use of multiscale features. The algorithm was evaluated on the PROMISE12 and ASPS13 datasets and presents the dice similarity coefficient of 92.88% and 92.65%, respectively, surpassing the state-of-the-art algorithms. In a variety of complex slice images, especially for the base and apex of slice sequences, the algorithm also achieved credible segmentation performance.","We appreciate the organizers of the 2012 prostate MR image segmentation challenge for sharing the data set and opening up various method rankings and test results. At the same time, we are grateful to the organizers of the 2013 Prostate Structure Segmentation Challenge for their efforts in collecting and sharing data sets.",This research was supported by the National Natural Science Foundation of China under Grant 51677123.,Applied Sciences,,,2020-09-24,2020,2020-09-24,,10,19,6678,All OA, Gold,Article,"Lu, Zhiying; Zhao, Mingyue; Pang, Yong","Lu, Zhiying (School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China); Zhao, Mingyue (School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China); Pang, Yong (Institute of Microelectronics of the Chinese Academy of Sciences, Beijing 100029, China;, pangyong@ime.ac.cn)","Lu, Zhiying (Tianjin University); Zhao, Mingyue (Tianjin University)","Lu, Zhiying (Tianjin University); Zhao, Mingyue (Tianjin University); Pang, Yong (Institute of Microelectronics)",7,7,,3.64,https://www.mdpi.com/2076-3417/10/19/6678/pdf?version=1601172721,https://app.dimensions.ai/details/publication/pub.1131181984,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
909,pub.1125953533,10.1007/978-3-030-40977-7_11,,,Active Contour Model in Deep Learning Era: A Revise and Review,"Active Contour (AC)-based segmentation has been widely used to solve many image processing problems, specially image segmentation. While these AC-based methods offer object shape constraints, they typically look for strong edges or statistical modeling for successful segmentation. Clearly, AC-based approaches lack a way to work with labeled images in a supervised machine learning framework. Furthermore, they are unsupervised approaches and strongly depend on many parameters which are chosen by empirical results. Recently, Deep Learning (DL) has become the go-to method for solving many problems in various areas. Over the past decade, DL has achieved remarkable success in various artificial intelligence research areas. DL is supervised methods and requires large volume ground-truth. This paper first provides a fundamental of both Active Contour techniques and Deep Learning framework. We then present the state-of-the-art approaches of Active Contour techniques incorporating in Deep Learning framework.",,,Studies in Computational Intelligence,Applications of Hybrid Metaheuristic Algorithms for Image Processing,,2020-03-28,2020,2020-03-28,2020,890,,231-260,Closed,Chapter,"Hoang Ngan Le, T.; Luu, Khoa; Duong, Chi Nhan; Quach, Kha Gia; Truong, Thanh Dat; Sadler, Kyle; Savvides, Marios","Hoang Ngan Le, T. (Department of Computer Science and Computer Engineering, University of Arkansas, 72701, Fayetteville, AR, USA); Luu, Khoa (Department of Computer Science and Computer Engineering, University of Arkansas, 72701, Fayetteville, AR, USA); Duong, Chi Nhan (Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada); Quach, Kha Gia (Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada); Truong, Thanh Dat (Department of Computer Science, University of Science, Ho Chi Minh, Vietnam); Sadler, Kyle (Department of Computer Science and Computer Engineering, University of Arkansas, 72701, Fayetteville, AR, USA); Savvides, Marios (Department of Electrical and Computer Engineering, Carnegie Mellon University, 5000 Forbes Ave, 15213, Pittsburgh, PA, USA)","Hoang Ngan Le, T. (University of Arkansas at Fayetteville)","Hoang Ngan Le, T. (University of Arkansas at Fayetteville); Luu, Khoa (University of Arkansas at Fayetteville); Duong, Chi Nhan (Concordia University); Quach, Kha Gia (Concordia University); Truong, Thanh Dat (Ho Chi Minh City University of Science); Sadler, Kyle (University of Arkansas at Fayetteville); Savvides, Marios (Carnegie Mellon University)",8,7,,4.12,,https://app.dimensions.ai/details/publication/pub.1125953533,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
909,pub.1123051838,10.48550/arxiv.1911.13273,,,Confidence Calibration and Predictive Uncertainty Estimation for Deep  Medical Image Segmentation,"Fully convolutional neural networks (FCNs), and in particular U-Nets, have
achieved state-of-the-art results in semantic segmentation for numerous medical
imaging applications. Moreover, batch normalization and Dice loss have been
used successfully to stabilize and accelerate training. However, these networks
are poorly calibrated i.e. they tend to produce overconfident predictions both
in correct and erroneous classifications, making them unreliable and hard to
interpret. In this paper, we study predictive uncertainty estimation in FCNs
for medical image segmentation. We make the following contributions: 1) We
systematically compare cross entropy loss with Dice loss in terms of
segmentation quality and uncertainty estimation of FCNs; 2) We propose model
ensembling for confidence calibration of the FCNs trained with batch
normalization and Dice loss; 3) We assess the ability of calibrated FCNs to
predict segmentation quality of structures and detect out-of-distribution test
examples. We conduct extensive experiments across three medical image
segmentation applications of the brain, the heart, and the prostate to evaluate
our contributions. The results of this study offer considerable insight into
the predictive uncertainty estimation and out-of-distribution detection in
medical image segmentation and provide practical recipes for confidence
calibration. Moreover, we consistently demonstrate that model ensembling
improves confidence calibration.",,,arXiv,,,2019-11-29,2019,,,,,,All OA, Green,Preprint,"Mehrtash, Alireza; Wells, William M.; Tempany, Clare M.; Abolmaesumi, Purang; Kapur, Tina","Mehrtash, Alireza (); Wells, William M. (); Tempany, Clare M. (); Abolmaesumi, Purang (); Kapur, Tina ()",,"Mehrtash, Alireza (); Wells, William M. (); Tempany, Clare M. (); Abolmaesumi, Purang (); Kapur, Tina ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1123051838,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
907,pub.1136235647,10.48550/arxiv.2103.04813,,,Boosting Semi-supervised Image Segmentation with Global and Local Mutual  Information Regularization,"The scarcity of labeled data often impedes the application of deep learning
to the segmentation of medical images. Semi-supervised learning seeks to
overcome this limitation by exploiting unlabeled examples in the learning
process. In this paper, we present a novel semi-supervised segmentation method
that leverages mutual information (MI) on categorical distributions to achieve
both global representation invariance and local smoothness. In this method, we
maximize the MI for intermediate feature embeddings that are taken from both
the encoder and decoder of a segmentation network. We first propose a global MI
loss constraining the encoder to learn an image representation that is
invariant to geometric transformations. Instead of resorting to
computationally-expensive techniques for estimating the MI on continuous
feature embeddings, we use projection heads to map them to a discrete cluster
assignment where MI can be computed efficiently. Our method also includes a
local MI loss to promote spatial consistency in the feature maps of the decoder
and provide a smoother segmentation. Since mutual information does not require
a strict ordering of clusters in two different assignments, we incorporate a
final consistency regularization loss on the output which helps align the
cluster labels throughout the network. We evaluate the method on four
challenging publicly-available datasets for medical image segmentation.
Experimental results show our method to outperform recently-proposed approaches
for semi-supervised segmentation and provide an accuracy near to full
supervision while training with very few annotated images.",,,arXiv,,,2021-03-08,2021,,,,,,All OA, Green,Preprint,"Peng, Jizong; Pedersoli, Marco; Desrosiers, Christian","Peng, Jizong (); Pedersoli, Marco (); Desrosiers, Christian ()",,"Peng, Jizong (); Pedersoli, Marco (); Desrosiers, Christian ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136235647,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4611 Machine Learning,,,,,,,,,
906,pub.1119386227,10.48550/arxiv.1904.10030,,,Reducing the Hausdorff Distance in Medical Image Segmentation with  Convolutional Neural Networks,"The Hausdorff Distance (HD) is widely used in evaluating medical image
segmentation methods. However, existing segmentation methods do not attempt to
reduce HD directly. In this paper, we present novel loss functions for training
convolutional neural network (CNN)-based segmentation methods with the goal of
reducing HD directly. We propose three methods to estimate HD from the
segmentation probability map produced by a CNN. One method makes use of the
distance transform of the segmentation boundary. Another method is based on
applying morphological erosion on the difference between the true and estimated
segmentation maps. The third method works by applying circular/spherical
convolution kernels of different radii on the segmentation probability maps.
Based on these three methods for estimating HD, we suggest three loss functions
that can be used for training to reduce HD. We use these loss functions to
train CNNs for segmentation of the prostate, liver, and pancreas in ultrasound,
magnetic resonance, and computed tomography images and compare the results with
commonly-used loss functions. Our results show that the proposed loss functions
can lead to approximately 18-45 % reduction in HD without degrading other
segmentation performance criteria such as the Dice similarity coefficient. The
proposed loss functions can be used for training medical image segmentation
methods in order to reduce the large segmentation errors.",,,arXiv,,,2019-04-22,2019,,,,,,All OA, Green,Preprint,"Karimi, Davood; Salcudean, Septimiu E.","Karimi, Davood (); Salcudean, Septimiu E. ()",,"Karimi, Davood (); Salcudean, Septimiu E. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1119386227,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
871,pub.1126507887,10.48550/arxiv.2004.04668,,,Test-Time Adaptable Neural Networks for Robust Medical Image  Segmentation,"Convolutional Neural Networks (CNNs) work very well for supervised learning
problems when the training dataset is representative of the variations expected
to be encountered at test time. In medical image segmentation, this premise is
violated when there is a mismatch between training and test images in terms of
their acquisition details, such as the scanner model or the protocol.
Remarkable performance degradation of CNNs in this scenario is well documented
in the literature. To address this problem, we design the segmentation CNN as a
concatenation of two sub-networks: a relatively shallow image normalization
CNN, followed by a deep CNN that segments the normalized image. We train both
these sub-networks using a training dataset, consisting of annotated images
from a particular scanner and protocol setting. Now, at test time, we adapt the
image normalization sub-network for \emph{each test image}, guided by an
implicit prior on the predicted segmentation labels. We employ an independently
trained denoising autoencoder (DAE) in order to model such an implicit prior on
plausible anatomical segmentation labels. We validate the proposed idea on
multi-center Magnetic Resonance imaging datasets of three anatomies: brain,
heart and prostate. The proposed test-time adaptation consistently provides
performance improvement, demonstrating the promise and generality of the
approach. Being agnostic to the architecture of the deep CNN, the second
sub-network, the proposed design can be utilized with any segmentation network
to increase robustness to variations in imaging scanners and protocols. Our
code is available at:
\url{https://github.com/neerakara/test-time-adaptable-neural-networks-for-domain-generalization}.",,,arXiv,,,2020-04-09,2020,,,,,,All OA, Green,Preprint,"Karani, Neerav; Erdil, Ertunc; Chaitanya, Krishna; Konukoglu, Ender","Karani, Neerav (); Erdil, Ertunc (); Chaitanya, Krishna (); Konukoglu, Ender ()",,"Karani, Neerav (); Erdil, Ertunc (); Chaitanya, Krishna (); Konukoglu, Ender ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1126507887,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
866,pub.1145516015,10.48550/arxiv.2202.05271,,,A Field of Experts Prior for Adapting Neural Networks at Test Time,"Performance of convolutional neural networks (CNNs) in image analysis tasks
is often marred in the presence of acquisition-related distribution shifts
between training and test images. Recently, it has been proposed to tackle this
problem by fine-tuning trained CNNs for each test image. Such
test-time-adaptation (TTA) is a promising and practical strategy for improving
robustness to distribution shifts as it requires neither data sharing between
institutions nor annotating additional data. Previous TTA methods use a helper
model to increase similarity between outputs and/or features extracted from a
test image with those of the training images. Such helpers, which are typically
modeled using CNNs, can be task-specific and themselves vulnerable to
distribution shifts in their inputs. To overcome these problems, we propose to
carry out TTA by matching the feature distributions of test and training
images, as modelled by a field-of-experts (FoE) prior. FoEs model complicated
probability distributions as products of many simpler expert distributions. We
use 1D marginal distributions of a trained task CNN's features as experts in
the FoE model. Further, we compute principal components of patches of the task
CNN's features, and consider the distributions of PCA loadings as additional
experts. We validate the method on 5 MRI segmentation tasks (healthy tissues in
4 anatomical regions and lesions in 1 one anatomy), using data from 17 clinics,
and on a MRI registration task, using data from 3 clinics. We find that the
proposed FoE-based TTA is generically applicable in multiple tasks, and
outperforms all previous TTA methods for lesion segmentation. For healthy
tissue segmentation, the proposed method outperforms other task-agnostic
methods, but a previous TTA method which is specifically designed for
segmentation performs the best for most of the tested datasets. Our code is
publicly available.",,,arXiv,,,2022-02-10,2022,,,,,,All OA, Green,Preprint,"Karani, Neerav; Brunner, Georg; Erdil, Ertunc; Fei, Simin; Tezcan, Kerem; Chaitanya, Krishna; Konukoglu, Ender","Karani, Neerav (); Brunner, Georg (); Erdil, Ertunc (); Fei, Simin (); Tezcan, Kerem (); Chaitanya, Krishna (); Konukoglu, Ender ()",,"Karani, Neerav (); Brunner, Georg (); Erdil, Ertunc (); Fei, Simin (); Tezcan, Kerem (); Chaitanya, Krishna (); Konukoglu, Ender ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1145516015,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
831,pub.1118740542,10.48550/arxiv.1604.07287,,,Diffusion MRI microstructure models with in vivo human brain Connectom  data: results from a multi-group comparison,"A large number of mathematical models have been proposed to describe the
measured signal in diffusion-weighted (DW) magnetic resonance imaging (MRI) and
infer properties about the white matter microstructure. However, a head-to-head
comparison of DW-MRI models is critically missing in the field. To address this
deficiency, we organized the ""White Matter Modeling Challenge"" during the
International Symposium on Biomedical Imaging (ISBI) 2015 conference. This
competition aimed at identifying the DW-MRI models that best predict unseen DW
data. in vivo DW-MRI data was acquired on the Connectom scanner at the
A.A.Martinos Center (Massachusetts General Hospital) using gradients strength
of up to 300 mT/m and a broad set of diffusion times. We focused on assessing
the DW signal prediction in two regions: the genu in the corpus callosum, where
the fibres are relatively straight and parallel, and the fornix, where the
configuration of fibres is more complex. The challenge participants had access
to three-quarters of the whole dataset, and their models were ranked on their
ability to predict the remaining unseen quarter of data. In this paper we
provide both an overview and a more in-depth description of each evaluated
model, report the challenge results, and infer trends about the model
characteristics that were associated with high model ranking. This work
provides a much needed benchmark for DW-MRI models. The acquired data and model
details for signal prediction evaluation are provided online to encourage a
larger scale assessment of diffusion models in the future.",,,arXiv,,,2016-04-25,2016,,,,,,All OA, Green,Preprint,"Ferizi, Uran; Scherrer, Benoit; Schneider, Torben; Alipoor, Mohammad; Eufracio, Odin; Fick, Rutger H. J.; Deriche, Rachid; Nilsson, Markus; Loya-Olivas, Ana K.; Rivera, Mariano; Poot, Dirk H. J.; Ramirez-Manzanares, Alonso; Marroquin, Jose L.; Rokem, Ariel; Pötter, Christian; Dougherty, Robert F.; Sakaie, Ken; Wheeler-Kingshott, Claudia; Warfield, Simon K.; Witzel, Thomas; Wald, Lawrence L.; Raya, José G.; Alexander, Daniel C.","Ferizi, Uran (); Scherrer, Benoit (); Schneider, Torben (); Alipoor, Mohammad (); Eufracio, Odin (); Fick, Rutger H. J. (); Deriche, Rachid (); Nilsson, Markus (); Loya-Olivas, Ana K. (); Rivera, Mariano (); Poot, Dirk H. J. (); Ramirez-Manzanares, Alonso (); Marroquin, Jose L. (); Rokem, Ariel (); Pötter, Christian (); Dougherty, Robert F. (); Sakaie, Ken (); Wheeler-Kingshott, Claudia (); Warfield, Simon K. (); Witzel, Thomas (); Wald, Lawrence L. (); Raya, José G. (); Alexander, Daniel C. ()",,"Ferizi, Uran (); Scherrer, Benoit (); Schneider, Torben (); Alipoor, Mohammad (); Eufracio, Odin (); Fick, Rutger H. J. (); Deriche, Rachid (); Nilsson, Markus (); Loya-Olivas, Ana K. (); Rivera, Mariano (); Poot, Dirk H. J. (); Ramirez-Manzanares, Alonso (); Marroquin, Jose L. (); Rokem, Ariel (); Pötter, Christian (); Dougherty, Robert F. (); Sakaie, Ken (); Wheeler-Kingshott, Claudia (); Warfield, Simon K. (); Witzel, Thomas (); Wald, Lawrence L. (); Raya, José G. (); Alexander, Daniel C. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1118740542,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences,,,,,,,,,
830,pub.1139533626,10.1155/2021/9955174,,,Intelligent Computer-Aided Prostate Cancer Diagnosis Systems: State-of-the-Art and Future Directions,"Prostate Cancer (PCa) is one of the common cancers among men in the world. About 16.67% of men will be affected by PCa in their life. Due to the integration of magnetic resonance imaging in the current clinical procedure for detecting prostate cancer and the apparent success of imaging techniques in the estimation of PCa volume in the gland, we provide a more detailed review of methodologies that use specific parameters for prostate tissue representation. After collecting over 200 researches on image-based systems for diagnosing prostate cancer, in this paper, we provide a detailed review of existing computer-aided diagnosis (CAD) methods and approaches to identify prostate cancer from images generated using Near-Infrared (NIR), Mid-Infrared (MIR), and Magnetic Resonance Imaging (MRI) techniques. Furthermore, we introduce two research methodologies to build intelligent CAD systems. The first methodology applies a fuzzy integral method to maintain the diversity and capacity of different classifiers aggregation to detect PCa tumor from NIR and MIR images. The second methodology investigates a typical workflow for developing an automated prostate cancer diagnosis using MRI images. Essentially, CAD development remains a helpful tool of radiology for diagnosing prostate cancer disease. Nonetheless, a complete implementation of effective and intelligent methods is still required for the PCa-diagnostic system. While some CAD applications work well, some limitations need to be solved for automated clinical PCa diagnostic. It is anticipated that more advances should be made in computational image analysis and computer-assisted approaches to satisfy clinical needs shortly in the coming years.","This project was funded by the National Plan for Science, Technology and Innovation (MAARIFAH), King Abdulaziz City for Science and Technology, Kingdom of Saudi Arabia, Award number 10-Bio-1905.",,Mathematical Problems in Engineering,,,2021-07-07,2021,,2021-07-07,2021,,1-17,All OA, Gold,Article,"Sammouda, Rachid; Gumaei, Abdu; El-Zaart, Ali","Sammouda, Rachid (Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia, ksu.edu.sa); Gumaei, Abdu (Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia, ksu.edu.sa); El-Zaart, Ali (Department of Mathematics and Computer Science, Faculty of Sciences, Beirut Arab University, Beirut, Lebanon, bau.edu.lb)","Sammouda, Rachid (King Saud University); Gumaei, Abdu (King Saud University)","Sammouda, Rachid (King Saud University); Gumaei, Abdu (King Saud University); El-Zaart, Ali (Beirut Arab University)",2,2,,,https://downloads.hindawi.com/journals/mpe/2021/9955174.pdf,https://app.dimensions.ai/details/publication/pub.1139533626,40 Engineering,,,,,,,,,,,
827,pub.1147931821,10.48550/arxiv.2205.07516,,,The use of deep learning in interventional radiotherapy (brachytherapy):  a review with a focus on open source and open data,"Deep learning advanced to one of the most important technologies in almost
all medical fields. Especially in areas, related to medical imaging it plays a
big role. However, in interventional radiotherapy (brachytherapy) deep learning
is still in an early phase. In this review, first, we investigated and
scrutinised the role of deep learning in all processes of interventional
radiotherapy and directly related fields. Additionally we summarised the most
recent developments. To reproduce results of deep learning algorithms both
source code and training data must be available. Therefore, a second focus of
this work was on the analysis of the availability of open source, open data and
open models. In our analysis, we were able to show that deep learning plays
already a major role in some areas of interventional radiotherapy, but is still
hardly presented in others. Nevertheless, its impact is increasing with the
years, partly self-propelled but also influenced by closely related fields.
Open source, data and models are growing in number but are still scarce and
unevenly distributed among different research groups. The reluctance in
publishing code, data and models limits reproducibility and restricts
evaluation to mono-institutional datasets. Summarised, deep learning will
change positively the workflow of interventional radiotherapy but there is room
for improvement when it comes to reproducible results and standardised
evaluation methods.",,,arXiv,,,2022-05-16,2022,,,,,,All OA, Green,Preprint,"Fechter, Tobias; Sachpazidis, Ilias; Baltas, Dimos","Fechter, Tobias (); Sachpazidis, Ilias (); Baltas, Dimos ()",,"Fechter, Tobias (); Sachpazidis, Ilias (); Baltas, Dimos ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1147931821,32 Biomedical and Clinical Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
823,pub.1140549294,10.48550/arxiv.2108.08467,,,Medical Image Segmentation with 3D Convolutional Neural Networks: A  Survey,"Computer-aided medical image analysis plays a significant role in assisting
medical practitioners for expert clinical diagnosis and deciding the optimal
treatment plan. At present, convolutional neural networks (CNN) are the
preferred choice for medical image analysis. In addition, with the rapid
advancements in three-dimensional (3D) imaging systems and the availability of
excellent hardware and software support to process large volumes of data, 3D
deep learning methods are gaining popularity in medical image analysis. Here,
we present an extensive review of the recently evolved 3D deep learning methods
in medical image segmentation. Furthermore, the research gaps and future
directions in 3D medical image segmentation are discussed.",,,arXiv,,,2021-08-18,2021,,,,,,All OA, Green,Preprint,"Niyas, S; Pawan, S J; Kumar, M Anand; Rajan, Jeny","Niyas, S (); Pawan, S J (); Kumar, M Anand (); Rajan, Jeny ()",,"Niyas, S (); Pawan, S J (); Kumar, M Anand (); Rajan, Jeny ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140549294,46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
795,pub.1113706436,10.1016/j.neucom.2019.01.110,,,AdaResU-Net: Multiobjective adaptive convolutional neural network for medical image segmentation,"Adapting an existing convolutional neural network architecture to a specific dataset for medical image segmentation remains a challenging task that requires extensive expertise and time to fine-tune the hyperparameters. Hyperparameter optimization approaches that automate the search have been proposed but have mainly focused on optimizing the segmentation performance. However, optimizing the network size is also important to prevent unnecessary and costly computational operations. In this paper, we present a multiobjective adaptive convolutional neural network (AdaResU-Net) for medical image segmentation that is able to automatically adapt to new datasets while minimizing the size of the network. The proposed AdaResU-Net is comprised of a fixed architecture that combines the structure of the state-of-the-art U-Net with a residual learning framework to improve information propagation and promote an efficient training. Then, a multiobjective evolutionary algorithm (MEA) that optimizes both segmentation accuracy and model size is proposed to evolve the AdaResU-Net networks with different hyperparameters. The presented model is tested on two publically available medical image datasets and compared with the U-Net. Results show that the AdaResU-Net achieves better segmentation performance with less than 30% the number of trainable parameters. Additionally, the MEA algorithm generated configurations that are smaller and perform better or equally well than configurations generated with a Bayesian hyperparameter optimization approach.",The authors would like to thank the Fulbright-Senecyt program for the support provided to Maria Baldeon-Calisto to pursue her Ph.D. degree.,"This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.",Neurocomputing,,,2020-06,2020,,2020-06,392,,325-340,Closed,Article,"Baldeon-Calisto, Maria; Lai-Yuen, Susana K.","Baldeon-Calisto, Maria (University of South Florida, 4202 E. Fowler Avenue, Tampa, FL 33620, USA); Lai-Yuen, Susana K. (University of South Florida, 4202 E. Fowler Avenue, Tampa, FL 33620, USA)","Lai-Yuen, Susana K. (University of South Florida)","Baldeon-Calisto, Maria (University of South Florida); Lai-Yuen, Susana K. (University of South Florida)",66,54,,31.69,,https://app.dimensions.ai/details/publication/pub.1113706436,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,
795,pub.1150264316,10.48550/arxiv.2208.06643,,,Medical image analysis based on transformer: A Review,"The transformer has dominated the natural language processing (NLP) field for
a long time. Recently, the transformer-based method has been adopted into the
computer vision (CV) field and shows promising results. As an important branch
of the CV field, medical image analysis joins the wave of the transformer-based
method rightfully. In this review, we illustrate the principle of the attention
mechanism, and the detailed structures of the transformer, and depict how the
transformer is adopted into medical image analysis. We organize the
transformer-based medical image analysis applications in a sequence of
different tasks, including classification, segmentation, synthesis,
registration, localization, detection, captioning, and denoising. For the
mainstream classification and segmentation tasks, we further divided the
corresponding works based on different medical imaging modalities. The datasets
corresponding to the related works are also organized. We include thirteen
modalities and more than twenty objects in our work.",,,arXiv,,,2022-08-13,2022,,,,,,All OA, Green,Preprint,"Liu, Zhaoshan; Lv, Qiujie; Lee, Chau Hung; Shen, Lei","Liu, Zhaoshan (); Lv, Qiujie (); Lee, Chau Hung (); Shen, Lei ()",,"Liu, Zhaoshan (); Lv, Qiujie (); Lee, Chau Hung (); Shen, Lei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150264316,46 Information and Computing Sciences, 4605 Data Management and Data Science,,,,,,,,,,
795,pub.1130750487,10.1016/b978-0-12-821259-2.00016-8,,,Chapter 16 Prospect and adversity of artificial intelligence in urology,"The emergence of artificial intelligence (AI) has opened a new avenue for tackling existing challenges in clinical routine. This chapter will briefly introduce potential applications of AI in urology and focus on its benefits and barriers in solving real clinical problems. First, the introduction section will generally discuss AI and existing data resources. Then, the chapter will explain the potential application of AI in urological endoscopy, urine, stone and andrology, imaging and the robotic surgery. Further, this chapter will briefly discuss some tools of risk predictions for urological cancer. Finally, the author will discuss the potential future direction of AI in urology.",,,,Artificial Intelligence in Medicine,,2021,2021,,2021,,,309-337,Closed,Chapter,"Eminaga, Okyaz; Liao, Joseph C.","Eminaga, Okyaz (); Liao, Joseph C. ()",,"Eminaga, Okyaz (); Liao, Joseph C. ()",1,1,,0.88,,https://app.dimensions.ai/details/publication/pub.1130750487,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,,
794,pub.1104156970,10.1016/j.patcog.2018.05.014,,,Deep learning for image-based cancer detection and diagnosis − A survey,"In this paper, we aim to provide a survey on the applications of deep learning for cancer detection and diagnosis and hope to provide an overview of the progress in this field. In the survey, we firstly provide an overview on deep learning and the popular architectures used for cancer detection and diagnosis. Especially we present four popular deep learning architectures, including convolutional neural networks, fully convolutional networks, auto-encoders, and deep belief networks in the survey. Secondly, we provide a survey on the studies exploiting deep learning for cancer detection and diagnosis. The surveys in this part are organized based on the types of cancers. Thirdly, we provide a and comments on the recent work on the applications of deep learning to cancer detection and diagnosis and propose some future research directions.",,,Pattern Recognition,,,2018-11,2018,,2018-11,83,,134-149,Closed,Article,"Hu, Zilong; Tang, Jinshan; Wang, Ziming; Zhang, Kai; Zhang, Ling; Sun, Qingling","Hu, Zilong (School of Technology, Michigan Technological University, Houghton, MI 49931, United States); Tang, Jinshan (School of Technology, Michigan Technological University, Houghton, MI 49931, United States; Department of Electrical Computer Engineering, Michigan Technological University, Houghton, MI 49931, United States; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan 430065, China); Wang, Ziming (Department of Electrical Computer Engineering, Michigan Technological University, Houghton, MI 49931, United States); Zhang, Kai (School of Technology, Michigan Technological University, Houghton, MI 49931, United States; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan 430065, China); Zhang, Ling (School of Technology, Michigan Technological University, Houghton, MI 49931, United States); Sun, Qingling (Sun Technologies Services, LLC, Clinton, MS, 39056 United States)","Tang, Jinshan (Michigan Technological University; Michigan Technological University; Wuhan University of Science and Technology)","Hu, Zilong (Michigan Technological University); Tang, Jinshan (Michigan Technological University; Michigan Technological University; Wuhan University of Science and Technology); Wang, Ziming (Michigan Technological University); Zhang, Kai (Michigan Technological University; Wuhan University of Science and Technology); Zhang, Ling (Michigan Technological University); Sun, Qingling ()",289,177,,91.21,,https://app.dimensions.ai/details/publication/pub.1104156970,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
790,pub.1144304733,10.32604/cmes.2022.018418,,,"Deep Learning-Based Cancer Detection-Recent Developments, Trend and Challenges","Cancer is one of the most critical diseases that has caused several deaths in today’s world. In most cases, doctors and practitioners are only able to diagnose cancer in its later stages. In the later stages, planning cancer treatment and increasing the patient’s survival rate becomes a very challenging task. Therefore, it becomes the need of the hour to detect cancer in the early stages for appropriate treatment and surgery planning. Analysis and interpretation of medical images such as MRI and CT scans help doctors and practitioners diagnose many diseases, including cancer disease. However, manual interpretation of medical images is costly, time-consuming and biased. Nowadays, deep learning, a subset of artificial intelligence, is gaining increasing attention from practitioners in automatically analysing and interpreting medical images without their intervention. Deep learning methods have reported extraordinary results in different fields due to their ability to automatically extract intrinsic features from images without any dependence on manually extracted features. This study provides a comprehensive review of deep learning methods in cancer detection and diagnosis, mainly focusing on breast cancer, brain cancer, skin cancer, and prostate cancer. This study describes various deep learning models and steps for applying deep learning models in detecting cancer. Recent developments in cancer detection based on deep learning methods have been critically analysed and summarised to identify critical challenges in applying them for detecting cancer accurately in the early stages. Based on the identified challenges, we provide a few promising future research directions for fellow researchers in the field. The outcome of this study provides many clues for developing practical and accurate cancer detection systems for its early diagnosis and treatment planning.",,,Computer Modeling in Engineering & Sciences,,,2021-12-29,2021,2021-12-29,2022,130,3,1271-1307,All OA, Gold,Article,"Kumar, Gulshan; Alqahtani, Hamed","Kumar, Gulshan (Shaheed Bhagat Singh State University, Ferozepur, India); Alqahtani, Hamed (King Khalid University, Abha, Saudi Arabia)","Kumar, Gulshan ","Kumar, Gulshan (); Alqahtani, Hamed (King Khalid University)",5,5,,5.54,https://file.techscience.com/ueditor/files/cmes/TSP_CMES-130-3/TSP_CMES_18418/TSP_CMES_18418.pdf,https://app.dimensions.ai/details/publication/pub.1144304733,49 Mathematical Sciences, 4901 Applied Mathematics, 4903 Numerical and Computational Mathematics,3 Good Health and Well Being,,,,,,,,
789,pub.1120842238,10.48550/arxiv.1909.00482,,,A Semi-Automated Usability Evaluation Framework for Interactive Image  Segmentation Systems,"For complex segmentation tasks, the achievable accuracy of fully automated
systems is inherently limited. Specifically, when a precise segmentation result
is desired for a small amount of given data sets, semi-automatic methods
exhibit a clear benefit for the user. The optimization of human computer
interaction (HCI) is an essential part of interactive image segmentation.
Nevertheless, publications introducing novel interactive segmentation systems
(ISS) often lack an objective comparison of HCI aspects. It is demonstrated,
that even when the underlying segmentation algorithm is the same throughout
interactive prototypes, their user experience may vary substantially. As a
result, users prefer simple interfaces as well as a considerable degree of
freedom to control each iterative step of the segmentation. In this article, an
objective method for the comparison of ISS is proposed, based on extensive user
studies. A summative qualitative content analysis is conducted via abstraction
of visual and verbal feedback given by the participants. A direct assessment of
the segmentation system is executed by the users via the system usability scale
(SUS) and AttrakDiff-2 questionnaires. Furthermore, an approximation of the
findings regarding usability aspects in those studies is introduced, conducted
solely from the system-measurable user actions during their usage of
interactive segmentation prototypes. The prediction of all questionnaire
results has an average relative error of 8.9%, which is close to the expected
precision of the questionnaire results themselves. This automated evaluation
scheme may significantly reduce the resources necessary to investigate each
variation of a prototype's user interface (UI) features and segmentation
methodologies.",,,arXiv,,,2019-09-01,2019,,,,,,All OA, Green,Preprint,"Amrehn, Mario; Steidl, Stefan; Kortekaas, Reinier; Strumia, Maddalena; Weingarten, Markus; Kowarschik, Markus; Maier, Andreas","Amrehn, Mario (); Steidl, Stefan (); Kortekaas, Reinier (); Strumia, Maddalena (); Weingarten, Markus (); Kowarschik, Markus (); Maier, Andreas ()",,"Amrehn, Mario (); Steidl, Stefan (); Kortekaas, Reinier (); Strumia, Maddalena (); Weingarten, Markus (); Kowarschik, Markus (); Maier, Andreas ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1120842238,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4608 Human-Centred Computing",,,,,,,,,,,
788,pub.1147138021,10.1016/j.neucom.2022.04.065,,,Medical image segmentation with 3D convolutional neural networks: A survey,"Computer-aided medical image analysis plays a significant role in assisting medical practitioners for expert clinical diagnosis and deciding the optimal treatment plan. At present, convolutional neural networks (CNNs) are the preferred choice for medical image analysis. In addition, with the rapid advancements in three-dimensional (3D) imaging systems and the availability of excellent hardware and software support to process large volumes of data, 3D deep learning methods are gaining popularity in medical image analysis. Here, we present an extensive review of the recently proposed 3D deep learning methods for medical image segmentation. Furthermore, the research gaps and future directions in 3D medical image segmentation are discussed.",,,Neurocomputing,,,2022-07,2022,,2022-07,493,,397-413,All OA, Green,Article,"Niyas, S.; Pawan, S.J.; Kumar, M. Anand; Rajan, Jeny","Niyas, S. (Department of Computer Science and Engineering, National Institute of Technology Karnataka, Surathkal, Mangalore – 575025, Karnataka, India); Pawan, S.J. (Department of Computer Science and Engineering, National Institute of Technology Karnataka, Surathkal, Mangalore – 575025, Karnataka, India); Kumar, M. Anand (Department of Information Technology, National Institute of Technology Karnataka, Surathkal, Mangalore – 575025, Karnataka, India); Rajan, Jeny (Department of Computer Science and Engineering, National Institute of Technology Karnataka, Surathkal, Mangalore – 575025, Karnataka, India)","Niyas, S. (National Institute of Technology Karnataka)","Niyas, S. (National Institute of Technology Karnataka); Pawan, S.J. (National Institute of Technology Karnataka); Kumar, M. Anand (National Institute of Technology Karnataka); Rajan, Jeny (National Institute of Technology Karnataka)",10,10,,,http://arxiv.org/pdf/2108.08467,https://app.dimensions.ai/details/publication/pub.1147138021,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
763,pub.1149847036,10.48550/arxiv.2207.14191,,,Learning with Limited Annotations: A Survey on Deep Semi-Supervised  Learning for Medical Image Segmentation,"Medical image segmentation is a fundamental and critical step in many
image-guided clinical approaches. Recent success of deep learning-based
segmentation methods usually relies on a large amount of labeled data, which is
particularly difficult and costly to obtain especially in the medical imaging
domain where only experts can provide reliable and accurate annotations.
Semi-supervised learning has emerged as an appealing strategy and been widely
applied to medical image segmentation tasks to train deep models with limited
annotations. In this paper, we present a comprehensive review of recently
proposed semi-supervised learning methods for medical image segmentation and
summarized both the technical novelties and empirical results. Furthermore, we
analyze and discuss the limitations and several unsolved problems of existing
approaches. We hope this review could inspire the research community to explore
solutions for this challenge and further promote the developments in medical
image segmentation field.",,,arXiv,,,2022-07-28,2022,,,,,,All OA, Green,Preprint,"Jiao, Rushi; Zhang, Yichi; Ding, Le; Cai, Rong; Zhang, Jicong","Jiao, Rushi (); Zhang, Yichi (); Ding, Le (); Cai, Rong (); Zhang, Jicong ()",,"Jiao, Rushi (); Zhang, Yichi (); Ding, Le (); Cai, Rong (); Zhang, Jicong ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1149847036,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
762,pub.1085079328,10.3390/info8020049,,,Automated Prostate Gland Segmentation Based on an Unsupervised Fuzzy C-Means Clustering Technique Using Multispectral T1w and T2w MR Imaging,"Prostate imaging analysis is difficult in diagnosis, therapy, and staging of prostate cancer. In clinical practice, Magnetic Resonance Imaging (MRI) is increasingly used thanks to its morphologic and functional capabilities. However, manual detection and delineation of prostate gland on multispectral MRI data is currently a time-expensive and operator-dependent procedure. Efficient computer-assisted segmentation approaches are not yet able to address these issues, but rather have the potential to do so. In this paper, a novel automatic prostate MR image segmentation method based on the Fuzzy C-Means (FCM) clustering algorithm, which enables multispectral T1-weighted (T1w) and T2-weighted (T2w) MRI anatomical data processing, is proposed. This approach, using an unsupervised Machine Learning technique, helps to segment the prostate gland effectively. A total of 21 patients with suspicion of prostate cancer were enrolled in this study. Volume-based metrics, spatial overlap-based metrics and spatial distance-based metrics were used to quantitatively evaluate the accuracy of the obtained segmentation results with respect to the gold-standard boundaries delineated manually by an expert radiologist. The proposed multispectral segmentation method was compared with the same processing pipeline applied on either T2w or T1w MR images alone. The multispectral approach considerably outperforms the monoparametric ones, achieving an average Dice Similarity Coefficient 90.77 ± 1.75, with respect to 81.90 ± 6.49 and 82.55 ± 4.93 by processing T2w and T1w imaging alone, respectively. Combining T2w and T1w MR image structural information significantly enhances prostate gland segmentation by exploiting the uniform gray appearance of the prostate on T1w MRI.","This research work was supported by funds from the University of Milano-Bicocca. We would like to thank Lucia Maria Valastro, Maria Gabriella Sabini and Davide D’Urso at Medical Physics Unit of the Cannizzaro Hospital, Catania, Italy, for supplying all the images analyzed in this study.",,Information,,,2017-04-28,2017,2017-04-28,,8,2,49,All OA, Gold,Article,"Rundo, Leonardo; Militello, Carmelo; Russo, Giorgio; Garufi, Antonio; Vitabile, Salvatore; Gilardi, Maria Carla; Mauri, Giancarlo","Rundo, Leonardo (Dipartimento di Informatica, Sistemistica e Comunicazione (DISCo), Università degli Studi di Milano-Bicocca, Viale Sarca 336, Milano 20126, Italy;, mauri@disco.unimib.it; Istituto di Bioimmagini e Fisiologia Molecolare-Consiglio Nazionale delle Ricerche (IBFM-CNR), Contrada Pietrapollastra-Pisciotto, Cefalù (PA) 90015, Italy;, carmelo.militello@ibfm.cnr.it, (C.M.);, giorgio.russo@ibfm.cnr.it, (G.R.);, mariacarla.gilardi@ibfm.cnr.it, (M.C.G.)); Militello, Carmelo (Istituto di Bioimmagini e Fisiologia Molecolare-Consiglio Nazionale delle Ricerche (IBFM-CNR), Contrada Pietrapollastra-Pisciotto, Cefalù (PA) 90015, Italy;, carmelo.militello@ibfm.cnr.it, (C.M.);, giorgio.russo@ibfm.cnr.it, (G.R.);, mariacarla.gilardi@ibfm.cnr.it, (M.C.G.)); Russo, Giorgio (Istituto di Bioimmagini e Fisiologia Molecolare-Consiglio Nazionale delle Ricerche (IBFM-CNR), Contrada Pietrapollastra-Pisciotto, Cefalù (PA) 90015, Italy;, carmelo.militello@ibfm.cnr.it, (C.M.);, giorgio.russo@ibfm.cnr.it, (G.R.);, mariacarla.gilardi@ibfm.cnr.it, (M.C.G.); Azienda Ospedaliera per l’Emergenza Cannizzaro, Via Messina 829, Catania 95126, Italy;, antoniogarufi@alice.it); Garufi, Antonio (Azienda Ospedaliera per l’Emergenza Cannizzaro, Via Messina 829, Catania 95126, Italy;, antoniogarufi@alice.it); Vitabile, Salvatore (Dipartimento di Biopatologia e Biotecnologie Mediche (DIBIMED), Università degli Studi di Palermo, Via del Vespro 129, Palermo 90127, Italy;, salvatore.vitabile@unipa.it); Gilardi, Maria Carla (Istituto di Bioimmagini e Fisiologia Molecolare-Consiglio Nazionale delle Ricerche (IBFM-CNR), Contrada Pietrapollastra-Pisciotto, Cefalù (PA) 90015, Italy;, carmelo.militello@ibfm.cnr.it, (C.M.);, giorgio.russo@ibfm.cnr.it, (G.R.);, mariacarla.gilardi@ibfm.cnr.it, (M.C.G.)); Mauri, Giancarlo (Dipartimento di Informatica, Sistemistica e Comunicazione (DISCo), Università degli Studi di Milano-Bicocca, Viale Sarca 336, Milano 20126, Italy;, mauri@disco.unimib.it)","Rundo, Leonardo (University of Milano-Bicocca; Institute of Molecular Bioimaging and Physiology; )","Rundo, Leonardo (University of Milano-Bicocca; Institute of Molecular Bioimaging and Physiology); Militello, Carmelo (Institute of Molecular Bioimaging and Physiology); Russo, Giorgio (Institute of Molecular Bioimaging and Physiology); Garufi, Antonio (); Vitabile, Salvatore (University of Palermo); Gilardi, Maria Carla (Institute of Molecular Bioimaging and Physiology); Mauri, Giancarlo (University of Milano-Bicocca)",45,16,,12.7,https://www.mdpi.com/2078-2489/8/2/49/pdf,https://app.dimensions.ai/details/publication/pub.1085079328,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4605 Data Management and Data Science,,,,,,,,,
757,pub.1128143400,10.1016/j.neucom.2020.05.070,,,A survey on U-shaped networks in medical image segmentations,"The U-shaped network is one of the end-to-end convolutional neural networks (CNNs). In electron microscope segmentation of ISBI challenge 2012, the concise architecture and outstanding performance of the U-shaped network are impressive. Then, a variety of segmentation models based on this architecture have been proposed for medical image segmentations. We present a comprehensive literature review of U-shaped networks applied to medical image segmentation tasks, focusing on the architectures, extended mechanisms and application areas in these studies. The aim of this survey is twofold. First, we report the different extended U-shaped networks, discuss main state-of-the-art extended mechanisms, including residual mechanism, dense mechanism, dilated mechanism, attention mechanism, multi-module mechanism, and ensemble mechanism, analyze their pros and cons. Second, this survey provides the overview of studies in main application areas of U-shaped networks, including brain tumor, stroke, white matter hyperintensities (WMHs), eye, cardiac, liver, musculoskeletal, skin cancer, and neuronal pathology. Finally, we summarize the current U-shaped networks, point out the open challenges and directions for future research.","The work described in this paper was supported by the National Natural Science Foundation of China under Grant Nos. U1909208, 61772552; the 111 Project (No.B18059); the Hunan Provincial Science and Technology Program (2018WK4001); the Natural Science Foundation of Hunan Province in China (2018JJ2534).",,Neurocomputing,,,2020-10,2020,,2020-10,409,,244-258,Closed,Article,"Liu, Liangliang; Cheng, Jianhong; Quan, Quan; Wu, Fang-Xiang; Wang, Yu-Ping; Wang, Jianxin","Liu, Liangliang (Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha 410083, PR China; Department of Network Center, Pingdingshan University, Pingdingshan 467000, PR China); Cheng, Jianhong (Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha 410083, PR China); Quan, Quan (Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha 410083, PR China); Wu, Fang-Xiang (Department of Mechanical Engineering and Division of Biomedical Engineering, University of Saskatchewan, Saskatoon, SK S7N5A9, Canada); Wang, Yu-Ping (School of Biomedical Engineering Department, Tulane University, New Orleans, LA 70118, USA); Wang, Jianxin (Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha 410083, PR China)","Wang, Jianxin (Central South University)","Liu, Liangliang (Central South University; Pingdingshan University); Cheng, Jianhong (Central South University); Quan, Quan (Central South University); Wu, Fang-Xiang (University of Saskatchewan); Wang, Yu-Ping (Tulane University); Wang, Jianxin (Central South University)",112,109,,57.72,,https://app.dimensions.ai/details/publication/pub.1128143400,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
697,pub.1154196947,10.48550/arxiv.2301.00265,,,Source-Free Unsupervised Domain Adaptation: A Survey,"Unsupervised domain adaptation (UDA) via deep learning has attracted
appealing attention for tackling domain-shift problems caused by distribution
discrepancy across different domains. Existing UDA approaches highly depend on
the accessibility of source domain data, which is usually limited in practical
scenarios due to privacy protection, data storage and transmission cost, and
computation burden. To tackle this issue, many source-free unsupervised domain
adaptation (SFUDA) methods have been proposed recently, which perform knowledge
transfer from a pre-trained source model to unlabeled target domain with source
data inaccessible. A comprehensive review of these works on SFUDA is of great
significance. In this paper, we provide a timely and systematic literature
review of existing SFUDA approaches from a technical perspective. Specifically,
we categorize current SFUDA studies into two groups, i.e., white-box SFUDA and
black-box SFUDA, and further divide them into finer subcategories based on
different learning strategies they use. We also investigate the challenges of
methods in each subcategory, discuss the advantages/disadvantages of white-box
and black-box SFUDA methods, conclude the commonly used benchmark datasets, and
summarize the popular techniques for improved generalizability of models
learned without using source data. We finally discuss several promising future
directions in this field.",,,arXiv,,,2022-12-31,2022,,,,,,All OA, Green,Preprint,"Fang, Yuqi; Yap, Pew-Thian; Lin, Weili; Zhu, Hongtu; Liu, Mingxia","Fang, Yuqi (); Yap, Pew-Thian (); Lin, Weili (); Zhu, Hongtu (); Liu, Mingxia ()",,"Fang, Yuqi (); Yap, Pew-Thian (); Lin, Weili (); Zhu, Hongtu (); Liu, Mingxia ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154196947,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
683,pub.1111990230,10.1201/9780429434334-5,,,Current Role and Evolution of MRI Fusion Biopsy for Prostate Cancer,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,43-51,Closed,Chapter,"Velez, Danielle; Brito, Joseph; Renzulli, Joseph","Velez, Danielle (); Brito, Joseph (); Renzulli, Joseph ()",,"Velez, Danielle (); Brito, Joseph (); Renzulli, Joseph ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1111990230,,,,,,,,,,,,,
683,pub.1111990225,10.1201/9780429434334-18,,,Diagnosing Prostate Cancer Based on Deep Learning with a Stacked Nonnegativity Constraint Autoencoder,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,325-347,Closed,Chapter,"Reda, Islam; Shalaby, Ahmed; Elmogy, Mohammed; Aboulfotouh, Ahmed; El-Ghar, Mohamed Abou; Elmagharaby, Adel; El-Baz, Ayman","Reda, Islam (); Shalaby, Ahmed (); Elmogy, Mohammed (); Aboulfotouh, Ahmed (); El-Ghar, Mohamed Abou (); Elmagharaby, Adel (); El-Baz, Ayman ()",,"Reda, Islam (); Shalaby, Ahmed (); Elmogy, Mohammed (); Aboulfotouh, Ahmed (); El-Ghar, Mohamed Abou (); Elmagharaby, Adel (); El-Baz, Ayman ()",2,0,,,,https://app.dimensions.ai/details/publication/pub.1111990225,,,,,,,,,,,,,
683,pub.1111990224,10.1201/9780429434334-17,,,Magnetic Resonance Imaging in the Detection of Prostate Cancer,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,317-324,Closed,Chapter,"McClure, Timothy D; Margolis, Daniel; Schlegel, Peter N","McClure, Timothy D (); Margolis, Daniel (); Schlegel, Peter N ()",,"McClure, Timothy D (); Margolis, Daniel (); Schlegel, Peter N ()",3,0,,,,https://app.dimensions.ai/details/publication/pub.1111990224,,,,,,,,,,,,,
683,pub.1111990222,10.1201/9780429434334-15,,,Precision Imaging of Prostate Cancer,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,259-293,Closed,Chapter,"Fei, Baowei","Fei, Baowei ()",,"Fei, Baowei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1111990222,,,,,,,,,,,,,
683,pub.1111990228,10.1201/9780429434334-3,,,Current Active Surveillance Protocol for Prostate Cancer,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,23-31,Closed,Chapter,"Greenberg, Scott; Yates, Jennifer","Greenberg, Scott (); Yates, Jennifer ()",,"Greenberg, Scott (); Yates, Jennifer ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1111990228,,,,,,,,,,,,,
679,pub.1111990226,10.1201/9780429434334-19,,,MRI Imaging of Seminal Vesicle Invasion (SVI) in Prostate Adenocarcinoma,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,349-370,Closed,Chapter,"Gold, Samuel A; Hale, Graham R; Rayn, Kareem N; Valera, Vladimir; Bloom, Jonathan B; Pinto, Peter A","Gold, Samuel A (); Hale, Graham R (); Rayn, Kareem N (); Valera, Vladimir (); Bloom, Jonathan B (); Pinto, Peter A ()",,"Gold, Samuel A (); Hale, Graham R (); Rayn, Kareem N (); Valera, Vladimir (); Bloom, Jonathan B (); Pinto, Peter A ()",3,0,,,,https://app.dimensions.ai/details/publication/pub.1111990226,,,,,,,,,,,,,
678,pub.1111990231,10.1201/9780429434334-6,,,Current Role of Focal Therapy for Prostate Cancer,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,53-62,Closed,Chapter,"Chiang, H Abraham; Haleblian, George E","Chiang, H Abraham (); Haleblian, George E ()",,"Chiang, H Abraham (); Haleblian, George E ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1111990231,,,,,,,,,,,,,
678,pub.1111990229,10.1201/9780429434334-4,,,Prostate MRI,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,33-42,Closed,Chapter,"Pereira, J; Pareek, Gyan; Grand, D","Pereira, J (); Pareek, Gyan (); Grand, D ()",,"Pereira, J (); Pareek, Gyan (); Grand, D ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1111990229,,,,,,,,,,,,,
678,pub.1111990227,10.1201/9780429434334-2,,,Transrectal Ultrasound (TRUS)-Guided Prostate Biopsy,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,9-21,Closed,Chapter,"Fantasia, Jennifer; Golijanin, Dragan; Gershman, Boris","Fantasia, Jennifer (); Golijanin, Dragan (); Gershman, Boris ()",,"Fantasia, Jennifer (); Golijanin, Dragan (); Gershman, Boris ()",1,0,,,,https://app.dimensions.ai/details/publication/pub.1111990227,,,,,,,,,,,,,
678,pub.1111990217,10.1201/9780429434334-10,,,Computer-Aided Diagnosis Systems for Prostate Cancer Detection,,,,,Prostate Cancer Imaging,,2018-10-31,2018,2018-10-31,,,,87-163,Closed,Chapter,"Lemaître, Guillaume; Martí, Robert; Meriaudeau, Fabrice","Lemaître, Guillaume (); Martí, Robert (); Meriaudeau, Fabrice ()",,"Lemaître, Guillaume (); Martí, Robert (); Meriaudeau, Fabrice ()",3,0,,,,https://app.dimensions.ai/details/publication/pub.1111990217,,,,,,,,,,,,,
648,pub.1144751841,10.1049/ipr2.12419,,,Medical image segmentation using deep learning: A survey,"Deep learning has been widely used for medical image segmentation and a large number of papers has been presented recording the success of deep learning in the field. A comprehensive thematic survey on medical image segmentation using deep learning techniques is presented. This paper makes two original contributions. Firstly, compared to traditional surveys that directly divide literatures of deep learning on medical image segmentation into many groups and introduce literatures in detail for each group, we classify currently popular literatures according to a multi‐level structure from coarse to fine. Secondly, this paper focuses on supervised and weakly supervised learning approaches, without including unsupervised approaches since they have been introduced in many old surveys and they are not popular currently. For supervised learning approaches, we analyse literatures in three aspects: the selection of backbone networks, the design of network blocks, and the improvement of loss functions. For weakly supervised learning approaches, we investigate literature according to data augmentation, transfer learning, and interactive segmentation, separately. Compared to existing surveys, this survey classifies the literatures very differently from before and is more convenient for readers to understand the relevant rationale and will guide them to think of appropriate improvements in medical image segmentation based on deep learning approaches.","This work was supported in part by Natural Science Basic Research Program of Shaanxi (Program No. 2021JC‐47), in part by the National Natural Science Foundation of China under Grant 61871259, and Grant 61861024, National Natural Science Foundation of China‐Royal Society: Grant 61811530325 (IECnNSFCn170396, Royal Society, UK), in part by Key Research and Development Program of Shaanxi (Program No. 2021ZDLGY08‐07), and in part by Shaanxi Joint Laboratory of Artificial Intelligence (Program No. 2020SS‐03).",,IET Image Processing,,,2022-01-17,2022,2022-01-17,2022-04,16,5,1243-1267,All OA, Green,Article,"Wang, Risheng; Lei, Tao; Cui, Ruixia; Zhang, Bingtao; Meng, Hongying; Nandi, Asoke K.","Wang, Risheng (Shaanxi Joint Laboratory of Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; The School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China); Lei, Tao (Shaanxi Joint Laboratory of Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; The School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China); Cui, Ruixia (The Laboratory of Hepatobiliary Surgery, First Affiliated Hospital' and 'National Engineering Laboratory of Big Data Algorithm and Analysis Technology Research'(Xi'an Jiaotong University), Xi'an, China); Zhang, Bingtao (The School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, China); Meng, Hongying (The Department of Electronic and Electrical Engineering, Brunel University London, UK); Nandi, Asoke K. (The Department of Electronic and Electrical Engineering, Brunel University London, UK)","Lei, Tao (Shaanxi University of Science and Technology; Shaanxi University of Science and Technology)","Wang, Risheng (Shaanxi University of Science and Technology; Shaanxi University of Science and Technology); Lei, Tao (Shaanxi University of Science and Technology; Shaanxi University of Science and Technology); Cui, Ruixia (Xi'an Jiaotong University); Zhang, Bingtao (Lanzhou Jiaotong University); Meng, Hongying (Brunel University London); Nandi, Asoke K. (Brunel University London)",36,36,,,http://arxiv.org/pdf/2009.13120,https://app.dimensions.ai/details/publication/pub.1144751841,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
648,pub.1131269932,10.48550/arxiv.2009.13120,,,Medical Image Segmentation Using Deep Learning: A Survey,"Deep learning has been widely used for medical image segmentation and a large
number of papers has been presented recording the success of deep learning in
the field. In this paper, we present a comprehensive thematic survey on medical
image segmentation using deep learning techniques. This paper makes two
original contributions. Firstly, compared to traditional surveys that directly
divide literatures of deep learning on medical image segmentation into many
groups and introduce literatures in detail for each group, we classify
currently popular literatures according to a multi-level structure from coarse
to fine. Secondly, this paper focuses on supervised and weakly supervised
learning approaches, without including unsupervised approaches since they have
been introduced in many old surveys and they are not popular currently. For
supervised learning approaches, we analyze literatures in three aspects: the
selection of backbone networks, the design of network blocks, and the
improvement of loss functions. For weakly supervised learning approaches, we
investigate literature according to data augmentation, transfer learning, and
interactive segmentation, separately. Compared to existing surveys, this survey
classifies the literatures very differently from before and is more convenient
for readers to understand the relevant rationale and will guide them to think
of appropriate improvements in medical image segmentation based on deep
learning approaches.",,,arXiv,,,2020-09-28,2020,,,,,,All OA, Green,Preprint,"Wang, Risheng; Lei, Tao; Cui, Ruixia; Zhang, Bingtao; Meng, Hongying; Nandi, Asoke K.","Wang, Risheng (); Lei, Tao (); Cui, Ruixia (); Zhang, Bingtao (); Meng, Hongying (); Nandi, Asoke K. ()",,"Wang, Risheng (); Lei, Tao (); Cui, Ruixia (); Zhang, Bingtao (); Meng, Hongying (); Nandi, Asoke K. ()",1,1,,0.52,,https://app.dimensions.ai/details/publication/pub.1131269932,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
648,pub.1119354523,10.48550/arxiv.1901.04056,,,The Liver Tumor Segmentation Benchmark (LiTS),"In this work, we report the set-up and results of the Liver Tumor
Segmentation Benchmark (LiTS), which was organized in conjunction with the IEEE
International Symposium on Biomedical Imaging (ISBI) 2017 and the International
Conferences on Medical Image Computing and Computer-Assisted Intervention
(MICCAI) 2017 and 2018. The image dataset is diverse and contains primary and
secondary tumors with varied sizes and appearances with various
lesion-to-background levels (hyper-/hypo-dense), created in collaboration with
seven hospitals and research institutions. Seventy-five submitted liver and
liver tumor segmentation algorithms were trained on a set of 131 computed
tomography (CT) volumes and were tested on 70 unseen test images acquired from
different patients. We found that not a single algorithm performed best for
both liver and liver tumors in the three events. The best liver segmentation
algorithm achieved a Dice score of 0.963, whereas, for tumor segmentation, the
best algorithms achieved Dices scores of 0.674 (ISBI 2017), 0.702 (MICCAI
2017), and 0.739 (MICCAI 2018). Retrospectively, we performed additional
analysis on liver tumor detection and revealed that not all top-performing
segmentation algorithms worked well for tumor detection. The best liver tumor
detection method achieved a lesion-wise recall of 0.458 (ISBI 2017), 0.515
(MICCAI 2017), and 0.554 (MICCAI 2018), indicating the need for further
research. LiTS remains an active benchmark and resource for research, e.g.,
contributing the liver-related segmentation tasks in
\url{http://medicaldecathlon.com/}. In addition, both data and online
evaluation are accessible via \url{www.lits-challenge.com}.",,,arXiv,,,2019-01-13,2019,,,,,,All OA, Green,Preprint,"Bilic, Patrick; Christ, Patrick; Li, Hongwei Bran; Vorontsov, Eugene; Ben-Cohen, Avi; Kaissis, Georgios; Szeskin, Adi; Jacobs, Colin; Mamani, Gabriel Efrain Humpire; Chartrand, Gabriel; Lohöfer, Fabian; Holch, Julian Walter; Sommer, Wieland; Hofmann, Felix; Hostettler, Alexandre; Lev-Cohain, Naama; Drozdzal, Michal; Amitai, Michal Marianne; Vivantik, Refael; Sosna, Jacob; Ezhov, Ivan; Sekuboyina, Anjany; Navarro, Fernando; Kofler, Florian; Paetzold, Johannes C.; Shit, Suprosanna; Hu, Xiaobin; Lipková, Jana; Rempfler, Markus; Piraud, Marie; Kirschke, Jan; Wiestler, Benedikt; Zhang, Zhiheng; Hülsemeyer, Christian; Beetz, Marcel; Ettlinger, Florian; Antonelli, Michela; Bae, Woong; Bellver, Míriam; Bi, Lei; Chen, Hao; Chlebus, Grzegorz; Dam, Erik B.; Dou, Qi; Fu, Chi-Wing; Georgescu, Bogdan; Giró-i-Nieto, Xavier; Gruen, Felix; Han, Xu; Heng, Pheng-Ann; Hesser, Jürgen; Moltz, Jan Hendrik; Igel, Christian; Isensee, Fabian; Jäger, Paul; Jia, Fucang; Kaluva, Krishna Chaitanya; Khened, Mahendra; Kim, Ildoo; Kim, Jae-Hun; Kim, Sungwoong; Kohl, Simon; Konopczynski, Tomasz; Kori, Avinash; Krishnamurthi, Ganapathy; Li, Fan; Li, Hongchao; Li, Junbo; Li, Xiaomeng; Lowengrub, John; Ma, Jun; Maier-Hein, Klaus; Maninis, Kevis-Kokitsi; Meine, Hans; Merhof, Dorit; Pai, Akshay; Perslev, Mathias; Petersen, Jens; Pont-Tuset, Jordi; Qi, Jin; Qi, Xiaojuan; Rippel, Oliver; Roth, Karsten; Sarasua, Ignacio; Schenk, Andrea; Shen, Zengming; Torres, Jordi; Wachinger, Christian; Wang, Chunliang; Weninger, Leon; Wu, Jianrong; Xu, Daguang; Yang, Xiaoping; Yu, Simon Chun-Ho; Yuan, Yading; Yu, Miao; Zhang, Liping; Cardoso, Jorge; Bakas, Spyridon; Braren, Rickmer; Heinemann, Volker; Pal, Christopher; Tang, An; Kadoury, Samuel; Soler, Luc; van Ginneken, Bram; Greenspan, Hayit; Joskowicz, Leo; Menze, Bjoern","Bilic, Patrick (); Christ, Patrick (); Li, Hongwei Bran (); Vorontsov, Eugene (); Ben-Cohen, Avi (); Kaissis, Georgios (); Szeskin, Adi (); Jacobs, Colin (); Mamani, Gabriel Efrain Humpire (); Chartrand, Gabriel (); Lohöfer, Fabian (); Holch, Julian Walter (); Sommer, Wieland (); Hofmann, Felix (); Hostettler, Alexandre (); Lev-Cohain, Naama (); Drozdzal, Michal (); Amitai, Michal Marianne (); Vivantik, Refael (); Sosna, Jacob (); Ezhov, Ivan (); Sekuboyina, Anjany (); Navarro, Fernando (); Kofler, Florian (); Paetzold, Johannes C. (); Shit, Suprosanna (); Hu, Xiaobin (); Lipková, Jana (); Rempfler, Markus (); Piraud, Marie (); Kirschke, Jan (); Wiestler, Benedikt (); Zhang, Zhiheng (); Hülsemeyer, Christian (); Beetz, Marcel (); Ettlinger, Florian (); Antonelli, Michela (); Bae, Woong (); Bellver, Míriam (); Bi, Lei (); Chen, Hao (); Chlebus, Grzegorz (); Dam, Erik B. (); Dou, Qi (); Fu, Chi-Wing (); Georgescu, Bogdan (); Giró-i-Nieto, Xavier (); Gruen, Felix (); Han, Xu (); Heng, Pheng-Ann (); Hesser, Jürgen (); Moltz, Jan Hendrik (); Igel, Christian (); Isensee, Fabian (); Jäger, Paul (); Jia, Fucang (); Kaluva, Krishna Chaitanya (); Khened, Mahendra (); Kim, Ildoo (); Kim, Jae-Hun (); Kim, Sungwoong (); Kohl, Simon (); Konopczynski, Tomasz (); Kori, Avinash (); Krishnamurthi, Ganapathy (); Li, Fan (); Li, Hongchao (); Li, Junbo (); Li, Xiaomeng (); Lowengrub, John (); Ma, Jun (); Maier-Hein, Klaus (); Maninis, Kevis-Kokitsi (); Meine, Hans (); Merhof, Dorit (); Pai, Akshay (); Perslev, Mathias (); Petersen, Jens (); Pont-Tuset, Jordi (); Qi, Jin (); Qi, Xiaojuan (); Rippel, Oliver (); Roth, Karsten (); Sarasua, Ignacio (); Schenk, Andrea (); Shen, Zengming (); Torres, Jordi (); Wachinger, Christian (); Wang, Chunliang (); Weninger, Leon (); Wu, Jianrong (); Xu, Daguang (); Yang, Xiaoping (); Yu, Simon Chun-Ho (); Yuan, Yading (); Yu, Miao (); Zhang, Liping (); Cardoso, Jorge (); Bakas, Spyridon (); Braren, Rickmer (); Heinemann, Volker (); Pal, Christopher (); Tang, An (); Kadoury, Samuel (); Soler, Luc (); van Ginneken, Bram (); Greenspan, Hayit (); Joskowicz, Leo (); Menze, Bjoern ()",,"Bilic, Patrick (); Christ, Patrick (); Li, Hongwei Bran (); Vorontsov, Eugene (); Ben-Cohen, Avi (); Kaissis, Georgios (); Szeskin, Adi (); Jacobs, Colin (); Mamani, Gabriel Efrain Humpire (); Chartrand, Gabriel (); Lohöfer, Fabian (); Holch, Julian Walter (); Sommer, Wieland (); Hofmann, Felix (); Hostettler, Alexandre (); Lev-Cohain, Naama (); Drozdzal, Michal (); Amitai, Michal Marianne (); Vivantik, Refael (); Sosna, Jacob (); Ezhov, Ivan (); Sekuboyina, Anjany (); Navarro, Fernando (); Kofler, Florian (); Paetzold, Johannes C. (); Shit, Suprosanna (); Hu, Xiaobin (); Lipková, Jana (); Rempfler, Markus (); Piraud, Marie (); Kirschke, Jan (); Wiestler, Benedikt (); Zhang, Zhiheng (); Hülsemeyer, Christian (); Beetz, Marcel (); Ettlinger, Florian (); Antonelli, Michela (); Bae, Woong (); Bellver, Míriam (); Bi, Lei (); Chen, Hao (); Chlebus, Grzegorz (); Dam, Erik B. (); Dou, Qi (); Fu, Chi-Wing (); Georgescu, Bogdan (); Giró-i-Nieto, Xavier (); Gruen, Felix (); Han, Xu (); Heng, Pheng-Ann (); Hesser, Jürgen (); Moltz, Jan Hendrik (); Igel, Christian (); Isensee, Fabian (); Jäger, Paul (); Jia, Fucang (); Kaluva, Krishna Chaitanya (); Khened, Mahendra (); Kim, Ildoo (); Kim, Jae-Hun (); Kim, Sungwoong (); Kohl, Simon (); Konopczynski, Tomasz (); Kori, Avinash (); Krishnamurthi, Ganapathy (); Li, Fan (); Li, Hongchao (); Li, Junbo (); Li, Xiaomeng (); Lowengrub, John (); Ma, Jun (); Maier-Hein, Klaus (); Maninis, Kevis-Kokitsi (); Meine, Hans (); Merhof, Dorit (); Pai, Akshay (); Perslev, Mathias (); Petersen, Jens (); Pont-Tuset, Jordi (); Qi, Jin (); Qi, Xiaojuan (); Rippel, Oliver (); Roth, Karsten (); Sarasua, Ignacio (); Schenk, Andrea (); Shen, Zengming (); Torres, Jordi (); Wachinger, Christian (); Wang, Chunliang (); Weninger, Leon (); Wu, Jianrong (); Xu, Daguang (); Yang, Xiaoping (); Yu, Simon Chun-Ho (); Yuan, Yading (); Yu, Miao (); Zhang, Liping (); Cardoso, Jorge (); Bakas, Spyridon (); Braren, Rickmer (); Heinemann, Volker (); Pal, Christopher (); Tang, An (); Kadoury, Samuel (); Soler, Luc (); van Ginneken, Bram (); Greenspan, Hayit (); Joskowicz, Leo (); Menze, Bjoern ()",6,6,,1.8,,https://app.dimensions.ai/details/publication/pub.1119354523,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
647,pub.1139631374,10.1007/978-3-030-69951-2_8,,,Medical Imaging Based Diagnosis Through Machine Learning and Data Analysis,"Machine learning techniques have played an essential role in computer-assisted medical image analysisMedical Image Analysis. In this chapter, we will introduce several of our recent achievements with machine learning methods for feature extraction and representation, classification, dense prediction (segmentation and synthesis), and multi-modality analysis, across the pipeline of computer-assisted diagnosis (CAD). These methods consist of both traditional machine learning techniques and state-of-the-art deep learningDeep Learning based approaches. They were proposed to address pain points in the techniques, for example, similarity metric learning for better classification, 3D and sample-adaptive dense prediction models for segmentation and synthesis, and effective multi-modal imaging data fusion. These methods have been employed in different levels of medical imaging applications, such as medical image synthesis within and across imaging modalities, brain tumor segmentation, and mental disease classification. Common approaches used for related research topics are also briefly reviewed.",,,Computational Biology,"Advances in Artificial Intelligence, Computation, and Data Science",,2021-07-13,2021,2021-07-13,2021,31,,179-225,Closed,Chapter,"Zhang, Jianjia; Wang, Yan; Zu, Chen; Yu, Biting; Wang, Lei; Zhou, Luping","Zhang, Jianjia (School of Biomedical Engineering, Sun yat-sen University, Guangzhou, China); Wang, Yan (School of Computer Science, Sichuan University, Chengdu, China); Zu, Chen (JD.com, Chengdu, China); Yu, Biting (School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia); Wang, Lei (School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia); Zhou, Luping (School of Electrical and Information Engineering, University of Sydney, Camperdown, NSW, Australia)","Zhou, Luping (The University of Sydney)","Zhang, Jianjia (Sun Yat-sen University); Wang, Yan (Sichuan University); Zu, Chen (Jingdong (China)); Yu, Biting (University of Wollongong); Wang, Lei (University of Wollongong); Zhou, Luping (The University of Sydney)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139631374,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation, 4605 Data Management and Data Science,3 Good Health and Well Being,,,,,,,,,
610,pub.1139044380,10.1109/access.2021.3090825,,,Recent Automatic Segmentation Algorithms of MRI Prostate Regions: A Review,"World-wide incidence rate of prostate cancer has progressively increased with time especially with the increased proportion of elderly population. Early detection of prostate cancer when it is confined to the prostate gland has the best chance of successful treatment and increase in surviving rate. Prostate cancer occurrence rate varies over the three prostate regions, peripheral zone (PZ), transitional zone (TZ), and central zone (CZ) and this characteristic is one of the important considerations is development of segmentation algorithm. In fact, the occurrence rate of cancer PZ, TZ and CZ regions is respectively. at 70-80%, 10-20%, 5% or less. In general application of medical imaging, segmentation tasks can be time consuming for the expert to delineate the region of interest, especially when involving large numbers of images. In addition, the manual segmentation is subjective depending on the expert’s experience. Hence, the need to develop automatic segmentation algorithms has rapidly increased along with the increased need of diagnostic tools for assisting medical practitioners, especially in the absence of radiologists. The prostate gland segmentation is challenging due to its shape variability in each zone from patient to patient and different tumor levels in each zone. This survey reviewed 22 machine learning and 88 deep learning-based segmentation of prostate MRI papers, including all MRI modalities. The review coverage includes the initial screening and imaging techniques, image pre-processing, segmentation techniques based on machine learning and deep learning techniques. Particular attention is given to different loss functions used for training segmentation based on deep learning techniques. Besides, a summary of publicly available prostate MRI image datasets is also provided. Finally, the future challenges and limitations of current deep learning-based approaches and suggestions of potential future research are also discussed.","This work was supported in part by the Yayasan Universiti Teknologi Petronas under Grant YUTP-FRG 015LC0-292, and in part by the Imagerie et Vision Artificielle (ImViA) / Imagerie Fonctionnelle et moléculaire et Traitement des Images Médicales (IFTIM) Research Grant.",,IEEE Access,,,2021-01-01,2021,2021-06-21,2021-01-01,9,,97878-97905,All OA, Gold,Article,"Khan, Zia; Yahya, Norashikin; Alsaih, Khaled; Al-Hiyali, Mohammed Isam; Meriaudeau, Fabrice","Khan, Zia (Centre for Intelligent Signal and Imaging Research, Electrical and Electronic Engineering Department, Universiti Teknologi PETRONAS, Seri Iskandar, 32610, Malaysia); Yahya, Norashikin (Centre for Intelligent Signal and Imaging Research, Electrical and Electronic Engineering Department, Universiti Teknologi PETRONAS, Seri Iskandar, 32610, Malaysia); Alsaih, Khaled (CNRS, IOGS, Université de Lyon, UJM-Saint-Etienne, Laboratoire Hubert Curien, UMR5516, 42023, Saint-Etienne, France); Al-Hiyali, Mohammed Isam (Centre for Intelligent Signal and Imaging Research, Electrical and Electronic Engineering Department, Universiti Teknologi PETRONAS, Seri Iskandar, 32610, Malaysia); Meriaudeau, Fabrice (ImViA/IFTIM, Universite Bourgogne Franche-Comté, 21000, Dijon, France)","Yahya, Norashikin (Universiti Teknologi Petronas)","Khan, Zia (Universiti Teknologi Petronas); Yahya, Norashikin (Universiti Teknologi Petronas); Alsaih, Khaled (Laboratoire Hubert Curien); Al-Hiyali, Mohammed Isam (Universiti Teknologi Petronas); Meriaudeau, Fabrice (Université Bourgogne Franche-Comté)",9,9,,,https://ieeexplore.ieee.org/ielx7/6287639/9312710/09461197.pdf,https://app.dimensions.ai/details/publication/pub.1139044380,40 Engineering, 46 Information and Computing Sciences,,,,,,,,,,
605,pub.1149822771,10.1016/j.neucom.2022.07.070,,,A review on the use of deep learning for medical images segmentation,"Deep learning (DL) algorithms have rapidly become a robust tool for analyzing medical images. They have been used extensively for medical image segmentation as the first and significant components of the diagnosis and treatment pipeline. Medical image segmentation is efficiently addressed by many types of deep neural networks, such as convolutional neural networks, fully convolutional network recurrent networks, adversarial networks, and U-shaped networks. This paper reviews the major DL models and applications pertinent to medical image segmentation and summarizes over 150 contributions to the field. Brief overviews of articles are provided by application area: anatomical structures such as organs, bones, and vessels, and abnormalities such as lesions and calcification. Moreover, we discuss current challenges and suggest directions for future research.",,,Neurocomputing,,,2022-09,2022,,2022-09,506,,311-335,Closed,Article,"Aljabri, Manar; AlGhamdi, Manal","Aljabri, Manar (Department of Computer Science, Umm Al-Qura University, Saudi Arabia); AlGhamdi, Manal (Department of Computer Science, Umm Al-Qura University, Saudi Arabia)","Aljabri, Manar (Umm al-Qura University); AlGhamdi, Manal (Umm al-Qura University)","Aljabri, Manar (Umm al-Qura University); AlGhamdi, Manal (Umm al-Qura University)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1149822771,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
572,pub.1141300020,10.48550/arxiv.2109.09658,,,FUTURE-AI: Guiding Principles and Consensus Recommendations for  Trustworthy Artificial Intelligence in Medical Imaging,"The recent advancements in artificial intelligence (AI) combined with the
extensive amount of data generated by today's clinical systems, has led to the
development of imaging AI solutions across the whole value chain of medical
imaging, including image reconstruction, medical image segmentation,
image-based diagnosis and treatment planning. Notwithstanding the successes and
future potential of AI in medical imaging, many stakeholders are concerned of
the potential risks and ethical implications of imaging AI solutions, which are
perceived as complex, opaque, and difficult to comprehend, utilise, and trust
in critical clinical applications. Despite these concerns and risks, there are
currently no concrete guidelines and best practices for guiding future AI
developments in medical imaging towards increased trust, safety and adoption.
To bridge this gap, this paper introduces a careful selection of guiding
principles drawn from the accumulated experiences, consensus, and best
practices from five large European projects on AI in Health Imaging. These
guiding principles are named FUTURE-AI and its building blocks consist of (i)
Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness
and (vi) Explainability. In a step-by-step approach, these guidelines are
further translated into a framework of concrete recommendations for specifying,
developing, evaluating, and deploying technically, clinically and ethically
trustworthy AI solutions into clinical practice.",,,arXiv,,,2021-09-20,2021,,,,,,All OA, Green,Preprint,"Lekadir, Karim; Osuala, Richard; Gallin, Catherine; Lazrak, Noussair; Kushibar, Kaisar; Tsakou, Gianna; Aussó, Susanna; Alberich, Leonor Cerdá; Marias, Kostas; Tsiknakis, Manolis; Colantonio, Sara; Papanikolaou, Nickolas; Salahuddin, Zohaib; Woodruff, Henry C; Lambin, Philippe; Martí-Bonmatí, Luis","Lekadir, Karim (); Osuala, Richard (); Gallin, Catherine (); Lazrak, Noussair (); Kushibar, Kaisar (); Tsakou, Gianna (); Aussó, Susanna (); Alberich, Leonor Cerdá (); Marias, Kostas (); Tsiknakis, Manolis (); Colantonio, Sara (); Papanikolaou, Nickolas (); Salahuddin, Zohaib (); Woodruff, Henry C (); Lambin, Philippe (); Martí-Bonmatí, Luis ()",,"Lekadir, Karim (); Osuala, Richard (); Gallin, Catherine (); Lazrak, Noussair (); Kushibar, Kaisar (); Tsakou, Gianna (); Aussó, Susanna (); Alberich, Leonor Cerdá (); Marias, Kostas (); Tsiknakis, Manolis (); Colantonio, Sara (); Papanikolaou, Nickolas (); Salahuddin, Zohaib (); Woodruff, Henry C (); Lambin, Philippe (); Martí-Bonmatí, Luis ()",1,1,,1.25,,https://app.dimensions.ai/details/publication/pub.1141300020,46 Information and Computing Sciences, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
571,pub.1119426691,10.48550/arxiv.1904.08128,,,Automated Design of Deep Learning Methods for Biomedical Image  Segmentation,"Biomedical imaging is a driver of scientific discovery and core component of
medical care, currently stimulated by the field of deep learning. While
semantic segmentation algorithms enable 3D image analysis and quantification in
many applications, the design of respective specialised solutions is
non-trivial and highly dependent on dataset properties and hardware conditions.
We propose nnU-Net, a deep learning framework that condenses the current domain
knowledge and autonomously takes the key decisions required to transfer a basic
architecture to different datasets and segmentation tasks. Without manual
tuning, nnU-Net surpasses most specialised deep learning pipelines in 19 public
international competitions and sets a new state of the art in the majority of
the 49 tasks. The results demonstrate a vast hidden potential in the systematic
adaptation of deep learning methods to different datasets. We make nnU-Net
publicly available as an open-source tool that can effectively be used
out-of-the-box, rendering state of the art segmentation accessible to
non-experts and catalyzing scientific progress as a framework for automated
method design.",,,arXiv,,,2019-04-17,2019,,,,,,All OA, Green,Preprint,"Isensee, Fabian; Jäger, Paul F.; Kohl, Simon A. A.; Petersen, Jens; Maier-Hein, Klaus H.","Isensee, Fabian (); Jäger, Paul F. (); Kohl, Simon A. A. (); Petersen, Jens (); Maier-Hein, Klaus H. ()",,"Isensee, Fabian (); Jäger, Paul F. (); Kohl, Simon A. A. (); Petersen, Jens (); Maier-Hein, Klaus H. ()",1,1,,0.38,,https://app.dimensions.ai/details/publication/pub.1119426691,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
538,pub.1145200613,10.1007/978-3-030-83047-2_11,,,Auto-contouring for Image-Guidance and Treatment Planning,"Manual contouring is a time-consuming task routinely performed in radiotherapy to identify each patient’s targets and anatomical structures. Auto-segmentation of targets and normal tissues has been growing in clinical use as it can mitigate the inter- and intra-observer differences of manual segmentation and significantly reduce contouring time. Auto-segmentation has gone through advances over the years as computer technology has improved. The first-generation techniques are low-level with no prior information included, such as intensity thresholds. The second-generation techniques use uncertainty models to mitigate issues arising from noise, intensity nonuniformity, and partial volume effect. The third-generation techniques incorporate higher-level knowledge, such as atlas-based contouring. The fourth generation is the most recent development and focuses on deep learning-based techniques. There are many different deep learning techniques, with convolutional neural networks being the most commonly used technique for segmentation tasks. Before implementation in clinics, careful QA must be carried out for auto-segmentation tasks, such as comparison with clinically approved manual contours. In a clinical setting, commissioning and continuous quality assurance must be performed on auto-segmentation systems to ensure safe and proper use. This chapter covers these topics from the generations of auto-segmentation to QA as well as provides some of the latest results for auto-segmentation of normal tissues and targets/tumor volumes.",,,,"Machine and Deep Learning in Oncology, Medical Physics and Radiology",,2022-02-02,2022,2022-02-02,2022,,,231-293,Closed,Chapter,"Ger, Rachel B.; Netherton, Tucker J.; Rhee, Dong Joo; Court, Laurence E.; Yang, Jinzhong; Cardenas, Carlos E.","Ger, Rachel B. (Department of Radiation Oncology and Molecular Radiation Sciences, Johns Hopkins School of Medicine, Baltimore, MD, USA); Netherton, Tucker J. (Department of Radiation Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA); Rhee, Dong Joo (Department of Radiation Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA); Court, Laurence E. (Department of Radiation Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA); Yang, Jinzhong (Department of Radiation Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA); Cardenas, Carlos E. (Department of Radiation Oncology, The University of Alabama at Birmingham, Birmingham, AL, USA)","Cardenas, Carlos E. (University of Alabama at Birmingham)","Ger, Rachel B. (Johns Hopkins University); Netherton, Tucker J. (The University of Texas MD Anderson Cancer Center); Rhee, Dong Joo (The University of Texas MD Anderson Cancer Center); Court, Laurence E. (The University of Texas MD Anderson Cancer Center); Yang, Jinzhong (The University of Texas MD Anderson Cancer Center); Cardenas, Carlos E. (University of Alabama at Birmingham)",1,1,,,,https://app.dimensions.ai/details/publication/pub.1145200613,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,
534,pub.1139148412,10.48550/arxiv.2106.12864,,,A Systematic Collection of Medical Image Datasets for Deep Learning,"The astounding success made by artificial intelligence (AI) in healthcare and
other fields proves that AI can achieve human-like performance. However,
success always comes with challenges. Deep learning algorithms are
data-dependent and require large datasets for training. The lack of data in the
medical imaging field creates a bottleneck for the application of deep learning
to medical image analysis. Medical image acquisition, annotation, and analysis
are costly, and their usage is constrained by ethical restrictions. They also
require many resources, such as human expertise and funding. That makes it
difficult for non-medical researchers to have access to useful and large
medical data. Thus, as comprehensive as possible, this paper provides a
collection of medical image datasets with their associated challenges for deep
learning research. We have collected information of around three hundred
datasets and challenges mainly reported between 2013 and 2020 and categorized
them into four categories: head & neck, chest & abdomen, pathology & blood, and
``others''. Our paper has three purposes: 1) to provide a most up to date and
complete list that can be used as a universal reference to easily find the
datasets for clinical image analysis, 2) to guide researchers on the
methodology to test and evaluate their methods' performance and robustness on
relevant datasets, 3) to provide a ``route'' to relevant algorithms for the
relevant medical topics, and challenge leaderboards.",,,arXiv,,,2021-06-24,2021,,,,,,All OA, Green,Preprint,"Li, Johann; Zhu, Guangming; Hua, Cong; Feng, Mingtao; BasheerBennamoun; Li, Ping; Lu, Xiaoyuan; Song, Juan; Shen, Peiyi; Xu, Xu; Mei, Lin; Zhang, Liang; Shah, Syed Afaq Ali; Bennamoun, Mohammed","Li, Johann (); Zhu, Guangming (); Hua, Cong (); Feng, Mingtao (); BasheerBennamoun (); Li, Ping (); Lu, Xiaoyuan (); Song, Juan (); Shen, Peiyi (); Xu, Xu (); Mei, Lin (); Zhang, Liang (); Shah, Syed Afaq Ali (); Bennamoun, Mohammed ()",,"Li, Johann (); Zhu, Guangming (); Hua, Cong (); Feng, Mingtao (); BasheerBennamoun (); Li, Ping (); Lu, Xiaoyuan (); Song, Juan (); Shen, Peiyi (); Xu, Xu (); Mei, Lin (); Zhang, Liang (); Shah, Syed Afaq Ali (); Bennamoun, Mohammed ()",1,1,,0.83,,https://app.dimensions.ai/details/publication/pub.1139148412,46 Information and Computing Sciences, 4602 Artificial Intelligence,3 Good Health and Well Being,,,,,,,,,
517,pub.1151124908,10.1007/978-3-031-16852-9,,,"Domain Adaptation and Representation Transfer, 4th MICCAI Workshop, DART 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings","This book constitutes the refereed proceedings of the 4th MICCAI Workshop on Domain Adaptation and Representation Transfer, DART 2022, held in conjunction with MICCAI 2022, in September 2022. DART 2022 accepted 13 papers from the 25 submissions received. The workshop aims at creating a discussion forum to compare, evaluate, and discuss methodological advancements and ideas that can improve the applicability of machine learning (ML)/deep learning (DL) approaches to clinical setting by making them robust and consistent across different domains.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13542,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16852-9%2F1,https://app.dimensions.ai/details/publication/pub.1151124908,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
416,pub.1139830237,10.48550/arxiv.2107.09543,,,Data synthesis and adversarial networks: A review and meta-analysis in  cancer imaging,"Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include inter-observer variability, class imbalance, dataset
shifts, inter- and intra-tumour heterogeneity, malignancy determination, and
treatment effect uncertainty. Given the recent advancements in Generative
Adversarial Networks (GANs), data synthesis, and adversarial training, we
assess the potential of these technologies to address a number of key
challenges of cancer imaging. We categorise these challenges into (a) data
scarcity and imbalance, (b) data access and privacy, (c) data annotation and
segmentation, (d) cancer detection and diagnosis, and (e) tumour profiling,
treatment planning and monitoring. Based on our analysis of 164 publications
that apply adversarial training techniques in the context of cancer imaging, we
highlight multiple underexplored solutions with research potential. We further
contribute the Synthesis Study Trustworthiness Test (SynTRUST), a meta-analysis
framework for assessing the validation rigour of medical image synthesis
studies. SynTRUST is based on 26 concrete measures of thoroughness,
reproducibility, usefulness, scalability, and tenability. Based on SynTRUST, we
analyse 16 of the most promising cancer imaging challenge solutions and observe
a high validation rigour in general, but also several desirable improvements.
With this work, we strive to bridge the gap between the needs of the clinical
cancer imaging community and the current and prospective research on data
synthesis and adversarial networks in the artificial intelligence community.",,,arXiv,,,2021-07-20,2021,,,,,,All OA, Green,Preprint,"Osuala, Richard; Kushibar, Kaisar; Garrucho, Lidia; Linardos, Akis; Szafranowska, Zuzanna; Klein, Stefan; Glocker, Ben; Diaz, Oliver; Lekadir, Karim","Osuala, Richard (); Kushibar, Kaisar (); Garrucho, Lidia (); Linardos, Akis (); Szafranowska, Zuzanna (); Klein, Stefan (); Glocker, Ben (); Diaz, Oliver (); Lekadir, Karim ()",,"Osuala, Richard (); Kushibar, Kaisar (); Garrucho, Lidia (); Linardos, Akis (); Szafranowska, Zuzanna (); Klein, Stefan (); Glocker, Ben (); Diaz, Oliver (); Lekadir, Karim ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139830237,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
415,pub.1140705694,10.48550/arxiv.2108.11510,,,Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey,"Deep reinforcement learning augments the reinforcement learning framework and
utilizes the powerful representation of deep neural networks. Recent works have
demonstrated the remarkable successes of deep reinforcement learning in various
domains including finance, medicine, healthcare, video games, robotics, and
computer vision. In this work, we provide a detailed review of recent and
state-of-the-art research advances of deep reinforcement learning in computer
vision. We start with comprehending the theories of deep learning,
reinforcement learning, and deep reinforcement learning. We then propose a
categorization of deep reinforcement learning methodologies and discuss their
advantages and limitations. In particular, we divide deep reinforcement
learning into seven main categories according to their applications in computer
vision, i.e. (i)landmark localization (ii) object detection; (iii) object
tracking; (iv) registration on both 2D image and 3D image volumetric data (v)
image segmentation; (vi) videos analysis; and (vii) other applications. Each of
these categories is further analyzed with reinforcement learning techniques,
network design, and performance. Moreover, we provide a comprehensive analysis
of the existing publicly available datasets and examine source code
availability. Finally, we present some open issues and discuss future research
directions on deep reinforcement learning in computer vision",,,arXiv,,,2021-08-25,2021,,,,,,All OA, Green,Preprint,"Le, Ngan; Rathour, Vidhiwar Singh; Yamazaki, Kashu; Luu, Khoa; Savvides, Marios","Le, Ngan (); Rathour, Vidhiwar Singh (); Yamazaki, Kashu (); Luu, Khoa (); Savvides, Marios ()",,"Le, Ngan (); Rathour, Vidhiwar Singh (); Yamazaki, Kashu (); Luu, Khoa (); Savvides, Marios ()",1,1,,0.82,,https://app.dimensions.ai/details/publication/pub.1140705694,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4611 Machine Learning,,,,,,,,,
415,pub.1141490662,10.1007/s10462-021-10061-9,,,Deep reinforcement learning in computer vision: a comprehensive survey,"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i) landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision.",This material is based upon work supported by the National Science Foundation under Award No OIA-1946391.,,Artificial Intelligence Review,,,2021-09-29,2021,2021-09-29,2022-04,55,4,2733-2819,All OA, Green,Article,"Le, Ngan; Rathour, Vidhiwar Singh; Yamazaki, Kashu; Luu, Khoa; Savvides, Marios","Le, Ngan (Department of Computer Science and Computer Engineering, University of Arkansas, 72703, Fayetteville, AR, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, 15213, Pittsburgh, PA, USA); Rathour, Vidhiwar Singh (Department of Computer Science and Computer Engineering, University of Arkansas, 72703, Fayetteville, AR, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, 15213, Pittsburgh, PA, USA); Yamazaki, Kashu (Department of Computer Science and Computer Engineering, University of Arkansas, 72703, Fayetteville, AR, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, 15213, Pittsburgh, PA, USA); Luu, Khoa (Department of Computer Science and Computer Engineering, University of Arkansas, 72703, Fayetteville, AR, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, 15213, Pittsburgh, PA, USA); Savvides, Marios (Department of Computer Science and Computer Engineering, University of Arkansas, 72703, Fayetteville, AR, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, 15213, Pittsburgh, PA, USA)","Le, Ngan (University of Arkansas at Fayetteville; Carnegie Mellon University)","Le, Ngan (University of Arkansas at Fayetteville; Carnegie Mellon University); Rathour, Vidhiwar Singh (University of Arkansas at Fayetteville; Carnegie Mellon University); Yamazaki, Kashu (University of Arkansas at Fayetteville; Carnegie Mellon University); Luu, Khoa (University of Arkansas at Fayetteville; Carnegie Mellon University); Savvides, Marios (University of Arkansas at Fayetteville; Carnegie Mellon University)",24,24,,19.77,http://arxiv.org/pdf/2108.11510,https://app.dimensions.ai/details/publication/pub.1141490662,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4611 Machine Learning,,,,,,,,,
399,pub.1123784839,10.1007/s13246-019-00826-6,31898241,,"EPSM 2019, Engineering and Physical Sciences in Medicine",,,,Physical and Engineering Sciences in Medicine,,,2020-01-02,2020,2020-01-02,2020-03,43,1,297-462,Closed,Article,,,,,4,1,1.25,1.64,,https://app.dimensions.ai/details/publication/pub.1123784839,40 Engineering, 4003 Biomedical Engineering, 51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,
381,pub.1138436629,10.48550/arxiv.2105.13137,,,"Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past,  Present and Future","With the advances of data-driven machine learning research, a wide variety of
prediction problems have been tackled. It has become critical to explore how
machine learning and specifically deep learning methods can be exploited to
analyse healthcare data. A major limitation of existing methods has been the
focus on grid-like data; however, the structure of physiological recordings are
often irregular and unordered which makes it difficult to conceptualise them as
a matrix. As such, graph neural networks have attracted significant attention
by exploiting implicit information that resides in a biological system, with
interactive nodes connected by edges whose weights can be either temporal
associations or anatomical junctions. In this survey, we thoroughly review the
different types of graph architectures and their applications in healthcare. We
provide an overview of these methods in a systematic manner, organized by their
domain of application including functional connectivity, anatomical structure
and electrical-based analysis. We also outline the limitations of existing
techniques and discuss potential directions for future research.",,,arXiv,,,2021-05-27,2021,,,,,,All OA, Green,Preprint,"Ahmedt-Aristizabal, David; Armin, Mohammad Ali; Denman, Simon; Fookes, Clinton; Petersson, Lars","Ahmedt-Aristizabal, David (); Armin, Mohammad Ali (); Denman, Simon (); Fookes, Clinton (); Petersson, Lars ()",,"Ahmedt-Aristizabal, David (); Armin, Mohammad Ali (); Denman, Simon (); Fookes, Clinton (); Petersson, Lars ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1138436629,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
327,pub.1151604270,10.1561/0600000097,,,A Comprehensive Review of Modern Object Segmentation Approaches,"Image segmentation is the task of associating pixels in an image with their respective object class labels. It has a wide range of applications in many industries including healthcare, transportation, robotics, fashion, home improvement, and tourism. Many deep learning-based approaches have been developed for image-level object recognition and pixel-level scene understanding — with the latter requiring a much denser annotation of scenes with a large set of objects. Extensions of image segmentation tasks include 3D and video segmentation, where units of voxels, point clouds, and video frames are classified into different objects. We use “Object Segmentation” to refer to the union of these segmentation tasks. In this monograph, we investigate both traditional and modern object segmentation approaches, comparing their strengths, weaknesses, and utilities. We examine in detail the wide range of deep learning-based segmentation techniques developed in recent years, provide a review of the widely used datasets and evaluation metrics, and discuss potential future research directions.",,,Foundations and Trends® in Computer Graphics and Vision,,,2022,2022,2022,2022,13,2-3,111-283,All OA, Gold,Article,"Wang, Yuanbo; Ahsan, Unaiza; Li, Hanyan; Hagen, Matthew","Wang, Yuanbo (Invitae Corporation, USA); Ahsan, Unaiza (The Home Depot, USA); Li, Hanyan (Indeed Inc., USA); Hagen, Matthew (Amazon.com, Inc., USA)",,"Wang, Yuanbo (); Ahsan, Unaiza (Home Depot (United States)); Li, Hanyan (); Hagen, Matthew (Amazon (United States))",2,2,,,https://doi.org/10.1561/0600000097,https://app.dimensions.ai/details/publication/pub.1151604270,"46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
267,pub.1121615375,10.1007/978-3-030-32486-5,,,"Artificial Intelligence in Radiation Therapy, First International Workshop, AIRT 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 17, 2019, Proceedings","This book constitutes the refereed proceedings of the First International Workshop on Connectomics in Artificial Intelligence in Radiation Therapy, AIRT 2019, held in conjunction with MICCAI 2019 in Shenzhen, China, in October 2019. The 20 full papers presented were carefully reviewed and selected from 24 submissions. The papers discuss the state of radiation therapy, the state of AI and related technologies, and hope to find a pathway to revolutionizing the field to ultimately improve cancer patient outcome and quality of life.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11850,,,All OA, Green,Edited Book,,,,,2,2,,,https://link.springer.com/content/pdf/bfm:978-3-030-32486-5/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1121615375,46 Information and Computing Sciences,3 Good Health and Well Being,,,,,,,,,,
258,pub.1111444832,10.1201/9780429434334,,,"Prostate Cancer Imaging, An Engineering and Clinical Perspective",,,,,,,2018-10-31,2018,2018-10-31,,,,,Closed,Edited Book,,,,,1,0,,,,https://app.dimensions.ai/details/publication/pub.1111444832,,,,,,,,,,,,,
252,pub.1138793611,10.1007/978-981-16-1550-4,,,"Emerging Technologies for Smart Cities, Select Proceedings of EGTET 2020","This book comprises the select proceedings of the International Conference on Emerging Global Trends in Engineering and Technology (EGTET 2020), held in Guwahati, India. The chapters in this book focus on the latest cleaner, greener, and efficient technologies being developed for the implementation of smart cities across the world. The broader topical sections include Smart Buildings, Infrastructures and Disaster Management; Smart Governance; Technologies for Smart Cities, and Wireless Connectivity for Smart Cities. This book will cater to students, researchers, industry professionals, and policy making bodies interested and involved in the planning and implementation of smart city projects.",,,Lecture Notes in Electrical Engineering,,,2021,2021,,2021,765,,,All OA, Green,Edited Book,,,,,0,0,,0.0,https://link.springer.com/content/pdf/bfm%3A978-981-16-1550-4%2F1,https://app.dimensions.ai/details/publication/pub.1138793611,46 Information and Computing Sciences, 48 Law and Legal Studies, 4802 Environmental and Resources Law,11 Sustainable Cities and Communities,,,,,,,,
251,pub.1149212892,10.1201/9781003240037,,,"Swarm Intelligence and Machine Learning, Applications in Healthcare","Today the healthcare sector is facing challenges such as detecting the cause of ailments, disease prevention, high operating costs, availability of skilled technicians and infrastructure bottlenecks. Intelligent healthcare management technologies are needed to manage these challenges. Healthcare organizations also need to continuously discover useful and actionable knowledge to gain insight from tons of data being generated for saving lives, reducing medical errors, enhancing efficiency, reducing costs and making the whole world a healthy place. The book introduces techniques that developed using machine learning along with swarm intelligence in healthcare informatics. It also discusses one of the major applications of artificial intelligence: using machine learning to extract useful information from multimodal data optimally by using swarm intelligence. It reviews optimization methods that help to minimize the error in developing patterns and classifications, which further helps improve prediction and decision-making. The objective of this book is to use swarm intelligence and machine learning techniques for various medical issues such as diagnosing cancer, brain tumor, diabetic retinopathy, heart diseases as well as drug design and development. The book will act as one-stop reference to think and explore swarm intelligence and machine learning algorithms seriously for real-time patient diagnosis. It will help future research be done in healthcare. It also explores the recent progress in several computing technologies and evaluates the performance based on today's betterment compared to previous development in the technology.",,,,,,2022-07-05,2022,2022-07-05,,,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1149212892,42 Health Sciences, 4203 Health Services and Systems, 46 Information and Computing Sciences, 4602 Artificial Intelligence,3 Good Health and Well Being,,,,,,,,
251,pub.1122554490,10.1007/978-3-030-35817-4,,,"Graph Learning in Medical Imaging, First International Workshop, GLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 17, 2019, Proceedings","This book constitutes the refereed proceedings of the First International Workshop on Graph Learning in Medical Imaging, GLMI 2019, held in conjunction with MICCAI 2019 in Shenzhen, China, in October 2019. The 21 full papers presented were carefully reviewed and selected from 42 submissions. The papers focus on major trends and challenges of graph learning in medical imaging and present original work aimed to identify new cutting-edge techniques and their applications in medical imaging.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11849,,,All OA, Green,Edited Book,,,,,2,2,,,https://link.springer.com/content/pdf/bfm:978-3-030-35817-4/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1122554490,46 Information and Computing Sciences,,,,,,,,,,,
226,pub.1151694516,10.1007/978-3-031-18523-6,,,"Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health, Third MICCAI Workshop, DeCaF 2022, and Second MICCAI Workshop, FAIR 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18 and 22, 2022, Proceedings","This book constitutes the refereed proceedings of the Third MICCAI Workshop on Distributed, Collaborative, and Federated Learning, DeCaF 2022, and the Second MICCAI Workshop on Affordable AI and Healthcare, FAIR 2022, held in conjunction with MICCAI 2022, in Singapore in September 2022. FAIR 2022 was held as a hybrid event. DeCaF 2022 accepted 14 papers from the 18 submissions received. The workshop aims at creating a scientific discussion focusing on the comparison, evaluation, and discussion of methodological advancement and practical ideas about machine learning applied to problems where data cannot be stored in centralized databases or where information privacy is a priority. For FAIR 2022, 4 papers from 9 submissions were accepted for publication. The topics of the accepted submissions focus on deep ultrasound segmentation, portable OCT image quality enhancement, self-attention deep networks and knowledge distillation in low-regime setting.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13573,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1151694516,46 Information and Computing Sciences, 4606 Distributed Computing and Systems Software,3 Good Health and Well Being,,,,,,,,,,
196,pub.1141326795,10.1007/978-3-030-87199-4,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2021, 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III","The eight-volume set LNCS 12901, 12902, 12903, 12904, 12905, 12906, 12907, and 12908 constitutes the refereed proceedings of the 24th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2021, held in Strasbourg, France, in September/October 2021.* The 531 revised full papers presented were carefully reviewed and selected from 1630 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: image segmentation Part II: machine learning - self-supervised learning; machine learning - semi-supervised learning; and machine learning - weakly supervised learning Part III: machine learning - advances in machine learning theory; machine learning - attention models; machine learning - domain adaptation; machine learning - federated learning; machine learning - interpretability / explainability; and machine learning - uncertainty Part IV: image registration; image-guided interventions and surgery; surgical data science; surgical planning and simulation; surgical skill and work flow analysis; and surgical visualization and mixed, augmented and virtual reality Part V: computer aided diagnosis; integration of imaging with non-imaging biomarkers; and outcome/disease prediction Part VI: image reconstruction; clinical applications - cardiac; and clinical applications - vascular Part VII: clinical applications - abdomen; clinical applications - breast; clinical applications - dermatology; clinical applications - fetal imaging; clinical applications - lung; clinical applications - neuroimaging - brain development; clinical applications - neuroimaging - DWI and tractography; clinical applications - neuroimaging - functional brain networks; clinical applications - neuroimaging – others; and clinical applications - oncology Part VIII: clinical applications - ophthalmology; computational (integrative) pathology; modalities - microscopy; modalities - histopathology; and modalities - ultrasound *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12903,,,All OA, Green,Edited Book,,,,,5,5,,4.09,https://link.springer.com/content/pdf/bfm%3A978-3-030-87199-4%2F1,https://app.dimensions.ai/details/publication/pub.1141326795,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
188,pub.1135198814,10.1038/nde/15487091/2021/18/2,,,Nature Methods,,,,Nature Methods,,,2021,2021,,,18,2,,Closed,Article,,,,,1,1,,,,https://app.dimensions.ai/details/publication/pub.1135198814,31 Biological Sciences,,,,,,,,,,,,
178,pub.1151032978,10.1007/978-3-031-16443-9,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2022, 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part V","The eight-volume set LNCS 13431, 13432, 13433, 13434, 13435, 13436, 13437, and 13438 constitutes the refereed proceedings of the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2022, which was held in Singapore in September 2022. The 574 revised full papers presented were carefully reviewed and selected from 1831 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: Brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; heart and lung imaging; dermatology; Part II: Computational (integrative) pathology; computational anatomy and physiology; ophthalmology; fetal imaging; Part III: Breast imaging; colonoscopy; computer aided diagnosis; Part IV: Microscopic image analysis; positron emission tomography; ultrasound imaging; video data analysis; image segmentation I; Part V: Image segmentation II; integration of imaging with non-imaging biomarkers; Part VI: Image registration; image reconstruction; Part VII: Image-Guided interventions and surgery; outcome and disease prediction; surgical data science; surgical planning and simulation; machine learning – domain adaptation and generalization; Part VIII: Machine learning – weakly-supervised learning; machine learning – model interpretation; machine learning – uncertainty; machine learning theory and methodologies.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13435,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16443-9%2F1,https://app.dimensions.ai/details/publication/pub.1151032978,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
178,pub.1109706443,10.1007/978-3-030-00937-3,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part IV","The four-volume set LNCS 11070, 11071, 11072, and 11073 constitutes the refereed proceedings of the 21st International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2018, held in Granada, Spain, in September 2018. The 373 revised full papers presented were carefully reviewed and selected from 1068 submissions in a double-blind review process. The papers have been organized in the following topical sections: Part I: Image Quality and Artefacts; Image Reconstruction Methods; Machine Learning in Medical Imaging; Statistical Analysis for Medical Imaging; Image Registration Methods. Part II: Optical and Histology Applications: Optical Imaging Applications; Histology Applications; Microscopy Applications; Optical Coherence Tomography and Other Optical Imaging Applications. Cardiac, Chest and Abdominal Applications: Cardiac Imaging Applications: Colorectal, Kidney and Liver Imaging Applications; Lung Imaging Applications; Breast Imaging Applications; Other Abdominal Applications. Part III: Diffusion Tensor Imaging and Functional MRI: Diffusion Tensor Imaging; Diffusion Weighted Imaging; Functional MRI; Human Connectome. Neuroimaging and Brain Segmentation Methods: Neuroimaging; Brain Segmentation Methods. Part IV: Computer Assisted Intervention: Image Guided Interventions and Surgery; Surgical Planning, Simulation and Work Flow Analysis; Visualization and Augmented Reality. Image Segmentation Methods: General Image Segmentation Methods, Measures and Applications; Multi-Organ Segmentation; Abdominal Segmentation Methods; Cardiac Segmentation Methods; Chest, Lung and Spine Segmentation; Other Segmentation Applications.",,,Lecture Notes in Computer Science,,,2018,2018,,2018,11073,,,Closed,Edited Book,,,,,20,13,,,,https://app.dimensions.ai/details/publication/pub.1109706443,46 Information and Computing Sciences,,,,,,,,,,,,
176,pub.1141327107,10.1007/978-3-030-87722-4,,,"Domain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health, Third MICCAI Workshop, DART 2021, and First MICCAI Workshop, FAIR 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27 and October 1, 2021, Proceedings","This book constitutes the refereed proceedings of the Third MICCAI Workshop on Domain Adaptation and Representation Transfer, DART 2021, and the First MICCAI Workshop on Affordable Healthcare and AI for Resource Diverse Global Health, FAIR 2021, held in conjunction with MICCAI 2021, in September/October 2021. The workshops were planned to take place in Strasbourg, France, but were held virtually due to the COVID-19 pandemic. DART 2021 accepted 13 papers from the 21 submissions received. The workshop aims at creating a discussion forum to compare, evaluate, and discuss methodological advancements and ideas that can improve the applicability of machine learning (ML)/deep learning (DL) approaches to clinical setting by making them robust and consistent across different domains. For FAIR 2021, 10 papers from 17 submissions were accepted for publication. They focus on Image-to-Image Translation particularly for low-dose or low-resolution settings; Model Compactness and Compression; Domain Adaptation and Transfer Learning; Active, Continual and Meta-Learning.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12968,,,All OA, Green,Edited Book,,,,,2,2,,1.64,https://link.springer.com/content/pdf/bfm%3A978-3-030-87722-4%2F1,https://app.dimensions.ai/details/publication/pub.1141327107,46 Information and Computing Sciences, 4611 Machine Learning,3 Good Health and Well Being,,,,,,,,,
143,pub.1138729672,10.48550/arxiv.2106.04381,,,Computer-Assisted Analysis of Biomedical Images,"Nowadays, the amount of heterogeneous biomedical data is increasing more and
more thanks to novel sensing techniques and high-throughput technologies. In
reference to biomedical image analysis, the advances in image acquisition
modalities and high-throughput imaging experiments are creating new challenges.
This huge information ensemble could overwhelm the analytic capabilities needed
by physicians in their daily decision-making tasks as well as by biologists
investigating complex biochemical systems. In particular, quantitative imaging
methods convey scientifically and clinically relevant information in
prediction, prognosis or treatment response assessment, by also considering
radiomics approaches. Therefore, the computational analysis of medical and
biological images plays a key role in radiology and laboratory applications. In
this regard, frameworks based on advanced Machine Learning and Computational
Intelligence can significantly improve traditional Image Processing and Pattern
Recognition approaches. However, conventional Artificial Intelligence
techniques must be tailored to address the unique challenges concerning
biomedical imaging data. This thesis aims at proposing novel and advanced
computer-assisted methods for biomedical image analysis, also as an instrument
in the development of Clinical Decision Support Systems, by always keeping in
mind the clinical feasibility of the developed solutions. In conclusion, the
ultimate goal of these research studies is to gain clinically and biologically
useful insights that can guide differential diagnosis and therapies, leading
towards biomedical data integration for personalized medicine. As a matter of
fact, the proposed computer-assisted bioimage analysis methods can be
beneficial for the definition of imaging biomarkers, as well as for
quantitative medicine and biology.",,,arXiv,,,2021-06-04,2021,,,,,,All OA, Green,Preprint,"Rundo, Leonardo","Rundo, Leonardo ()",,"Rundo, Leonardo ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1138729672,46 Information and Computing Sciences, 4605 Data Management and Data Science,3 Good Health and Well Being,,,,,,,,,
134,pub.1091382400,10.1007/978-3-319-56904-8,,,Multidisciplinary Approaches to Neural Computing,"This book presents a collection of contributions in the field of Artificial Neural Networks (ANNs). The themes addressed are multidisciplinary in nature, and closely connected in their ultimate aim to identify features from dynamic realistic signal exchanges and invariant machine representations that can be exploited to improve the quality of life of their end users. Mathematical tools like ANNs are currently exploited in many scientific domains because of their solid theoretical background and effectiveness in providing solutions to many demanding tasks such as appropriately processing (both for extracting features and recognizing) mono- and bi-dimensional dynamic signals, solving strong nonlinearities in the data and providing general solutions for deep and fully connected architectures. Given the multidisciplinary nature of their use and the interdisciplinary characterization of the problems they are applied to – which range from medicine to psychology, industrial and social robotics, computer vision, and signal processing (among many others) – ANNs may provide a basis for redefining the concept of information processing. These reflections are supported by theoretical models and applications presented in the chapters of this book. This book is of primary importance for: (a) the academic research community, (b) the ICT market, (c) PhD students and early-stage researchers, (d) schools, hospitals, rehabilitation and assisted-living centers, and (e) representatives of multimedia industries and standardization bodies.",,,"Smart Innovation, Systems and Technologies",,,2018,2018,,2018,69,,,All OA, Green,Edited Book,,,,,4,3,,1.26,https://link.springer.com/content/pdf/bfm:978-3-319-56904-8/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1091382400,40 Engineering, 46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,
134,pub.1139631360,10.1007/978-3-030-69951-2,,,"Advances in Artificial Intelligence, Computation, and Data Science, For Medicine and Life Science","Artificial intelligence (AI) has become pervasive in most areas of research and applications. While computation can significantly reduce mental efforts for complex problem solving, effective computer algorithms allow continuous improvement of AI tools to handle complexity—in both time and memory requirements—for machine learning in large datasets. Meanwhile, data science is an evolving scientific discipline that strives to overcome the hindrance of traditional skills that are too limited to enable scientific discovery when leveraging research outcomes. Solutions to many problems in medicine and life science, which cannot be answered by these conventional approaches, are urgently needed for society. This edited book attempts to report recent advances in the complementary domains of AI, computation, and data science with applications in medicine and life science. The benefits to the reader are manifold as researchers from similar or different fields can be aware of advanced developments and novel applications that can be useful for either immediate implementations or future scientific pursuit. Features: Considers recent advances in AI, computation, and data science for solving complex problems in medicine, physiology, biology, chemistry, and biochemistry Provides recent developments in three evolving key areas and their complementary combinations: AI, computation, and data science Reports on applications in medicine and physiology, including cancer, neuroscience, and digital pathology Examines applications in life science, including systems biology, biochemistry, and even food technology This unique book, representing research from a team of international contributors, has not only real utility in academia for those in the medical and life sciences communities, but also a much wider readership from industry, science, and other areas of technology and education. Tuan D. Pham is professor and founding director of the Center for Artificial Intelligence at Prince Mohammad Bin Fahd University, Saudi Arabia. Hong Yan is currently chair professor of computer engineering at City University of Hong Kong. Dr. Muhammad Waqar Ashraf is professor and dean of College of Sciences & Human Studies at Prince Mohammad Bin Fahd University. Folke Sjöberg is professor of burn surgery and critical care at Linköping University, Sweden, and director of the Burn Center at the Linköping University Hospital.",,,Computational Biology,,,2021,2021,,2021,31,,,All OA, Green,Edited Book,,,,,3,3,,2.49,https://link.springer.com/content/pdf/bfm%3A978-3-030-69951-2%2F1,https://app.dimensions.ai/details/publication/pub.1139631360,46 Information and Computing Sciences, 4602 Artificial Intelligence,4 Quality Education,,,,,,,,,
134,pub.1141326734,10.1007/978-3-030-87196-3,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2021, 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II","The eight-volume set LNCS 12901, 12902, 12903, 12904, 12905, 12906, 12907, and 12908 constitutes the refereed proceedings of the 24th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2021, held in Strasbourg, France, in September/October 2021.* The 531 revised full papers presented were carefully reviewed and selected from 1630 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: image segmentation Part II: machine learning - self-supervised learning; machine learning - semi-supervised learning; and machine learning - weakly supervised learning Part III: machine learning - advances in machine learning theory; machine learning - attention models; machine learning - domain adaptation; machine learning - federated learning; machine learning - interpretability / explainability; and machine learning - uncertainty Part IV: image registration; image-guided interventions and surgery; surgical data science; surgical planning and simulation; surgical skill and work flow analysis; and surgical visualization and mixed, augmented and virtual reality Part V: computer aided diagnosis; integration of imaging with non-imaging biomarkers; and outcome/disease prediction Part VI: image reconstruction; clinical applications - cardiac; and clinical applications - vascular Part VII: clinical applications - abdomen; clinical applications - breast; clinical applications - dermatology; clinical applications - fetal imaging; clinical applications - lung; clinical applications - neuroimaging - brain development; clinical applications - neuroimaging - DWI and tractography; clinical applications - neuroimaging - functional brain networks; clinical applications - neuroimaging – others; and clinical applications - oncology Part VIII: clinical applications - ophthalmology; computational (integrative) pathology; modalities - microscopy; modalities - histopathology; and modalities - ultrasound *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12902,,,All OA, Green,Edited Book,,,,,3,3,,2.46,https://link.springer.com/content/pdf/bfm%3A978-3-030-87196-3%2F1,https://app.dimensions.ai/details/publication/pub.1141326734,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
133,pub.1051847078,10.1007/978-3-319-24888-2,,,"Machine Learning in Medical Imaging, 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings","This book constitutes the proceedings of the 6th International Workshop on Machine Learning in Medical Imaging, MLMI 2015, held in conjunction with MICCAI 2015, in Munich in October 2015. The 40 full papers presented in this volume were carefully reviewed and selected from 69 submissions. The workshop focuses on major trends and challenges in the area of machine learning in medical imaging and present works aimed to identify new cutting-edge techniques and their use in medical imaging.",,,Lecture Notes in Computer Science,,,2015,2015,,2015,9352,,,Closed,Edited Book,,,,,8,4,,,,https://app.dimensions.ai/details/publication/pub.1051847078,46 Information and Computing Sciences,,,,,,,,,,,,
121,pub.1141301941,10.1007/978-3-030-87193-2,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2021, 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I","The eight-volume set LNCS 12901, 12902, 12903, 12904, 12905, 12906, 12907, and 12908 constitutes the refereed proceedings of the 24th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2021, held in Strasbourg, France, in September/October 2021.* The 531 revised full papers presented were carefully reviewed and selected from 1630 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: image segmentation Part II: machine learning - self-supervised learning; machine learning - semi-supervised learning; and machine learning - weakly supervised learning Part III: machine learning - advances in machine learning theory; machine learning - attention models; machine learning - domain adaptation; machine learning - federated learning; machine learning - interpretability / explainability; and machine learning - uncertainty Part IV: image registration; image-guided interventions and surgery; surgical data science; surgical planning and simulation; surgical skill and work flow analysis; and surgical visualization and mixed, augmented and virtual reality Part V: computer aided diagnosis; integration of imaging with non-imaging biomarkers; and outcome/disease prediction Part VI: image reconstruction; clinical applications - cardiac; and clinical applications - vascular Part VII: clinical applications - abdomen; clinical applications - breast; clinical applications - dermatology; clinical applications - fetal imaging; clinical applications - lung; clinical applications - neuroimaging - brain development; clinical applications - neuroimaging - DWI and tractography; clinical applications - neuroimaging - functional brain networks; clinical applications - neuroimaging – others; and clinical applications - oncology Part VIII: clinical applications - ophthalmology; computational (integrative) pathology; modalities - microscopy; modalities - histopathology; and modalities - ultrasound *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12901,,,All OA, Green,Edited Book,,,,,4,4,,3.27,https://link.springer.com/content/pdf/bfm%3A978-3-030-87193-2%2F1,https://app.dimensions.ai/details/publication/pub.1141301941,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
120,pub.1125948321,10.1007/978-3-030-40977-7,,,Applications of Hybrid Metaheuristic Algorithms for Image Processing,"This book presents a collection of the most recent hybrid methods for image processing. The algorithms included consider evolutionary, swarm, machine learning and deep learning. The respective chapters explore different areas of image processing, from image segmentation to the recognition of objects using complex approaches and medical applications. The book also discusses the theory of the methodologies used to provide an overview of the applications of these tools in image processing. The book is primarily intended for undergraduate and postgraduate students of science, engineering and computational mathematics, and can also be used for courses on artificial intelligence, advanced image processing, and computational intelligence. Further, it is a valuable resource for researchers from the evolutionary computation, artificial intelligence and image processing communities.",,,Studies in Computational Intelligence,,,2020,2020,,2020,890,,,Closed,Edited Book,,,,,3,2,,1.52,,https://app.dimensions.ai/details/publication/pub.1125948321,46 Information and Computing Sciences, 4602 Artificial Intelligence,,,,,,,,,,,
119,pub.1109700333,10.1007/978-3-030-00919-9,,,"Machine Learning in Medical Imaging, 9th International Workshop, MLMI 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings","This book constitutes the proceedings of the 9th International Workshop on Machine Learning in Medical Imaging, MLMI 2018, held in conjunction with MICCAI 2018 in Granada, Spain, in September 2018. The 45 papers presented in this volume were carefully reviewed and selected from 82 submissions. They focus on major trends and challenges in the area of machine learning in medical imaging and aim to identify new cutting-edge techniques and their use in medical imaging.",,,Lecture Notes in Computer Science,,,2018,2018,,2018,11046,,,Closed,Edited Book,,,,,9,3,,,,https://app.dimensions.ai/details/publication/pub.1109700333,46 Information and Computing Sciences,,,,,,,,,,,,
108,pub.1129070467,10.1007/978-3-030-52791-4,,,"Medical Image Understanding and Analysis, 24th Annual Conference, MIUA 2020, Oxford, UK, July 15-17, 2020, Proceedings","This book constitutes the refereed proceedings of the 24th Conference on Medical Image Understanding and Analysis, MIUA 2020, held in July 2020. Due to COVID-19 pandemic the conference was held virtually. The 29 full papers and 5 short papers presented were carefully reviewed and selected from 70 submissions. They were organized according to following topical sections: image segmentation; image registration, reconstruction and enhancement; radiomics, predictive models, and quantitative imaging biomarkers; ocular imaging analysis; biomedical simulation and modelling.",,,Communications in Computer and Information Science,,,2020,2020,,2020,1248,,,All OA, Green,Edited Book,,,,,2,2,,5.75,https://link.springer.com/content/pdf/bfm:978-3-030-52791-4/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1129070467,48 Law and Legal Studies, 4806 Private Law and Civil Obligations,,,,,,,,,,
99,pub.1124287610,10.1007/978-3-030-39343-4,,,"Medical Image Understanding and Analysis, 23rd Conference, MIUA 2019, Liverpool, UK, July 24–26, 2019, Proceedings","This book constitutes the refereed proceedings of the 23rd Conference on Medical Image Understanding and Analysis, MIUA 2019, held in Liverpool, UK, in July 2019. The 43 full papers presented were carefully reviewed and selected from 70 submissions. There were organized in topical sections named: oncology and tumour imaging; lesion, wound and ulcer analysis; biostatistics; fetal imaging; enhancement and reconstruction; diagnosis, classification and treatment; vessel and nerve analysis; image registration; image segmentation; ophthalmic imaging; and posters.",,,Communications in Computer and Information Science,,,2020,2020,,2020,1065,,,Closed,Edited Book,,,,,2,1,,0.74,,https://app.dimensions.ai/details/publication/pub.1124287610,32 Biomedical and Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,,
99,pub.1121091027,10.1007/978-981-13-8950-4,,,Neural Approaches to Dynamics of Signal Exchanges,"The book presents research that contributes to the development of intelligent dialog systems to simplify diverse aspects of everyday life, such as medical diagnosis and entertainment. Covering major thematic areas: machine learning and artificial neural networks; algorithms and models; and social and biometric data for applications in human–computer interfaces, it discusses processing of audio-visual signals for the detection of user-perceived states, the latest scientific discoveries in processing verbal (lexicon, syntax, and pragmatics), auditory (voice, intonation, vocal expressions) and visual signals (gestures, body language, facial expressions), as well as algorithms for detecting communication disorders, remote health-status monitoring, sentiment and affect analysis, social behaviors and engagement. Further, it examines neural and machine learning algorithms for the implementation of advanced telecommunication systems, communication with people with special needs, emotion modulation by computer contents, advanced sensors for tracking changes in real-life and automatic systems, as well as the development of advanced human–computer interfaces. The book does not focus on solving a particular problem, but instead describes the results of research that has positive effects in different fields and applications.",,,"Smart Innovation, Systems and Technologies",,,2020,2020,,2020,151,,,All OA, Green,Edited Book,,,,,7,4,,3.69,https://link.springer.com/content/pdf/bfm:978-981-13-8950-4/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1121091027,46 Information and Computing Sciences, 4608 Human-Centred Computing,,,,,,,,,,
90,pub.1148654954,10.1007/978-3-031-08223-8,,,"Engineering Applications of Neural Networks, 23rd International Conference, EAAAI/EANN 2022, Chersonissos, Crete, Greece, June 17–20, 2022, Proceedings","This book constitutes the refereed proceedings of the 23rd International Conference on Engineering Applications of Neural Networks, EANN 2022, held in Chersonisos, Crete, Greece, in June 2022. The 37 revised full papers and 5 revised short papers presented were carefully reviewed and selected from 72 submissions. The papers are organized in topical sections on Bio inspired Modeling / Novel Neural Architectures; Classification / Clustering; Machine Learning; Convolutional / Deep Learning; Datamining / Learning / Autoencoders; Deep Learning / Blockchain; Machine Learning for Medical Images / Genome Classification; Reinforcement /Adversarial / Echo State Neural Networks; Robotics / Autonomous Vehicles, Photonic Neural Networks; Text Classification / Natural Language.",,,Communications in Computer and Information Science,,,2022,2022,,2022,1600,,,All OA, Green,Edited Book,,,,,1,1,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-08223-8%2F1,https://app.dimensions.ai/details/publication/pub.1148654954,46 Information and Computing Sciences, 4602 Artificial Intelligence, 4611 Machine Learning,,,,,,,,,
73,pub.1090610744,10.1007/978-3-319-59876-5,,,"Image Analysis and Recognition, 14th International Conference, ICIAR 2017, Montreal, QC, Canada, July 5–7, 2017, Proceedings","This book constitutes the thoroughly refereed proceedings of the 14th International Conference on Image Analysis and Recognition, ICIAR 2017, held in Montreal, QC, Canada, in July 2017. The 73 revised full papers presented were carefully reviewed and selected from 133 submissions. The papers are organized in the following topical sections: machine learning in image recognition; machine learning for medical image computing; image enhancement and reconstruction; image segmentation; motion and tracking; 3D computer vision; feature extraction; detection and classification; biomedical image analysis; image analysis in ophthalmology; remote sensing; applications.",,,Lecture Notes in Computer Science,,,2017,2017,,2017,10317,,,All OA, Green,Edited Book,,,,,13,4,,3.74,https://link.springer.com/content/pdf/bfm%3A978-3-319-59876-5%2F1,https://app.dimensions.ai/details/publication/pub.1090610744,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
73,pub.1084915440,10.1007/978-3-319-46726-9,,,"Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016, 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part III","The three-volume set LNCS 9900, 9901, and 9902 constitutes the refereed proceedings of the 19th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2016, held in Athens, Greece, in October 2016. Based on rigorous peer reviews, the program committee carefully selected 228 revised regular papers from 756 submissions for presentation in three volumes. The papers have been organized in the following topical sections: Part I: brain analysis, brain analysis - connectivity; brain analysis - cortical morphology; Alzheimer disease; surgical guidance and tracking; computer aided interventions; ultrasound image analysis; cancer image analysis; Part II: machine learning and feature selection; deep learning in medical imaging; applications of machine learning; segmentation; cell image analysis; Part III: registration and deformation estimation; shape modeling; cardiac and vascular image analysis; image reconstruction; and MR image analysis.",,,Lecture Notes in Computer Science,,,2016,2016,,2016,9902,,,Closed,Edited Book,,,,,22,5,,,,https://app.dimensions.ai/details/publication/pub.1084915440,46 Information and Computing Sciences,,,,,,,,,,,,
69,pub.1122947450,10.1007/978-3-030-34110-7,,,"Image and Graphics, 10th International Conference, ICIG 2019, Beijing, China, August 23–25, 2019, Proceedings, Part II","This three-volume set LNCS 11901, 11902, and 11903 constitutes the refereed conference proceedings of the 10thth International Conference on Image and Graphics, ICIG 2019, held in Beijing, China, in August 2019. The 183 full papers presented were selected from 384 submissions and focus on advances of theory, techniques and algorithms as well as innovative technologies of image, video and graphics processing and fostering innovation, entrepreneurship, and networking.",,,Lecture Notes in Computer Science,,,2019,2019,,2019,11902,,,All OA, Green,Edited Book,,,,,1,1,,,https://link.springer.com/content/pdf/bfm%3A978-3-030-34110-7%2F1,https://app.dimensions.ai/details/publication/pub.1122947450,46 Information and Computing Sciences,,,,,,,,,,,
61,pub.1150793449,10.1007/978-3-031-15937-4,,,"Artificial Neural Networks and Machine Learning – ICANN 2022, 31st International Conference on Artificial Neural Networks, Bristol, UK, September 6–9, 2022, Proceedings; Part IV","The 4-volumes set of LNCS 13529, 13530, 13531, and 13532 constitutes the proceedings of the 31st International Conference on Artificial Neural Networks, ICANN 2022, held in Bristol, UK, in September 2022. The total of 255 full papers presented in these proceedings was carefully reviewed and selected from 561 submissions. ICANN 2022 is a dual-track conference featuring tracks in brain inspired computing and machine learning and artificial neural networks, with strong cross-disciplinary interactions and applications.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13532,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-15937-4%2F1,https://app.dimensions.ai/details/publication/pub.1150793449,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
61,pub.1131394427,10.1007/978-3-030-59713-9,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2020, 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part II","The seven-volume set LNCS 12261, 12262, 12263, 12264, 12265, 12266, and 12267 constitutes the refereed proceedings of the 23rd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2020, held in Lima, Peru, in October 2020. The conference was held virtually due to the COVID-19 pandemic. The 542 revised full papers presented were carefully reviewed and selected from 1809 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: machine learning methodologies Part II: image reconstruction; prediction and diagnosis; cross-domain methods and reconstruction; domain adaptation; machine learning applications; generative adversarial networks Part III: CAI applications; image registration; instrumentation and surgical phase detection; navigation and visualization; ultrasound imaging; video image analysis Part IV: segmentation; shape models and landmark detection Part V: biological, optical, microscopic imaging; cell segmentation and stain normalization; histopathology image analysis; opthalmology Part VI: angiography and vessel analysis; breast imaging; colonoscopy; dermatology; fetal imaging; heart and lung imaging; musculoskeletal imaging Part VI: brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; positron emission tomography",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12262,,,All OA, Green,Edited Book,,,,,4,4,,2.06,https://link.springer.com/content/pdf/bfm:978-3-030-59713-9/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1131394427,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
61,pub.1135776660,10.1007/978-3-030-69525-5,,,"Computer Vision – ACCV 2020, 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part I","The six volume set of LNCS 12622-12627 constitutes the proceedings of the 15th Asian Conference on Computer Vision, ACCV 2020, held in Kyoto, Japan, in November/ December 2020.* The total of 254 contributions was carefully reviewed and selected from 768 submissions during two rounds of reviewing and improvement. The papers focus on the following topics: Part I: 3D computer vision; segmentation and grouping Part II: low-level vision, image processing; motion and tracking Part III: recognition and detection; optimization, statistical methods, and learning; robot vision Part IV: deep learning for computer vision, generative models for computer vision Part V: face, pose, action, and gesture; video analysis and event recognition; biomedical image analysis Part VI: applications of computer vision; vision for X; datasets and performance analysis *The conference was held virtually.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12622,,,Closed,Edited Book,,,,,0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1135776660,46 Information and Computing Sciences, 4608 Human-Centred Computing,,,,,,,,,,,
61,pub.1151072692,10.1007/978-3-031-16449-1,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2022, 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part VII","The eight-volume set LNCS 13431, 13432, 13433, 13434, 13435, 13436, 13437, and 13438 constitutes the refereed proceedings of the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2022, which was held in Singapore in September 2022. The 574 revised full papers presented were carefully reviewed and selected from 1831 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: Brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; heart and lung imaging; dermatology; Part II: Computational (integrative) pathology; computational anatomy and physiology; ophthalmology; fetal imaging; Part III: Breast imaging; colonoscopy; computer aided diagnosis; Part IV: Microscopic image analysis; positron emission tomography; ultrasound imaging; video data analysis; image segmentation I; Part V: Image segmentation II; integration of imaging with non-imaging biomarkers; Part VI: Image registration; image reconstruction; Part VII: Image-Guided interventions and surgery; outcome and disease prediction; surgical data science; surgical planning and simulation; machine learning – domain adaptation and generalization; Part VIII: Machine learning – weakly-supervised learning; machine learning – model interpretation; machine learning – uncertainty; machine learning theory and methodologies.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13437,,,All OA, Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16449-1%2F1,https://app.dimensions.ai/details/publication/pub.1151072692,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
55,pub.1108489393,10.1007/978-3-030-00928-1,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I","The four-volume set LNCS 11070, 11071, 11072, and 11073 constitutes the refereed proceedings of the 21st International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2018, held in Granada, Spain, in September 2018. The 373 revised full papers presented were carefully reviewed and selected from 1068 submissions in a double-blind review process. The papers have been organized in the following topical sections: Part I: Image Quality and Artefacts; Image Reconstruction Methods; Machine Learning in Medical Imaging; Statistical Analysis for Medical Imaging; Image Registration Methods. Part II: Optical and Histology Applications: Optical Imaging Applications; Histology Applications; Microscopy Applications; Optical Coherence Tomography and Other Optical Imaging Applications. Cardiac, Chest and Abdominal Applications: Cardiac Imaging Applications: Colorectal, Kidney and Liver Imaging Applications; Lung Imaging Applications; Breast Imaging Applications; Other Abdominal Applications. Part III: Diffusion Tensor Imaging and Functional MRI: Diffusion Tensor Imaging; Diffusion Weighted Imaging; Functional MRI; Human Connectome. Neuroimaging and Brain Segmentation Methods: Neuroimaging; Brain Segmentation Methods. Part IV: Computer Assisted Intervention: Image Guided Interventions and Surgery; Surgical Planning, Simulation and Work Flow Analysis; Visualization and Augmented Reality. Image Segmentation Methods: General Image Segmentation Methods, Measures and Applications; Multi-Organ Segmentation; Abdominal Segmentation Methods; Cardiac Segmentation Methods; Chest, Lung and Spine Segmentation; Other Segmentation Applications.",,,Lecture Notes in Computer Science,,,2018,2018,,2018,11070,,,All OA, Green,Edited Book,,,,,11,6,,,https://link.springer.com/content/pdf/bfm:978-3-030-00928-1/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1108489393,46 Information and Computing Sciences,,,,,,,,,,,
55,pub.1037724307,10.1007/978-3-319-24574-4,,,"Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III","The three-volume set LNCS 9349, 9350, and 9351 constitutes the refereed proceedings of the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2015, held in Munich, Germany, in October 2015. Based on rigorous peer reviews, the program committee carefully selected 263 revised papers from 810 submissions for presentation in three volumes. The papers have been organized in the following topical sections: quantitative image analysis I: segmentation and measurement; computer-aided diagnosis: machine learning; computer-aided diagnosis: automation; quantitative image analysis II: classification, detection, features, and morphology; advanced MRI: diffusion, fMRI, DCE; quantitative image analysis III: motion, deformation, development and degeneration; quantitative image analysis IV: microscopy, fluorescence and histological imagery; registration: method and advanced applications; reconstruction, image formation, advanced acquisition - computational imaging; modelling and simulation for diagnosis and interventional planning; computer-assisted and image-guided interventions.",,,Lecture Notes in Computer Science,,,2015,2015,,2015,9351,,,Closed,Edited Book,,,,,295,156,,,,https://app.dimensions.ai/details/publication/pub.1037724307,46 Information and Computing Sciences,,,,,,,,,,,,
50,pub.1053080397,10.1007/978-3-319-10404-1,,,"Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014, 17th International Conference, Boston, MA, USA, September 14-18, 2014, Proceedings, Part I","The three-volume set LNCS 8673, 8674, and 8675 constitutes the refereed proceedings of the 17th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2014, held in Boston, MA, USA, in September 2014. Based on rigorous peer reviews, the program committee carefully selected 253 revised papers from 862 submissions for presentation in three volumes. The 100 papers included in the first volume have been organized in the following topical sections: microstructure imaging; image reconstruction and enhancement; registration; segmentation; intervention planning and guidance; oncology; and optical imaging.",,,Lecture Notes in Computer Science,,,2014,2014,,2014,8673,,,Closed,Edited Book,,,,,20,4,,,,https://app.dimensions.ai/details/publication/pub.1053080397,46 Information and Computing Sciences,,,,,,,,,,,,
6767,pub.1154790774,10.1007/s00330-023-09410-9,36690774,,Automated prostate multi-regional segmentation in magnetic resonance using fully convolutional neural networks,"Abstract
ObjectiveAutomatic MR imaging segmentation of the prostate provides relevant clinical benefits for prostate cancer evaluation such as calculation of automated PSA density and other critical imaging biomarkers. Further, automated T2-weighted image segmentation of central-transition zone (CZ-TZ), peripheral zone (PZ), and seminal vesicle (SV) can help to evaluate clinically significant cancer following the PI-RADS v2.1 guidelines. Therefore, the main objective of this work was to develop a robust and reproducible CNN-based automatic prostate multi-regional segmentation model using an intercontinental cohort of prostate MRI.MethodsA heterogeneous database of 243 T2-weighted prostate studies from 7 countries and 10 machines of 3 different vendors, with the CZ-TZ, PZ, and SV regions manually delineated by two experienced radiologists (ground truth), was used to train (n = 123) and test (n = 120) a U-Net-based model with deep supervision using a cyclical learning rate. The performance of the model was evaluated by means of dice similarity coefficient (DSC), among others. Segmentation results with a DSC above 0.7 were considered accurate.ResultsThe proposed method obtained a DSC of 0.88 ± 0.01, 0.85 ± 0.02, 0.72 ± 0.02, and 0.72 ± 0.02 for the prostate gland, CZ-TZ, PZ, and SV respectively in the 120 studies of the test set when comparing the predicted segmentations with the ground truth. No statistically significant differences were found in the results obtained between manufacturers or continents.ConclusionProstate multi-regional T2-weighted MR images automatic segmentation can be accurately achieved by U-Net like CNN, generalizable in a highly variable clinical environment with different equipment, acquisition configurations, and population.Key Points• Deep learning techniques allows the accurate segmentation of the prostate in three different regions on MR T2w images.• Multi-centric database proved the generalization of the CNN model on different institutions across different continents.• CNN models can be used to aid on the diagnosis and follow-up of patients with prostate cancer.",,The authors declare that this work has not received any funding.,European Radiology,,,2023-01-24,2023,2023-01-24,2023-01-24,,,1-10,Closed,Article,"Jimenez-Pastor, Ana; Lopez-Gonzalez, Rafael; Fos-Guarinos, Belén; Garcia-Castro, Fabio; Wittenberg, Mark; Torregrosa-Andrés, Asunción; Marti-Bonmati, Luis; Garcia-Fontes, Margarita; Duarte, Pablo; Gambini, Juan Pablo; Bittencourt, Leonardo Kayat; Kitamura, Felipe Campos; Venugopal, Vasantha Kumar; Mahajan, Vidur; Ros, Pablo; Soria-Olivas, Emilio; Alberich-Bayarri, Angel","Jimenez-Pastor, Ana (Quantitative Imaging Biomarkers in Medicine (Quibim S.L.), Aragon Avenue, 30, 13th floor, Office I-J, 46021, Valencia, Spain); Lopez-Gonzalez, Rafael (Quantitative Imaging Biomarkers in Medicine (Quibim S.L.), Aragon Avenue, 30, 13th floor, Office I-J, 46021, Valencia, Spain; Intelligent Data Analysis Laboratory (IDAL), ETSE, Universidad de Valencia, Valencia, Spain); Fos-Guarinos, Belén (Quantitative Imaging Biomarkers in Medicine (Quibim S.L.), Aragon Avenue, 30, 13th floor, Office I-J, 46021, Valencia, Spain); Garcia-Castro, Fabio (Quantitative Imaging Biomarkers in Medicine (Quibim S.L.), Aragon Avenue, 30, 13th floor, Office I-J, 46021, Valencia, Spain); Wittenberg, Mark (Quantitative Imaging Biomarkers in Medicine (Quibim S.L.), Aragon Avenue, 30, 13th floor, Office I-J, 46021, Valencia, Spain); Torregrosa-Andrés, Asunción (Department of Radiology and GIBI230 Research Group On Biomedical Imaging, Hospital Universitario Y Politécnico de La Fe and Instituto de Investigación Sanitaria La Fe, Valencia, Spain); Marti-Bonmati, Luis (Department of Radiology and GIBI230 Research Group On Biomedical Imaging, Hospital Universitario Y Politécnico de La Fe and Instituto de Investigación Sanitaria La Fe, Valencia, Spain); Garcia-Fontes, Margarita (Centro Uruguayo de Imagenologia Molecular (CUDIM), Montevideo, Uruguay); Duarte, Pablo (Centro Uruguayo de Imagenologia Molecular (CUDIM), Montevideo, Uruguay); Gambini, Juan Pablo (Centro Uruguayo de Imagenologia Molecular (CUDIM), Montevideo, Uruguay); Bittencourt, Leonardo Kayat (University Hospitals Cleveland Medical Center, Cleveland, USA; DasaInova, Dasa, Unifesp, São Paulo, Brazil); Kitamura, Felipe Campos (DasaInova, Dasa, Unifesp, São Paulo, Brazil); Venugopal, Vasantha Kumar (Mahajan Imaging, New Delhi, India); Mahajan, Vidur (Mahajan Imaging, New Delhi, India); Ros, Pablo (Stony Brook University, Stony Brook, NY, USA); Soria-Olivas, Emilio (Intelligent Data Analysis Laboratory (IDAL), ETSE, Universidad de Valencia, Valencia, Spain); Alberich-Bayarri, Angel (Quantitative Imaging Biomarkers in Medicine (Quibim S.L.), Aragon Avenue, 30, 13th floor, Office I-J, 46021, Valencia, Spain)","Jimenez-Pastor, Ana ","Jimenez-Pastor, Ana (); Lopez-Gonzalez, Rafael (University of Valencia); Fos-Guarinos, Belén (); Garcia-Castro, Fabio (); Wittenberg, Mark (); Torregrosa-Andrés, Asunción (Instituto de Investigación Sanitaria La Fe); Marti-Bonmati, Luis (Instituto de Investigación Sanitaria La Fe); Garcia-Fontes, Margarita (Centro Uruguayo de Imagenología Molecular); Duarte, Pablo (Centro Uruguayo de Imagenología Molecular); Gambini, Juan Pablo (Centro Uruguayo de Imagenología Molecular); Bittencourt, Leonardo Kayat (University Hospitals Cleveland Medical Center); Kitamura, Felipe Campos (); Venugopal, Vasantha Kumar (); Mahajan, Vidur (); Ros, Pablo (Stony Brook University); Soria-Olivas, Emilio (University of Valencia); Alberich-Bayarri, Angel ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154790774,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,,
5382,pub.1155132149,10.1002/mp.16280,36738103,,Surface‐GCN: Learning interaction experience for organ segmentation in 3D medical images,"BACKGROUND: Accurate segmentation of organs has a great significance for clinical diagnosis, but it is still hard work due to the obscure imaging boundaries caused by tissue adhesion on medical images. Based on the image continuity in medical image volumes, segmentation on these slices could be inferred from adjacent slices with a clear organ boundary. Radiologists can delineate a clear organ boundary by observing adjacent slices.
PURPOSE: Inspired by the radiologists' delineating procedure, we design an organ segmentation model based on boundary information of adjacent slices and a human-machine interactive learning strategy to introduce clinical experience.
METHODS: We propose an interactive organ segmentation method for medical image volume based on Graph Convolution Network (GCN) called Surface-GCN. First, we propose a Surface Feature Extraction Network (SFE-Net) to capture surface features of a target organ, and supervise it by a Mini-batch Adaptive Surface Matching (MBASM) module. Then, to predict organ boundaries precisely, we design an automatic segmentation module based on a Surface Convolution Unit (SCU), which propagates information on organ surfaces to refine the generated boundaries. In addition, an interactive segmentation module is proposed to learn radiologists' experience of interactive corrections on organ surfaces to reduce interaction clicks.
RESULTS: We evaluate the proposed method on one prostate MR image dataset and two abdominal multi-organ CT datasets. The experimental results show that our method outperforms other state-of-the-art methods. For prostate segmentation, the proposed method conducts a DSC score of 94.49% on PROMISE12 test dataset. For abdominal multi-organ segmentation, the proposed method achieves DSC scores of 95, 91, 95, and 88% for the left kidney, gallbladder, spleen, and esophagus, respectively. For interactive segmentation, the proposed method reduces 5-10 interaction clicks to reach the same accuracy.
CONCLUSIONS: To overcome the medical organ segmentation challenge, we propose a Graph Convolutional Network called Surface-GCN by imitating radiologist interactions and learning clinical experience. On single and multiple organ segmentation tasks, the proposed method could obtain more accurate segmentation boundaries compared with other state-of-the-art methods.","This work was supported by the National Natural Science Foundation of China under Grant No. 62173269, the Key Research and Development Program of Shaanxi Province of China under Grant No. 2020GXLH‐Y‐008, the Natural Science Basic Research Plan in Shaanxi Province of China under Grant No. 2022JM‐324, and the Shaanxi Provincial Social Science Fund under Grant No. 2021K014.",,Medical Physics,,,2023-02-03,2023,2023-02-10,2023-02-03,,,,Closed,Article,"Tian, Fengrui; Tian, Zhiqiang; Chen, Zhang; Zhang, Dong; Du, Shaoyi","Tian, Fengrui (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China); Tian, Zhiqiang (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China); Chen, Zhang (School of Software Engineering, Xi'an Jiaotong University, Xi'an, China); Zhang, Dong (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China; School of Automation Science and Engineering, Xi'an Jiaotong University, Xi'an, China); Du, Shaoyi (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China)","Tian, Zhiqiang (Xi'an Jiaotong University); Du, Shaoyi (Xi'an Jiaotong University)","Tian, Fengrui (Xi'an Jiaotong University; Xi'an Jiaotong University); Tian, Zhiqiang (Xi'an Jiaotong University); Chen, Zhang (Xi'an Jiaotong University); Zhang, Dong (Xi'an Jiaotong University; Xi'an Jiaotong University); Du, Shaoyi (Xi'an Jiaotong University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155132149,51 Physical Sciences, 5105 Medical and Biological Physics,,,,,,,,,,,
5055,pub.1155332607,10.1186/s12880-023-00974-y,36774463,PMC9921124,Comparison of automated segmentation techniques for magnetic resonance images of the prostate,"BackgroundContouring of anatomical regions is a crucial step in the medical workflow and is both time-consuming and prone to intra- and inter-observer variability. This study compares different strategies for automatic segmentation of the prostate in T2-weighted MRIs.MethodsThis study included 100 patients diagnosed with prostate adenocarcinoma who had undergone multi-parametric MRI and prostatectomy. From the T2-weighted MR images, ground truth segmentation masks were established by consensus from two expert radiologists. The prostate was then automatically contoured with six different methods: (1) a multi-atlas algorithm, (2) a proprietary algorithm in the Syngo.Via medical imaging software, and four deep learning models: (3) a V-net trained from scratch, (4) a pre-trained 2D U-net, (5) a GAN extension of the 2D U-net, and (6) a segmentation-adapted EfficientDet architecture. The resulting segmentations were compared and scored against the ground truth masks with one 70/30 and one 50/50 train/test data split. We also analyzed the association between segmentation performance and clinical variables.ResultsThe best performing method was the adapted EfficientDet (model 6), achieving a mean Dice coefficient of 0.914, a mean absolute volume difference of 5.9%, a mean surface distance (MSD) of 1.93 pixels, and a mean 95th percentile Hausdorff distance of 3.77 pixels. The deep learning models were less prone to serious errors (0.854 minimum Dice and 4.02 maximum MSD), and no significant relationship was found between segmentation performance and clinical variables.ConclusionsDeep learning-based segmentation techniques can consistently achieve Dice coefficients of 0.9 or above with as few as 50 training patients, regardless of architectural archetype. The atlas-based and Syngo.via methods found in commercial clinical software performed significantly worse (0.855-\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}0.887 Dice).","LJI and SV are PhD students at the European School of Molecular Medicine (SEMM), Milan, Italy.",This study was partially supported by the Italian Ministry of Health with Ricerca Corrente and 5x1000 funds.,BMC Medical Imaging,,"Male; Humans; Prostate; Image Processing, Computer-Assisted; Algorithms; Prostatic Neoplasms; Magnetic Resonance Imaging",2023-02-11,2023,2023-02-11,,23,1,32,All OA, Gold,Article,"Isaksson, Lars Johannes; Pepa, Matteo; Summers, Paul; Zaffaroni, Mattia; Vincini, Maria Giulia; Corrao, Giulia; Mazzola, Giovanni Carlo; Rotondi, Marco; Lo Presti, Giuliana; Raimondi, Sara; Gandini, Sara; Volpe, Stefania; Haron, Zaharudin; Alessi, Sarah; Pricolo, Paola; Mistretta, Francesco Alessandro; Luzzago, Stefano; Cattani, Federica; Musi, Gennaro; Cobelli, Ottavio De; Cremonesi, Marta; Orecchia, Roberto; Marvaso, Giulia; Petralia, Giuseppe; Jereczek-Fossa, Barbara Alicja","Isaksson, Lars Johannes (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy); Pepa, Matteo (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy); Summers, Paul (Division of Radiology, IEO European Institute of Oncology IRCCS, Milan, Italy); Zaffaroni, Mattia (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy); Vincini, Maria Giulia (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy); Corrao, Giulia (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy); Mazzola, Giovanni Carlo (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy; Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy); Rotondi, Marco (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy; Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy); Lo Presti, Giuliana (Molecular and Pharmaco-Epidemiology Unit, Department of Experimental Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy); Raimondi, Sara (Molecular and Pharmaco-Epidemiology Unit, Department of Experimental Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy); Gandini, Sara (Molecular and Pharmaco-Epidemiology Unit, Department of Experimental Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy); Volpe, Stefania (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy; Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy); Haron, Zaharudin (Radiology Department, National Cancer Institute, Putrajaya, Malaysia); Alessi, Sarah (Division of Radiology, IEO European Institute of Oncology IRCCS, Milan, Italy); Pricolo, Paola (Division of Radiology, IEO European Institute of Oncology IRCCS, Milan, Italy); Mistretta, Francesco Alessandro (Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy; Division of Urology, IEO European Institute of Oncology IRCCS, Milan, Italy); Luzzago, Stefano (Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy; Division of Urology, IEO European Institute of Oncology IRCCS, Milan, Italy); Cattani, Federica (Medical Physics Unit, IEO European Institute of Oncology IRCCS, Milan, Italy); Musi, Gennaro (Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy; Division of Urology, IEO European Institute of Oncology IRCCS, Milan, Italy); Cobelli, Ottavio De (Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy; Division of Urology, IEO European Institute of Oncology IRCCS, Milan, Italy); Cremonesi, Marta (Radiation Research Unit, IEO European Institute of Oncology IRCCS, Milan, Italy); Orecchia, Roberto (Scientific Direction, IEO European Institute of Oncology IRCCS, Milan, Italy); Marvaso, Giulia (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy; Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy); Petralia, Giuseppe (Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy; Precision Imaging and Research Unit, Department of Medical Imaging and Radiation Sciences, IEO European Institute of Oncology IRCCS, Milan, Italy); Jereczek-Fossa, Barbara Alicja (Department of Radiation Oncology, IEO European Institute of Oncology IRCCS, Milan, Italy; Department of Oncology and Hemato-Oncology, University of Milan, Milan, Italy)","Isaksson, Lars Johannes (European Institute of Oncology); Zaffaroni, Mattia (European Institute of Oncology)","Isaksson, Lars Johannes (European Institute of Oncology); Pepa, Matteo (European Institute of Oncology); Summers, Paul (European Institute of Oncology); Zaffaroni, Mattia (European Institute of Oncology); Vincini, Maria Giulia (European Institute of Oncology); Corrao, Giulia (European Institute of Oncology); Mazzola, Giovanni Carlo (European Institute of Oncology; University of Milan); Rotondi, Marco (European Institute of Oncology; University of Milan); Lo Presti, Giuliana (European Institute of Oncology); Raimondi, Sara (European Institute of Oncology); Gandini, Sara (European Institute of Oncology); Volpe, Stefania (European Institute of Oncology; University of Milan); Haron, Zaharudin (National Cancer Institute); Alessi, Sarah (European Institute of Oncology); Pricolo, Paola (European Institute of Oncology); Mistretta, Francesco Alessandro (University of Milan; European Institute of Oncology); Luzzago, Stefano (University of Milan; European Institute of Oncology); Cattani, Federica (European Institute of Oncology); Musi, Gennaro (University of Milan; European Institute of Oncology); Cobelli, Ottavio De (University of Milan; European Institute of Oncology); Cremonesi, Marta (European Institute of Oncology); Orecchia, Roberto (European Institute of Oncology); Marvaso, Giulia (European Institute of Oncology; University of Milan); Petralia, Giuseppe (University of Milan; European Institute of Oncology); Jereczek-Fossa, Barbara Alicja (European Institute of Oncology; University of Milan)",0,0,,,https://bmcmedimaging.biomedcentral.com/counter/pdf/10.1186/s12880-023-00974-y,https://app.dimensions.ai/details/publication/pub.1155332607,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
1357,pub.1155391550,10.1016/j.jrras.2023.100549,,,Evaluation of prostate multi parameter bone structures for martial arts practitioners based on magnetic resonance imaging,"Objective In this paper, we studied the feasibility of automatic segmentation of bone structures for rehabilitation and disease prevention from diffusion-weighted and apparent diffusion coefficient images obtained from prostate multi-parameter magnetic resonance imaging (mpMRI) in the healthy promotion of Chinese Taijiquan martial arts. Methods The mpMRI images of 15 patients practicing Taijiquan martial arts were analyzed retrospectively. We manually annotate bone structures on DWI (b = 800 s/mm2), DWI (b = 0S/mm2), and ADC images. Then, we use different sequence combinations as input data to test the segmentation model, and evaluate the impact of six different sequence combinations on the region-based segmentation performance, such as the watershed model. The model evaluation indicators include quantitative indicators (DICE coefficient, label capacity) and qualitative indicators (subjective score). The model evaluation standard calculates the coincidence rate of all sequences in the test set, and more than 80% are considered to meet the clinical application requirements. Results The DICE value of the watershed segmentation model was 0.75 (0.70–0.81) −0.81 (0.73–0.85) on DWI images, and the ADC value was 0.79 (0.78–0.81) −0.83 (0.80–0.85). However, there was no significant difference in DICE value between different models (HDWI = 2.978, PDWI>0.05; HADC = 1.140, PADC>0.05). There was no significant difference in the volume difference between model prediction and manual labeling among different models (HDWI = 2.900, PDWI>0.05; HADC = 2.236, PADC>0.05). Qualitative scoring models 1 and 3 achieved the highest standard rate in DWI and ADC image segmentation, both above 80%. Conclusion After Taijiquan martial arts exercise, it is found that the DWI segmentation model based on watershed region combined with ADC sequence can achieve high-performance segmentation of pelvic bone structure in prostate mpMRI, meet the needs of clinical application, and is conducive to healthy sports for all.",Thank the staff of the imaging department of the hospital for their help in medical scanning.,This research is not funded.,Journal of Radiation Research and Applied Sciences,,,2023-06,2023,,2023-06,16,2,100549,Closed,Article,"Xue, Meng; Liu, Yan; Cai, Xiaomei","Xue, Meng (Jiangmen Polytechnic, Jiangmen, Guangdong, 529000, China); Liu, Yan (Department of Orthopedic Surgery, Jiangmen TCM Affiliated Hospital of Jinan University, Jiangmen, Guangdong, 529000, China); Cai, Xiaomei (Guangdong Jiangmen Chinese Medical College, Jiangmen, Guangdong, 529000, China)","Cai, Xiaomei ","Xue, Meng (Jiangmen Polytechnic); Liu, Yan (); Cai, Xiaomei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155391550,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences,,,,,,,,,,,
1215,pub.1154964172,10.24840/2183-6493_009-001_001551,,,U-Net Architectures for Prostate Cancer Radiation Therapy: A Literature Review,"Prostate Cancer (PCa) was first diagnosed in 1853 as cirrhosis of the prostate gland. Silently asymptomatic, PCa is usually diagnosed with Digital Rectal Examination (DRE) and Prostate Specific Antigen (PSA) levels. Since the first treatment of an advanced prostatic malignancy with X-rays by Imbert and Imbert in 1904, External Beam Radiation Therapy (EBRT) is now a curative option for localised and locally advanced disease and a palliative option for the metastatic low-volume disease. With the introduction of computers in EBRT and better imaging techniques, volume delineation is still a very time-consuming task. While Deep Learning methods are urged, EBRT systems still rely on manual or semi-automatic segmentation techniques. The U-Net architecture was specially designed for medical image segmentation presenting promising results. This literature review gathers work using U-Net architectures, outlining methods, techniques and obtained outcomes as a potential foundation for an automated segmentation framework for PCa.",,,U Porto Journal of Engineering,,,2023-01-23,2023,2023-01-23,,9,1,177-190,All OA, Gold,Article,"Mendes, Bruno; Domingues, Inês; Santos, João","Mendes, Bruno (Universidade do Porto : Faculdade de Engenharia); Domingues, Inês (Politécnico de Coimbra); Santos, João (Centro de Investigação do Instituto Português de Oncologia do Porto (CI-IPOP))",,"Mendes, Bruno (University of Porto); Domingues, Inês (); Santos, João ()",0,0,,,https://journalengineering.fe.up.pt/index.php/upjeng/article/download/2183-6493_009-001_001551/743,https://app.dimensions.ai/details/publication/pub.1154964172,32 Biomedical and Clinical Sciences, 3202 Clinical Sciences, 3211 Oncology and Carcinogenesis,,,,,,,,,
1179,pub.1155160325,10.1109/wacv56688.2023.00265,,,Training Auxiliary Prototypical Classifiers for Explainable Anomaly Detection in Medical Image Segmentation,"Machine learning-based algorithms using fully convolutional networks (FCNs) have been a promising option for medical image segmentation. However, such deep networks silently fail if input samples are drawn far from the training data distribution, thus causing critical problems in automatic data processing pipelines. To overcome such out-of-distribution (OoD) problems, we propose a novel OoD score formulation and its regularization strategy by applying an auxiliary add-on classifier to an intermediate layer of an FCN, where the auxiliary module is helfpul for analyzing the encoder output features by taking their class information into account. Our regularization strategy train the module along with the FCN via the principle of outlier exposure so that our model can be trained to distinguish OoD samples from normal ones without modifying the original network architecture. Our extensive experiment results demonstrate that the proposed approach can successfully conduct effective OoD detection without loss of segmentation performance. In addition, our module can provide reasonable explanation maps along with OoD scores, which can enable users to analyze the reliability of predictions.","This work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1A2B5B02001913), and the National Supercomputing Center with supercomputing resources including technical support (KSC-2021-CRE-0186).","This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1A2B5B02001913), and the National Supercomputing Center with supercomputing resources including technical support (KSC-2021-CRE-0186).",,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),,2023-01-07,2023,,2023-01-07,0,,2623-2632,Closed,Proceeding,"Cho, Wonwoo; Park, Jeonghoon; Choo, Jaegul","Cho, Wonwoo (KAIST, Daejeon, Republic of Korea; Letsur Inc, Seoul, Republic of Korea); Park, Jeonghoon (KAIST, Daejeon, Republic of Korea); Choo, Jaegul (KAIST, Daejeon, Republic of Korea; Letsur Inc, Seoul, Republic of Korea)","Cho, Wonwoo (Korea Advanced Institute of Science and Technology; )","Cho, Wonwoo (Korea Advanced Institute of Science and Technology); Park, Jeonghoon (Korea Advanced Institute of Science and Technology); Choo, Jaegul (Korea Advanced Institute of Science and Technology)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155160325,46 Information and Computing Sciences, 4605 Data Management and Data Science, 4611 Machine Learning,,,,,,,,,,
1133,pub.1154305577,10.26599/tst.2022.9010023,,,SUNet++: A Deep Network with Channel Attention for Small-Scale Object Segmentation on 3D Medical Images,"As a deep learning network with an encoder-decoder architecture, UNet and its series of improved versions have been widely used in medical image segmentation with great applications. However, when used to segment targets in 3D medical images such as magnetic resonance imaging (MRI), computed tomography (CT), these models do not model the relevance of images in vertical space, resulting in poor accurate analysis of consecutive slices of the same patient. On the other hand, the large amount of detail lost during the encoding process makes these models incapable of segmenting small-scale tumor targets. Aiming at the scene of small-scale target segmentation in 3D medical images, a fully new neural network model SUNet++ is proposed on the basis of UNet and UNet++. SUNet++ improves the existing models mainly in three aspects: 1) the modeling strategy of slice superposition is used to thoroughly excavate the three dimensional information of the data; 2) by adding an attention mechanism during the decoding process, small scale targets in the picture are retained and amplified; 3) in the up-sampling process, the transposed convolution operation is used to further enhance the effect of the model. In order to verify the effect of the model, we collected and produced a dataset of hyperintensity MRI liver-stage images containing over 400 cases of liver nodules. Experimental results on both public and proprietary datasets demonstrate the superiority of SUNet++ in small-scale target segmentation of three-dimensional medical images.",,,Tsinghua Science & Technology,,,2023-08,2023,,,28,4,628-638,Closed,Article,"Zhang, Lan; Zhang, Kejia; Pan, Haiwei","Zhang, Lan (College of Computer Science and Technology, Harbin Engineering University,Harbin,China,150001); Zhang, Kejia (College of Computer Science and Technology, Harbin Engineering University,Harbin,China,150001); Pan, Haiwei (College of Computer Science and Technology, Harbin Engineering University,Harbin,China,150001)",,"Zhang, Lan (Harbin Engineering University); Zhang, Kejia (Harbin Engineering University); Pan, Haiwei (Harbin Engineering University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154305577,40 Engineering, 4008 Electrical Engineering,,,,,,,,,,,
1076,pub.1154955040,10.48550/arxiv.2301.12053,,,Weakly Supervised Image Segmentation Beyond Tight Bounding Box  Annotations,"Weakly supervised image segmentation approaches in the literature usually
achieve high segmentation performance using tight bounding box supervision and
decrease the performance greatly when supervised by loose bounding boxes.
However, compared with loose bounding box, it is much more difficult to acquire
tight bounding box due to its strict requirements on the precise locations of
the four sides of the box. To resolve this issue, this study investigates
whether it is possible to maintain good segmentation performance when loose
bounding boxes are used as supervision. For this purpose, this work extends our
previous parallel transformation based multiple instance learning (MIL) for
tight bounding box supervision by integrating an MIL strategy based on polar
transformation to assist image segmentation. The proposed polar transformation
based MIL formulation works for both tight and loose bounding boxes, in which a
positive bag is defined as pixels in a polar line of a bounding box with one
endpoint located inside the object enclosed by the box and the other endpoint
located at one of the four sides of the box. Moreover, a weighted smooth
maximum approximation is introduced to incorporate the observation that pixels
closer to the origin of the polar transformation are more likely to belong to
the object in the box. The proposed approach was evaluated on two public
datasets using dice coefficient when bounding boxes at different precision
levels were considered in the experiments. The results demonstrate that the
proposed approach achieves state-of-the-art performance for bounding boxes at
all precision levels and is robust to mild and moderate errors in the loose
bounding box annotations. The codes are available at
\url{https://github.com/wangjuan313/wsis-beyond-tightBB}.",,,arXiv,,,2023-01-27,2023,,,,,,All OA, Green,Preprint,"Wang, Juan; Xia, Bin","Wang, Juan (); Xia, Bin ()",,"Wang, Juan (); Xia, Bin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154955040,46 Information and Computing Sciences, 4603 Computer Vision and Multimedia Computation,,,,,,,,,,
1071,pub.1155331428,10.1016/j.bspc.2023.104651,,,NMNet: Learning Multi-level semantic information from scale extension domain for improved medical image segmentation,"Medical image segmentation methods based on encoder-decoder network structure have gained great success. However, these methods inevitably cause feature loss due to the pooling operation on the features during the encoding stage. Furthermore, there exists semantic gaps caused by the difference between low-level features and high-level features in the encoder-decoder network structure. By fusing the contextual features through simple skip-connections, it will limit the segmentation performance. To address these problems, this paper presents a new network for medical image segmentation, termed as NMNet, which mainly consists of a reverse encoder-decoder major structure with new attention modules. Specifically, in this network, we first design an N-shaped reverse encoder-decoder medical image segmentation structure (NNet), which can effectively reduce the impact of feature loss during the encoding process by performing feature representation compensation from the scale extension domain. Then, we build a Multi-scale Cross-attention Mechanism (MSC) in the skip-connections, which can enhance low-level features to bridge the semantic gaps. Extensive experiments on three benchmark datasets show that our NMNet performs favorably against most state-of-the-art methods under different evaluation metrics.","The author would like to thank Xiangyang Xu, who worked as an instructor at the School of Computer Science Technology, Huazhong University of Science and Technology, for his help and contributions, who checked the writing language of the article and made the necessary corrections.",,Biomedical Signal Processing and Control,,,2023-05,2023,,2023-05,83,,104651,Closed,Article,"Song, Enmin; Zhan, Bangcheng; Liu, Hong; Cetinkaya, Coskun; Hung, Chih-Cheng","Song, Enmin (School of Computer Science Technology, Huazhong University of Science and Technology, Wuhan, China); Zhan, Bangcheng (School of Computer Science Technology, Huazhong University of Science and Technology, Wuhan, China); Liu, Hong (School of Computer Science Technology, Huazhong University of Science and Technology, Wuhan, China); Cetinkaya, Coskun (The Center for Machine Vision and Security Research, Kennesaw State University, Marietta, GA, USA); Hung, Chih-Cheng (The Center for Machine Vision and Security Research, Kennesaw State University, Marietta, GA, USA)","Song, Enmin (Huazhong University of Science and Technology)","Song, Enmin (Huazhong University of Science and Technology); Zhan, Bangcheng (Huazhong University of Science and Technology); Liu, Hong (Huazhong University of Science and Technology); Cetinkaya, Coskun (Kennesaw State University); Hung, Chih-Cheng (Kennesaw State University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155331428,40 Engineering, 4003 Biomedical Engineering,,,,,,,,,,,
1067,pub.1155644749,10.1109/tmi.2023.3247941,,,Shape-aware Joint Distribution Alignment for Cross-domain Image Segmentation,"We present an unsupervised domain adaptation method for image segmentation which aligns high-order statistics, computed for the source and target domains, encoding domain-invariant spatial relationships between segmentation classes. Our method first estimates the joint distribution of predictions for pair of pixels whose relative position corresponds to a given spatial displacement. Domain adaptation is then achieved by aligning the joint distributions of source and target images, computed for a set of displacements. Two enhancements of this method are proposed. The first one uses an efficient multi-scale strategy that enables capturing long-range relationships in the statistics. The second one extends the joint distribution alignment loss to features in intermediate layers of the network by computing their cross-correlation. We test our method on the task of unpaired multi-modal cardiac segmentation using the Multi-Modality Whole Heart Segmentation Challenge dataset and prostate segmentation task where images from two datasets are taken as data in different domains. Our results show the advantages of our method compared to recent approaches for cross-domain image segmentation. Code is available at https://github.com/WangPing521/Domain_adaptation_shape_prior.",,,IEEE Transactions on Medical Imaging,,,2023-02-22,2023,2023-02-22,,PP,99,1-1,Closed,Article,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (Software and IT department, &#x00C9;cole de technologie sup&#x00E9;rieure(ETS), Montreal, Canada); Peng, Jizong (Software and IT department, ETS, Montreal, Canada); Pedersoli, Marco (Software and IT department, ETS, Montreal, Canada); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, China); Zhang, Caiming (School of Software, Shandong University, Jinan, China); Desrosiers, Christian (Software and IT department, ETS, Montreal, Canada)",,"Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (Shandong University); Zhang, Caiming (Shandong University); Desrosiers, Christian ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155644749,46 Information and Computing Sciences, 4601 Applied Computing,,,,,,,,,,,
1015,pub.1151488479,10.1016/b978-0-32-399851-2.00016-8,,,Chapter 8 Domain generalization of deep networks for medical image segmentation via meta learning,"The generalization capacity of deep models is crucial for real-world clinical deployment, where domain shifts usually exist due to changes of image acquisition conditions. This chapter presents domain generalization methods for medical image segmentation with meta learning technique, aiming to generalize the deep models learned from multiple source domains to unseen target domains. We present solutions under two realistic scenarios, i.e., centralized training and decentralized federated training. For the first scenario, we establish meta learning with multidomain data to construct virtual training and testing domains and enhance the model optimization towards simulated domain shifts with two shape-aware metaobjectives. Under the second scenario, a frequency-space interpolation mechanism is devised to enable each client access the multisource distributions under the constraint of data decentralization. The meta learning scheme is then established locally with explicit feature regularization imposed around the ambiguous regions to promote generalizable model parameters for segmentation problems. Experimental results on two medical image segmentation tasks have validated the effectiveness of our methods on the generalization problems under the two realistic training scenarios.",,,,Meta Learning With Medical Imaging and Health Informatics Applications,,2023,2023,,2023,,,117-139,Closed,Chapter,"Liu, Quande; Dou, Qi; Chen, Cheng; Heng, Pheng-Ann","Liu, Quande (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China); Dou, Qi (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China); Chen, Cheng (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China); Heng, Pheng-Ann (Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China)",,"Liu, Quande (Chinese University of Hong Kong); Dou, Qi (Chinese University of Hong Kong); Chen, Cheng (Chinese University of Hong Kong); Heng, Pheng-Ann (Chinese University of Hong Kong)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1151488479,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
915,pub.1154404820,10.1109/tmi.2023.3235757,,,FedDM: Federated Weakly Supervised Segmentation via Annotation Calibration and Gradient De-conflicting,"Weakly supervised segmentation (WSS) aims to exploit weak forms of annotations to achieve the segmentation training, thereby reducing the burden on annotation. However, existing methods rely on large-scale centralized datasets, which are difficult to construct due to privacy concerns on medical data. Federated learning (FL) provides a cross-site training paradigm and shows great potential to address this problem. In this work, we represent the first effort to formulate federated weakly supervised segmentation (FedWSS) and propose a novel Federated Drift Mitigation (FedDM) framework to learn segmentation models across multiple sites without sharing their raw data. FedDM is devoted to solving two main challenges (i.e., local drift on client-side optimization and global drift on server-side aggregation) caused by weak supervision signals in FL setting via Collaborative Annotation Calibration (CAC) and Hierarchical Gradient De-conflicting (HGD). To mitigate the local drift, CAC customizes a distal peer and a proximal peer for each client via a Monte Carlo sampling strategy, and then employs inter-client knowledge agreement and disagreement to recognize clean labels and correct noisy labels, respectively. Moreover, in order to alleviate the global drift, HGD online builds a client hierarchy under the guidance of history gradient of the global model in each communication round. Through de-conflicting clients under the same parent nodes from bottom layers to top layers, HGD achieves robust gradient aggregation at the server side. Furthermore, we theoretically analyze FedDM and conduct extensive experiments on public datasets. The experimental results demonstrate the superior performance of our method compared with state-of-the-art approaches. The source code is available at https://github.com/CityU-AIM-Group/FedDM.",,,IEEE Transactions on Medical Imaging,,,2023-01-10,2023,2023-01-10,,PP,99,1-1,Closed,Article,"Zhu, Meilu; Chen, Zhen; Yuan, Yixuan","Zhu, Meilu (Department of Mechanical Engineering, City University of Hong Kong, China); Chen, Zhen (Centre for Artificial Intelligence and Robotics (CAIR), HKISI, CAS, China); Yuan, Yixuan (Department of Electronic Engineering, Chinese University of Hong Kong, China)",,"Zhu, Meilu (City University of Hong Kong); Chen, Zhen (Chinese Academy of Sciences); Yuan, Yixuan (Chinese University of Hong Kong)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154404820,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
825,pub.1155159970,10.1109/tmi.2023.3243069,,,CAT: Constrained Adversarial Training for Anatomically-plausible Semi-supervised Segmentation,"Deep learning models for semi-supervised medical image segmentation have achieved unprecedented performance for a wide range of tasks. Despite their high accuracy, these models may however yield predictions that are considered anatomically impossible by clinicians. Moreover, incorporating complex anatomical constraints into standard deep learning frameworks remains challenging due to their non-differentiable nature. To address these limitations, we propose a Constrained Adversarial Training (CAT) method that learns how to produce anatomically plausible segmentations. Unlike approaches focusing solely on accuracy measures like Dice, our method considers complex anatomical constraints like connectivity, convexity, and symmetry which cannot be easily modeled in a loss function. The problem of non-differentiable constraints is solved using a Reinforce algorithm which enables to obtain a gradient for violated constraints. To generate constraint-violating examples on the fly, and thereby obtain useful gradients, our method adopts an adversarial training strategy which modifies training images to maximize the constraint loss, and then updates the network to be robust to these adversarial examples. The proposed method offers a generic and efficient way to add complex segmentation constraints on top of any segmentation network. Experiments on synthetic data and four clinically-relevant datasets demonstrate the effectiveness of our method in terms of segmentation accuracy and anatomical plausibility.",,,IEEE Transactions on Medical Imaging,,,2023-02-07,2023,2023-02-07,,PP,99,1-1,Closed,Article,"Wang, Ping; Peng, Jizong; Pedersoli, Marco; Zhou, Yuanfeng; Zhang, Caiming; Desrosiers, Christian","Wang, Ping (Software and IT department, &#x00C9;cole de technologie sup&#x00E9;rieure(ETS), Montreal, Canada); Peng, Jizong (Software and IT department, ETS, Montreal, Canada); Pedersoli, Marco (Software and IT department, ETS, Montreal, Canada); Zhou, Yuanfeng (School of Software, Shandong University, Jinan, China); Zhang, Caiming (School of Software, Shandong University, Jinan, China); Desrosiers, Christian (Software and IT department, ETS, Montreal, Canada)",,"Wang, Ping (); Peng, Jizong (); Pedersoli, Marco (); Zhou, Yuanfeng (Shandong University); Zhang, Caiming (Shandong University); Desrosiers, Christian ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1155159970,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,,
763,pub.1155169716,10.32604/cmc.2023.035888,,,Deep Learning for Image Segmentation: A Focus on Medical Imaging,"Image segmentation is crucial for various research areas. Many computer vision applications depend on segmenting images to understand the scene, such as autonomous driving, surveillance systems, robotics, and medical imaging. With the recent advances in deep learning (DL) and its confounding results in image segmentation, more attention has been drawn to its use in medical image segmentation. This article introduces a survey of the state-of-the-art deep convolution neural network (CNN) models and mechanisms utilized in image segmentation. First, segmentation models are categorized based on their model architecture and primary working principle. Then, CNN categories are described, and various models are discussed within each category. Compared with other existing surveys, several applications with multiple architectural adaptations are discussed within each category. A comparative summary is included to give the reader insights into utilized architectures in different applications and datasets. This study focuses on medical image segmentation applications, where the most widely used architectures are illustrated, and other promising models are suggested that have proven their success in different domains. Finally, the present work discusses current limitations and solutions along with future trends in the field.",,"This research work was supported by the Information Technology Industry Development Agency (ITIDA), Egypt (Project No. CFP181).",Computers Materials & Continua,,,2023-01-24,2023,2023-01-24,2023,75,1,1995-2024,All OA, Gold,Article,"Khalifa, Ali F.; Badr, Eman","Khalifa, Ali F. (Faculty of Computers and Artificial Intelligence, Cairo University, Giza, 12613, Egypt); Badr, Eman (Faculty of Computers and Artificial Intelligence, Cairo University, Giza, 12613, Egypt; Zewail City of Science and Technology, Giza, 12578, Egypt)","Badr, Eman (Cairo University; Zewail City of Science and Technology)","Khalifa, Ali F. (Cairo University); Badr, Eman (Cairo University; Zewail City of Science and Technology)",0,0,,,https://file.techscience.com/files/cmc/2023/TSP_CMC-75-1/TSP_CMC_35888/TSP_CMC_35888.pdf,https://app.dimensions.ai/details/publication/pub.1155169716,46 Information and Computing Sciences, 4611 Machine Learning,,,,,,,,,,
324,pub.1154675240,10.48550/arxiv.2301.07499,,,A Comprehensive Review of Modern Object Segmentation Approaches,"Image segmentation is the task of associating pixels in an image with their
respective object class labels. It has a wide range of applications in many
industries including healthcare, transportation, robotics, fashion, home
improvement, and tourism. Many deep learning-based approaches have been
developed for image-level object recognition and pixel-level scene
understanding-with the latter requiring a much denser annotation of scenes with
a large set of objects. Extensions of image segmentation tasks include 3D and
video segmentation, where units of voxels, point clouds, and video frames are
classified into different objects. We use ""Object Segmentation"" to refer to the
union of these segmentation tasks. In this monograph, we investigate both
traditional and modern object segmentation approaches, comparing their
strengths, weaknesses, and utilities. We examine in detail the wide range of
deep learning-based segmentation techniques developed in recent years, provide
a review of the widely used datasets and evaluation metrics, and discuss
potential future research directions.",,,arXiv,,,2023-01-13,2023,,,,,,All OA, Green,Preprint,"Wang, Yuanbo; Ahsan, Unaiza; Li, Hanyan; Hagen, Matthew","Wang, Yuanbo (); Ahsan, Unaiza (); Li, Hanyan (); Hagen, Matthew ()",,"Wang, Yuanbo (); Ahsan, Unaiza (); Li, Hanyan (); Hagen, Matthew ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154675240,"46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4607 Graphics, Augmented Reality and Games",,,,,,,,,,,
