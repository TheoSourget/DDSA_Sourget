"About the data: Exported on Mar 06, 2023. Criteria: '""Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark Study from Multi-Sequence Cardiac MR Segmentation Challenge""' in full data. © 2023 Digital Science &amp; Research Solutions Inc. All rights reserved. Parts of this work may also be protected by copyright of content providers and other third parties, which together with all rights of Digital Science, user agrees not to violate. Redistribution / external use of this work (or parts thereof) is prohibited without prior written approval. Please contact info@dimensions.ai for further information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rank,Publication ID,DOI,PMID,PMCID,Title,Abstract,Acknowledgements,Funding,Source title,Anthology title,MeSH terms,Publication Date,PubYear,Publication Date (online),Publication Date (print),Volume,Issue,Pagination,Open Access,Publication Type,Authors,Authors (Raw Affiliation),Corresponding Authors,Authors Affiliations,Times cited,Recent citations,RCR,FCR,Source Linkout,Dimensions URL,Fields of Research (ANZSRC 2020),Sustainable Development Goals
9752,pub.1147812714,10.3389/fcvm.2022.847825,35647044,PMC9133416,Radiomics and Machine Learning for Detecting Scar Tissue on CT Delayed Enhancement Imaging,"Background: Delayed enhancement CT (CT-DE) has been evaluated as a tool for the detection of myocardial scar and compares well to the gold standard of MRI with late gadolinium enhancement (MRI-LGE). Prior work has established that high performance can be achieved with manual reading; however, few studies have looked at quantitative measures to differentiate scar and healthy myocardium on CT-DE or automated analysis.
Methods: Eighteen patients with clinically indicated MRI-LGE were recruited for CT-DE at multiple 80 and 100 kV post contrast imaging. Left ventricle segmentation was performed on both imaging modalities, along with scar segmentation on MRI-LGE. Segmentations were registered together and scar regions were estimated on CT-DE. 93 radiomic features were calculated and analysed for their ability to differentiate between scarred and non-scarred myocardium regions. Machine learning (ML) classifiers were trained using the strongest set of radiomic features to classify segments containing scar on CT-DE. Features and classifiers were compared across both tube voltages and combined-energy images.
Results: There were 59 and 51 statistically significant features in the 80 and 100 kV images respectively. Combined-energy imaging increased this to 63 with more features having area under the curve (AUC) above 0.9. The 10 highest AUC features for each image were used in the ML classifiers. The 100 kV images produced the best ML classifier, a support vector machine with an AUC of 0.88 (95% CI 0.87-0.90). Comparable performance was achieved with both the 80 kV and combined-energy images.
Conclusions: CT-DE can be quantitatively analyzed using radiomic feature calculations. These features may be suitable for ML classification techniques to prospectively identify AHA segments with performance comparable to previously reported manual reading. Future work on larger CT-DE datasets is warranted to establish optimum imaging parameters and features.",,"MW (FS/ICRF/20/26002) was supported by the British Heart Foundation. HO'B would like to acknowledge funding from the EPSRC Centre for Doctoral Training in Medical Imaging (EP/L015226/1). The study was funded by Edinburgh & Lothians Health Foundation (49–187), and by core funding from the Wellcome/EPSRC Centre for Medical Engineering [WT203148/Z/16/Z]. For the purpose of Open Access, the author has applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission. The software used for segmenting MRI and CTA datasets was provided by Siemens Healthineers. The software is currently in a prototype stage and commercial availability can't be guaranteed.",Frontiers in Cardiovascular Medicine,,,2022-05-12,2022,2022-05-12,,9,,847825,All OA; Gold,Article,"O'Brien, Hugh; Williams, Michelle C.; Rajani, Ronak; Niederer, Steven","O'Brien, Hugh (School of Biomedical Engineering and Imaging Sciences, King's College London, London, United Kingdom); Williams, Michelle C. (Centre for Cardiovascular Science, University of Edinburgh, Edinburgh, United Kingdom); Rajani, Ronak (School of Biomedical Engineering and Imaging Sciences, King's College London, London, United Kingdom; Cardiology Department, Guy's and St Thomas' NHS Foundation Trust, London, United Kingdom); Niederer, Steven (School of Biomedical Engineering and Imaging Sciences, King's College London, London, United Kingdom)","O'Brien, Hugh (King's College London); Williams, Michelle C. (University of Edinburgh)","O'Brien, Hugh (King's College London); Williams, Michelle C. (University of Edinburgh); Rajani, Ronak (King's College London; Guy's and St Thomas' NHS Foundation Trust); Niederer, Steven (King's College London)",3,3,,,https://www.frontiersin.org/articles/10.3389/fcvm.2022.847825/pdf,https://app.dimensions.ai/details/publication/pub.1147812714,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
9682,pub.1149359829,10.1016/j.media.2022.102528,35834896,,Cardiac segmentation on late gadolinium enhancement MRI: A benchmark study from multi-sequence cardiac MR segmentation challenge,"Accurate computing, analysis and modeling of the ventricles and myocardium from medical images are important, especially in the diagnosis and treatment management for patients suffering from myocardial infarction (MI). Late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an important protocol to visualize MI. However, compared with the other sequences LGE CMR images with gold standard labels are particularly limited. This paper presents the selective results from the Multi-Sequence Cardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019. The challenge offered a data set of paired MS-CMR images, including auxiliary CMR sequences as well as LGE CMR, from 45 patients who underwent cardiomyopathy. It was aimed to develop new algorithms, as well as benchmark existing ones for LGE CMR segmentation focusing on myocardial wall of the left ventricle and blood cavity of the two ventricles. In addition, the paired MS-CMR images could enable algorithms to combine the complementary information from the other sequences for the ventricle segmentation of LGE CMR. Nine representative works were selected for evaluation and comparisons, among which three methods are unsupervised domain adaptation (UDA) methods and the other six are supervised. The results showed that the average performance of the nine methods was comparable to the inter-observer variations. Particularly, the top-ranking algorithms from both the supervised and UDA methods could generate reliable and robust segmentation results. The success of these methods was mainly attributed to the inclusion of the auxiliary sequences from the MS-CMR images, which provide important label information for the training of deep neural networks. The challenge continues as an ongoing resource, and the gold standard segmentation as well as the MS-CMR images of both the training and test data are available upon registration via its homepage (www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mscmrseg/).","This work was funded by the National Natural Science Foundation of China (Grant no. 61971142, 62111530195 and 62011540404) and the development fund for Shanghai talents (No. 2020015).",,Medical Image Analysis,,Benchmarking; Contrast Media; Gadolinium; Heart; Humans; Magnetic Resonance Imaging; Myocardial Infarction; Myocardium,2022-07-09,2022,2022-07-09,2022-10,81,,102528,All OA; Green,Article,"Zhuang, Xiahai; Xu, Jiahang; Luo, Xinzhe; Chen, Chen; Ouyang, Cheng; Rueckert, Daniel; Campello, Victor M; Lekadir, Karim; Vesal, Sulaiman; RaviKumar, Nishant; Liu, Yashu; Luo, Gongning; Chen, Jingkun; Li, Hongwei; Ly, Buntheng; Sermesant, Maxime; Roth, Holger; Zhu, Wentao; Wang, Jiexiang; Ding, Xinghao; Wang, Xinyue; Yang, Sen; Li, Lei","Zhuang, Xiahai (School of Data Science, Fudan University, Shanghai, China. Electronic address: https://www.sdspeople.fudan.edu.cn/zhuangxiahai/?); Xu, Jiahang (School of Data Science, Fudan University, Shanghai, China. Electronic address: jhxu18@fudan.edu.cn.); Luo, Xinzhe (School of Data Science, Fudan University, Shanghai, China.); Chen, Chen (Biomedical Image Analysis Group, Imperial College London, London, UK.); Ouyang, Cheng (Biomedical Image Analysis Group, Imperial College London, London, UK.); Rueckert, Daniel (Biomedical Image Analysis Group, Imperial College London, London, UK.); Campello, Victor M (Department Mathematics & Computer Science, Universitat de Barcelona, Barcelona, Spain.); Lekadir, Karim (Department Mathematics & Computer Science, Universitat de Barcelona, Barcelona, Spain.); Vesal, Sulaiman (Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany.); RaviKumar, Nishant (Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany.); Liu, Yashu (School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China.); Luo, Gongning (School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China.); Chen, Jingkun (Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China.); Li, Hongwei (Department of Informatics, Technical University of Munich, Germany.); Ly, Buntheng (INRIA, Université Côte d'Azur, Sophia Antipolis, France.); Sermesant, Maxime (INRIA, Université Côte d'Azur, Sophia Antipolis, France.); Roth, Holger (NVIDIA, Bethesda, USA.); Zhu, Wentao (NVIDIA, Bethesda, USA.); Wang, Jiexiang (School of Informatics, Xiamen University, Xiamen, China.); Ding, Xinghao (School of Informatics, Xiamen University, Xiamen, China.); Wang, Xinyue (College of Electrical Engineering, Sichuan University, Chengdu, China.); Yang, Sen (College of Electrical Engineering, Sichuan University, Chengdu, China; Tencent AI Lab, Shenzhen, China.); Li, Lei (School of Data Science, Fudan University, Shanghai, China; School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China. Electronic address: lilei.sky@sjtu.edu.cn.)","Zhuang, Xiahai (Fudan University); Xu, Jiahang (Fudan University); Li, Lei (Fudan University; Shanghai Jiao Tong University)","Zhuang, Xiahai (Fudan University); Xu, Jiahang (Fudan University); Luo, Xinzhe (Fudan University); Chen, Chen (Imperial College London); Ouyang, Cheng (Imperial College London); Rueckert, Daniel (Imperial College London); Campello, Victor M (University of Barcelona); Lekadir, Karim (University of Barcelona); Vesal, Sulaiman (University of Erlangen-Nuremberg); RaviKumar, Nishant (University of Erlangen-Nuremberg); Liu, Yashu (Harbin Institute of Technology); Luo, Gongning (Harbin Institute of Technology); Chen, Jingkun (Southern University of Science and Technology); Li, Hongwei (Technical University of Munich); Ly, Buntheng (); Sermesant, Maxime (); Roth, Holger (Nvidia (United States)); Zhu, Wentao (Nvidia (United States)); Wang, Jiexiang (Xiamen University); Ding, Xinghao (Xiamen University); Wang, Xinyue (Sichuan University); Yang, Sen (Sichuan University; Tencent (China)); Li, Lei (Fudan University; Shanghai Jiao Tong University)",8,7,,,http://arxiv.org/pdf/2006.12434,https://app.dimensions.ai/details/publication/pub.1149359829,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
7923,pub.1139463444,10.1161/circulationaha.121.054432,34229451,PMC8378544,Towards Replacing Late Gadolinium Enhancement with Artificial Intelligence Virtual Native Enhancement for Gadolinium-Free Cardiovascular Magnetic Resonance Tissue Characterization in Hypertrophic Cardiomyopathy,"BACKGROUND: Late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) imaging is the gold standard for noninvasive myocardial tissue characterization but requires intravenous contrast agent administration. It is highly desired to develop a contrast agent-free technology to replace LGE for faster and cheaper CMR scans.
METHODS: A CMR virtual native enhancement (VNE) imaging technology was developed using artificial intelligence. The deep learning model for generating VNE uses multiple streams of convolutional neural networks to exploit and enhance the existing signals in native T1 maps (pixel-wise maps of tissue T1 relaxation times) and cine imaging of cardiac structure and function, presenting them as LGE-equivalent images. The VNE generator was trained using generative adversarial networks. This technology was first developed on CMR datasets from the multicenter Hypertrophic Cardiomyopathy Registry, using hypertrophic cardiomyopathy as an exemplar. The datasets were randomized into 2 independent groups for deep learning training and testing. The test data of VNE and LGE were scored and contoured by experienced human operators to assess image quality, visuospatial agreement, and myocardial lesion burden quantification. Image quality was compared using a nonparametric Wilcoxon test. Intra- and interobserver agreement was analyzed using intraclass correlation coefficients (ICC). Lesion quantification by VNE and LGE were compared using linear regression and ICC.
RESULTS: A total of 1348 hypertrophic cardiomyopathy patients provided 4093 triplets of matched T1 maps, cines, and LGE datasets. After randomization and data quality control, 2695 datasets were used for VNE method development and 345 were used for independent testing. VNE had significantly better image quality than LGE, as assessed by 4 operators (n=345 datasets; P<0.001 [Wilcoxon test]). VNE revealed lesions characteristic of hypertrophic cardiomyopathy in high visuospatial agreement with LGE. In 121 patients (n=326 datasets), VNE correlated with LGE in detecting and quantifying both hyperintensity myocardial lesions (r=0.77-0.79; ICC=0.77-0.87; P<0.001) and intermediate-intensity lesions (r=0.70-0.76; ICC=0.82-0.85; P<0.001). The native CMR images (cine plus T1 map) required for VNE can be acquired within 15 minutes and producing a VNE image takes less than 1 second.
CONCLUSIONS: VNE is a new CMR technology that resembles conventional LGE but without the need for contrast administration. VNE achieved high agreement with LGE in the distribution and quantification of lesions, with significantly better image quality.",,,Circulation,,"Artificial Intelligence; Cardiomyopathy, Hypertrophic; Contrast Media; Deep Learning; Gadolinium; Humans; Image Enhancement; Image Processing, Computer-Assisted; Magnetic Resonance Imaging",2021-07-07,2021,2021-07-07,2021-08-24,144,8,589-599,All OA; Hybrid,Article,"Zhang, Qiang; Burrage, Matthew K.; Lukaschuk, Elena; Shanmuganathan, Mayooran; Popescu, Iulia A.; Nikolaidou, Chrysovalantou; Mills, Rebecca; Werys, Konrad; Hann, Evan; Barutcu, Ahmet; Polat, Suleyman D.; ; Salerno, Michael; Jerosch-Herold, Michael; Kwong, Raymond Y.; Watkins, Hugh C.; Kramer, Christopher M.; Neubauer, Stefan; Ferreira, Vanessa M.; Piechnik, Stefan K.","Zhang, Qiang (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Burrage, Matthew K. (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Lukaschuk, Elena (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Shanmuganathan, Mayooran (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Popescu, Iulia A. (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Nikolaidou, Chrysovalantou (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Mills, Rebecca (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Werys, Konrad (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Hann, Evan (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Barutcu, Ahmet (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.)); Polat, Suleyman D. (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.));  (); Salerno, Michael (Department of Medicine, University of Virginia Health System, Charlottesville, VA (M.Salerno, C.M.K.).); Jerosch-Herold, Michael (Cardiovascular Division, Department of Medicine, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA (M.J-H., R.Y.K.).); Kwong, Raymond Y. (Cardiovascular Division, Department of Medicine, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA (M.J-H., R.Y.K.).); Watkins, Hugh C. (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Kramer, Christopher M. (Department of Medicine, University of Virginia Health System, Charlottesville, VA (M.Salerno, C.M.K.).); Neubauer, Stefan (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Ferreira, Vanessa M. (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.); Piechnik, Stefan K. (Oxford Centre for Clinical Magnetic Resonance Research, Oxford Biomedical Research Centre National Institute for Health Research, Division of Cardiovascular (Q.Z., M.J.B., E.L., M.Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., A.B., S.D.P., H.C.W., S.N., V.M.F., S.K.P.); Radcliffe Department of Medicine (Q.Z., M.J.B., E.L., M. Shanmuganathan, I.A.P., C.N., R.M., K.W., E.H., H.C.W., S.N., V.M.F., S.K.P.), University of Oxford, UK.)","Zhang, Qiang (Oxford Biomedical Research; University of Oxford)","Zhang, Qiang (Oxford Biomedical Research; University of Oxford); Burrage, Matthew K. (Oxford Biomedical Research; University of Oxford); Lukaschuk, Elena (Oxford Biomedical Research; University of Oxford); Shanmuganathan, Mayooran (Oxford Biomedical Research; University of Oxford); Popescu, Iulia A. (Oxford Biomedical Research; University of Oxford); Nikolaidou, Chrysovalantou (Oxford Biomedical Research; University of Oxford); Mills, Rebecca (Oxford Biomedical Research; University of Oxford); Werys, Konrad (Oxford Biomedical Research; University of Oxford); Hann, Evan (Oxford Biomedical Research; University of Oxford); Barutcu, Ahmet (Oxford Biomedical Research); Polat, Suleyman D. (Oxford Biomedical Research);  (); Salerno, Michael (University of Virginia Health System); Jerosch-Herold, Michael (Brigham and Women's Hospital; Harvard University); Kwong, Raymond Y. (Brigham and Women's Hospital; Harvard University); Watkins, Hugh C. (Oxford Biomedical Research; University of Oxford); Kramer, Christopher M. (University of Virginia Health System); Neubauer, Stefan (Oxford Biomedical Research; University of Oxford); Ferreira, Vanessa M. (Oxford Biomedical Research; University of Oxford); Piechnik, Stefan K. (Oxford Biomedical Research; University of Oxford)",28,28,4.74,28.7,https://www.ahajournals.org/doi/pdf/10.1161/CIRCULATIONAHA.121.054432,https://app.dimensions.ai/details/publication/pub.1139463444,32 Biomedical and Clinical Sciences; 3201 Cardiovascular Medicine and Haematology; 3202 Clinical Sciences; 42 Health Sciences; 4207 Sports Science and Exercise,
7908,pub.1150455007,10.1016/j.compbiomed.2022.106052,36055164,,Domain generalization in deep learning for contrast-enhanced imaging,"BACKGROUND: The domain generalization problem has been widely investigated in deep learning for non-contrast imaging over the last years, but it received limited attention for contrast-enhanced imaging. However, there are marked differences in contrast imaging protocols across clinical centers, in particular in the time between contrast injection and image acquisition, while access to multi-center contrast-enhanced image data is limited compared to available datasets for non-contrast imaging. This calls for new tools for generalizing single-domain, single-center deep learning models across new unseen domains and clinical centers in contrast-enhanced imaging.
METHODS: In this paper, we present an exhaustive evaluation of deep learning techniques to achieve generalizability to unseen clinical centers for contrast-enhanced image segmentation. To this end, several techniques are investigated, optimized and systematically evaluated, including data augmentation, domain mixing, transfer learning and domain adaptation. To demonstrate the potential of domain generalization for contrast-enhanced imaging, the methods are evaluated for ventricular segmentation in contrast-enhanced cardiac magnetic resonance imaging (MRI).
RESULTS: The results are obtained based on a multi-center cardiac contrast-enhanced MRI dataset acquired in four hospitals located in three countries (France, Spain and China). They show that the combination of data augmentation and transfer learning can lead to single-center models that generalize well to new clinical centers not included during training.
CONCLUSIONS: Single-domain neural networks enriched with suitable generalization procedures can reach and even surpass the performance of multi-center, multi-vendor models in contrast-enhanced imaging, hence eliminating the need for comprehensive multi-center datasets to train generalizable models.",,"This work received funding from the European Union’s 2020 research and innovation programme under grant agreement No. 825903 (euCanSHare project), as well as from the Spanish Ministry of Science, Innovation and Universities under grant agreement RTI2018-099898-B-I00. Guala A. received funding from the Spanish Ministry of Science, Innovation and Universities (IJC2018-037349-I).",Computers in Biology and Medicine,,"Deep Learning; Heart; Heart Ventricles; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer",2022-08-24,2022,2022-08-24,2022-10,149,,106052,All OA; Hybrid,Article,"Sendra-Balcells, Carla; Campello, Víctor M; Martín-Isla, Carlos; Viladés, David; Descalzo, Martín L; Guala, Andrea; Rodríguez-Palomares, José F; Lekadir, Karim","Sendra-Balcells, Carla (Dept. de Matemàtiques i Informàtica, Universitat de Barcelona, Spain. Electronic address: carla.sendra@ub.edu.); Campello, Víctor M (Dept. de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.); Martín-Isla, Carlos (Dept. de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.); Viladés, David (Hospital de la Santa Creu i Sant Pau, Universitat Autònoma de Barcelona, Spain.); Descalzo, Martín L (Hospital de la Santa Creu i Sant Pau, Universitat Autònoma de Barcelona, Spain.); Guala, Andrea (Cardiovascular Imaging Unit, Hospital Universitari Vall d'Hebron, Barcelona, Spain.); Rodríguez-Palomares, José F (Cardiovascular Imaging Unit, Hospital Universitari Vall d'Hebron, Barcelona, Spain.); Lekadir, Karim (Dept. de Matemàtiques i Informàtica, Universitat de Barcelona, Spain.)","Sendra-Balcells, Carla (University of Barcelona)","Sendra-Balcells, Carla (University of Barcelona); Campello, Víctor M (University of Barcelona); Martín-Isla, Carlos (University of Barcelona); Viladés, David (); Descalzo, Martín L (); Guala, Andrea (Vall d'Hebron Hospital Universitari); Rodríguez-Palomares, José F (Vall d'Hebron Hospital Universitari); Lekadir, Karim (University of Barcelona)",1,1,,,https://doi.org/10.1016/j.compbiomed.2022.106052,https://app.dimensions.ai/details/publication/pub.1150455007,46 Information and Computing Sciences; 4601 Applied Computing,
7438,pub.1145342220,10.1109/jbhi.2022.3149114,35130178,,Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion,"Multi-atlas segmentation (MAS) is a promising framework for medical image segmentation. Generally, MAS methods register multiple atlases, i.e., medical images with corresponding labels, to a target image; and the transformed atlas labels can be combined to generate target segmentation via label fusion schemes. Many conventional MAS methods employed the atlases from the same modality as the target image. However, the number of atlases with the same modality may be limited or even missing in many clinical applications. Besides, conventional MAS methods suffer from the computational burden of registration or label fusion procedures. In this work, we design a novel cross-modality MAS framework, which uses available atlases from a certain modality to segment a target image from another modality. To boost the computational efficiency of the framework, both the image registration and label fusion are achieved by well-designed deep neural networks. For the atlas-to-target image registration, we propose a bi-directional registration network (BiRegNet), which can efficiently align images from different modalities. For the label fusion, we design a similarity estimation network (SimNet), which estimates the fusion weight of each atlas by measuring its similarity to the target image. SimNet can learn multi-scale information for similarity estimation to improve the performance of label fusion. The proposed framework was evaluated by the left ventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets, respectively. Results have shown that the framework is effective for cross-modality MAS in both registration and label fusion https://github.com/NanYoMy/cmmas.",,"This work was supported in part by the National Nature Science Foundation of China under Grants 62011540404, 62111530195, and 61971142, in part by Fujian Provincial Natural Science Foundation Project under Grants 2021J02019, 2021J01578 and 2019Y9070, and in part by Fuzhou Science and Technology under Grant 2020-GX-17.",IEEE Journal of Biomedical and Health Informatics,,"Heart Ventricles; Humans; Magnetic Resonance Imaging; Neural Networks, Computer",2022-07-01,2022,2022-07-01,2022-07,26,7,3104-3115,All OA; Green,Article,"Ding, Wangbin; Li, Lei; Zhuang, Xiahai; Huang, Liqin","Ding, Wangbin (College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350117, China); Li, Lei (School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200230, China); Zhuang, Xiahai (School of Data Science, Fudan University, Shanghai, 200433, China); Huang, Liqin (College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350117, China)","Zhuang, Xiahai (Fudan University)","Ding, Wangbin (Fuzhou University); Li, Lei (Shanghai Jiao Tong University); Zhuang, Xiahai (Fudan University); Huang, Liqin (Fuzhou University)",1,1,,,http://arxiv.org/pdf/2202.02000,https://app.dimensions.ai/details/publication/pub.1145342220,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
7041,pub.1153163732,10.1016/j.media.2022.102694,36495601,,MyoPS-Net: Myocardial pathology segmentation with flexible combination of multi-sequence CMR images,"Myocardial pathology segmentation (MyoPS) can be a prerequisite for the accurate diagnosis and treatment planning of myocardial infarction. However, achieving this segmentation is challenging, mainly due to the inadequate and indistinct information from an image. In this work, we develop an end-to-end deep neural network, referred to as MyoPS-Net, to flexibly combine five-sequence cardiac magnetic resonance (CMR) images for MyoPS. To extract precise and adequate information, we design an effective yet flexible architecture to extract and fuse cross-modal features. This architecture can tackle different numbers of CMR images and complex combinations of modalities, with output branches targeting specific pathologies. To impose anatomical knowledge on the segmentation results, we first propose a module to regularize myocardium consistency and localize the pathologies, and then introduce an inclusiveness loss to utilize relations between myocardial scars and edema. We evaluated the proposed MyoPS-Net on two datasets, i.e., a private one consisting of 50 paired multi-sequence CMR images and a public one from MICCAI2020 MyoPS Challenge. Experimental results showed that MyoPS-Net could achieve state-of-the-art performance in various scenarios. Note that in practical clinics, the subjects may not have full sequences, such as missing LGE CMR or mapping CMR scans. We therefore conducted extensive experiments to investigate the performance of the proposed method in dealing with such complex combinations of different CMR sequences. Results proved the superiority and generalizability of MyoPS-Net, and more importantly, indicated a practical clinical application. The code has been released via https://github.com/QJYBall/MyoPS-Net.","This work was funded by the National Natural Science Foundation of China (grant No. 61971142, 62111530195 and 62011540404) and the development fund for Shanghai talents (No. 2020015).",,Medical Image Analysis,,"Humans; Heart; Myocardium; Magnetic Resonance Imaging; Myocardial Infarction; Neural Networks, Computer; Image Processing, Computer-Assisted",2022-11-28,2022,2022-11-28,2023-02,84,,102694,All OA; Green,Article,"Qiu, Junyi; Li, Lei; Wang, Sihan; Zhang, Ke; Chen, Yinyin; Yang, Shan; Zhuang, Xiahai","Qiu, Junyi (School of Data Science, Fudan University, Shanghai, China.); Li, Lei (Institute of Biomedical Engineering, University of Oxford, Oxford, UK.); Wang, Sihan (School of Data Science, Fudan University, Shanghai, China.); Zhang, Ke (School of Data Science, Fudan University, Shanghai, China.); Chen, Yinyin (Department of Radiology, Zhongshan Hospital, Fudan University, Shanghai, China; Department of Medical Imaging, Shanghai Medical School, Fudan University and Shanghai Institute of Medical Imaging, Shanghai, China.); Yang, Shan (Department of Radiology, Zhongshan Hospital, Fudan University, Shanghai, China; Department of Medical Imaging, Shanghai Medical School, Fudan University and Shanghai Institute of Medical Imaging, Shanghai, China.); Zhuang, Xiahai (School of Data Science, Fudan University, Shanghai, China. Electronic address: zxh@fudan.edu.cn.)","Zhuang, Xiahai (Fudan University)","Qiu, Junyi (Fudan University); Li, Lei (University of Oxford); Wang, Sihan (Fudan University); Zhang, Ke (Fudan University); Chen, Yinyin (Fudan University; Zhongshan Hospital); Yang, Shan (Fudan University; Zhongshan Hospital); Zhuang, Xiahai (Fudan University)",1,1,,,http://arxiv.org/pdf/2211.03062,https://app.dimensions.ai/details/publication/pub.1153163732,32 Biomedical and Clinical Sciences; 40 Engineering,
6704,pub.1136481879,10.1109/tmi.2021.3066683,33729930,,Adapt Everywhere: Unsupervised Adaptation of Point-Clouds and Entropy Minimization for Multi-Modal Cardiac Image Segmentation,"Deep learning models are sensitive to domain shift phenomena. A model trained on images from one domain cannot generalise well when tested on images from a different domain, despite capturing similar anatomical structures. It is mainly because the data distribution between the two domains is different. Moreover, creating annotation for every new modality is a tedious and time-consuming task, which also suffers from high inter- and intra- observer variability. Unsupervised domain adaptation (UDA) methods intend to reduce the gap between source and target domains by leveraging source domain labelled data to generate labels for the target domain. However, current state-of-the-art (SOTA) UDA methods demonstrate degraded performance when there is insufficient data in source and target domains. In this paper, we present a novel UDA method for multi-modal cardiac image segmentation. The proposed method is based on adversarial learning and adapts network features between source and target domain in different spaces. The paper introduces an end-to-end framework that integrates: a) entropy minimization, b) output feature space alignment and c) a novel point-cloud shape adaptation based on the latent features learned by the segmentation model. We validated our method on two cardiac datasets by adapting from the annotated source domain, bSSFP-MRI (balanced Steady-State Free Procession-MRI), to the unannotated target domain, LGE-MRI (Late-gadolinium enhance-MRI), for the multi-sequence dataset; and from MRI (source) to CT (target) for the cross-modality dataset. The results highlighted that by enforcing adversarial learning in different parts of the network, the proposed method delivered promising performance, compared to other SOTA methods.",This work was supported by the Project EFI-BIG-THERA: Integrative “BigData Modeling” for the development of novel therapeutic approaches for breast cancer. The authors would like to thank NVIDIA for donating a Titan X-Pascal GPU.,,IEEE Transactions on Medical Imaging,,"Entropy; Heart; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging",2021-06-30,2021,2021-06-30,2021-07,40,7,1838-1851,All OA; Green,Article,"Vesal, Sulaiman; Gu, Mingxuan; Kosti, Ronak; Maier, Andreas; Ravikumar, Nishant","Vesal, Sulaiman (Pattern Recognition Lab, Friedrich-Alexander-University Erlangen-Nuremberg, 91054, Erlangen, Germany); Gu, Mingxuan (Pattern Recognition Lab, Friedrich-Alexander-University Erlangen-Nuremberg, 91054, Erlangen, Germany); Kosti, Ronak (Pattern Recognition Lab, Friedrich-Alexander-University Erlangen-Nuremberg, 91054, Erlangen, Germany); Maier, Andreas (Pattern Recognition Lab, Friedrich-Alexander-University Erlangen-Nuremberg, 91054, Erlangen, Germany); Ravikumar, Nishant (Center for Computational Imaging & Simulation Technologies in Biomedicine (CISTIB), School of Computing, Leeds Institute of Cardiovascular and Metabolic Medicine (LICAMM), School of Medicine, University of Leeds, Leeds, LS2 9JT, U.K.)","Vesal, Sulaiman (University of Erlangen-Nuremberg)","Vesal, Sulaiman (University of Erlangen-Nuremberg); Gu, Mingxuan (University of Erlangen-Nuremberg); Kosti, Ronak (University of Erlangen-Nuremberg); Maier, Andreas (University of Erlangen-Nuremberg); Ravikumar, Nishant (University of Leeds)",10,10,0.91,8.66,https://eprints.whiterose.ac.uk/174516/7/Adapt-everywhere.pdf,https://app.dimensions.ai/details/publication/pub.1136481879,"46 Information and Computing Sciences; 4607 Graphics, Augmented Reality and Games; 4611 Machine Learning",
6660,pub.1151790345,10.1109/tmi.2022.3213372,36219664,,Semi-supervised Unpaired Medical Image Segmentation Through Task-affinity Consistency,"Deep learning-based semi-supervised learning (SSL) algorithms are promising in reducing the cost of manual annotation of clinicians by using unlabelled data, when developing medical image segmentation tools. However, to date, most existing semi-supervised learning (SSL) algorithms treat the labelled images and unlabelled images separately and ignore the explicit connection between them; this disregards essential shared information and thus hinders further performance improvements. To mine the shared information between the labelled and unlabelled images, we introduce a class-specific representation extraction approach, in which a task-affinity module is specifically designed for representation extraction. We further cast the representation into two different views of feature maps; one is focusing on low-level context, while the other concentrates on structural information. The two views of feature maps are incorporated into the task-affinity module, which then extracts the class-specific representations to aid the knowledge transfer from the labelled images to the unlabelled images. In particular, a task-affinity consistency loss between the labelled images and unlabelled images based on the multi-scale class-specific representations is formulated, leading to a significant performance improvement. Experimental results on three datasets show that our method consistently outperforms existing state-of-the-art methods. Our findings highlight the potential of consistency between class-specific knowledge for semi-supervised medical image segmentation. The code and models are to be made publicly available at https://github.com/jingkunchen/TAC.",,,IEEE Transactions on Medical Imaging,,,2022-10-11,2022,2022-10-11,2022-10-11,PP,99,1-1,All OA; Green,Article,"Chen, Jingkun; Zhang, Jianguo; Debattista, Kurt; Han, Jungong","Chen, Jingkun (Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China); Zhang, Jianguo (Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China); Debattista, Kurt (Warwick Manufacturing Group, University of Warwick, Coventry, UK); Han, Jungong (Warwick Manufacturing Group, University of Warwick, Coventry, UK)",,"Chen, Jingkun (Southern University of Science and Technology); Zhang, Jianguo (Southern University of Science and Technology); Debattista, Kurt (University of Warwick); Han, Jungong (University of Warwick)",1,1,,,http://wrap.warwick.ac.uk/170166/1/WRAP-semi-supervised-unpaired-medical-image-segmentation-through-task-affinity-consistency-2022.pdf,https://app.dimensions.ai/details/publication/pub.1151790345,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4603 Computer Vision and Multimedia Computation; 4605 Data Management and Data Science; 4611 Machine Learning,
6644,pub.1151551617,10.1109/tmi.2022.3211195,36191115,,Domain-incremental Cardiac Image Segmentation with Style-oriented Replay and Domain-sensitive Feature Whitening,"Contemporary methods have shown promising results on cardiac image segmentation, but merely in static learning, i.e., optimizing the network once for all, ignoring potential needs for model updating. In real-world scenarios, new data continues to be gathered from multiple institutions over time and new demands keep growing to pursue more satisfying performance. The desired model should incrementally learn from each incoming dataset and progressively update with improved functionality as time goes by. As the datasets sequentially delivered from multiple sites are normally heterogenous with domain discrepancy, each updated model should not catastrophically forget previously learned domains while well generalizing to currently arrived domains or even unseen domains. In medical scenarios, this is particularly challenging as accessing or storing past data is commonly not allowed due to data privacy. To this end, we propose a novel domain-incremental learning framework to recover past domain inputs first and then regularly replay them during model optimization. Particularly, we first present a style-oriented replay module to enable structure-realistic and memory-efficient reproduction of past data, and then incorporate the replayed past data to jointly optimize the model with current data to alleviate catastrophic forgetting. During optimization, we additionally perform domain-sensitive feature whitening to suppress model's dependency on features that are sensitive to domain changes (e.g., domain-distinctive style features) to assist domain-invariant feature exploration and gradually improve the generalization performance of the network. We have extensively evaluated our approach with the M&Ms Dataset in single-domain and compound-domain incremental learning settings. Our approach outperforms other comparison methods with less forgetting on past domains and better generalization on current domains and unseen domains.",,,IEEE Transactions on Medical Imaging,,,2022-10-03,2022,2022-10-03,2022-10-03,PP,99,1-1,All OA; Green,Article,"Li, Kang; Yu, Lequan; Heng, Pheng-Ann","Li, Kang (Department of Computer Science and Engineering, and the Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, HKSAR, China); Yu, Lequan (Department of Statistics and Actuarial Science, The University of Hong Kong, HKSAR, China); Heng, Pheng-Ann (Department of Computer Science and Engineering, and the Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, HKSAR, China)",,"Li, Kang (Chinese University of Hong Kong); Yu, Lequan (University of Hong Kong); Heng, Pheng-Ann (Chinese University of Hong Kong)",0,0,,,http://arxiv.org/pdf/2211.04862,https://app.dimensions.ai/details/publication/pub.1151551617,46 Information and Computing Sciences; 4611 Machine Learning,
6310,pub.1153013167,10.1109/tmi.2022.3224067,36417741,,Causality-inspired Single-source Domain Generalization for Medical Image Segmentation,"Deep learning models usually suffer from the domain shift issue, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data are only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. In this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation scenarios: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-site prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains.",,,IEEE Transactions on Medical Imaging,,,2022-11-23,2022,2022-11-23,2022-11-23,PP,99,1-1,All OA; Green,Article,"Ouyang, Cheng; Chen, Chen; Li, Surui; Li, Zeju; Qin, Chen; Bai, Wenjia; Rueckert, Daniel","Ouyang, Cheng (Department of Computing, Imperial College London, UK); Chen, Chen (Department of Computing, Imperial College London, UK); Li, Surui (Department of Computing, Imperial College London, UK); Li, Zeju (Department of Computing, Imperial College London, UK); Qin, Chen (Department of Electrical and Electronic Engineering, Imperial College London, UK); Bai, Wenjia (Department of Computing, Imperial College London, UK); Rueckert, Daniel (Department of Computing, Imperial College London, UK)",,"Ouyang, Cheng (Imperial College London); Chen, Chen (Imperial College London); Li, Surui (Imperial College London); Li, Zeju (Imperial College London); Qin, Chen (Imperial College London); Bai, Wenjia (Imperial College London); Rueckert, Daniel (Imperial College London)",5,5,,,http://arxiv.org/pdf/2111.12525,https://app.dimensions.ai/details/publication/pub.1153013167,46 Information and Computing Sciences; 4611 Machine Learning,
6310,pub.1146513990,10.1016/j.media.2022.102426,35367712,,Towards reliable cardiac image segmentation: Assessing image-level and pixel-level segmentation quality via self-reflective references,"Cardiac image segmentation is a fundamental step in cardiovascular disease diagnosis, where many deep learning models have achieved promising performance. However, when deploying these well-trained models for real clinical usage, the network will inevitably produce inferior results due to domain shifts, motion artifacts, etc. How to avoid the potential poor-quality segmentations involved in clinical decision making is crucial for reliable computer-aided cardiac disease diagnosis. To this end, we develop a quality control method to identify failure segmentations by measuring their qualities, and report them to physicians for professional opinions. In specific, we propose a reference-based framework to assess the image-level quality (i.e. per-class Dice) for overall evaluation and pixel-level quality (i.e. pixel-wise correct map) to locate mis-segmented regions. Following previous works, we create informative references first, and investigate their relative relationships (e.g. differences) to the inputs to expose segmentation failures. However, we generate and leverage the references in different ways. We instantiate the references by recovering input images from segmentations by a self-reflective reference generator. If the segmentation is of good quality, the reference (i.e. the reconstructed image) will be close to the input image, and the inconsistency between them would be a good indicator to deduce the qualities. To effectively explore these inconsistency, we employ a difference investigator equipped with semantic class-aware compactness constraint to force the correctly-segmented features more separable to the wrongly-segmented ones. The experiments on ACDC and MSCMR datasets demonstrated our method could effectively capture segmentation failures, and the results on low-quality (Dice∈[0,0.6)), medium-quality (Dice∈[0.6,0.8)) and high-quality (Dice∈[0.8,1.0)) segmentations showed satisfying robustness of our method.","This work is supported by Key-Area Research and Development Program of Guangdong Province, China under Grant 2020B010165004, Hong Kong RGC TRS Project No. T42-409/18-R, Hong Kong Innovation and Technology Fund Project No. GHP/110/19SZ, National Natural Science Foundation of China with Project No. U1813204, and HKU Startup Fund and HKU Seed Fund for Basic Research for New Staff with Project No. 202009185079.",,Medical Image Analysis,,"Heart; Heart Diseases; Humans; Image Processing, Computer-Assisted",2022-03-24,2022,2022-03-24,2022-05,78,,102426,Closed,Article,"Li, Kang; Yu, Lequan; Heng, Pheng-Ann","Li, Kang (The Department of Computer Science and Engineering, The Chinese University of Hong Kong, HKSAR, China. Electronic address: kli@cse.cuhk.edu.hk.); Yu, Lequan (The Department of Statistics and Actuarial Science, The University of Hong Kong, HKSAR, China.); Heng, Pheng-Ann (The Department of Computer Science and Engineering, The Chinese University of Hong Kong, HKSAR, China.)","Li, Kang (Chinese University of Hong Kong)","Li, Kang (Chinese University of Hong Kong); Yu, Lequan (University of Hong Kong); Heng, Pheng-Ann (Chinese University of Hong Kong)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1146513990,32 Biomedical and Clinical Sciences; 40 Engineering,
6018,pub.1140657766,10.1016/j.compmedimag.2021.101982,34481237,,MA-SOCRATIS: An automatic pipeline for robust segmentation of the left ventricle and scar,"Multi-atlas segmentation of cardiac regions and total infarct scar (MA-SOCRATIS) is an unsupervised automatic pipeline to segment left ventricular myocardium and scar from late gadolinium enhanced MR images (LGE-MRI) of the heart. We implement two different pipelines for myocardial and scar segmentation from short axis LGE-MRI. Myocardial segmentation has two steps; initial segmentation and re-estimation. The initial segmentation step makes a first estimate of myocardium boundaries by using multi-atlas segmentation techniques. The re-estimation step refines the myocardial segmentation by a combination of k-means clustering and a geometric median shape variation technique. An active contour technique determines the unhealthy and healthy myocardial wall. The scar segmentation pipeline is a combination of a Rician-Gaussian mixture model and full width at half maximum (FWHM) thresholding, to determine the intensity pixels in scar regions. Following this step a watershed method with an automatic seed-points framework segments the final scar region. MA-SOCRATIS was evaluated using two different datasets. In both datasets ground truths were based on manual segmentation of short axis images from LGE-MRI scans. The first dataset included 40 patients from the MS-CMRSeg 2019 challenge dataset (STACOM at MICCAI 2019). The second is a collection of 20 patients with scar regions that are challenging to segment. MA-SOCRATIS achieved robust and accurate performance in automatic segmentation of myocardium and scar regions without the need of training or tuning in both cohorts, compared with state-of-the-art techniques (intra-observer and inter observer myocardium segmentation: 81.9% and 70% average Dice value, and scar (intra-observer and inter observer segmentation: 70.5% and 70.5% average Dice value).","We acknowledge the use of facilities of the Research Software Engineering Sheffield (RSE) UK and the JADE Tier 2 HPC UK system specification. This work was funded by University of Sheffield Department of Computer Science Scholarship. We thank Gavin Bainbridge, Caroline Richmond, Margaret Saysell, and Petra Bijsterveld for their invaluable assistance in recruiting and collecting data for this study. All authors declare that they have no competing interests.",,Computerized Medical Imaging and Graphics,,"Cicatrix; Gadolinium; Heart Ventricles; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Myocardial Infarction",2021-08-26,2021,2021-08-26,2021-10,93,,101982,All OA; Green,Article,"Mamalakis, Michail; Garg, Pankaj; Nelson, Tom; Lee, Justin; Wild, Jim M; Clayton, Richard H","Mamalakis, Michail (Insigneo Institute for In-Silico Medicine, University of Sheffield, Sheffield, UK; Department of Computer Science, University of Sheffield, Regent Court, Sheffield S1 4DP, UK. Electronic address: mmamalakis1@sheffield.ac.uk.); Garg, Pankaj (Department of Cardiology, Sheffield Teaching Hospitals NHS Trust, Sheffield S5 7AU, UK.); Nelson, Tom (Department of Cardiology, Sheffield Teaching Hospitals NHS Trust, Sheffield S5 7AU, UK.); Lee, Justin (Department of Cardiology, Sheffield Teaching Hospitals NHS Trust, Sheffield S5 7AU, UK.); Wild, Jim M (Insigneo Institute for In-Silico Medicine, University of Sheffield, Sheffield, UK; Polaris, Imaging Sciences, Department of Infection, Immunity and Cardiovascular Disease, University of Sheffield, Sheffield, UK.); Clayton, Richard H (Insigneo Institute for In-Silico Medicine, University of Sheffield, Sheffield, UK; Department of Computer Science, University of Sheffield, Regent Court, Sheffield S1 4DP, UK.)","Mamalakis, Michail (University of Sheffield)","Mamalakis, Michail (University of Sheffield); Garg, Pankaj (Sheffield Teaching Hospitals NHS Foundation Trust); Nelson, Tom (Sheffield Teaching Hospitals NHS Foundation Trust); Lee, Justin (Sheffield Teaching Hospitals NHS Foundation Trust); Wild, Jim M (University of Sheffield); Clayton, Richard H (University of Sheffield)",4,4,0.58,3.51,https://eprints.whiterose.ac.uk/180842/1/MA-SOCRATIS%20An%20automatic%20pipeline%20for%20robust%20segmentation%20of%20the%20left%20ventricle%20and%20scar.pdf,https://app.dimensions.ai/details/publication/pub.1140657766,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
5744,pub.1149790980,10.3389/fcvm.2022.894503,36051279,PMC9426684,Predicting post-contrast information from contrast agent free cardiac MRI using machine learning: Challenges and methods,"Objectives: Currently, administering contrast agents is necessary for accurately visualizing and quantifying presence, location, and extent of myocardial infarction (MI) with cardiac magnetic resonance (CMR). In this study, our objective is to investigate and analyze pre- and post-contrast CMR images with the goal of predicting post-contrast information using pre-contrast information only. We propose methods and identify challenges.
Methods: The study population consists of 272 retrospectively selected CMR studies with diagnoses of MI (n = 108) and healthy controls (n = 164). We describe a pipeline for pre-processing this dataset for analysis. After data feature engineering, 722 cine short-axis (SAX) images and segmentation mask pairs were used for experimentation. This constitutes 506, 108, and 108 pairs for the training, validation, and testing sets, respectively. We use deep learning (DL) segmentation (UNet) and classification (ResNet50) models to discover the extent and location of the scar and classify between the ischemic cases and healthy cases (i.e., cases with no regional myocardial scar) from the pre-contrast cine SAX image frames, respectively. We then capture complex data patterns that represent subtle signal and functional changes in the cine SAX images due to MI using optical flow, rate of change of myocardial area, and radiomics data. We apply this dataset to explore two supervised learning methods, namely, the support vector machines (SVM) and the decision tree (DT) methods, to develop predictive models for classifying pre-contrast cine SAX images as being a case of MI or healthy.
Results: Overall, for the UNet segmentation model, the performance based on the mean Dice score for the test set (n = 108) is 0.75 (±0.20) for the endocardium, 0.51 (±0.21) for the epicardium and 0.20 (±0.17) for the scar. For the classification task, the accuracy, F1 and precision scores of 0.68, 0.69, and 0.64, respectively, were achieved with the SVM model, and of 0.62, 0.63, and 0.72, respectively, with the DT model.
Conclusion: We have presented some promising approaches involving DL, SVM, and DT methods in an attempt to accurately predict contrast information from non-contrast images. While our initial results are modest for this challenging task, this area of research still poses several open problems.",,"This work forms part of the research areas contributed to the translational research portfolio of the Biomedical Research Centre at Barts which was supported and funded by the National Institute for Health Research. MA and SP acknowledged support from the CAP-AI program (led by Capital Enterprise in partnership with Barts Health NHS Trust and Digital Catapult and funded by the European Regional Development Fund and Barts Charity) and Health Data Research UK [HDR UK—an initiative funded by UK Research and Innovation, Department of Health and Social Care (England) and the devolved administrations, and leading medical research charities; www.hdruk.ac.uk]. SP acknowledged support from the SmartHeart EPSRC program grant (www.nihr.ac.uk; EP/P001009/1). SP had received funding from the European Union’s Horizon 2020 Research and Innovation Program under grant agreement No. 825903 (euCanSHare project). SP and ER acknowledged support by the London Medical Imaging and Artificial Intelligence Centre for Value Based Healthcare (AI4VBH), which was funded from the Data to Early Diagnosis and Precision Medicine strand of the government’s Industrial Strategy Challenge Fund, managed, and delivered by Innovate UK on behalf of UK Research and Innovation (UKRI). NA was supported by a Wellcome Trust Research Training Fellowship (wellcome.ac.uk; 203553/Z/Z). NA recognises the National Institute for Health Research (NIHR) Integrated Academic Training programme which supports his Academic Clinical Lectureship post.",Frontiers in Cardiovascular Medicine,,,2022-07-27,2022,2022-07-27,,9,,894503,All OA; Gold,Article,"Abdulkareem, Musa; Kenawy, Asmaa A.; Rauseo, Elisa; Lee, Aaron M.; Sojoudi, Alireza; Amir-Khalili, Alborz; Lekadir, Karim; Young, Alistair A.; Barnes, Michael R.; Barckow, Philipp; Khanji, Mohammed Y.; Aung, Nay; Petersen, Steffen E.","Abdulkareem, Musa (Barts Heart Centre, Barts Health National Health Service (NHS) Trust, London, United Kingdom; National Institute for Health Research (NIHR) Barts Biomedical Research Centre, William Harvey Research Institute, Queen Mary University of London, London, United Kingdom; Health Data Research UK, London, United Kingdom); Kenawy, Asmaa A. (Barts Heart Centre, Barts Health National Health Service (NHS) Trust, London, United Kingdom; National Institute for Health Research (NIHR) Barts Biomedical Research Centre, William Harvey Research Institute, Queen Mary University of London, London, United Kingdom); Rauseo, Elisa (Barts Heart Centre, Barts Health National Health Service (NHS) Trust, London, United Kingdom; National Institute for Health Research (NIHR) Barts Biomedical Research Centre, William Harvey Research Institute, Queen Mary University of London, London, United Kingdom); Lee, Aaron M. (Barts Heart Centre, Barts Health National Health Service (NHS) Trust, London, United Kingdom; National Institute for Health Research (NIHR) Barts Biomedical Research Centre, William Harvey Research Institute, Queen Mary University of London, London, United Kingdom); Sojoudi, Alireza (Circle Cardiovascular Imaging Inc., Calgary, AB, Canada); Amir-Khalili, Alborz (Circle Cardiovascular Imaging Inc., Calgary, AB, Canada); Lekadir, Karim (Artificial Intelligence in Medicine Lab (BCN-AIM), Faculty of Mathematics and Computer Science, University of Barcelona, Barcelona, Spain); Young, Alistair A. (Department of Biomedical Engineering, King’s College London, London, United Kingdom); Barnes, Michael R. (Centre for Translational Bioinformatics, William Harvey Research Institute, Faculty of Medicine and Dentistry, Queen Mary University of London, London, United Kingdom); Barckow, Philipp (Circle Cardiovascular Imaging Inc., Calgary, AB, Canada); Khanji, Mohammed Y. (Barts Heart Centre, Barts Health National Health Service (NHS) Trust, London, United Kingdom; National Institute for Health Research (NIHR) Barts Biomedical Research Centre, William Harvey Research Institute, Queen Mary University of London, London, United Kingdom; Newham University Hospital, Barts Health National Health Service (NHS) Trust, London, United Kingdom); Aung, Nay (Barts Heart Centre, Barts Health National Health Service (NHS) Trust, London, United Kingdom; National Institute for Health Research (NIHR) Barts Biomedical Research Centre, William Harvey Research Institute, Queen Mary University of London, London, United Kingdom); Petersen, Steffen E. (Barts Heart Centre, Barts Health National Health Service (NHS) Trust, London, United Kingdom; National Institute for Health Research (NIHR) Barts Biomedical Research Centre, William Harvey Research Institute, Queen Mary University of London, London, United Kingdom; Health Data Research UK, London, United Kingdom; The Alan Turing Institute, London, United Kingdom)","Abdulkareem, Musa (St Bartholomew's Hospital; Queen Mary University of London; Health Data Research UK)","Abdulkareem, Musa (St Bartholomew's Hospital; Queen Mary University of London; Health Data Research UK); Kenawy, Asmaa A. (St Bartholomew's Hospital; Queen Mary University of London); Rauseo, Elisa (St Bartholomew's Hospital; Queen Mary University of London); Lee, Aaron M. (St Bartholomew's Hospital; Queen Mary University of London); Sojoudi, Alireza (Circle Cardiovascular Imaging); Amir-Khalili, Alborz (Circle Cardiovascular Imaging); Lekadir, Karim (University of Barcelona); Young, Alistair A. (King's College London); Barnes, Michael R. (Queen Mary University of London); Barckow, Philipp (Circle Cardiovascular Imaging); Khanji, Mohammed Y. (St Bartholomew's Hospital; Queen Mary University of London; Newham University Hospital); Aung, Nay (St Bartholomew's Hospital; Queen Mary University of London); Petersen, Steffen E. (St Bartholomew's Hospital; Queen Mary University of London; Health Data Research UK; The Alan Turing Institute)",1,1,,,https://www.frontiersin.org/articles/10.3389/fcvm.2022.894503/pdf,https://app.dimensions.ai/details/publication/pub.1149790980,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
5267,pub.1153212736,10.1109/tpami.2022.3225418,36445992,,"<inline-formula><tex-math notation=""LaTeX"">$\mathcal {X}$</tex-math></inline-formula>-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing","This paper presents a generic probabilistic framework for estimating the statistical dependency and finding the anatomical correspondences among an arbitrary number of medical images. The method builds on a novel formulation of the N-dimensional joint intensity distribution by representing the common anatomy as latent variables and estimating the appearance model with nonparametric estimators. Through connection to maximum likelihood and the expectation-maximization algorithm, an information-theoretic metric called X-metric and a co-registration algorithm named X-CoReg are induced, allowing groupwise registration of the N observed images with computational complexity of O(N). Moreover, the method naturally extends for a weakly-supervised scenario where anatomical labels of certain images are provided. This leads to a combined-computing framework implemented with deep learning, which performs registration and segmentation simultaneously and collaboratively in an end-to-end fashion. Extensive experiments were conducted to demonstrate the versatility and applicability of our model, including multimodal groupwise registration, motion correction for dynamic contrast enhanced magnetic resonance images, and deep combined computing for multimodal medical images. Results show the superiority of our method in various applications in terms of both accuracy and efficiency, highlighting the advantage of the proposed representation of the imaging process.",,,IEEE Transactions on Pattern Analysis and Machine Intelligence,,,2022-11-29,2022,2022-11-29,2022-11-29,PP,99,1-18,All OA; Green,Article,"Luo, Xinzhe; Zhuang, Xiahai","Luo, Xinzhe (School of Data Science, Fudan University, Shanghai, China); Zhuang, Xiahai (School of Data Science, Fudan University, Shanghai, China)",,"Luo, Xinzhe (Fudan University); Zhuang, Xiahai (Fudan University)",1,1,,,http://arxiv.org/pdf/2211.01631,https://app.dimensions.ai/details/publication/pub.1153212736,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
3728,pub.1145093048,10.1016/j.media.2022.102360,35124370,PMC7614005,Medical image analysis on left atrial LGE MRI for atrial fibrillation studies: A review,"Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly used to visualize and quantify left atrial (LA) scars. The position and extent of LA scars provide important information on the pathophysiology and progression of atrial fibrillation (AF). Hence, LA LGE MRI computing and analysis are essential for computer-assisted diagnosis and treatment stratification of AF patients. Since manual delineations can be time-consuming and subject to intra- and inter-expert variability, automating this computing is highly desired, which nevertheless is still challenging and under-researched. This paper aims to provide a systematic review on computing methods for LA cavity, wall, scar, and ablation gap segmentation and quantification from LGE MRI, and the related literature for AF studies. Specifically, we first summarize AF-related imaging techniques, particularly LGE MRI. Then, we review the methodologies of the four computing tasks in detail and summarize the validation strategies applied in each task as well as state-of-the-art results on public datasets. Finally, the possible future developments are outlined, with a brief survey on the potential clinical applications of the aforementioned methods. The review indicates that the research into this topic is still in the early stages. Although several methods have been proposed, especially for the LA cavity segmentation, there is still a large scope for further algorithmic developments due to performance issues related to the high variability of enhancement appearance and differences in image acquisition.","This work was supported by the National Natural Science Foundation of China (61971142, 62111530195 and 62011540404) and the development fund for Shanghai talents (2020015). L Li was partially supported by the CSC Scholarship. JA Schnabel and VA Zimmer would like to acknowledge funding from a Wellcome Trust IEH Award (WT 102431), an EPSRC programme grant (EP/P001009/1), and the Wellcome/EPSRC Center for Medical Engineering (WT 203148/Z/16/Z). XH Zhuang and JA Schnabel would also like to acknowledge funding from the Royal Society Sino-British Fellowship Trust International Exchanges Award.",,Medical Image Analysis,,Atrial Fibrillation; Cicatrix; Contrast Media; Gadolinium; Heart Atria; Humans; Magnetic Resonance Imaging,2022-01-29,2022,2022-01-29,2022-04,77,,102360,All OA; Green,Article,"Li, Lei; Zimmer, Veronika A.; Schnabel, Julia A.; Zhuang, Xiahai","Li, Lei (School of Data Science, Fudan University, Shanghai, China; School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, UK); Zimmer, Veronika A. (School of Biomedical Engineering and Imaging Sciences, King’s College London, London, UK; Department of Informatics, Technical University of Munich, Germany); Schnabel, Julia A. (School of Biomedical Engineering and Imaging Sciences, King’s College London, London, UK; Department of Informatics, Technical University of Munich, Germany; Helmholtz Center Munich, Germany); Zhuang, Xiahai (School of Data Science, Fudan University, Shanghai, China)","Zhuang, Xiahai (Fudan University)","Li, Lei (Fudan University; Shanghai Jiao Tong University; King's College London); Zimmer, Veronika A. (King's College London; Technical University of Munich); Schnabel, Julia A. (King's College London; Technical University of Munich; Helmholtz Zentrum München); Zhuang, Xiahai (Fudan University)",4,4,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7614005,https://app.dimensions.ai/details/publication/pub.1145093048,40 Engineering; 4003 Biomedical Engineering,
2472,pub.1132369677,10.48550/arxiv.2011.02580,,,DeepReg: a deep learning toolkit for medical image registration,"DeepReg (https://github.com/DeepRegNet/DeepReg) is a community-supported
open-source toolkit for research and education in medical image registration
using deep learning.",,,arXiv,,,2020-11-04,2020,,,,,,All OA; Green,Preprint,"Fu, Yunguan; Brown, Nina Montaña; Saeed, Shaheer U.; Casamitjana, Adrià; Baum, Zachary M. C.; Delaunay, Rémi; Yang, Qianye; Grimwood, Alexander; Min, Zhe; Blumberg, Stefano B.; Iglesias, Juan Eugenio; Barratt, Dean C.; Bonmati, Ester; Alexander, Daniel C.; Clarkson, Matthew J.; Vercauteren, Tom; Hu, Yipeng","Fu, Yunguan (); Brown, Nina Montaña (); Saeed, Shaheer U. (); Casamitjana, Adrià (); Baum, Zachary M. C. (); Delaunay, Rémi (); Yang, Qianye (); Grimwood, Alexander (); Min, Zhe (); Blumberg, Stefano B. (); Iglesias, Juan Eugenio (); Barratt, Dean C. (); Bonmati, Ester (); Alexander, Daniel C. (); Clarkson, Matthew J. (); Vercauteren, Tom (); Hu, Yipeng ()",,"Fu, Yunguan (); Brown, Nina Montaña (); Saeed, Shaheer U. (); Casamitjana, Adrià (); Baum, Zachary M. C. (); Delaunay, Rémi (); Yang, Qianye (); Grimwood, Alexander (); Min, Zhe (); Blumberg, Stefano B. (); Iglesias, Juan Eugenio (); Barratt, Dean C. (); Bonmati, Ester (); Alexander, Daniel C. (); Clarkson, Matthew J. (); Vercauteren, Tom (); Hu, Yipeng ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1132369677,51 Physical Sciences; 5105 Medical and Biological Physics,
2357,pub.1129909520,10.48550/arxiv.2008.01216,,,Generalisable Cardiac Structure Segmentation via Attentional and Stacked  Image Adaptation,"Tackling domain shifts in multi-centre and multi-vendor data sets remains
challenging for cardiac image segmentation. In this paper, we propose a
generalisable segmentation framework for cardiac image segmentation in which
multi-centre, multi-vendor, multi-disease datasets are involved. A generative
adversarial networks with an attention loss was proposed to translate the
images from existing source domains to a target domain, thus to generate
good-quality synthetic cardiac structure and enlarge the training set. A stack
of data augmentation techniques was further used to simulate real-world
transformation to boost the segmentation performance for unseen domains.We
achieved an average Dice score of 90.3% for the left ventricle, 85.9% for the
myocardium, and 86.5% for the right ventricle on the hidden validation set
across four vendors. We show that the domain shifts in heterogeneous cardiac
imaging datasets can be drastically reduced by two aspects: 1) good-quality
synthetic data by learning the underlying target domain distribution, and 2)
stacked classical image processing techniques for data augmentation.",,,arXiv,,,2020-08-03,2020,,,,,,All OA; Green,Preprint,"Li, Hongwei; Zhang, Jianguo; Menze, Bjoern","Li, Hongwei (); Zhang, Jianguo (); Menze, Bjoern ()",,"Li, Hongwei (); Zhang, Jianguo (); Menze, Bjoern ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1129909520,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
2352,pub.1134951308,10.1007/978-3-030-68107-4_30,,,Generalisable Cardiac Structure Segmentation via Attentional and Stacked Image Adaptation,"Tackling domain shifts in multi-centre and multi-vendor data sets remains challenging for cardiac image segmentation. In this paper, we propose a generalisable segmentation framework for cardiac image segmentation in which multi-centre, multi-vendor, multi-disease datasets are involved. A generative adversarial networks with an attention loss was proposed to translate the images from existing source domains to a target domain, thus to generate good-quality synthetic cardiac structure and enlarge the training set. A stack of data augmentation techniques was further used to simulate real-world transformation to boost the segmentation performance for unseen domains. We achieved an average Dice score of 90.3% for the left ventricle, 85.9% for the myocardium, and 86.5% for the right ventricle on the hidden validation set across four vendors. We show that the domain shifts in heterogeneous cardiac imaging datasets can be drastically reduced by two aspects: 1) good-quality synthetic data by learning the underlying target domain distribution, and 2) stacked classical image processing techniques for data augmentation.",,,Lecture Notes in Computer Science,Statistical Atlases and Computational Models of the Heart. M&Ms and EMIDEC Challenges,,2021-01-29,2021,2021-01-29,2021,12592,,297-304,All OA; Green,Chapter,"Li, Hongwei; Zhang, Jianguo; Menze, Bjoern","Li, Hongwei (Department of Computer Science, Technical University of Munich, Munich, Germany); Zhang, Jianguo (Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China); Menze, Bjoern (Department of Computer Science, Technical University of Munich, Munich, Germany)","Zhang, Jianguo (Southern University of Science and Technology)","Li, Hongwei (Technical University of Munich); Zhang, Jianguo (Southern University of Science and Technology); Menze, Bjoern (Technical University of Munich)",8,7,,6.37,http://arxiv.org/pdf/2008.01216,https://app.dimensions.ai/details/publication/pub.1134951308,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
2254,pub.1144687364,10.1007/978-3-030-93722-5_28,,,Right Ventricular Segmentation from Short- and Long-Axis MRIs via Information Transition,"Right ventricular (RV) segmentation from magnetic resonance imaging (MRI) is a crucial step for cardiac morphology and function analysis. However, automatic RV segmentation from MRI is still challenging, mainly due to the heterogeneous intensity, the complex variable shapes, and the unclear RV boundary. Moreover, current methods for the RV segmentation tend to suffer from performance degradation at the basal and apical slices of MRI. In this work, we propose an automatic RV segmentation framework, where the information from long-axis (LA) views is utilized to assist the segmentation of short-axis (SA) views via information transition. Specifically, we employed the transformed segmentation from LA views as a prior information, to extract the ROI from SA views for better segmentation. The information transition aims to remove the surrounding ambiguous regions in the SA views. We tested our model on a public dataset with 360 multi-center, multi-vendor and multi-disease subjects that consist of both LA and SA MRIs. Our experimental results show that including LA views can be effective to improve the accuracy of the SA segmentation. Our model is publicly available at https://github.com/NanYoMy/MMs-2.","This work was funded by the National Natural Science Foundation of China (grant no. 61971142, 62111530195 and 62011540404) and the development fund for Shanghai talents (no. 2020015).",,Lecture Notes in Computer Science,"Statistical Atlases and Computational Models of the Heart. Multi-Disease, Multi-View, and Multi-Center Right Ventricular Segmentation in Cardiac MRI Challenge",,2022-01-14,2022,2022-01-14,2022,13131,,259-267,All OA; Green,Chapter,"Li, Lei; Ding, Wangbin; Huang, Liqin; Zhuang, Xiahai","Li, Lei (School of Data Science, Fudan University, Shanghai, China; School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China); Ding, Wangbin (College of Physics and Information Engineering, Fuzhou University, Fuzhou, China); Huang, Liqin (College of Physics and Information Engineering, Fuzhou University, Fuzhou, China); Zhuang, Xiahai (School of Data Science, Fudan University, Shanghai, China)","Zhuang, Xiahai (Fudan University)","Li, Lei (Fudan University; Shanghai Jiao Tong University); Ding, Wangbin (Fuzhou University); Huang, Liqin (Fuzhou University); Zhuang, Xiahai (Fudan University)",2,2,,,http://arxiv.org/pdf/2109.02171,https://app.dimensions.ai/details/publication/pub.1144687364,46 Information and Computing Sciences,
2251,pub.1140941656,10.48550/arxiv.2109.02171,,,Right Ventricular Segmentation from Short- and Long-Axis MRIs via  Information Transition,"Right ventricular (RV) segmentation from magnetic resonance imaging (MRI) is
a crucial step for cardiac morphology and function analysis. However, automatic
RV segmentation from MRI is still challenging, mainly due to the heterogeneous
intensity, the complex variable shapes, and the unclear RV boundary. Moreover,
current methods for the RV segmentation tend to suffer from performance
degradation at the basal and apical slices of MRI. In this work, we propose an
automatic RV segmentation framework, where the information from long-axis (LA)
views is utilized to assist the segmentation of short-axis (SA) views via
information transition. Specifically, we employed the transformed segmentation
from LA views as a prior information, to extract the ROI from SA views for
better segmentation. The information transition aims to remove the surrounding
ambiguous regions in the SA views. %, such as the tricuspid valve regions. We
tested our model on a public dataset with 360 multi-center, multi-vendor and
multi-disease subjects that consist of both LA and SA MRIs. Our experimental
results show that including LA views can be effective to improve the accuracy
of the SA segmentation. Our model is publicly available at
https://github.com/NanYoMy/MMs-2.",,,arXiv,,,2021-09-05,2021,,,,,,All OA; Green,Preprint,"Li, Lei; Ding, Wangbin; Huang, Liqun; Zhuang, Xiahai","Li, Lei (); Ding, Wangbin (); Huang, Liqun (); Zhuang, Xiahai ()",,"Li, Lei (); Ding, Wangbin (); Huang, Liqun (); Zhuang, Xiahai ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1140941656,32 Biomedical and Clinical Sciences; 3201 Cardiovascular Medicine and Haematology; 46 Information and Computing Sciences,
2202,pub.1134364271,10.48550/arxiv.2101.01513,,,Deep Class-Specific Affinity-Guided Convolutional Network for Multimodal  Unpaired Image Segmentation,"Multi-modal medical image segmentation plays an essential role in clinical
diagnosis. It remains challenging as the input modalities are often not
well-aligned spatially. Existing learning-based methods mainly consider sharing
trainable layers across modalities and minimizing visual feature discrepancies.
While the problem is often formulated as joint supervised feature learning,
multiple-scale features and class-specific representation have not yet been
explored. In this paper, we propose an affinity-guided fully convolutional
network for multimodal image segmentation. To learn effective representations,
we design class-specific affinity matrices to encode the knowledge of
hierarchical feature reasoning, together with the shared convolutional layers
to ensure the cross-modality generalization. Our affinity matrix does not
depend on spatial alignments of the visual features and thus allows us to train
with unpaired, multimodal inputs. We extensively evaluated our method on two
public multimodal benchmark datasets and outperform state-of-the-art methods.",,,arXiv,,,2021-01-05,2021,,,,,,All OA; Green,Preprint,"Chen, Jingkun; Li, Wenqi; Li, Hongwei; Zhang, Jianguo","Chen, Jingkun (); Li, Wenqi (); Li, Hongwei (); Zhang, Jianguo ()",,"Chen, Jingkun (); Li, Wenqi (); Li, Hongwei (); Zhang, Jianguo ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1134364271,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
2157,pub.1154882203,10.1007/978-3-031-23443-9_32,,,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels,"Cardiac segmentation is in great demand for clinical practice. Due to the enormous labor of manual delineation, unsupervised segmentation is desired. The ill-posed optimization problem of this task is inherently challenging, requiring well-designed constraints. In this work, we propose an unsupervised framework for multi-class segmentation with both intensity and shape constraints. Firstly, we extend a conventional non-convex energy function as an intensity constraint and implement it with U-Net. For shape constraint, synthetic images are generated from anatomical labels via image-to-image translation, as shape supervision for the segmentation network. Moreover, augmentation invariance is applied to facilitate the segmentation network to learn the latent features in terms of shape. We evaluated the proposed framework using the public datasets from MICCAI2019 MSCMR Challenge, and achieved promising results on cardiac MRIs with Dice scores of 0.5737, 0.7796, and 0.6287 in Myo, LV, and RV, respectively.",,,Lecture Notes in Computer Science,Statistical Atlases and Computational Models of the Heart. Regular and CMRxMotion Challenge Papers,,2022,2022,2023-01-28,2022,13593,,349-358,All OA; Green,Chapter,"Wang, Sihan; Wu, Fuping; Li, Lei; Gao, Zheyao; Hong, Byung-Woo; Zhuang, Xiahai","Wang, Sihan (School of Data Science, Fudan University, Shanghai, China); Wu, Fuping (School of Data Science, Fudan University, Shanghai, China); Li, Lei (Institute of Biomedical Engineering, University of Oxford, Oxford, UK); Gao, Zheyao (School of Data Science, Fudan University, Shanghai, China); Hong, Byung-Woo (Computer Science Department, Chung-Ang University, Seoul, Korea); Zhuang, Xiahai (School of Data Science, Fudan University, Shanghai, China)","Zhuang, Xiahai (Fudan University)","Wang, Sihan (Fudan University); Wu, Fuping (Fudan University); Li, Lei (University of Oxford); Gao, Zheyao (Fudan University); Hong, Byung-Woo (Chung-Ang University); Zhuang, Xiahai (Fudan University)",0,0,,,http://arxiv.org/pdf/2301.06043,https://app.dimensions.ai/details/publication/pub.1154882203,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
2157,pub.1154627682,10.48550/arxiv.2301.06043,,,Unsupervised Cardiac Segmentation Utilizing Synthesized Images from  Anatomical Labels,"Cardiac segmentation is in great demand for clinical practice. Due to the
enormous labor of manual delineation, unsupervised segmentation is desired. The
ill-posed optimization problem of this task is inherently challenging,
requiring well-designed constraints. In this work, we propose an unsupervised
framework for multi-class segmentation with both intensity and shape
constraints. Firstly, we extend a conventional non-convex energy function as an
intensity constraint and implement it with U-Net. For shape constraint,
synthetic images are generated from anatomical labels via image-to-image
translation, as shape supervision for the segmentation network. Moreover,
augmentation invariance is applied to facilitate the segmentation network to
learn the latent features in terms of shape. We evaluated the proposed
framework using the public datasets from MICCAI2019 MSCMR Challenge and
achieved promising results on cardiac MRIs with Dice scores of 0.5737, 0.7796,
and 0.6287 in Myo, LV, and RV, respectively.",,,arXiv,,,2023-01-15,2023,,,,,,All OA; Green,Preprint,"Wang, Sihan; Wu, Fuping; Li, Lei; Gao, Zheyao; Hong, Byung-Woo; Zhuang, Xiahai","Wang, Sihan (); Wu, Fuping (); Li, Lei (); Gao, Zheyao (); Hong, Byung-Woo (); Zhuang, Xiahai ()",,"Wang, Sihan (); Wu, Fuping (); Li, Lei (); Gao, Zheyao (); Hong, Byung-Woo (); Zhuang, Xiahai ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154627682,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
2155,pub.1131397901,10.1007/978-3-030-59719-1_19,,,Deep Class-Specific Affinity-Guided Convolutional Network for Multimodal Unpaired Image Segmentation,"Multi-modal medical image segmentation plays an essential role in clinical diagnosis. It remains challenging as the input modalities are often not well-aligned spatially. Existing learning-based methods mainly consider sharing trainable layers across modalities and minimizing visual feature discrepancies. While the problem is often formulated as joint supervised feature learning, multiple-scale features and class-specific representation have not yet been explored. In this paper, we propose an affinity-guided fully convolutional network for multimodal image segmentation. To learn effective representations, we design class-specific affinity matrices to encode the knowledge of hierarchical feature reasoning, together with the shared convolutional layers to ensure the cross-modality generalization. Our affinity matrix does not depend on spatial alignments of the visual features and thus allows us to train with unpaired, multimodal inputs. We extensively evaluated our method on two public multimodal benchmark datasets and outperform state-of-the-art methods.",,,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,,2020-09-29,2020,2020-09-29,2020,12264,,187-196,All OA; Green,Chapter,"Chen, Jingkun; Li, Wenqi; Li, Hongwei; Zhang, Jianguo","Chen, Jingkun (Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China); Li, Wenqi (NVIDIA, Santa Clara, USA); Li, Hongwei (Technical University of Munich, Munich, Germany); Zhang, Jianguo (Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China)","Zhang, Jianguo (Southern University of Science and Technology)","Chen, Jingkun (Southern University of Science and Technology); Li, Wenqi (Nvidia (United States)); Li, Hongwei (Technical University of Munich); Zhang, Jianguo (Southern University of Science and Technology)",6,6,,2.97,http://arxiv.org/pdf/2101.01513,https://app.dimensions.ai/details/publication/pub.1131397901,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
2153,pub.1133717591,10.1007/978-3-030-65651-5_4,,,Multi-modality Pathology Segmentation Framework: Application to Cardiac Magnetic Resonance Images,"Multi-sequence of cardiac magnetic resonance (CMR) images can provide complementary information for myocardial pathology (scar and edema). However, it is still challenging to fuse these underlying information for pathology segmentation effectively. This work presents an automatic cascade pathology segmentation framework based on multi-modality CMR images. It mainly consists of two neural networks: an anatomical structure segmentation network (ASSN) and a pathological region segmentation network (PRSN). Specifically, the ASSN aims to segment the anatomical structure where the pathology may exist, and it can provide a spatial prior for the pathological region segmentation. In addition, we integrate a denoising auto-encoder (DAE) into the ASSN to generate segmentation results with plausible shapes. The PRSN is designed to segment pathological region based on the result of ASSN, in which a fusion block based on channel attention is proposed to better aggregate multi-modality information from multi-modality CMR images. Experiments from the MyoPS2020 challenge dataset show that our framework can achieve promising performance for myocardial scar and edema segmentation.",,,Lecture Notes in Computer Science,Myocardial Pathology Segmentation Combining Multi-Sequence Cardiac Magnetic Resonance Images,,2020-12-21,2020,2020-12-21,2020,12554,,37-48,All OA; Green,Chapter,"Zhang, Zhen; Liu, Chenyu; Ding, Wangbin; Wang, Sihan; Pei, Chenhao; Yang, Mingjing; Huang, Liqin","Zhang, Zhen (College of Physics and Information Engineering, Fuzhou University, Fuzhou, China); Liu, Chenyu (College of Physics and Information Engineering, Fuzhou University, Fuzhou, China); Ding, Wangbin (College of Physics and Information Engineering, Fuzhou University, Fuzhou, China); Wang, Sihan (School of Basic Medical Science, Fudan University, Shanghai, China); Pei, Chenhao (College of Physics and Information Engineering, Fuzhou University, Fuzhou, China); Yang, Mingjing (College of Physics and Information Engineering, Fuzhou University, Fuzhou, China); Huang, Liqin (College of Physics and Information Engineering, Fuzhou University, Fuzhou, China)","Yang, Mingjing (Fuzhou University)","Zhang, Zhen (Fuzhou University); Liu, Chenyu (Fuzhou University); Ding, Wangbin (Fuzhou University); Wang, Sihan (Fudan University); Pei, Chenhao (Fuzhou University); Yang, Mingjing (Fuzhou University); Huang, Liqin (Fuzhou University)",10,10,,4.76,http://arxiv.org/pdf/2008.05780,https://app.dimensions.ai/details/publication/pub.1133717591,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
2112,pub.1130100375,10.48550/arxiv.2008.05780,,,Multi-Modality Pathology Segmentation Framework: Application to Cardiac  Magnetic Resonance Images,"Multi-sequence of cardiac magnetic resonance (CMR) images can provide
complementary information for myocardial pathology (scar and edema). However,
it is still challenging to fuse these underlying information for pathology
segmentation effectively. This work presents an automatic cascade pathology
segmentation framework based on multi-modality CMR images. It mainly consists
of two neural networks: an anatomical structure segmentation network (ASSN) and
a pathological region segmentation network (PRSN). Specifically, the ASSN aims
to segment the anatomical structure where the pathology may exist, and it can
provide a spatial prior for the pathological region segmentation. In addition,
we integrate a denoising auto-encoder (DAE) into the ASSN to generate
segmentation results with plausible shapes. The PRSN is designed to segment
pathological region based on the result of ASSN, in which a fusion block based
on channel attention is proposed to better aggregate multi-modality information
from multi-modality CMR images. Experiments from the MyoPS2020 challenge
dataset show that our framework can achieve promising performance for
myocardial scar and edema segmentation.",,,arXiv,,,2020-08-13,2020,,,,,,All OA; Green,Preprint,"Zhang, Zhen; Liu, Chenyu; Ding, Wangbin; Wang, Sihan; Pei, Chenhao; Yang, Mingjing; Huang, Liqin","Zhang, Zhen (); Liu, Chenyu (); Ding, Wangbin (); Wang, Sihan (); Pei, Chenhao (); Yang, Mingjing (); Huang, Liqin ()",,"Zhang, Zhen (); Liu, Chenyu (); Ding, Wangbin (); Wang, Sihan (); Pei, Chenhao (); Yang, Mingjing (); Huang, Liqin ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1130100375,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
2108,pub.1148489502,10.48550/arxiv.2206.02377,,,BInGo: Bayesian Intrinsic Groupwise Registration via Explicit  Hierarchical Disentanglement,"Multimodal groupwise registration aligns internal structures in a group of
medical images. Current approaches to this problem involve developing
similarity measures over the joint intensity profile of all images, which may
be computationally prohibitive for large image groups and unstable under
various conditions. To tackle these issues, we propose BInGo, a general
unsupervised hierarchical Bayesian framework based on deep learning, to learn
intrinsic structural representations to measure the similarity of multimodal
images. Particularly, a variational auto-encoder with a novel posterior is
proposed, which facilitates the disentanglement learning of structural
representations and spatial transformations, and characterizes the imaging
process from the common structure with shape transition and appearance
variation. Notably, BInGo is scalable to learn from small groups, whereas being
tested for large-scale groupwise registration, thus significantly reducing
computational costs. We compared BInGo with five iterative or deep learning
methods on three public intrasubject and intersubject datasets, i.e. BraTS,
MS-CMR of the heart, and Learn2Reg abdomen MR-CT, and demonstrated its superior
accuracy and computational efficiency, even for very large group sizes (e.g.,
over 1300 2D images from MS-CMR in each group).",,,arXiv,,,2022-06-06,2022,,,,,,All OA; Green,Preprint,"Wang, Xin; Luo, Xinzhe; Zhuang, Xiahai","Wang, Xin (); Luo, Xinzhe (); Zhuang, Xiahai ()",,"Wang, Xin (); Luo, Xinzhe (); Zhuang, Xiahai ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148489502,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
2108,pub.1148555964,10.48550/arxiv.2206.03888,,,ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical  Image Segmentation,"Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to an unlabeled target domain. Contrastive learning
(CL) in the context of UDA can help to better separate classes in feature
space. However, in image segmentation, the large memory footprint due to the
computation of the pixel-wise contrastive loss makes it prohibitive to use.
Furthermore, labeled target data is not easily available in medical imaging,
and obtaining new samples is not economical. As a result, in this work, we
tackle a more challenging UDA task when there are only a few (fewshot) or a
single (oneshot) image available from the target domain. We apply a style
transfer module to mitigate the scarcity of target samples. Then, to align the
source and target features and tackle the memory issue of the traditional
contrastive loss, we propose the centroid-based contrastive learning (CCL) and
a centroid norm regularizer (CNR) to optimize the contrastive pairs in both
direction and magnitude. In addition, we propose multi-partition centroid
contrastive learning (MPCCL) to further reduce the variance in the target
features. Fewshot evaluation on MS-CMRSeg dataset demonstrates that ConFUDA
improves the segmentation performance by 0.34 of the Dice score on the target
domain compared with the baseline, and 0.31 Dice score improvement in a more
rigorous oneshot setting.",,,arXiv,,,2022-06-08,2022,,,,,,All OA; Green,Preprint,"Gu, Mingxuan; Vesal, Sulaiman; Thies, Mareike; Pan, Zhaoya; Wagner, Fabian; Rusu, Mirabela; Maier, Andreas; Kosti, Ronak","Gu, Mingxuan (); Vesal, Sulaiman (); Thies, Mareike (); Pan, Zhaoya (); Wagner, Fabian (); Rusu, Mirabela (); Maier, Andreas (); Kosti, Ronak ()",,"Gu, Mingxuan (); Vesal, Sulaiman (); Thies, Mareike (); Pan, Zhaoya (); Wagner, Fabian (); Rusu, Mirabela (); Maier, Andreas (); Kosti, Ronak ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148555964,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
2025,pub.1151032985,10.1007/978-3-031-16443-9_15,,,MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation,"Convolutional neural networks (CNNs) have achieved remarkable segmentation accuracy on benchmark datasets where training and test sets are from the same domain, yet their performance can degrade significantly on unseen domains, which hinders the deployment of CNNs in many clinical scenarios. Most existing works improve model out-of-domain (OOD) robustness by collecting multi-domain datasets for training, which is expensive and may not always be feasible due to privacy and logistical issues. In this work, we focus on improving model robustness using a single-domain dataset only. We propose a novel data augmentation framework called MaxStyle, which maximizes the effectiveness of style augmentation for model OOD performance. It attaches an auxiliary style-augmented image decoder to a segmentation network for robust feature learning and data augmentation. Importantly, MaxStyle augments data with improved image style diversity and hardness, by expanding the style space with noise and searching for the worst-case style composition of latent features via adversarial training. With extensive experiments on multiple public cardiac and prostate MR datasets, we demonstrate that MaxStyle leads to significantly improved out-of-distribution robustness against unseen corruptions as well as common distribution shifts across multiple, different, unseen sites and unknown image sequences under both low- and high-training data settings. The code can be found at https://github.com/cherise215/MaxStyle.","This work was supported by two EPSRC Programme Grants (EP/P001009/1, EP/W01842X/1) and the UKRI Innovate UK Grant (No.104691).",,Lecture Notes in Computer Science,Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,,2022-09-16,2022,2022-09-16,2022,13435,,151-161,All OA; Green,Chapter,"Chen, Chen; Li, Zeju; Ouyang, Cheng; Sinclair, Matthew; Bai, Wenjia; Rueckert, Daniel","Chen, Chen (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Li, Zeju (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Ouyang, Cheng (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Sinclair, Matthew (BioMedIA Group, Department of Computing, Imperial College London, London, UK; HeartFlow, Mountain View, USA); Bai, Wenjia (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Data Science Institute, Imperial College London, London, UK; Department of Brain Sciences, Imperial College London, London, UK); Rueckert, Daniel (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Klinikum rechts der Isar, Technical University of Munich, Munich, Germany)","Chen, Chen (Imperial College London)","Chen, Chen (Imperial College London); Li, Zeju (Imperial College London); Ouyang, Cheng (Imperial College London); Sinclair, Matthew (Imperial College London; HeartFlow (United States)); Bai, Wenjia (Imperial College London; Imperial College London; Imperial College London); Rueckert, Daniel (Imperial College London; Rechts der Isar Hospital; Technical University of Munich)",0,0,,,http://arxiv.org/pdf/2206.01737,https://app.dimensions.ai/details/publication/pub.1151032985,46 Information and Computing Sciences; 4611 Machine Learning,
2013,pub.1128692081,10.48550/arxiv.2006.12434,,,Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark  Study from Multi-Sequence Cardiac MR Segmentation Challenge,"Accurate computing, analysis and modeling of the ventricles and myocardium
from medical images are important, especially in the diagnosis and treatment
management for patients suffering from myocardial infarction (MI). Late
gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an
important protocol to visualize MI. However, automated segmentation of LGE CMR
is still challenging, due to the indistinguishable boundaries, heterogeneous
intensity distribution and complex enhancement patterns of pathological
myocardium from LGE CMR. Furthermore, compared with the other sequences LGE CMR
images with gold standard labels are particularly limited, which represents
another obstacle for developing novel algorithms for automatic segmentation of
LGE CMR. This paper presents the selective results from the Multi-Sequence
Cardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019.
The challenge offered a data set of paired MS-CMR images, including auxiliary
CMR sequences as well as LGE CMR, from 45 patients who underwent
cardiomyopathy. It was aimed to develop new algorithms, as well as benchmark
existing ones for LGE CMR segmentation and compare them objectively. In
addition, the paired MS-CMR images could enable algorithms to combine the
complementary information from the other sequences for the segmentation of LGE
CMR. Nine representative works were selected for evaluation and comparisons,
among which three methods are unsupervised methods and the other six are
supervised. The results showed that the average performance of the nine methods
was comparable to the inter-observer variations. The success of these methods
was mainly attributed to the inclusion of the auxiliary sequences from the
MS-CMR images, which provide important label information for the training of
deep neural networks.",,,arXiv,,,2020-06-22,2020,,,,,,All OA; Green,Preprint,"Zhuang, Xiahai; Xu, Jiahang; Luo, Xinzhe; Chen, Chen; Ouyang, Cheng; Rueckert, Daniel; Campello, Victor M.; Lekadir, Karim; Vesal, Sulaiman; RaviKumar, Nishant; Liu, Yashu; Luo, Gongning; Chen, Jingkun; Li, Hongwei; Ly, Buntheng; Sermesant, Maxime; Roth, Holger; Zhu, Wentao; Wang, Jiexiang; Ding, Xinghao; Wang, Xinyue; Yang, Sen; Li, Lei","Zhuang, Xiahai (); Xu, Jiahang (); Luo, Xinzhe (); Chen, Chen (); Ouyang, Cheng (); Rueckert, Daniel (); Campello, Victor M. (); Lekadir, Karim (); Vesal, Sulaiman (); RaviKumar, Nishant (); Liu, Yashu (); Luo, Gongning (); Chen, Jingkun (); Li, Hongwei (); Ly, Buntheng (); Sermesant, Maxime (); Roth, Holger (); Zhu, Wentao (); Wang, Jiexiang (); Ding, Xinghao (); Wang, Xinyue (); Yang, Sen (); Li, Lei ()",,"Zhuang, Xiahai (); Xu, Jiahang (); Luo, Xinzhe (); Chen, Chen (); Ouyang, Cheng (); Rueckert, Daniel (); Campello, Victor M. (); Lekadir, Karim (); Vesal, Sulaiman (); RaviKumar, Nishant (); Liu, Yashu (); Luo, Gongning (); Chen, Jingkun (); Li, Hongwei (); Ly, Buntheng (); Sermesant, Maxime (); Roth, Holger (); Zhu, Wentao (); Wang, Jiexiang (); Ding, Xinghao (); Wang, Xinyue (); Yang, Sen (); Li, Lei ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1128692081,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
1950,pub.1151072775,10.1007/978-3-031-16749-2_6,,,Improved Post-hoc Probability Calibration for Out-of-Domain MRI Segmentation,"Probability calibration for deep models is highly desirable in safety-critical applications such as medical imaging. It makes output probabilities of deep networks interpretable, by aligning prediction probability with the actual accuracy in test data. In image segmentation, well-calibrated probabilities allow radiologists to identify regions where model-predicted segmentations are unreliable. These unreliable predictions often occur to out-of-domain (OOD) images that are caused by imaging artifacts or unseen imaging protocols. Unfortunately, most previous calibration methods for image segmentation perform sub-optimally on OOD images. To reduce the calibration error when confronted with OOD images, we propose a novel post-hoc calibration model. Our model leverages the pixel susceptibility against perturbations at the local level, and the shape prior information at the global level. The model is tested on cardiac MRI segmentation datasets that contain unseen imaging artifacts and images from an unseen imaging protocol. We demonstrate reduced calibration errors compared with the state-of-the-art calibration algorithm.","This work was in part supported by EPSRC Programme Grants (EP/P001009/1, EP/W01842X/1) and in part by the UKRI London Medical Imaging and Artificial Intelligence Centre for Value Based Healthcare (No. 104691). S.W. was also supported by the Shanghai Sailing Programs of Shanghai Municipal Science and Technology Committee (22YF1409300).",,Lecture Notes in Computer Science,Uncertainty for Safe Utilization of Machine Learning in Medical Imaging,,2022-09-14,2022,2022-09-14,2022,13563,,59-69,All OA; Green,Chapter,"Ouyang, Cheng; Wang, Shuo; Chen, Chen; Li, Zeju; Bai, Wenjia; Kainz, Bernhard; Rueckert, Daniel","Ouyang, Cheng (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Wang, Shuo (School of Basic Medical Sciences, Fudan University, Shanghai, China); Chen, Chen (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Li, Zeju (BioMedIA Group, Department of Computing, Imperial College London, London, UK); Bai, Wenjia (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Department of Brain Sciences, Imperial College London, London, UK; Data Science Institute, Imperial College London, London, UK); Kainz, Bernhard (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany); Rueckert, Daniel (BioMedIA Group, Department of Computing, Imperial College London, London, UK; Klinikum rechts der Isar, Technical University of Munich, Munich, Germany)","Ouyang, Cheng (Imperial College London)","Ouyang, Cheng (Imperial College London); Wang, Shuo (Fudan University); Chen, Chen (Imperial College London); Li, Zeju (Imperial College London); Bai, Wenjia (Imperial College London; Imperial College London; Imperial College London); Kainz, Bernhard (Imperial College London; University of Erlangen-Nuremberg); Rueckert, Daniel (Imperial College London; Technical University of Munich; Rechts der Isar Hospital)",0,0,,,http://arxiv.org/pdf/2208.02870,https://app.dimensions.ai/details/publication/pub.1151072775,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1948,pub.1148488878,10.48550/arxiv.2206.01737,,,MaxStyle: Adversarial Style Composition for Robust Medical Image  Segmentation,"Convolutional neural networks (CNNs) have achieved remarkable segmentation
accuracy on benchmark datasets where training and test sets are from the same
domain, yet their performance can degrade significantly on unseen domains,
which hinders the deployment of CNNs in many clinical scenarios. Most existing
works improve model out-of-domain (OOD) robustness by collecting multi-domain
datasets for training, which is expensive and may not always be feasible due to
privacy and logistical issues. In this work, we focus on improving model
robustness using a single-domain dataset only. We propose a novel data
augmentation framework called MaxStyle, which maximizes the effectiveness of
style augmentation for model OOD performance. It attaches an auxiliary
style-augmented image decoder to a segmentation network for robust feature
learning and data augmentation. Importantly, MaxStyle augments data with
improved image style diversity and hardness, by expanding the style space with
noise and searching for the worst-case style composition of latent features via
adversarial training. With extensive experiments on multiple public cardiac and
prostate MR datasets, we demonstrate that MaxStyle leads to significantly
improved out-of-distribution robustness against unseen corruptions as well as
common distribution shifts across multiple, different, unseen sites and unknown
image sequences under both low- and high-training data settings. The code can
be found at https://github.com/cherise215/MaxStyle.",,,arXiv,,,2022-06-02,2022,,,,,,All OA; Green,Preprint,"Chen, Chen; Li, Zeju; Ouyang, Cheng; Sinclair, Matt; Bai, Wenjia; Rueckert, Daniel","Chen, Chen (); Li, Zeju (); Ouyang, Cheng (); Sinclair, Matt (); Bai, Wenjia (); Rueckert, Daniel ()",,"Chen, Chen (); Li, Zeju (); Ouyang, Cheng (); Sinclair, Matt (); Bai, Wenjia (); Rueckert, Daniel ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1148488878,46 Information and Computing Sciences; 4611 Machine Learning,
1946,pub.1150069047,10.48550/arxiv.2208.02870,,,Improved post-hoc probability calibration for out-of-domain MRI  segmentation,"Probability calibration for deep models is highly desirable in
safety-critical applications such as medical imaging. It makes output
probabilities of deep networks interpretable, by aligning prediction
probability with the actual accuracy in test data. In image segmentation,
well-calibrated probabilities allow radiologists to identify regions where
model-predicted segmentations are unreliable. These unreliable predictions
often occur to out-of-domain (OOD) images that are caused by imaging artifacts
or unseen imaging protocols. Unfortunately, most previous calibration methods
for image segmentation perform sub-optimally on OOD images. To reduce the
calibration error when confronted with OOD images, we propose a novel post-hoc
calibration model. Our model leverages the pixel susceptibility against
perturbations at the local level, and the shape prior information at the global
level. The model is tested on cardiac MRI segmentation datasets that contain
unseen imaging artifacts and images from an unseen imaging protocol. We
demonstrate reduced calibration errors compared with the state-of-the-art
calibration algorithm.",,,arXiv,,,2022-08-04,2022,,,,,,All OA; Green,Preprint,"Ouyang, Cheng; Wang, Shuo; Chen, Chen; Li, Zeju; Bai, Wenjia; Kainz, Bernhard; Rueckert, Daniel","Ouyang, Cheng (); Wang, Shuo (); Chen, Chen (); Li, Zeju (); Bai, Wenjia (); Kainz, Bernhard (); Rueckert, Daniel ()",,"Ouyang, Cheng (); Wang, Shuo (); Chen, Chen (); Li, Zeju (); Bai, Wenjia (); Kainz, Bernhard (); Rueckert, Daniel ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150069047,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1690,pub.1147620728,10.1016/j.knosys.2022.108942,,,Deep U-Net architecture with curriculum learning for myocardial pathology segmentation in multi-sequence cardiac magnetic resonance images,"Myocardial pathology segmentation is essential for the diagnosis and treatment of patients suffering from myocardial infarction. In this work, we propose an end-to-end deep learning based segmentation method for automatically delineating the area of left ventricle (LV) myocardial infarct and edema regions. The proposed method uses the 6 layers deep U-Net architecture as the segmentation backbone, which adopts a hierarchical feature representation with symmetrical encoder–decoder paths. Skip connections are added between encoder and decoder paths, to concatenate low-level and high-level information for better feature representation. Moreover, three other modules, direction field module (DFM), channel self-attention module (CAM) and selective kernel module (SKM), have also been implemented for further exploration of performance improvement. The proposed method is tested on the public MyoPS 2020 (myocardial pathology segmentation combining multi-sequence cardiac magnetic resonance) challenge dataset. Compared with extra self-attention module or selective kernel module, plain deep U-Net with curriculum learning achieves better results on testing dataset. Extensive ablation experiments are performed to explore the optimal depth of U-Net, multiple loss functions and different data augmentation methods. Using the official evaluation kit, our solution outperforms state-of-the-art single stage approaches, and achieves comparable performance with other advanced multi-stage methods. The evaluation results demonstrate our method’s effectiveness on myocardial pathology segmentation in multi-sequence cardiac magnetic resonance (CMR) data, and the superiority to the current state-of-the-art single stage methods.","The study was supported in part by the National Natural Science Foundation of China under Grants 61801393 and 62171377, and in part by the Fundamental Research Funds for the Central Universities under Grant 3102020QD1001.",,Knowledge-Based Systems,,,2022-08,2022,,2022-08,249,,108942,Closed,Article,"Cui, Hengfei; Jiang, Lei; Yuwen, Chang; Xia, Yong; Zhang, Yanning","Cui, Hengfei (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China; Centre for Multidisciplinary Convergence Computing (CMCC), School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China); Jiang, Lei (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China; Centre for Multidisciplinary Convergence Computing (CMCC), School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China); Yuwen, Chang (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China; Centre for Multidisciplinary Convergence Computing (CMCC), School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China); Xia, Yong (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China; Centre for Multidisciplinary Convergence Computing (CMCC), School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China); Zhang, Yanning (National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China)","Cui, Hengfei (Northwestern Polytechnical University; Northwestern Polytechnical University)","Cui, Hengfei (Northwestern Polytechnical University; Northwestern Polytechnical University); Jiang, Lei (Northwestern Polytechnical University; Northwestern Polytechnical University); Yuwen, Chang (Northwestern Polytechnical University; Northwestern Polytechnical University); Xia, Yong (Northwestern Polytechnical University; Northwestern Polytechnical University); Zhang, Yanning (Northwestern Polytechnical University)",4,4,,,,https://app.dimensions.ai/details/publication/pub.1147620728,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4611 Machine Learning,
1686,pub.1153198877,10.48550/arxiv.2211.14805,,,Rethinking Data Augmentation for Single-source Domain Generalization in  Medical Image Segmentation,"Single-source domain generalization (SDG) in medical image segmentation is a
challenging yet essential task as domain shifts are quite common among clinical
image datasets. Previous attempts most conduct global-only/random augmentation.
Their augmented samples are usually insufficient in diversity and
informativeness, thus failing to cover the possible target domain distribution.
In this paper, we rethink the data augmentation strategy for SDG in medical
image segmentation. Motivated by the class-level representation invariance and
style mutability of medical images, we hypothesize that unseen target data can
be sampled from a linear combination of $C$ (the class number) random
variables, where each variable follows a location-scale distribution at the
class level. Accordingly, data augmented can be readily made by sampling the
random variables through a general form. On the empirical front, we implement
such strategy with constrained B$\acute{\rm e}$zier transformation on both
global and local (i.e. class-level) regions, which can largely increase the
augmentation diversity. A Saliency-balancing Fusion mechanism is further
proposed to enrich the informativeness by engaging the gradient information,
guiding augmentation with proper orientation and magnitude. As an important
contribution, we prove theoretically that our proposed augmentation can lead to
an upper bound of the generalization risk on the unseen target domain, thus
confirming our hypothesis. Combining the two strategies, our Saliency-balancing
Location-scale Augmentation (SLAug) exceeds the state-of-the-art works by a
large margin in two challenging SDG tasks. Code is available at
https://github.com/Kaiseem/SLAug .",,,arXiv,,,2022-11-27,2022,,,,,,All OA; Green,Preprint,"Su, Zixian; Yao, Kai; Yang, Xi; Wang, Qiufeng; Sun, Jie; Huang, Kaizhu","Su, Zixian (); Yao, Kai (); Yang, Xi (); Wang, Qiufeng (); Sun, Jie (); Huang, Kaizhu ()",,"Su, Zixian (); Yao, Kai (); Yang, Xi (); Wang, Qiufeng (); Sun, Jie (); Huang, Kaizhu ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1153198877,46 Information and Computing Sciences; 4611 Machine Learning,
1581,pub.1131970475,10.48550/arxiv.2010.11081,,,Anatomically-Informed Deep Learning on Contrast-Enhanced Cardiac MRI for  Scar Segmentation and Clinical Feature Extraction,"Visualizing disease-induced scarring and fibrosis in the heart on cardiac
magnetic resonance (CMR) imaging with contrast enhancement (LGE) is paramount
in characterizing disease progression and quantifying pathophysiological
substrates of arrhythmias. However, segmentation and scar/fibrosis
identification from LGE-CMR is an intensive manual process prone to large
inter-observer variability. Here, we present a novel fully-automated
anatomically-informed deep learning solution for left ventricle (LV) and
scar/fibrosis segmentation and clinical feature extraction from LGE-CMR. The
technology involves three cascading convolutional neural networks that segment
myocardium and scar/fibrosis from raw LGE-CMR images and constrain these
segmentations within anatomical guidelines, thus facilitating seamless
derivation of clinically-significant parameters. In addition to available
LGE-CMR images, training used ""LGE-like"" synthetically enhanced cine scans.
Results show excellent agreement with those of trained experts in terms of
segmentation (balanced accuracy of $96\%$ and $75\%$ for LV and scar
segmentation), clinical features ($2\%$ difference in mean scar-to-LV wall
volume fraction), and anatomical fidelity. Our segmentation technology is
extendable to other computer vision medical applications and to problems
requiring guidelines adherence of predicted outputs.",,,arXiv,,,2020-10-21,2020,,,,,,All OA; Green,Preprint,"Abramson, Haley G.; Popescu, Dan M.; Yu, Rebecca; Lai, Changxin; Shade, Julie K.; Wu, Katherine C.; Maggioni, Mauro; Trayanova, Natalia A.","Abramson, Haley G. (); Popescu, Dan M. (); Yu, Rebecca (); Lai, Changxin (); Shade, Julie K. (); Wu, Katherine C. (); Maggioni, Mauro (); Trayanova, Natalia A. ()",,"Abramson, Haley G. (); Popescu, Dan M. (); Yu, Rebecca (); Lai, Changxin (); Shade, Julie K. (); Wu, Katherine C. (); Maggioni, Mauro (); Trayanova, Natalia A. ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1131970475,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
1580,pub.1141914787,10.48550/arxiv.2110.07360,,,Domain generalization in deep learning for contrast-enhanced imaging,"The domain generalization problem has been widely investigated in deep
learning for non-contrast imaging over the last years, but it received limited
attention for contrast-enhanced imaging. However, there are marked differences
in contrast imaging protocols across clinical centers, in particular in the
time between contrast injection and image acquisition, while access to
multi-center contrast-enhanced image data is limited compared to available
datasets for non-contrast imaging. This calls for new tools for generalizing
single-domain, single-center deep learning models across new unseen domains and
clinical centers in contrast-enhanced imaging. In this paper, we present an
exhaustive evaluation of deep learning techniques to achieve generalizability
to unseen clinical centers for contrast-enhanced image segmentation. To this
end, several techniques are investigated, optimized and systematically
evaluated, including data augmentation, domain mixing, transfer learning and
domain adaptation. To demonstrate the potential of domain generalization for
contrast-enhanced imaging, the methods are evaluated for ventricular
segmentation in contrast-enhanced cardiac magnetic resonance imaging (MRI). The
results are obtained based on a multi-center cardiac contrast-enhanced MRI
dataset acquired in four hospitals located in three countries (France, Spain
and China). They show that the combination of data augmentation and transfer
learning can lead to single-center models that generalize well to new clinical
centers not included during training. Single-domain neural networks enriched
with suitable generalization procedures can reach and even surpass the
performance of multi-center, multi-vendor models in contrast-enhanced imaging,
hence eliminating the need for comprehensive multi-center datasets to train
generalizable models.",,,arXiv,,,2021-10-14,2021,,,,,,All OA; Green,Preprint,"Sendra-Balcells, Carla; Campello, Víctor M.; Martín-Isla, Carlos; Viladés, David; Descalzo, Martín L.; Guala, Andrea; Rodríguez-Palomares, José F.; Lekadir, Karim","Sendra-Balcells, Carla (); Campello, Víctor M. (); Martín-Isla, Carlos (); Viladés, David (); Descalzo, Martín L. (); Guala, Andrea (); Rodríguez-Palomares, José F. (); Lekadir, Karim ()",,"Sendra-Balcells, Carla (); Campello, Víctor M. (); Martín-Isla, Carlos (); Viladés, David (); Descalzo, Martín L. (); Guala, Andrea (); Rodríguez-Palomares, José F. (); Lekadir, Karim ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141914787,46 Information and Computing Sciences; 4611 Machine Learning,
1498,pub.1143055508,10.48550/arxiv.2111.12525,,,Causality-inspired Single-source Domain Generalization for Medical Image  Segmentation,"Deep learning models usually suffer from domain shift issues, where models
trained on one source domain do not generalize well to other unseen domains. In
this work, we investigate the single-source domain generalization problem:
training a deep network that is robust to unseen domains, under the condition
that training data is only available from one source domain, which is common in
medical imaging applications. We tackle this problem in the context of
cross-domain medical image segmentation. Under this scenario, domain shifts are
mainly caused by different acquisition processes. We propose a simple
causality-inspired data augmentation approach to expose a segmentation model to
synthesized domain-shifted training examples. Specifically, 1) to make the deep
model robust to discrepancies in image intensities and textures, we employ a
family of randomly-weighted shallow networks. They augment training images
using diverse appearance transformations. 2) Further we show that spurious
correlations among objects in an image are detrimental to domain robustness.
These correlations might be taken by the network as domain-specific clues for
making predictions, and they may break on unseen domains. We remove these
spurious correlations via causal intervention. This is achieved by resampling
the appearances of potentially correlated objects independently. The proposed
approach is validated on three cross-domain segmentation tasks: cross-modality
(CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI
segmentation, and cross-center prostate MRI segmentation. The proposed approach
yields consistent performance gains compared with competitive methods when
tested on unseen domains.",,,arXiv,,,2021-11-24,2021,,,,,,All OA; Green,Preprint,"Ouyang, Cheng; Chen, Chen; Li, Surui; Li, Zeju; Qin, Chen; Bai, Wenjia; Rueckert, Daniel","Ouyang, Cheng (); Chen, Chen (); Li, Surui (); Li, Zeju (); Qin, Chen (); Bai, Wenjia (); Rueckert, Daniel ()",,"Ouyang, Cheng (); Chen, Chen (); Li, Surui (); Li, Zeju (); Qin, Chen (); Bai, Wenjia (); Rueckert, Daniel ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1143055508,46 Information and Computing Sciences; 4611 Machine Learning,
1415,pub.1152637746,10.48550/arxiv.2211.04862,,,Domain-incremental Cardiac Image Segmentation with Style-oriented Replay  and Domain-sensitive Feature Whitening,"Contemporary methods have shown promising results on cardiac image
segmentation, but merely in static learning, i.e., optimizing the network once
for all, ignoring potential needs for model updating. In real-world scenarios,
new data continues to be gathered from multiple institutions over time and new
demands keep growing to pursue more satisfying performance. The desired model
should incrementally learn from each incoming dataset and progressively update
with improved functionality as time goes by. As the datasets sequentially
delivered from multiple sites are normally heterogenous with domain
discrepancy, each updated model should not catastrophically forget previously
learned domains while well generalizing to currently arrived domains or even
unseen domains. In medical scenarios, this is particularly challenging as
accessing or storing past data is commonly not allowed due to data privacy. To
this end, we propose a novel domain-incremental learning framework to recover
past domain inputs first and then regularly replay them during model
optimization. Particularly, we first present a style-oriented replay module to
enable structure-realistic and memory-efficient reproduction of past data, and
then incorporate the replayed past data to jointly optimize the model with
current data to alleviate catastrophic forgetting. During optimization, we
additionally perform domain-sensitive feature whitening to suppress model's
dependency on features that are sensitive to domain changes (e.g.,
domain-distinctive style features) to assist domain-invariant feature
exploration and gradually improve the generalization performance of the
network. We have extensively evaluated our approach with the M&Ms Dataset in
single-domain and compound-domain incremental learning settings with improved
performance over other comparison approaches.",,,arXiv,,,2022-11-09,2022,,,,,,All OA; Green,Preprint,"Li, Kang; Yu, Lequan; Heng, Pheng-Ann","Li, Kang (); Yu, Lequan (); Heng, Pheng-Ann ()",,"Li, Kang (); Yu, Lequan (); Heng, Pheng-Ann ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152637746,46 Information and Computing Sciences; 4611 Machine Learning,
1406,pub.1152570721,10.48550/arxiv.2211.03062,,,MyoPS-Net: Myocardial Pathology Segmentation with Flexible Combination  of Multi-Sequence CMR Images,"Myocardial pathology segmentation (MyoPS) can be a prerequisite for the
accurate diagnosis and treatment planning of myocardial infarction. However,
achieving this segmentation is challenging, mainly due to the inadequate and
indistinct information from an image. In this work, we develop an end-to-end
deep neural network, referred to as MyoPS-Net, to flexibly combine
five-sequence cardiac magnetic resonance (CMR) images for MyoPS. To extract
precise and adequate information, we design an effective yet flexible
architecture to extract and fuse cross-modal features. This architecture can
tackle different numbers of CMR images and complex combinations of modalities,
with output branches targeting specific pathologies. To impose anatomical
knowledge on the segmentation results, we first propose a module to regularize
myocardium consistency and localize the pathologies, and then introduce an
inclusiveness loss to utilize relations between myocardial scars and edema. We
evaluated the proposed MyoPS-Net on two datasets, i.e., a private one
consisting of 50 paired multi-sequence CMR images and a public one from
MICCAI2020 MyoPS Challenge. Experimental results showed that MyoPS-Net could
achieve state-of-the-art performance in various scenarios. Note that in
practical clinics, the subjects may not have full sequences, such as missing
LGE CMR or mapping CMR scans. We therefore conducted extensive experiments to
investigate the performance of the proposed method in dealing with such complex
combinations of different CMR sequences. Results proved the superiority and
generalizability of MyoPS-Net, and more importantly, indicated a practical
clinical application.",,,arXiv,,,2022-11-06,2022,,,,,,All OA; Green,Preprint,"Qiu, Junyi; Li, Lei; Wang, Sihan; Zhang, Ke; Chen, Yinyin; Yang, Shan; Zhuang, Xiahai","Qiu, Junyi (); Li, Lei (); Wang, Sihan (); Zhang, Ke (); Chen, Yinyin (); Yang, Shan (); Zhuang, Xiahai ()",,"Qiu, Junyi (); Li, Lei (); Wang, Sihan (); Zhang, Ke (); Chen, Yinyin (); Yang, Shan (); Zhuang, Xiahai ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152570721,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1340,pub.1145333990,10.48550/arxiv.2202.02000,,,Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label  Fusion,"Multi-atlas segmentation (MAS) is a promising framework for medical image
segmentation. Generally, MAS methods register multiple atlases, i.e., medical
images with corresponding labels, to a target image; and the transformed atlas
labels can be combined to generate target segmentation via label fusion
schemes. Many conventional MAS methods employed the atlases from the same
modality as the target image. However, the number of atlases with the same
modality may be limited or even missing in many clinical applications. Besides,
conventional MAS methods suffer from the computational burden of registration
or label fusion procedures. In this work, we design a novel cross-modality MAS
framework, which uses available atlases from a certain modality to segment a
target image from another modality. To boost the computational efficiency of
the framework, both the image registration and label fusion are achieved by
well-designed deep neural networks. For the atlas-to-target image registration,
we propose a bi-directional registration network (BiRegNet), which can
efficiently align images from different modalities. For the label fusion, we
design a similarity estimation network (SimNet), which estimates the fusion
weight of each atlas by measuring its similarity to the target image. SimNet
can learn multi-scale information for similarity estimation to improve the
performance of label fusion. The proposed framework was evaluated by the left
ventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets,
respectively. Results have shown that the framework is effective for
cross-modality MAS in both registration and label fusion.",,,arXiv,,,2022-02-04,2022,,,,,,All OA; Green,Preprint,"Ding, Wangbin; Li, Lei; Zhuang, Xiahai; Huang, Liqin","Ding, Wangbin (); Li, Lei (); Zhuang, Xiahai (); Huang, Liqin ()",,"Ding, Wangbin (); Li, Lei (); Zhuang, Xiahai (); Huang, Liqin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1145333990,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1264,pub.1136426375,10.48550/arxiv.2103.08219,,,Adapt Everywhere: Unsupervised Adaptation of Point-Clouds and Entropy  Minimisation for Multi-modal Cardiac Image Segmentation,"Deep learning models are sensitive to domain shift phenomena. A model trained
on images from one domain cannot generalise well when tested on images from a
different domain, despite capturing similar anatomical structures. It is mainly
because the data distribution between the two domains is different. Moreover,
creating annotation for every new modality is a tedious and time-consuming
task, which also suffers from high inter- and intra- observer variability.
Unsupervised domain adaptation (UDA) methods intend to reduce the gap between
source and target domains by leveraging source domain labelled data to generate
labels for the target domain. However, current state-of-the-art (SOTA) UDA
methods demonstrate degraded performance when there is insufficient data in
source and target domains. In this paper, we present a novel UDA method for
multi-modal cardiac image segmentation. The proposed method is based on
adversarial learning and adapts network features between source and target
domain in different spaces. The paper introduces an end-to-end framework that
integrates: a) entropy minimisation, b) output feature space alignment and c) a
novel point-cloud shape adaptation based on the latent features learned by the
segmentation model. We validated our method on two cardiac datasets by adapting
from the annotated source domain, bSSFP-MRI (balanced Steady-State Free
Procession-MRI), to the unannotated target domain, LGE-MRI (Late-gadolinium
enhance-MRI), for the multi-sequence dataset; and from MRI (source) to CT
(target) for the cross-modality dataset. The results highlighted that by
enforcing adversarial learning in different parts of the network, the proposed
method delivered promising performance, compared to other SOTA methods.",,,arXiv,,,2021-03-15,2021,,,,,,All OA; Green,Preprint,"Vesal, Sulaiman; Gu, Mingxuan; Kosti, Ronak; Maier, Andreas; Ravikumar, Nishant","Vesal, Sulaiman (); Gu, Mingxuan (); Kosti, Ronak (); Maier, Andreas (); Ravikumar, Nishant ()",,"Vesal, Sulaiman (); Gu, Mingxuan (); Kosti, Ronak (); Maier, Andreas (); Ravikumar, Nishant ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1136426375,46 Information and Computing Sciences; 4611 Machine Learning,
1264,pub.1144584935,10.48550/arxiv.2201.03186,,,MyoPS: A Benchmark of Myocardial Pathology Segmentation Combining  Three-Sequence Cardiac Magnetic Resonance Images,"Assessment of myocardial viability is essential in diagnosis and treatment
management of patients suffering from myocardial infarction, and classification
of pathology on myocardium is the key to this assessment. This work defines a
new task of medical image analysis, i.e., to perform myocardial pathology
segmentation (MyoPS) combining three-sequence cardiac magnetic resonance (CMR)
images, which was first proposed in the MyoPS challenge, in conjunction with
MICCAI 2020. The challenge provided 45 paired and pre-aligned CMR images,
allowing algorithms to combine the complementary information from the three CMR
sequences for pathology segmentation. In this article, we provide details of
the challenge, survey the works from fifteen participants and interpret their
methods according to five aspects, i.e., preprocessing, data augmentation,
learning strategy, model architecture and post-processing. In addition, we
analyze the results with respect to different factors, in order to examine the
key obstacles and explore potential of solutions, as well as to provide a
benchmark for future research. We conclude that while promising results have
been reported, the research is still in the early stage, and more in-depth
exploration is needed before a successful application to the clinics. Note that
MyoPS data and evaluation tool continue to be publicly available upon
registration via its homepage
(www.sdspeople.fudan.edu.cn/zhuangxiahai/0/myops20/).",,,arXiv,,,2022-01-10,2022,,,,,,All OA; Green,Preprint,"Li, Lei; Wu, Fuping; Wang, Sihan; Luo, Xinzhe; Martin-Isla, Carlos; Zhai, Shuwei; Zhang, Jianpeng; Liu7, Yanfei; Zhang, Zhen; Ankenbrand, Markus J.; Jiang, Haochuan; Zhang, Xiaoran; Wang, Linhong; Arega, Tewodros Weldebirhan; Altunok, Elif; Zhao, Zhou; Li, Feiyan; Ma, Jun; Yang, Xiaoping; Puybareau, Elodie; Oksuz, Ilkay; Bricq, Stephanie; Li, Weisheng; Punithakumar, Kumaradevan; Tsaftaris, Sotirios A.; Schreiber, Laura M.; Yang, Mingjing; Liu, Guocai; Xia, Yong; Wang, Guotai; Escalera, Sergio; Zhuang, Xiahai","Li, Lei (); Wu, Fuping (); Wang, Sihan (); Luo, Xinzhe (); Martin-Isla, Carlos (); Zhai, Shuwei (); Zhang, Jianpeng (); Liu7, Yanfei (); Zhang, Zhen (); Ankenbrand, Markus J. (); Jiang, Haochuan (); Zhang, Xiaoran (); Wang, Linhong (); Arega, Tewodros Weldebirhan (); Altunok, Elif (); Zhao, Zhou (); Li, Feiyan (); Ma, Jun (); Yang, Xiaoping (); Puybareau, Elodie (); Oksuz, Ilkay (); Bricq, Stephanie (); Li, Weisheng (); Punithakumar, Kumaradevan (); Tsaftaris, Sotirios A. (); Schreiber, Laura M. (); Yang, Mingjing (); Liu, Guocai (); Xia, Yong (); Wang, Guotai (); Escalera, Sergio (); Zhuang, Xiahai ()",,"Li, Lei (); Wu, Fuping (); Wang, Sihan (); Luo, Xinzhe (); Martin-Isla, Carlos (); Zhai, Shuwei (); Zhang, Jianpeng (); Liu7, Yanfei (); Zhang, Zhen (); Ankenbrand, Markus J. (); Jiang, Haochuan (); Zhang, Xiaoran (); Wang, Linhong (); Arega, Tewodros Weldebirhan (); Altunok, Elif (); Zhao, Zhou (); Li, Feiyan (); Ma, Jun (); Yang, Xiaoping (); Puybareau, Elodie (); Oksuz, Ilkay (); Bricq, Stephanie (); Li, Weisheng (); Punithakumar, Kumaradevan (); Tsaftaris, Sotirios A. (); Schreiber, Laura M. (); Yang, Mingjing (); Liu, Guocai (); Xia, Yong (); Wang, Guotai (); Escalera, Sergio (); Zhuang, Xiahai ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1144584935,46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1205,pub.1154562507,10.48550/arxiv.2301.05392,,,Multi-Target Landmark Detection with Incomplete Images via Reinforcement  Learning and Shape Prior,"Medical images are generally acquired with limited field-of-view (FOV), which
could lead to incomplete regions of interest (ROI), and thus impose a great
challenge on medical image analysis. This is particularly evident for the
learning-based multi-target landmark detection, where algorithms could be
misleading to learn primarily the variation of background due to the varying
FOV, failing the detection of targets. Based on learning a navigation policy,
instead of predicting targets directly, reinforcement learning (RL)-based
methods have the potential totackle this challenge in an efficient manner.
Inspired by this, in this work we propose a multi-agent RL framework for
simultaneous multi-target landmark detection. This framework is aimed to learn
from incomplete or (and) complete images to form an implicit knowledge of
global structure, which is consolidated during the training stage for the
detection of targets from either complete or incomplete test images. To further
explicitly exploit the global structural information from incomplete images, we
propose to embed a shape model into the RL process. With this prior knowledge,
the proposed RL model can not only localize dozens of targetssimultaneously,
but also work effectively and robustly in the presence of incomplete images. We
validated the applicability and efficacy of the proposed method on various
multi-target detection tasks with incomplete images from practical clinics,
using body dual-energy X-ray absorptiometry (DXA), cardiac MRI and head CT
datasets. Results showed that our method could predict whole set of landmarks
with incomplete training images up to 80% missing proportion (average distance
error 2.29 cm on body DXA), and could detect unseen landmarks in regions with
missing image information outside FOV of target images (average distance error
6.84 mm on 3D half-head CT).",,,arXiv,,,2023-01-13,2023,,,,,,All OA; Green,Preprint,"Wan, Kaiwen; Li, Lei; Jia, Dengqiang; Gao, Shangqi; Qian, Wei; Wu, Yingzhi; Lin, Huandong; Mu, Xiongzheng; Gao, Xin; Wang, Sijia; Wu, Fuping; Zhuang, Xiahai","Wan, Kaiwen (); Li, Lei (); Jia, Dengqiang (); Gao, Shangqi (); Qian, Wei (); Wu, Yingzhi (); Lin, Huandong (); Mu, Xiongzheng (); Gao, Xin (); Wang, Sijia (); Wu, Fuping (); Zhuang, Xiahai ()",,"Wan, Kaiwen (); Li, Lei (); Jia, Dengqiang (); Gao, Shangqi (); Qian, Wei (); Wu, Yingzhi (); Lin, Huandong (); Mu, Xiongzheng (); Gao, Xin (); Wang, Sijia (); Wu, Fuping (); Zhuang, Xiahai ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1154562507,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4603 Computer Vision and Multimedia Computation; 4611 Machine Learning,
1151,pub.1142498366,10.48550/arxiv.2111.04736,,,Multi-Modality Cardiac Image Analysis with Deep Learning,"Accurate cardiac computing, analysis and modeling from multi-modality images
are important for the diagnosis and treatment of cardiac disease. Late
gadolinium enhancement magnetic resonance imaging (LGE MRI) is a promising
technique to visualize and quantify myocardial infarction (MI) and atrial
scars. Automating quantification of MI and atrial scars can be challenging due
to the low image quality and complex enhancement patterns of LGE MRI. Moreover,
compared with the other sequences LGE MRIs with gold standard labels are
particularly limited, which represents another obstacle for developing novel
algorithms for automatic segmentation and quantification of LGE MRIs. This
chapter aims to summarize the state-of-the-art and our recent advanced
contributions on deep learning based multi-modality cardiac image analysis.
Firstly, we introduce two benchmark works for multi-sequence cardiac MRI based
myocardial and pathology segmentation. Secondly, two novel frameworks for left
atrial scar segmentation and quantification from LGE MRI were presented.
Thirdly, we present three unsupervised domain adaptation techniques for
cross-modality cardiac image segmentation.",,,arXiv,,,2021-11-08,2021,,,,,,All OA; Green,Preprint,"Li, Lei; Wu, Fuping; Wang, Sihang; Zhuang, Xiahai","Li, Lei (); Wu, Fuping (); Wang, Sihang (); Zhuang, Xiahai ()",,"Li, Lei (); Wu, Fuping (); Wang, Sihang (); Zhuang, Xiahai ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1142498366,40 Engineering; 4003 Biomedical Engineering; 46 Information and Computing Sciences,
1102,pub.1152449074,10.48550/arxiv.2211.01631,,,$\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework  for Groupwise Registration and Deep Combined Computing,"This paper presents a generic probabilistic framework for estimating the
statistical dependency and finding the anatomical correspondences among an
arbitrary number of medical images. The method builds on a novel formulation of
the $N$-dimensional joint intensity distribution by representing the common
anatomy as latent variables and estimating the appearance model with
nonparametric estimators. Through connection to maximum likelihood and the
expectation-maximization algorithm, an information\hyp{}theoretic metric called
$\mathcal{X}$-metric and a co-registration algorithm named $\mathcal{X}$-CoReg
are induced, allowing groupwise registration of the $N$ observed images with
computational complexity of $\mathcal{O}(N)$. Moreover, the method naturally
extends for a weakly-supervised scenario where anatomical labels of certain
images are provided. This leads to a combined\hyp{}computing framework
implemented with deep learning, which performs registration and segmentation
simultaneously and collaboratively in an end-to-end fashion. Extensive
experiments were conducted to demonstrate the versatility and applicability of
our model, including multimodal groupwise registration, motion correction for
dynamic contrast enhanced magnetic resonance images, and deep combined
computing for multimodal medical images. Results show the superiority of our
method in various applications in terms of both accuracy and efficiency,
highlighting the advantage of the proposed representation of the imaging
process.",,,arXiv,,,2022-11-03,2022,,,,,,All OA; Green,Preprint,"Luo, Xinzhe; Zhuang, Xiahai","Luo, Xinzhe (); Zhuang, Xiahai ()",,"Luo, Xinzhe (); Zhuang, Xiahai ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1152449074,46 Information and Computing Sciences; 4611 Machine Learning,
746,pub.1150593121,10.48550/arxiv.2208.12881,,,Multi-Modality Cardiac Image Computing: A Survey,"Multi-modality cardiac imaging plays a key role in the management of patients
with cardiovascular diseases. It allows a combination of complementary
anatomical, morphological and functional information, increases diagnosis
accuracy, and improves the efficacy of cardiovascular interventions and
clinical outcomes. Fully-automated processing and quantitative analysis of
multi-modality cardiac images could have a direct impact on clinical research
and evidence-based patient management. However, these require overcoming
significant challenges including inter-modality misalignment and finding
optimal methods to integrate information from different modalities.
  This paper aims to provide a comprehensive review of multi-modality imaging
in cardiology, the computing methods, the validation strategies, the related
clinical workflows and future perspectives. For the computing methodologies, we
have a favored focus on the three tasks, i.e., registration, fusion and
segmentation, which generally involve multi-modality imaging data,
\textit{either combining information from different modalities or transferring
information across modalities}. The review highlights that multi-modality
cardiac imaging data has the potential of wide applicability in the clinic,
such as trans-aortic valve implantation guidance, myocardial viability
assessment, and catheter ablation therapy and its patient selection.
Nevertheless, many challenges remain unsolved, such as missing modality,
combination of imaging and non-imaging data, and uniform analysis and
representation of different modalities. There is also work to do in defining
how the well-developed techniques fit in clinical workflows and how much
additional and relevant information they introduce. These problems are likely
to continue to be an active field of research and the questions to be answered
in the future.",,,arXiv,,,2022-08-26,2022,,,,,,All OA; Green,Preprint,"Li, Lei; Ding, Wangbin; Huang, Liqun; Zhuang, Xiahai; Grau, Vicente","Li, Lei (); Ding, Wangbin (); Huang, Liqun (); Zhuang, Xiahai (); Grau, Vicente ()",,"Li, Lei (); Ding, Wangbin (); Huang, Liqun (); Zhuang, Xiahai (); Grau, Vicente ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1150593121,32 Biomedical and Clinical Sciences; 3202 Clinical Sciences,
740,pub.1139041071,10.48550/arxiv.2106.09862,,,Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation  Studies: A Review,"Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly
used to visualize and quantify left atrial (LA) scars. The position and extent
of scars provide important information of the pathophysiology and progression
of atrial fibrillation (AF). Hence, LA scar segmentation and quantification
from LGE MRI can be useful in computer-assisted diagnosis and treatment
stratification of AF patients. Since manual delineation can be time-consuming
and subject to intra- and inter-expert variability, automating this computing
is highly desired, which nevertheless is still challenging and
under-researched.
  This paper aims to provide a systematic review on computing methods for LA
cavity, wall, scar and ablation gap segmentation and quantification from LGE
MRI, and the related literature for AF studies. Specifically, we first
summarize AF-related imaging techniques, particularly LGE MRI. Then, we review
the methodologies of the four computing tasks in detail, and summarize the
validation strategies applied in each task. Finally, the possible future
developments are outlined, with a brief survey on the potential clinical
applications of the aforementioned methods. The review shows that the research
into this topic is still in early stages. Although several methods have been
proposed, especially for LA segmentation, there is still large scope for
further algorithmic developments due to performance issues related to the high
variability of enhancement appearance and differences in image acquisition.",,,arXiv,,,2021-06-17,2021,,,,,,All OA; Green,Preprint,"Li, Lei; Zimmer, Veronika A.; Schnabel, Julia A.; Zhuang, Xiahai","Li, Lei (); Zimmer, Veronika A. (); Schnabel, Julia A. (); Zhuang, Xiahai ()",,"Li, Lei (); Zimmer, Veronika A. (); Schnabel, Julia A. (); Zhuang, Xiahai ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1139041071,40 Engineering; 4003 Biomedical Engineering,
394,pub.1151072765,10.1007/978-3-031-16749-2,,,"Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, 4th International Workshop, UNSURE 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings","This book constitutes the refereed proceedings of the Fourth Workshop on Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, UNSURE 2022, held in conjunction with MICCAI 2022. The conference was hybrid event held from Singapore. For this workshop, 13 papers from 22 submissions were accepted for publication. They focus on developing awareness and encouraging research in the field of uncertainty modelling to enable safe implementation of machine learning tools in the clinical world.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13563,,,All OA; Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16749-2%2F1,https://app.dimensions.ai/details/publication/pub.1151072765,46 Information and Computing Sciences,
372,pub.1133716556,10.1007/978-3-030-65651-5,,,"Myocardial Pathology Segmentation Combining Multi-Sequence Cardiac Magnetic Resonance Images, First Challenge, MyoPS 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings","This book constitutes the First Myocardial Pathology Segmentation Combining Multi-Sequence CMR Challenge, MyoPS 2020, which was held in conjunction with the 23rd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2020, in Lima, Peru, in October 2020. The challenge took place virtually due to the COVID-19 crisis. The 12 full and 4 short papers presented in this volume were carefully reviewed and selected form numerous submissions. This challenge aims not only to benchmark various myocardial pathology segmentation algorithms, but also to cover the topic of general cardiac image segmentation, registration and modeling, and raise discussions for further technical development and clinical deployment.",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12554,,,All OA; Green,Edited Book,,,,,5,5,,,https://link.springer.com/content/pdf/bfm:978-3-030-65651-5/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1133716556,46 Information and Computing Sciences,
168,pub.1134960455,10.1007/978-3-030-68107-4,,,"Statistical Atlases and Computational Models of the Heart. M&Ms and EMIDEC Challenges, 11th International Workshop, STACOM 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised Selected Papers","This book constitutes the proceedings of the 11th International Workshop on Statistical Atlases and Computational Models of the Heart, STACOM 2020, as well as two challenges: M&Ms - The Multi-Centre, Multi-Vendor, Multi-Disease Segmentation Challenge, and EMIDEC - Automatic Evaluation of Myocardial Infarction from Delayed-Enhancement Cardiac MRI Challenge. The 43 full papers included in this volume were carefully reviewed and selected from 70 submissions. They deal with cardiac imaging and image processing, machine learning applied to cardiac imaging and image analysis, atlas construction, artificial intelligence, statistical modelling of cardiac function across different patient populations, cardiac computational physiology, model customization, atlas based functional analysis, ontological schemata for data and results, integrated functional and structural analyses, as well as the pre-clinical and clinical applicability of these methods.",,,Lecture Notes in Computer Science,,,2021,2021,,2021,12592,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1134960455,46 Information and Computing Sciences,
137,pub.1154882177,10.1007/978-3-031-23443-9,,,"Statistical Atlases and Computational Models of the Heart. Regular and CMRxMotion Challenge Papers, 13th International Workshop, STACOM 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Revised Selected Papers","This book constitutes the proceedings of the 13th International Workshop on Statistical Atlases and Computational Models of the Heart, STACOM 2022, held in conjunction with the 25th MICCAI conference. The 34 regular workshop papers included in this volume were carefully reviewed and selected after being revised and deal with topics such as: common cardiac segmentation and modelling problems to more advanced generative modelling for ageing hearts, learning cardiac motion using biomechanical networks, physics-informed neural networks for left atrial appendage occlusion, biventricular mechanics for Tetralogy of Fallot, ventricular arrhythmia prediction by using graph convolutional network, and deeper analysis of racial and sex biases from machine learning-based cardiac segmentation. In addition, 14 papers from the CMRxMotion challenge are included in the proceedings which aim to assess the effects of respiratory motion on cardiac MRI (CMR) imaging quality and examine the robustness of segmentation models in face of respiratory motion artefacts. A total of 48 submissions to the workshop was received.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13593,,,Closed,Edited Book,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1154882177,46 Information and Computing Sciences,
85,pub.1151032978,10.1007/978-3-031-16443-9,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2022, 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part V","The eight-volume set LNCS 13431, 13432, 13433, 13434, 13435, 13436, 13437, and 13438 constitutes the refereed proceedings of the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2022, which was held in Singapore in September 2022. The 574 revised full papers presented were carefully reviewed and selected from 1831 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: Brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; heart and lung imaging; dermatology; Part II: Computational (integrative) pathology; computational anatomy and physiology; ophthalmology; fetal imaging; Part III: Breast imaging; colonoscopy; computer aided diagnosis; Part IV: Microscopic image analysis; positron emission tomography; ultrasound imaging; video data analysis; image segmentation I; Part V: Image segmentation II; integration of imaging with non-imaging biomarkers; Part VI: Image registration; image reconstruction; Part VII: Image-Guided interventions and surgery; outcome and disease prediction; surgical data science; surgical planning and simulation; machine learning – domain adaptation and generalization; Part VIII: Machine learning – weakly-supervised learning; machine learning – model interpretation; machine learning – uncertainty; machine learning theory and methodologies.",,,Lecture Notes in Computer Science,,,2022,2022,,2022,13435,,,All OA; Green,Edited Book,,,,,0,0,,,https://link.springer.com/content/pdf/bfm%3A978-3-031-16443-9%2F1,https://app.dimensions.ai/details/publication/pub.1151032978,46 Information and Computing Sciences; 4611 Machine Learning,
77,pub.1131399582,10.1007/978-3-030-59719-1,,,"Medical Image Computing and Computer Assisted Intervention – MICCAI 2020, 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part IV","The seven-volume set LNCS 12261, 12262, 12263, 12264, 12265, 12266, and 12267 constitutes the refereed proceedings of the 23rd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2020, held in Lima, Peru, in October 2020. The conference was held virtually due to the COVID-19 pandemic. The 542 revised full papers presented were carefully reviewed and selected from 1809 submissions in a double-blind review process. The papers are organized in the following topical sections: Part I: machine learning methodologies Part II: image reconstruction; prediction and diagnosis; cross-domain methods and reconstruction; domain adaptation; machine learning applications; generative adversarial networks Part III: CAI applications; image registration; instrumentation and surgical phase detection; navigation and visualization; ultrasound imaging; video image analysis Part IV: segmentation; shape models and landmark detection Part V: biological, optical, microscopic imaging; cell segmentation and stain normalization; histopathology image analysis; opthalmology Part VI: angiography and vessel analysis; breast imaging; colonoscopy; dermatology; fetal imaging; heart and lung imaging; musculoskeletal imaging Part VI: brain development and atlases; DWI and tractography; functional brain networks; neuroimaging; positron emission tomography",,,Lecture Notes in Computer Science,,,2020,2020,,2020,12264,,,All OA; Green,Edited Book,,,,,6,6,,3.09,https://link.springer.com/content/pdf/bfm:978-3-030-59719-1/1?pdf=chapter%20toc,https://app.dimensions.ai/details/publication/pub.1131399582,46 Information and Computing Sciences; 4611 Machine Learning,
