title,doi,abstract,task
Hand Segmentation with Structured Convolutional Learning,https://doi.org/10.1007/978-3-319-16811-1_45,"the availability of cheap and effective depth sensors has resulted in recent advances in human pose estimation and tracking detailed estimation of hand pose however remains a challenge since fingers are often occluded and may only represent just a few pixels moreover labelled data is difficult to obtain we propose a deep learning basedapproach for hand pose estimation targeting gesture recognition that requires very little labelled data it leverages both unlabeled data and synthetic data from renderings the key to making it work is to integrate structural information not into the model architecture which would slow down inference but into the training objective we show that adding unlabelled realworld samples significantly improves results compared to a purely supervised setting",Other
A functional network estimation method of resting-state fMRI using a hierarchical Markov random field,https://doi.org/10.1016/j.neuroimage.2014.06.001,"we propose a hierarchical markov random field model for estimating both group and subject functional networks simultaneously the model takes into account the withinsubject spatial coherence as well as the betweensubject consistency of the network label maps the statistical dependency between group and subject networks acts as a regularization which helps the network estimation on both layers we use gibbs sampling to approximate the posterior density of the network labels and monte carlo expectation maximization to estimate the model parameters we compare our method with two alternative segmentation methods based on kmeans and normalized cuts using synthetic and real fmri data the experimental results show that our proposed model is able to identify both group and subject functional networks with higher accuracy on synthetic data more robustness and intersession consistency on the real data",Other
Perfect MCMC Sampling in Bayesian MRFs for Uncertainty Estimation in Segmentation,https://doi.org/10.1007/978-3-030-00928-1_76,"typical segmentation methods produce a single optimal solution and fail to inform about i the confidenceuncertainty in the object boundaries or ii alternate closetooptimal solutions to estimate uncertainty some methods intend to sample segmentations from an associated posterior model using markov chain monte carlo mcmc sampling or perturbation models however they cannot guarantee sampling from the true posterior deviating significantly in practice we propose a novel method that guarantees exact mcmc sampling in finite time of multilabel segmentations from generic bayesian markov random field mrf models for exact sampling we propose fills strategy and extend it to generic mrf models via a novel bounding chain algorithm results on simulated data and clinical brain images from 4 classic problems show that our uncertainty estimates gain accuracy over the state of the art",Other
Semi-automated Method for the Glaucoma Monitoring,https://doi.org/10.1007/978-3-319-63754-9_11,"the current trend of computer vision and image processing systems in biomedical field is the application of the computational intelligence ci approaches which include the use of tools as machine learning and soft computing the ci approaches bring a new solution to automatic feature extraction for a particular task based on that techniques we have proposed in this work a semiautomated method for the glaucoma monitoring through retinal images glaucoma is a disease caused by neurodegeneration of the optic nerve leading to blindness it can be assessed by monitoring intraocular pressure iop by the visual field and the aspect of the optic disc ratio cupdisc glaucoma increases the rate of cupdisc cdr which affects the loss of peripheral vision in this work a segmentation method of cups and discs regions is proposed in a semisupervised pixelbased classification paradigm to automate the cupdisc ratio calculation for the concrete medical supervision of the glaucoma disease the idea is to canvas the medical expert for labeling the regions of interest roi three retinal images and automate the segmentation by intelligent region growing based on machine learning a comparative study of semisupervised and supervised methods is carried out in this proposal by mono approaches decision tree and setred and multiclassifiers random forest and coforest our proposition is evaluated on real images of normal and glaucoma cases the obtained results are very promising and demonstrate the efficacy and potency of segmentation by the multiclassifier systems in semiautomatic segmentation",Other
OBJECT-BASED ANALYSIS FOR URBAN LAND COVER MAPPING USING THE INTERIMAGE AND THE SIPINA FREE SOFTWARE PACKAGES,https://doi.org/10.1590/s1982-21702018000100001,"abstract in this work we introduce an objectbased method applied to urban land cover mapping the method is implemented with two opensource tools sipina a data mining software package and interimage an objectbased image analysis system initially segmentation feature extraction and sample selection procedures are performed with interimage in order to reduce the time and subjectivity involved to develop the decision rules in interimage a data mining step is then carried out with sipina in sequence the decision trees delivered by sipina are analysed and encoded into interimage decision rules for the final classification step experiments were conducted using a subset of a geoeye image acquired in january 01 2013 covering the urban portion of the municipality of goianésia brazil five decision tree induction algorithms available in sipina were tested id3 c45 gid3 assistant86 and chaid the tau and kappa coefficients were used to evaluate the results the tau values obtained were in the range of 066 and 070 while those for kappa varied from 065 to 069",Other
Performance Categorization for Personalized Learning in Vocational Training Simulators,https://doi.org/10.1109/icalt.2018.00022,"simulators have been increasingly used at many industries to provide skill training to its trainees or employees a haptics based rebar bending simulator is being used at leading construction training centers in india the simulator collects the realtime training data and uses it to generate accurate assessment reports similar to the manually generated reports to enhance the learning environment the simulator should learn to recognize the differences between the trainee performances and should be capable to model the successive stages of skill learning in this paper we discuss how the timeseries segmentation technique can be applied to extract process relevant data from the training history collected by the rebar bending simulator we applied machine learning methods on this transformed data to model the varying levels of expertise in future these models can be utilized to enhance the simulator to provide personalized feedback and learning experiences",Other
An Entropy-Based Approach to Automatic Detection of Critical Changes in Human-Machine Interaction,https://doi.org/10.1109/coginfocom.2018.8639869,"this paper introduces and exemplifies an entropybased approach to automatic detection of critical changes in flow of humanmachine interaction the underlying intuition is that a critical change is a local phenomenon associated with a dialogue fragment that can be detected by examining patterns of interactional entropy by means of segmentation and thresholding in terms of units of analysis the notion of normalized interactional entropy is introduced at the level of dialogue act types in order to account for the sequential order of dialogue acts in a given dialogue fragment interactional entropy is reevaluated with each ensuing dialogue act the proposed approach is validated for thirtysix dialogues between children and a conversational humanlike robot recorded in therapeutic settings finally the generalizability of the proposed approach is discussed",Other
Plant Disease Detection System for Agricultural Applications in Cloud Using SVM and PSO,https://doi.org/10.1007/978-3-030-24643-3_99,"in agricultural industry disease in plants is very difficult to identify what type of disease affect and how the disease is to treat the infected plant then the information is store in cloud computing for easily accessing the data for farmers whenever they want and learn about the diseases of the plant and treat it very fast without any loss of plants life the latest technology eagriculture service is very important for the development of rural and it contribute the farmers get more knowledge regarding the market place as well as finding varies disease infect the plant by taking image of the affected plant leaves in this paper internet of things capture the disease affected plant image and it is given to the cloud server then the image is preprocessed with top hat filter ostus thresholding segmentation support vector machine svm classifier is used and particle swarm optimization technique is perform for task scheduling by using these techniques execution time and system performance is improved",Other
Three-objective antenna optimization by means of kriging surrogates and domain segmentation,https://doi.org/10.23919/mikon.2018.8405222,"in this paper an optimization framework for multiobjective design of antenna structures is discussed which exploits datadriven surrogates a multiobjective evolutionary algorithm response correction techniques for design refinement as well as generalized domain segmentation the last mechanism is introduced to constrain the design space region subjected to sampling which permits reduction of the number of training data samples required for surrogate model identification the generalized segmentation technique works for any number of design objectives here it is demonstrated using a threeobjective case study of a uwb monopole optimized for best inband reflection minimum gain variability and minimum size the numerical results indicate that segmentation leads to reducing the cost of initial pareto identification by around 21 percent as compared to the conventional surrogateassisted approach",Other
Improving Aspect Identification with Reviews Segmentation,https://doi.org/10.1007/978-3-319-99495-6_35,"aspect identification a key subtask in aspectbased sentiment analysis absa aims to identify aspect categories from online user reviews inspired by the observation that different segments of a review usually express different aspect categories we propose a reviewssegmentationbased method to improve aspect identification specifically we divide a review into several segments according to the sentence structure and then automatically transfer aspect labels from the original review to its derived segments trained with the new constructed segmentlevel dataset a classifier can achieve better performance for aspect identification another contribution of this paper is extracting alignment features which can be leveraged to further improve aspect identification the experimental results show the effectiveness of our proposed method",Other
GAPLE: Generalizable Approaching Policy LEarning for Robotic Object  Searching in Indoor Environment,https://doi.org/10.48550/arxiv.1809.08287,"we study the problem of learning a generalizable action policy for an intelligent agent to actively approach an object of interest in an indoor environment solely from its visual inputs while scenedriven or recognitiondriven visual navigation has been widely studied prior efforts suffer severely from the limited generalization capability in this paper we first argue the object searching task is environment dependent while the approaching ability is general to learn a generalizable approaching policy we present a novel solution dubbed as gaple which adopts two channels of visual features depth and semantic segmentation as the inputs to the policy learning module the empirical studies conducted on the house3d dataset as well as on a physical platform in a real world scenario validate our hypothesis and we further provide indepth qualitative analysis",Other
An Efficient Deep Representation Based Framework for Large-Scale Terrain Classification,https://doi.org/10.1109/icpr.2018.8545021,"in this paper we present a novel terrain classification framework for largescale remote sensing images a wellperforming multiscale superpixel tessellation based segmentation approach is employed to generate homogeneous and irregularly shaped regions and a transfer learning technique is sequentially deployed to derive representative deep features by utilizing successful pretrained convolutional neural network cnn models this design is aimed to overcome the big problem of lacking available groundtruth data and to increase the generalization power of the multipixel descriptor in the subsequent classification step we train a fast and robust support vector machine svm to assign the pixellevel labels its maximummargin property can be easily combined with a graph laplacian propagation approach moreover we analyze the advantages of applying a feature selection technique to the deep cnn features which are extracted by transfer learning in the experiments we evaluate the whole framework based on different geographical types compared with other regionbased classification methods the results show that our framework can obtain stateoftheart performance wrt both classification accuracy and computational efficiency",Other
Multi-column Point-CNN for Sketch Segmentation,https://doi.org/10.48550/arxiv.1812.11029,"traditional sketch segmentation methods mainly rely on handcrafted features and complicate models and their performance is far from satisfactory due to the abstract representation of sketches recent success of deep neural networks dnns in related tasks suggests dnns could be a practical solution for this problem yet the suitable datasets for learning and evaluating dnns are limited to this end we introduce sketchseg a large dataset consisting of 10000 pixelwisely labeled sketchesbesides due to the lack of colors and textures in sketches conventional dnns learned on natural images are not optimal for tackling our problemtherefore we further propose the multicolumn pointcnn mcpnet which 1 directly takes sampled points as its input to reduce computational costs and 2 adopts multiple columns with different filter sizes to better capture the structures of sketches extensive experiments validate that the mcpnet is superior to conventional dnns like fcn the sketchseg dataset is publicly available on httpsdrivegooglecomopenid1opcbvkinhxvfahuvsspdeppb8ixfc3c",Other
Point Cloud Semantic Segmentation Algorithm Based on Multi-information Markov Random Field,https://doi.org/10.1109/ssci.2018.8628817,"there is a strong spatial relationship between point clouds obtained from the same object nevertheless it usually takes a great quantity of time to directly establish the relationship model between individual point clouds in this paper we draw on this idea and propose a new semantic segmentation method based on 3d mesh model which is applied to do the semantic segmentation of point cloud data in traffic scene firstly the markov random field is used to model according to the attribute information of the triangular patches of the 3d mesh model and the spatial dependence between the bins secondly the attribute information of the triangle patch is represented by the intensity information of the point cloud data included in each triangle patch and it is clustered by gaussian mixture model which describes the matching degree of each attribute with each class the algorithm combines topology information of grid model and intensity information of point cloud data eliminates the over segmentation effectively and makes the boundary of the partition smooth finally isdf improved shape and diameter function is proposed to determine the final class of two side triangular patches on the boundary the proposed method is evaluated on two public point cloud datasets and shows the competitive performance",Other
A systematic literature review of machine learning application in COVID-19 medical image classification,https://doi.org/10.1016/j.procs.2022.12.192,"detecting covid19 as early as possible and quickly is one way to stop the spread of covid19 machine learning development can help to diagnose covid19 more quickly and accurately this report aims to find out how far research has progressed and what lessons can be learned for future research in this sector by filtering titles abstracts and content in the google scholar database this literature review was able to find 19 related papers to answer two research questions ie what medical images are commonly used for covid19 classification and what are the methods for covid19 classification according to the findings chest xray were the most commonly used data to categorize covid19 and transfer learning techniques were the method used in this study researchers also concluded that lung segmentation and use of multimodal data could improve performance",Other
Diagnosis of some apple fruit diseases by using image processing and artificial neural network,https://doi.org/10.1016/j.foodcont.2022.109484,"farmers typically lack the knowledge of diagnosis and control of different apple diseases however some apple diseases have visual symptoms and can be diagnosed by eyes but their diagnosis by eyes is timeconsuming and costly for farmers a solution is to design an automatic disease diagnosis system using image processing techniques this paper presents a lowcost method of apple disease diagnosis using a neural network and fruit classification into four classes of scab bitter rot black rot and healthy fruits this method uses color and texture features the research used a multilayer perceptron neural network whose input was the features extracted from the images and its output was the defined classes after the network was trained by 60 of the images and the remaining images were reserved for its testing the accuracy of the proposed method was assessed with different structures of singlelayer and twolayer neural network structures based on the results the application of a twolayer structure with eight neurons in the first layer and eight neurons in the second layer resulted in an optimal accuracy of 737 developing a novel automatically method for diagnosing apple fruit diseases using image processing and artificial neural network for diagnosing apple fruit diseases providing a new dataset of apple images in field of diagnosing fruit diseases",Other
Deep learning models-based CT-scan image classification for automated screening of COVID-19,https://doi.org/10.1016/j.bspc.2022.104268,"covid19 is the most transmissible disease caused by the sarscov2 virus that severely infects the lungs and the upper respiratory tract of the human body this virus badly affected the lives and wellness of millions of people worldwide and spread widely early diagnosis timely treatment and proper confinement of the infected patients are some possible ways to control the spreading of coronavirus computed tomography ct scanning has proven useful in diagnosing several respiratory lung problems including covid19 infections automated detection of covid19 using chest ctscan images may reduce the clinicians load and save the lives of thousands of people this study proposes a robust framework for the automated screening of covid19 using chest ctscan images and deep learningbased techniques in this work a publically accessible ctscan image dataset contains the 1252 covid19 and 1230 noncovid chest ct images two pretrained deep learning models dlms namely mobilenetv2 and darknet19 and a newlydesigned lightweight dlm are utilized for the automated screening of covid19 a repeated tenfold holdout validation method is utilized for the training validation and testing of dlms the highest classification accuracy of 9891 is achieved using transferlearned darknet19 the proposed framework is ready to be tested with more ct images the simulation results with the publicly available covid19 ct scan image dataset are included to show the effectiveness of the presented study",Other
Deep learning for image inpainting: A survey,https://doi.org/10.1016/j.patcog.2022.109046,"we summarize existing deep learningbased image inpainting algorithms in three aspects including inpainting strategies network structures and loss functions we introduce the open source codes popular used datasets evaluation metrics and application scenarios we compare the inpainting algorithms with released source codes we outline the challenges and possible future directions image inpainting has been widely exploited in the field of computer vision and image processing the main purpose of image inpainting is to produce visually plausible structure and texture for the missing regions of damaged images in the past decade the success of deep learning has brought new opportunities to many vision tasks which promoted the development of a large number of deep learningbased image inpainting methods although these methods have many similarities they also have their own characteristics due to the differences in data types application scenarios computing platforms etc it is necessary to classify and summarize these methods to provide a reference for the research community in this survey we present a comprehensive overview of recent advances in deep learningbased image inpainting first we categorize the deep learningbased techniques from multiple perspectives inpainting strategies network structures and loss functions second we summarize the open source codes and representative public datasets and introduce the evaluation metrics for quantitative comparisons third we summarize the realworld applications of image inpainting in different scenarios and give a detailed analysis on the performance of different inpainting algorithms at last we conclude the survey and discuss about the future directions",Other
Plant trait estimation and classification studies in plant phenotyping using machine vision – A review,https://doi.org/10.1016/j.inpa.2021.02.006,"imaging techniques used for plant phenotyping machine vision methodologies used for plant trait estimation and classification plant image segmentation techniques for plant growth tracking publicly available dataset for plant phenotyping future research directions in plant phenotyping today there is a rapid development taking place in phenotyping of plants using nondestructive image based machine vision techniques machine vision based plant phenotyping ranges from single plant trait estimation to broad assessment of crop canopy for thousands of plants in the field plant phenotyping systems either use single imaging method or integrative approach signifying simultaneous use of some of the imaging techniques like visible red green and blue rgb imaging thermal imaging chlorophyll fluorescence imaging cfim hyperspectral imaging 3dimensional 3d imaging or high resolution volumetric imaging this paper provides an overview of imaging techniques and their applications in the field of plant phenotyping this paper presents a comprehensive survey on recent machine vision methods for plant trait estimation and classification in this paper information about publicly available datasets is provided for uniform comparison among the stateoftheart phenotyping methods this paper also presents future research directions related to the use of deep learning based machine vision algorithms for structural 2d and 3d physiological and temporal trait estimation and classification studies in plants",Other
Automatic classification of textile visual pollutants using deep learning networks,https://doi.org/10.1016/j.aej.2022.07.039,"urban pollution is a massive global problem especially in industrialized and developing nations visual pollution is an issue concerned with the external noticeable appearance of the modern urban areas causing human health disorders emotional distress driving distraction environmental hazards etc amidst the plethora of different forms of environmental pollution visual pollution deteriorates the aesthetics of an urban environment endorsing the importance of research and assessing it from different dimensions the main objective of this study is to initialize a new concept of automatic identification and classification of visible contaminants related to textile industries using computer vision techniques in this work deep learning techniques have been applied for the automatic detection and classification of three categories of textilebased visual pollutants ie cloth garbage advertising billboards and signages and textile dyeing waste materials initially 1709 visual pollutants images were obtained through web crawling of search engines additionally 954 images were collected from two local garments factories roadside vendors and shopping malls of bangladesh next the dataset was manually annotated by an opensource labeling tool finally various deep learning techniques faster rcnn yolov5 and efficientdet have been used to classify the obtained dataset automatically the efficientdet framework achieved the best performance with 97 and 93 training and test accuracies respectively the yolov5 approach exhibits acceptable precision with a considerably lower number of epochs the proposed automated classification system is expected to create future visual pollution ratings for the textile industries consequently the corresponding stakeholders industry owners government authorities factory workers etc can introduce regulatory frameworks and control the proliferation of visual pollution the opensource images obtained by web crawling locally collected visual pollutants dataset and implementation code of this work are available at httpsgithubcomsadiaafrin163textilevisualpollutantsdataset",Other
Fruit quality and defect image classification with conditional GAN data augmentation,https://doi.org/10.1016/j.scienta.2021.110684,"cgan data augmentation improves fruit quality classification gradcam shows cgan generates useful features for classification pruning reduces model size to 50 of original size retaining high accuracy code and models made publicly available for future work contemporary artificial intelligence technologies allow for the employment of computer vision to discern good crops from bad providing a step in the pipeline of selecting healthy fruit from undesirable fruit such as those which are mouldy or damaged stateoftheart works in the field report high accuracy results on small datasets 1000 images which are not representative of the population regarding realworld usage the goals of this study are to further enable realworld usage by improving generalisation with data augmentation as well as to reduce overfitting and energy usage through model pruning in this work we suggest a machine learning pipeline that combines the ideas of finetuning transfer learning and generative modelbased training data augmentation towards improving fruit quality image classification a linear network topology search is performed to tune a vgg16 lemon quality classification model using a publiclyavailable dataset of 2690 images we find that appending a 4096 neuron fully connected layer to the convolutional layers leads to an image classification accuracy of 8377 we then train a conditional generative adversarial network on the training data for 2000 epochs and it learns to generate relatively realistic images gradcam analysis of the model trained on real photographs shows that the synthetic images can exhibit classifiable characteristics such as shape mould and gangrene a higher image classification accuracy of 8875 is then attained by augmenting the training with synthetic images arguing that conditional generative adversarial networks have the ability to produce new data to alleviate issues of data scarcity finally model pruning is performed via polynomial decay where we find that the conditional ganaugmented classification network can retain 8116 classification accuracy when compressed to 50 of its original size",Other
Underwater Image Enhancement via Minimal Color Loss and Locally Adaptive Contrast Enhancement,https://doi.org/10.1109/TIP.2022.3177129,"underwater images typically suffer from color deviations and low visibility due to the wavelengthdependent light absorption and scattering to deal with these degradation issues we propose an efficient and robust underwater image enhancement method called mlle specifically we first locally adjust the color and details of an input image according to a minimum color loss principle and a maximum attenuation mapguided fusion strategy afterward we employ the integral and squared integral maps to compute the mean and variance of local image blocks which are used to adaptively adjust the contrast of the input image meanwhile a color balance strategy is introduced to balance the color differences between channel a and channel b in the cielab color space our enhanced results are characterized by vivid color improved contrast and enhanced details extensive experiments on three underwater image enhancement datasets demonstrate that our method outperforms the stateoftheart methods our method is also appealing in its fast processing speed within 1s for processing an image of size 1024x00d71024x00d73 on a single cpu experiments further suggest that our method can effectively improve the performance of underwater image segmentation keypoint detection and saliency detection the project page is available at httpslichongyigithubioprojmmlehtml",Other
PolyLoss: A Polynomial Expansion Perspective of Classification Loss  Functions,https://doi.org/10.48550/arXiv.2204.12511,"crossentropy loss and focal loss are the most common choices when training deep neural networks for classification problems generally speaking however a good loss function can take on much more flexible forms and should be tailored for different tasks and datasets motivated by how functions can be approximated via taylor expansion we propose a simple framework named polyloss to view and design loss functions as a linear combination of polynomial functions our polyloss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets while naturally subsuming the aforementioned crossentropy loss and focal loss as special cases extensive experimental results show that the optimal choice within the polyloss is indeed dependent on the task and dataset simply by introducing one extra hyperparameter and adding one line of code our poly1 formulation outperforms the crossentropy loss and focal loss on 2d image classification instance segmentation object detection and 3d object detection tasks sometimes by a large margin",Other
Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions,https://doi.org/10.1609/aaai.v36i2.20072,"though deep learningbased object detection methods have achieved promising results on the conventional datasets it is still challenging to locate objects from the lowquality images captured in adverse weather conditions the existing methods either have difficulties in balancing the tasks of image enhancement and object detection or often ignore the latent information beneficial for detection to alleviate this problem we propose a novel imageadaptive yolo iayolo framework where each image can be adaptively enhanced for better detection performance specifically a differentiable image processing dip module is presented to take into account the adverse weather conditions for yolo detector whose parameters are predicted by a small convolutional neural network cnnpp we learn cnnpp and yolov3 jointly in an endtoend fashion which ensures that cnnpp can learn an appropriate dip to enhance the image for detection in a weakly supervised manner our proposed iayolo approach can adaptively process images in both normal and adverse weather conditions the experimental results are very encouraging demonstrating the effectiveness of our proposed iayolo method in both foggy and lowlight scenarios the source code can be found at httpsgithubcomwenyyuimageadaptiveyolo",Other
Large Scale Visual Food Recognition,https://doi.org/10.1109/TPAMI.2023.3237871,"food recognition plays an important role in food choice and intake which is essential to the health and wellbeing of humans it is thus of importance to the computer vision community and can further support many foodoriented vision and multimodal tasks eg food detection and segmentation crossmodal recipe retrieval and generation unfortunately we have witnessed remarkable advancements in generic visual recognition for released largescale datasets yet largely lags in the food domain in this paper we introduce food2k which is the largest food recognition dataset with 2000 categories and over 1 million images compared with existing food recognition datasets food2k bypasses them in both categories and images by one order of magnitude and thus establishes a new challenging benchmark to develop advanced models for food visual representation learning furthermore we propose a deep progressive region enhancement network for food recognition which mainly consists of two components namely progressive local feature learning and region feature enhancement the former adopts improved progressive training to learn diverse and complementary local features while the latter utilizes selfattention to incorporate richer context with multiple scales into local features for further local feature enhancement extensive experiments on food2k demonstrate the effectiveness of our proposed method more importantly we have verified better generalization ability of food2k in various tasks including food image recognition food image retrieval crossmodal recipe retrieval food detection and segmentation food2k can be further explored to benefit more foodrelevant tasks including emerging and more complex ones eg nutritional understanding of food and the trained models on food2k can be expected as backbones to improve the performance of more foodrelevant tasks we also hope food2k can serve as a large scale finegrained visual recognition benchmark and contributes to the development of large scale finegrained visual analysis the dataset code and models are publicly available at uri xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinkhttp123574289foodprojecthtmluri",Other
Glass Segmentation with RGB-Thermal Image Pairs,https://doi.org/10.1109/TIP.2023.3256762,"this paper proposes a new glass segmentation method utilizing paired rgb and thermal images due to the large difference between the transmission property of visible light and that of the thermal energy through the glass where most glass is transparent to the visible light but opaque to thermal energy glass regions of a scene are made more distinguishable with a pair of rgb and thermal images than solely with an rgb image to exploit such a unique property we propose a neural network architecture that effectively combines an rgbthermal image pair with a new multimodal fusion module based on attention and integrate cnn and transformer to extract local features and nonlocal dependencies respectively as well we have collected a new dataset containing 5551 rgbthermal image pairs with groundtruth segmentation annotations the qualitative and quantitative evaluations demonstrate the effectiveness of the proposed approach on fusing rgb and thermal data for glass segmentation our code and data are available at httpsgithubcomdonghuorgbtglasssegmentation",Other
Palette: Image-to-Image Diffusion Models,https://doi.org/10.1145/3528233.3530757,"we introduce palette a simple and general framework for imagetoimage translation using conditional diffusion models on four challenging imagetoimage translation tasks colorization inpainting uncropping and jpeg decompression palette outperforms strong gan and regression baselines and establishes a new state of the art this is accomplished without taskspecific hyperparameter tuning architecture customization or any auxiliary loss demonstrating a desirable degree of generality and flexibility we uncover the impact of using l2 vs l1 loss in the denoising diffusion objective on sample diversity and demonstrate the importance of selfattention through empirical architecture studies importantly we advocate a unified evaluation protocol based on imagenet and report several sample quality scores including fid inception score classification accuracy of a pretrained resnet50 and perceptual distance against reference images for various baselines we expect this standardized evaluation protocol to play a critical role in advancing imagetoimage translation research finally we show that a single generalist palette model trained on 3 tasks colorization inpainting jpeg decompression performs as well or better than taskspecific specialist counterparts",Other
Automatic soil desiccation crack recognition using deep learning,https://doi.org/10.1680/jgeot.20.P.091,"soil desiccation cracking is a common natural phenomenon the existence of cracks can negatively impact both the mechanical and hydraulic properties of soil accurate acquisition of soil crack networks is not only the basis for obtaining the relevant geometrical parameters of crack networks but also an important foundation and premise for further study about the formation mechanism of shrinkage and desiccation cracking this study proposes a new automatic soil cracks recognition method based on a unet convolutional neural network cnn architecture for segmentation on soil desiccation crack images the backbone of the unet encoder is selected as resnet and a new loss function combining both binary crossentropy bce loss and dice loss is used during the training stage to fit an imbalance problem subsequently the unet with an encoder based on resnet and a decoder part is trained from end to end on a subset of 524 labelled crack images with 224 224 pixels for semantic segmentation the unet architecture achieves 9438 7443 and 8113 for precision recall and dice scores on test sets which are better than all results using the otsu threshold method employed in the traditional crack image processing technique experimental results reveal that deep learning can achieve higher accuracy than the traditional method binarisation by thresholding in quantifying surface crack ratio average crack width total crack length and crack number moreover deep learning can not only accurately identify cracks or spots by means of crack edge features but also can accurately separate soil cracks and clod areas under a bad photographing condition such as uneven illumination a field environment or poor photographing angle overall the proposed deep learningbased method presents a satisfactory performance in soil crack image recognition and quantification it may also be applied to other materials with cracks",Other
Enhanced deep convolutional neural network for malarial parasite classification,https://doi.org/10.1080/1206212X.2019.1672277,"the standard method for examining malarial disease is performed through the examination of blood smears under the microscope for parasiteinfected red blood cells and this is done by qualified tech",Other
An improved YOLOv5 model based on visual attention mechanism: Application to recognition of tomato virus disease,https://doi.org/10.1016/j.compag.2022.106780,"a deep learning model based on attention mechanism is proposed for tomato virus disease recognition the recognition accuracy is improved while maintaining the same detection speed it provides technical support for other researches related to plant disease recognition traditional target detection methods cannot effectively screen key features which leads to overfitting and produces a model with a weak generalization ability in this paper an improved seyolov5 network model is proposed for the recognition of tomato virus diseases images of tomato diseases in greenhouses were collected using a mobile phone and the collected images were expanded a squeezeandexcitation se module was added to a yolov5 model to realize the extraction of key features using a human visual attention mechanism for reference the trained network model was evaluated on the test set of tomato virus diseases the accuracy was 9107 which was 712 1785 and 891 higher than that of the faster regions with convolutional neural network features rcnn model singleshot multibox detector ssd model and yolov5 model respectively meanwhile the mean average precision map 05 was 9410 which was 123 1677 and 178 higher than that of the faster rcnn model ssd model and yolov5 model the proposed seyolov5 model can effectively detect regions of tomato virus disease which provides disease identification and control theoretical research and technical support",Other
Deep Transfer Learning Based Classification Model for COVID-19 Disease,https://doi.org/10.1016/j.irbm.2020.05.003,"the covid19 infection is increasing at a rapid rate with the availability of limited number of testing kits therefore the development of covid19 testing kits is still an open area of research recently many studies have shown that chest computed tomography ct images can be used for covid19 testing as chest ct images show a bilateral change in covid19 infected patients however the classification of covid19 patients from chest ct images is not an easy task as predicting the bilateral change is defined as an illposed problem therefore in this paper a deep transfer learning technique is used to classify covid19 infected patients additionally a top2 smooth loss function with costsensitive attributes is also utilized to handle noisy and imbalanced covid19 dataset kind of problems experimental results reveal that the proposed deep transfer learningbased covid19 classification model provides efficient results as compared to the other supervised learning models",Other
Weakly Supervised 3D Scene Segmentation with Region-Level Boundary Awareness and Instance Discrimination,https://doi.org/10.1007/978-3-031-19815-1_3,"abstractcurrent stateoftheart 3d scene understanding methods are merely designed in a fullsupervised way however in the limited reconstruction cases only limited 3d scenes can be reconstructed and annotated we are in need of a framework that can concurrently be applied to 3d point cloud semantic segmentation and instance segmentation particularly in circumstances where labels are rather scarce the paper introduces an effective approach to tackle the 3d scene understanding problem when labeled scenes are limited to leverage the boundary information we propose a novel energybased loss with boundary awareness benefiting from the regionlevel boundary labels predicted by the boundary prediction network to encourage latent instance discrimination and guarantee efficiency we propose the first unsupervised regionlevel semantic contrastive learning scheme for point clouds which uses confident predictions of the network to discriminate the intermediate feature embeddings in multiple stages in the limited reconstruction case our proposed approach termed ws3d has pioneer performance on the largescale scannet on semantic segmentation and instance segmentation also our proposed ws3d achieves stateoftheart performance on the other indoor and outdoor datasets s3dis and semantickitti keywords3d scene understandingweaklysupervisedsemisupervised learningregionlevel contrastenergy functionsegmentation",Other
Exploring Local Detail Perception for Scene Sketch Semantic Segmentation,https://doi.org/10.1109/TIP.2022.3142511,"in this paper we aim to explore the finegrained perception ability of deep models for the newly proposed scene sketch semantic segmentation task scene sketches are abstract drawings containing multiple related objects it plays a vital role in daily communication and humancomputer interaction the study has only recently started due to a main obstacle of the absence of largescale datasets the currently available dataset sketchyscene is composed of clip artstyle edge maps which lacks abstractness and diversity to drive further research we contribute two new largescale datasets based on real handdrawn object sketches a general automatic scene sketch synthesis process is developed to assist with new dataset composition furthermore we propose to enhancing local detail perception in deep models to realize accurate strokeoriented scene sketch segmentation due to the inherent differences between handdrawn sketches and natural images extreme lowlevel local features of strokes are incorporated to improve detail discrimination stroke masks are also integrated into model training to guide the learning attention extensive experiments are conducted on three largescale scene sketch datasets our method achieves stateoftheart performance under four evaluation metrics and yields meaningful interpretability via visual analytics",Other
Surface Reflectance: A Metric for Untextured Surgical Scene Segmentation,https://doi.org/10.1007/978-981-19-7528-8_17,"abstractsegmentation is a process to understand scene context captured by a camera and it is commonly used to solve many robotic problems such as localization and tracking the utility of semantic segmentation in robotassisted minimally invasive surgery mis is increasing as it can provide situational awareness in this work we present a new approach for image segmentation for knee arthroscopy by capturing the evolution of tissuedependent spectral reflectance within the cavity our approach relies on using spectral sensitivity of the image sensor that provides a cue to estimate surface reflectance from color tristimulus values we demonstrate that light reflectance conveys pivotal information about the tissue surfaces that could be used to segment surgical frames which are difficult to visualize as limited scene context and lack of texture and features the methodology conducted in cadaveric experiments confirms the robustness of reconstructed spectral reflectance in binary segmentations of surgical sites to the best of our knowledge this is the first use of surface reflectance as a segmentation tool for identifying tissue types in endoscopic imageskeywordsspectral reflectancesensor spectral sensitivitysegmentationdecision tree classifiersupport vector machineknee arthroscopymisroboticassisted surgery",Other
A review of natural scene text detection methods,https://doi.org/10.1016/j.procs.2022.01.185,"natural scene text detection has an important role to play in getting textual information from natural scenes with the continuous development of deep learning natural scene text detection methods are emerging and achieving better results on detection tasks in this paper analysis and summary of the current stage of deep learningbased text algorithms for natural scenes can be divided into two types region of the proposal and semantic segmentation and the content of these two series of related algorithms is described secondly a publicly available dataset and detection performance metrics for scene text detection are presented ultimately the research in scene text detection is summarized and looked forward to in the hope of providing new research directions for subsequent algorithms",Other
Exploring Intra- and Inter-Video Relation for Surgical Semantic Scene Segmentation,https://doi.org/10.1109/TMI.2022.3177077,"automatic surgical scene segmentation is fundamental for facilitating cognitive intelligence in the modern operating theatre previous works rely on conventional aggregation modules eg dilated convolution convolutional lstm which only make use of the local context in this paper we propose a novel framework stswincl that explores the complementary intra and intervideo relations to boost segmentation performance by progressively capturing the global context we firstly develop a hierarchy transformer to capture intravideo relation that includes richer spatial and temporal cues from neighbor pixels and previous frames a joint spacetime window shift scheme is proposed to efficiently aggregate these two cues into each pixel embedding then we explore intervideo relation via pixeltopixel contrastive learning which well structures the global embedding space a multisource contrast training objective is developed to group the pixel embeddings across videos with the groundtruth guidance which is crucial for learning the global property of the whole data we extensively validate our approach on two public surgical video benchmarks including endovis18 challenge and cadis dataset experimental results demonstrate the promising performance of our method which consistently exceeds previous stateoftheart approaches code is available at uri xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinkhttpsgithubcomyuemingjinstswincluri",Other
UAVformer: A Composite Transformer Network for Urban Scene Segmentation of UAV Images,https://doi.org/10.1016/j.patcog.2022.109019 	,"a novel transformerbased semantic segmentation network with a composite structure backbone is proposed for urban scene segmentation of uav images adaptive fusion modules afm are implemented to adaptively fuse the multilevel extracted features an aggregation window multihead selfattention awmsa mechanism is designed in the transformer block for accurately segmented scale variation objects in uav images a vshaped decoder with the capacity to fully utilise multilevel features is proposed to ideally preserve segmented object boundaries urban scenes segmentation based on uav unmanned aerial vehicle view is a fundamental task for the applications of smart city such as city planning land use monitoring traffic monitoring and crowd estimation while urban scenes in uav image characteristic by large scale variation of objects size and complexity background which posed challenges to urban scenes segmentation of uav image the feature extracting backbone of existing networks cannot extract complex features of uav image effectively which limits the performance of urban scenes segmentation to design segmentation network capable of extracting features of large scale variation urban ground scenes this study proposed a novel composite transformer network for urban scenes segmentation of uav image a composite backbone with aggregation windows multihead selfattention transformer blocks is proposed to make the extracted features more representatives by adaptive multilevel features fusion and the full utilisation of contextual information and local information position attention modules are inserted in each stage between encoder and decoder to further enhance the spatial attention of extracted feature maps finally a vshaped decoder which is capable of utilising multilevel features is designed to get accurately dense prediction the accuracy of urban scenes segmentation could significantly be enhanced in this way and successfully segmented the large scale variation objects from uav views extensive ablation experiments and comparative experiments for the proposed network have been conducted on the public available urban scenes segmentation datasets for uav imagery experimental results have demonstrated the effectiveness of designed network structure and the superiority of proposed network over stateoftheart methods specifically reached 532 miou on the uavid dataset and 776 miou on the udd6 dataset respectively",Other
RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset  for Natural Disaster Damage Assessment,https://doi.org/10.48550/arXiv.2202.12361,"due to climate change we can observe a recent surge of natural disasters all around the world these disasters are causing disastrous impact on both nature and human lives economic losses are getting greater due to the hurricanes quick and prompt response of the rescue teams are crucial in saving human lives and reducing economic cost deep learning based computer vision techniques can help in scene understanding and help rescue teams with precise damage assessment semantic segmentation an active research area in computer vision can put labels to each pixel of an image and therefore can be a valuable arsenal in the effort of reducing the impacts of hurricanes unfortunately available datasets for natural disaster damage assessment lack detailed annotation of the affected areas and therefore do not support the deep learning models in total damage assessment to this end we introduce the rescuenet a high resolution post disaster dataset for semantic segmentation to assess damages after natural disasters the rescuenet consists of post disaster images collected after hurricane michael the data is collected using unmanned aerial vehicles uavs from several areas impacted by the hurricane the uniqueness of the rescuenet comes from the fact that this dataset provides high resolution postdisaster images and comprehensive annotation of each image while most of the existing dataset offer annotation of only part of the scene like building road or river rescuenet provides pixel level annotation of all the classes including building road pool tree debris and so on we further analyze the usefulness of the dataset by implementing stateoftheart segmentation models on the rescuenet the experiments demonstrate that our dataset can be valuable in further improvement of the existing methodologies for natural disaster damage assessment",Other
Cursive Text Recognition in Natural Scene Images Using Deep Convolutional Recurrent Neural Network,https://doi.org/10.1109/ACCESS.2022.3144844,"text recognition in natural scene images is a challenging problem in computer vision different than the optical character recognition ocr text recognition in natural scene images is more complex due to variations in text size colors fonts orientations complex backgrounds occlusion illuminations and uneven lighting conditions in this paper we propose a segmentationfree method based on a deep convolutional recurrent neural network to solve the problem of cursive text recognition particularly focusing on urdu text in natural scenes compared to the noncursive scripts urdu text recognition is more complex due to variations in the writing styles several shapes of the same character connected text ligature overlapping stretched diagonal and condensed text the proposed model gets a whole word image as an input without presegmenting into individual characters and then transforms into the sequence of the relevant features our model is based on three components a deep convolutional neural network cnn with shortcut connections to extract and encode the features a recurrent neural network rnn to decode the convolutional features and a connectionist temporal classification ctc to map the predicted sequences into the target labels to increase the text recognition accuracy further we explore deeper cnn architectures like vgg16 vgg19 resnet18 and resnet34 to extract more appropriate urdu text features and compare the recognition results to conduct the experiments a new largescale benchmark dataset of cropped urdu word images in natural scenes is developed the experimental results show that the proposed deep crnn network with shortcut connections outperform than other network architectures the dataset is publicly available and can be downloaded from uri xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinkhttpsdatamendeleycomdatasetsk5fz57zd9z1uri",Other
Transformer-based Flood Scene Segmentation for Developing Countries,https://doi.org/10.48550/arXiv.2210.04218,"floods are largescale natural disasters that often induce a massive number of deaths extensive material damage and economic turmoil the effects are more extensive and longerlasting in highpopulation and lowresource developing countries early warning systems ews constantly assess water levels and other factors to forecast floods to help minimize damage postdisaster disaster response teams undertake a post disaster needs assessment pdsa to assess structural damage and determine optimal strategies to respond to highly affected neighbourhoods however even today in developing countries ews and pdsa analysis of large volumes of image and video data is largely a manual process undertaken by first responders and volunteers we propose floodtransformer which to the best of our knowledge is the first visual transformerbased model to detect and segment flooded areas from aerial images at disaster sites we also propose a custom metric flood capacity fc to measure the spatial extent of water coverage and quantify the segmented flooded area for ews and pdsa analyses we use the swoc flood segmentation dataset and achieve 093 miou outperforming all other methods we further show the robustness of this approach by validating across unseen flood images from other flood data sources",Other
CARL-D: A vision benchmark suite and large scale dataset for vehicle detection and scene segmentation,https://doi.org/10.1016/j.image.2022.116667,"visionbased object detection and scene understanding are becoming key features of environment perception and autonomous driving in the past couple of years numerous largescale datasets for visual object detection and semantic understanding have been released which has enormously benefited the environment perception in selfdriving cars however these datasets like kitti and cityscapes only focus on well organized and urban road traffic scenarios of european countries while ignoring the dense and unpattern traffic conditions of subcontinent countries like pakistan india bangladesh and sri lanka consequently the environment perception system developed on these datasets cannot efficiently assist selfdriving cars in traffic scenarios of subcontinent countries to this end we present carld a largescale dataset and benchmark suite for develop 2d object detection and instancepixellevel segmentation methods for selfdriving cars carld comprises largescale stereo visionbased driving videos captured from more than 100 cities of pakistan including motorways dense and unpattern traffic scenarios of urban rural and hilly areas as a benchmark selection 15000 suitable images are labeled for 2d object detection and recognition whereas semantic segmentation benchmark contains 2500 images with pixellevel highquality fine annotations and 5000 coarseannotated images which could help in enabling the deep neural networks to leverage the weakly labeled data alongside the dataset we also present transfer learningbased 2d vehicle detection and scene segmentation methods to evaluate the performance of existing stateoftheart deep neural networks on our dataset lastly an extensive experimental evaluation along with the comparative study has been carried out which demonstrates the upper edge of our dataset in terms of interclassdiversity scenevariability and annotations richness the proposed benchmark suite is available at httpscarldatasetgithubioindex a largescale dataset for 2d vehicle detection and scene understanding in selfdriving cars in unpattern rural urban and hilly road traffic scenarios scene segmentation benchmark 7500 pixellevel annotated images including 44 classes grouped into nature infrastructure moving and static objects signboards and misc vehicle detection recognition benchmark 15000 annotated images covering 25 classes with 50348 labels a comparative analysis of existing vehicle detection and scene segmentation dataset with our proposed benchmarks",Other
Embedded Control Gate Fusion and Attention Residual Learning for RGB–Thermal Urban Scene Parsing,https://doi.org/10.1109/TITS.2023.3242651,"the semantic segmentation of road scenes is an important task in autonomous driving deep learning has enabled the development of a variety of semantic segmentation networks using rgb and depth data however poor lighting conditions and longdistance sensing limit the applicability of rgb and depth cameras nevertheless many existing methods still rely on precise depth maps for scene segmentation unlike depth information thermal imaging provides a visual heat representation that remains accurate under a variety of lighting conditions and over longer distances for robust and accurate segmentation of scenes collected during autonomous driving we used the advanced mobilenetv2 network for feature extraction and a fusion strategy with an embedded control gate in addition we adopted an encoderdecoder scheme for semantic segmentation and developed an attention residual learning strategy to restore the resolution of the feature map finally semantic and boundary supervision is introduced to optimize parameters of the proposed network experimental results show that the proposed network outperforms existing networks on segmentation of urban scenes and our network can be generalized to depth data",Other
Semantic Segmentation of Coastal Zone on Airborne Lidar Bathymetry Point Clouds,https://doi.org/10.1109/LGRS.2022.3161191,"largescale semantic segmentation point cloud is an ongoing research topic for onland environments however there is a rare deep learning research study for the subsurface environment although pointnet and its successor pointnet have become the cornerstone of point cloud segmentation however these techniques handle a relatively small number of points this poses a natural difficulty in a large spatial scene with millions of possible points in particular for shallow water of coastal zone the small number of points where the seabed and water surface meet close points may belong to different classes in our work we present the semantic segmentation on a largescale airborne lidar bathymetry alb point cloud containing millions of sample points into two classes of water surface and seabed with the voxel sampling preprocessing vsp approach the proposed approach will allow us to capture the complicated outdoor natural scene components of water surface and seabed more accurately and more realistic through nonuniform voxelization in the mixture of dense and sparse points of the alb point cloud the performance of validation results show improvement in a perpoint accuracy of 7245 compared with other stateoftheart deep learningbased methods",Other
Crack detection algorithm for concrete structures based on super-resolution reconstruction and segmentation network,https://doi.org/10.1016/j.autcon.2022.104346,"crack images collected from civil infrastructures through unmanned aerial vehicles suffer from motion blur and insufficient resolution which reduces the accuracy of microcrack detection therefore an automatic microcrack detection method based on superresolution reconstruction srr and semantic segmentation is proposed superresolution sr images reconstructed by the proposed deep learningbased srr model were input into the proposed semantic segmentation network for crack segmentation and the length and width of cracks were measured through an improved medial axis transform approach the accuracy of crack segmentation and feature quantification for sr images obtained using the deep learningbased srr is significantly improved compared with lowresolution fuzzy images the effects of three parameters on the results were analyzed compared with the bicubic testset the intersectionoverunion of the sr testset is improved by 17 when a magnification factor of 4 is adopted the results show that the proposed method achieves good performance in detecting concrete cracks a crack detection method based on deep learning srr and segmentation is proposed srfbn with the least number of parameters achieves the best results on crack srr the f1score and iou obtained from the sr testset are improved by 13 and 17 the training set combining natural scene images with crack images performs best the effects of training sets and magnification factors on crack srr are discussed",Other
Visual segmentation of complex naturalistic structures in an infant eye-tracking search task,https://doi.org/10.1371/journal.pone.0266158,"an infants everyday visual environment is composed of a complex array of entities some of which are well integrated into their surroundings although infants are already sensitive to some categories in their first year of life it is not clear which visual information supports their detection of meaningful elements within naturalistic scenes here we investigated the impact of image characteristics on 8montholds search performance using a gaze contingent eyetracking search task infants had to detect a target patch on a background image the stimuli consisted of images taken from three categories vegetation nonliving natural elements eg stones and manmade artifacts for which we also assessed target background differences in lower and higherlevel visual properties our results showed that larger targetbackground differences in the statistical properties scaling invariance and entropy and also stimulus backgrounds including low pictorial depth predicted better detection performance furthermore category membership only affected search performance if supported by luminance contrast data from an adult comparison group also indicated that infants search performance relied more on lowerorder visual properties than adults taken together these results suggest that infants use a combination of property and categoryrelated information to parse complex visual stimuli",Other
Waymo Open Dataset: Panoramic Video Panoptic Segmentation,https://doi.org/10.1007/978-3-031-19818-2_4,"abstractpanoptic image segmentation is the computer vision task of finding groups of pixels in an image and assigning semantic classes and object instance identifiers to them research in image segmentation has become increasingly popular due to its critical applications in robotics and autonomous driving the research community thereby relies on publicly available benchmark dataset to advance the stateoftheart in computer vision due to the high costs of densely labeling the images however there is a shortage of publicly available ground truth labels that are suitable for panoptic segmentation the high labeling costs also make it challenging to extend existing datasets to the video domain and to multicamera setups we therefore present the waymo open dataset panoramic video panoptic segmentation a largescale dataset that offers highquality panoptic segmentation labels for autonomous driving we generate our dataset using the publicly available waymo open dataset wod leveraging the diverse set of camera images our labels are consistent over time for video processing and consistent across multiple cameras mounted on the vehicles for full panoramic scene understanding specifically we offer labels for 28 semantic categories and 2860 temporal sequences that were captured by five cameras mounted on autonomous vehicles driving in three different geographical locations leading to a total of 100k labeled camera images to the best of our knowledge this makes our dataset an order of magnitude larger than existing datasets that offer video panoptic segmentation labels we further propose a new benchmark for panoramic video panoptic segmentation and establish a number of strong baselines based on the deeplab family of models we have made the benchmark and the code publicly available which we hope will facilitate future research on holistic scene understanding our dataset can be found at waymocomopenkeywordpanoramic video panoptic segmentation",Other
DocSegTr: An Instance-Level End-to-End Document Image Segmentation  Transformer,https://doi.org/10.48550/arXiv.2201.11438,"understanding documents with rich layouts is an essential step towards information extraction business intelligence processes often require the extraction of useful semantic content from documents at a large scale for subsequent decisionmaking tasks in this context instancelevel segmentation of different document objects title sections figures etc has emerged as an interesting problem for the document analysis and understanding community to advance the research in this direction we present a transformerbased model called emphdocsegtr for endtoend instance segmentation of complex layouts in document images the method adapts a twin attention module for semantic reasoning which helps to become highly computationally efficient compared with the stateoftheart to the best of our knowledge this is the first work on transformerbased document segmentation extensive experimentation on competitive benchmarks like publaynet prima historical japanese hj and tablebank demonstrate that our model achieved comparable or better segmentation performance than the existing stateoftheart approaches with the average precision of 894 403 834 and 933 this simple and flexible framework could serve as a promising baseline for instancelevel recognition tasks in document images",Other
Improved DeepLabV3+ Network Segmentation Method for Urban Road Scenes,https://doi.org/10.1109/ITAIC54216.2022.9836922,"aiming at the poor accuracy and slow segmentation speed of the current road scene semantic segmentation an improved deeplabv3neural network segmentation algorithm is proposedfirstly the encoder network is changed to a more lightweight mobilenetv3 network which increases the segmentation speed without sacrificing segmentation accuracysecondly the atrous spatial convolutional pyramid pooling aspp is improved in the form of dense connection which further expands the receptive field while maintaining the size of the feature spatial resolution and increases the feature reuse efficiency and the dual attention mechanism is added to the encoder and denseaspp module which improves the recognition effect of the edge area of the model and improves the model segmentation accuracyfinally the multiscale lowlevel semantic features extracted at the encoder are fused at the decoder thereby reducing the loss of spatial information due to subsampling",Other
Multi-Dataset Hyper-CNN for Hyperspectral Image Segmentation of Remote Sensing Images,https://doi.org/10.3390/pr11020435,"this research paper presents novel condensed cnn architecture for the recognition of multispectral images which has been developed to address the lack of attention paid to neural network designs for multispectral and hyperspectral photography in comparison to rgb photographs the proposed architecture is able to recognize 10band multispectral images and has fewer parameters than popular deep designs such as resnet and densenet thanks to recent advancements in more efficient smaller cnns the proposed architecture is trained from scratch and it outperforms a comparable network that was trained on rgb images in terms of accuracy and efficiency the study also demonstrates the use of a bayesian variant of cnn architecture to show that a network able to process multispectral information greatly reduces the uncertainty associated with class predictions in comparison to standard rgb images the results of the study are demonstrated by comparing the accuracy of the networks predictions to the images",Other
Segmentation of Handwritten Arabic Graphemes Using a Directed Convolutional Neural Network and Mathematical Morphology Operations,https://doi.org/10.1016/j.patcog.2021.108288,"a directed convolutional neural network model is proposed partial dilationglobal erosion generates resistance against information loss several recent approaches for the segmentation of graphemescharacters are discussed the overtraces issue is addressed for the first time in the literature mmos can provide real solutions if a convenient strategy is adopted due to the nature of arabic handwriting segmenting words into charactersgraphemes is the most difficult and critical task of the recognition system the present paper proposes an approach to segment handwritten arabic words into graphemes based on a directed convolutional neural network cnn and mathematical morphology operations mmo arabic script is cursive which means that almost all graphemes are connected via horizontal links therefore a technique to remove links will facilitate the segmentation of graphemes in general an mmo such as erosion seems suitable for getting the job done but since arabic handwriting is difficult mmos cause information loss and suffer from many issues such as diacritics and overtraces which lead to overunderbad segmentations to overcome limitations the present paper addresses these issues in the following order the overtraces issue is addressed for the first time in the literature a robust algorithm for diacritics extraction is provided and finally the main segmentation algorithm adopts a strategy based on a partial dilation pdglobal erosion ge technique to combat the information loss issue the pd phase amplifies important regions while ge eliminates links between graphemes the complementarity between pd and ge facilitates the extraction of graphemes and creates resistance against information loss to properly tackle these difficult problems this article exploits the robustness of cnns so a new directed cnn model is suggested the idea is to draw the models attention to certain targeted features which are selected according to the nature of the problem addressed the proposed directed cnn is used in all phases of the segmentation process the experimental results are very encouraging and show that the proposed directed cnn model outperformed basic cnn in many experiments the results also reveal that the followed strategy improved the ability of mmos to perform segmentation and to compete with other approaches in this research area",Other
Food Quality Inspection and Grading Using Efficient Image Segmentation and Machine Learning-Based System,https://doi.org/10.1155/2022/5262294,"one of the most critical aspects of quality assurance is inspecting products for defects before they are sold or shipped a good product is more vital than having more of the same item for a customers enjoyment the client has a significant role in determining the quality of a product another way to think about quality is as the total of all the characteristics that contribute to the creation of items that the client enjoys recently the application of machine vision and image processing technology to improve the surface quality of fruits and other foods has increased significantly this is primarily because these technologies make significant advancements in areas where the human eye falls short this means that by utilizing computer vision and image processing techniques timeconsuming and subjective industrial quality control processes can be eliminated this article discusses how to check and assess food using picture segmentation and machine learning it is capable of classifying fruits and determining whether a piece of fruit is rotten to begin gaussian elimination is used to remove noise from images then photos are subjected to histogram equalization in order to improve their quality segmentation of the image is carried out using the kmeans clustering technique then fruit photos are classified using machine learning methods such as knn svm and c45 these algorithms determine if a fruit is damaged or not",Other
Fish Disease Detection Using Image Based Machine Learning Technique in Aquaculture,https://doi.org/10.1016/j.jksuci.2021.05.003	,"fish diseases in aquaculture constitute a significant hazard to nutriment security identification of infected fishes in aquaculture remains challenging to find out at the early stage due to the dearth of necessary infrastructure the identification of infected fish timely is an obligatory step to thwart from spreading disease in this work we want to find out the salmon fish disease in aquaculture as salmon aquaculture is the fastestgrowing food production system globally accounting for 70 percent 25 million tons of the market in the alliance of flawless image processing and machine learning mechanism we identify the infected fishes caused by the various pathogen this work divides into two portions in the rudimentary portion image preprocessing and segmentation have been applied to reduce noise and exaggerate the image respectively in the second portion we extract the involved features to classify the diseases with the help of the support vector machine svm algorithm of machine learning with a kernel function the processed images of the first portion have passed through this svm model then we harmonize a comprehensive experiment with the proposed combination of techniques on the salmon fish image dataset used to examine the fish disease we have conveyed this work on a novel dataset compromising with and without image augmentation the results have bought a judgment of our applied svm performs notably with 9142 and 9412 percent of accuracy respectively with and without augmentation",Other
TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation,https://doi.org/10.48550/arXiv.2204.05525,"although vision transformers vits have achieved great success in computer vision the heavy computational cost hampers their applications to dense prediction tasks such as semantic segmentation on mobile devices in this paper we present a mobilefriendly architecture named textbftoken textbfpyramid vision transtextbfformer textbftopformer the proposed textbftopformer takes tokens from various scales as input to produce scaleaware semantic features which are then injected into the corresponding tokens to augment the representation experimental results demonstrate that our method significantly outperforms cnn and vitbased networks across several semantic segmentation datasets and achieves a good tradeoff between accuracy and latency on the ade20k dataset topformer achieves 5 higher accuracy in miou than mobilenetv3 with lower latency on an armbased mobile device furthermore the tiny version of topformer achieves realtime inference on an armbased mobile device with competitive results the code and models are available at httpsgithubcomhustvltopformer",Other
Image positioning and identification method and system for coal and gangue sorting robot,https://doi.org/10.1080/19392699.2020.1760855,"accurately positioning and identifying coal and gangue under the heterogeneous and complex scenarios of coal production is a key and difficult problem for coal and gangue sorting robot this paper",Other
UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery,https://doi.org/10.1016/j.isprsjprs.2022.06.008,"semantic segmentation of remotely sensed urban scene images is required in a wide range of practical applications such as land cover mapping urban change detection environmental protection and economic assessmentdriven by rapid developments in deep learning technologies the convolutional neural network cnn has dominated semantic segmentation for many years cnn adopts hierarchical feature representation demonstrating strong capabilities for local information extraction however the local property of the convolution layer limits the network from capturing the global context recently as a hot topic in the domain of computer vision transformer has demonstrated its great potential in global information modelling boosting many visionrelated tasks such as image classification object detection and particularly semantic segmentation in this paper we propose a transformerbased decoder and construct a unetlike transformer unetformer for realtime urban scene segmentation for efficient segmentation the unetformer selects the lightweight resnet18 as the encoder and develops an efficient globallocal attention mechanism to model both global and local information in the decoder extensive experiments reveal that our method not only runs faster but also produces higher accuracy compared with stateoftheart lightweight models specifically the proposed unetformer achieved 678 and 524 miou on the uavid and loveda datasets respectively while the inference speed can achieve up to 3224 fps with a 512x512 input on a single nvidia gtx 3090 gpu in further exploration the proposed transformerbased decoder combined with a swin transformer encoder also achieves the stateoftheart result 913 f1 and 841 miou on the vaihingen dataset the source code will be freely available at httpsgithubcomwanglibo1995geoseg",Other
UrbanLF: A Comprehensive Light Field Dataset for Semantic Segmentation of Urban Scenes,https://doi.org/10.1109/TCSVT.2022.3187664,"as one of the fundamental technologies for scene understanding semantic segmentation has been widely explored in the last few years light field cameras encode the geometric information by simultaneously recording the spatial information and angular information of light rays which provides us with a new way to solve this issue in this paper we propose a highquality and challenging urban scene dataset containing 1074 samples composed of realworld and synthetic light field images as well as pixelwise annotations for 14 semantic classes to the best of our knowledge it is the largest and the most diverse light field dataset for semantic segmentation we further design two new semantic segmentation baselines tailored for light field and compare them with stateoftheart rgb video and rgbdbased methods using the proposed dataset the outperforming results of our baselines demonstrate the advantages of the geometric information in light field for this task we also provide evaluations of superresolution and depth estimation methods showing that the proposed dataset presents new challenges and supports detailed comparisons among different methods we expect this work inspires new research direction and stimulates scientific progress in related fields the complete dataset is available at uri xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinkhttpsgithubcomhawkeyegroupurbanlfuri",Other
Automatic pediatric congenital heart disease classification based on heart sound signal,https://doi.org/10.1016/j.artmed.2022.102257,"congenital heart diseases chd are the most common birth defects and the early diagnosis of chd is crucial for chd therapy however there are relatively few studies on intelligent auscultation for pediatric chd due to the fact that effective cooperation of the patient is required for the acquisition of useable heart sounds by electronic stethoscopes yet the quality of heart sounds in pediatric is poor compared to adults due to the factors such as crying and breath sounds this paper presents a novel pediatric chd intelligent auscultation method based on electronic stethoscope firstly a pediatric chd heart sound database with a total of 941 pcg signal is established then a segmentbased heart sound segmentation algorithm is proposed which is based on pcg segment to achieve the segmentation of cardiac cycles and therefore can reduce the influence of local noise to the global finally the accurate classification of chd is achieved using a majority voting classifier with random forest and adaboost classifier based on 84 features containing time domain and frequency domain experimental results show that the performance of the proposed method is competitive and the accuracy sensitivity specificity and f1score of classification for chd are 0953 0946 0961 and 0953 respectively",Other
Image and audio caps: automated captioning of background sounds and images using deep learning,https://doi.org/10.1007/s00530-022-00902-0,"abstract image recognition based on computers is something human beings have been working on for many years it is one of the most difficult tasks in the field of computer science and improvements to this system are made when we speak in this paper we propose a methodology to automatically propose an appropriate title and add a specific sound to the image two models have been extensively trained and combined to achieve this effect sounds are recommended based on the image scene and the headings are generated using a combination of natural language processing and stateoftheart computer vision models a top 5 accuracy of 67 and a top 1 accuracy of 53 have been achieved it is also worth mentioning that this is also the first model of its kind to make this forecast",Other
The Effect of Signal Duration on the Classification of Heart Sounds: A Deep Learning Approach,https://doi.org/10.3390/s22062261,"deep learning techniques are the future trend for designing heart sound classification methods making conventional heart sound segmentation dispensable however despite using fixed signal duration for training no study has assessed its effect on the final performance in detail therefore this study aims at analysing the duration effect on the commonly used deep learning methods to provide insight for future studies in data processing classifier and feature selection the results of this study revealed that 1 very short heart sound signal duration 1 s weakens the performance of recurrent neural networks rnns whereas no apparent decrease in the tested convolutional neural network cnn model was found 2 rnn outperformed cnn using melfrequency cepstrum coefficients mfccs as features there was no difference between rnn models lstm bilstm gru or bigru 3 adding dynamic information and ²mfccs of the heart sound as a feature did not improve the rnns performance and the improvement on cnn was also minimal 25 in macc the findings provided a theoretical basis for further heart sound classification using deep learning techniques when selecting the input length",Other
Robust and Interpretable Temporal Convolution Network for Event Detection in Lung Sound Recordings,https://doi.org/10.1109/JBHI.2022.3144314,"italic xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinkobjectivei this paper proposes a novel framework for lung sound event detection segmenting continuous lung sound recordings into discrete events and performing recognition of each event italic xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinkmethodsi we propose the use of a multibranch tcn architecture and exploit a novel fusion strategy to combine the resultant features from these branches this not only allows the network to retain the most salient information across different temporal granularities and disregards irrelevant information but also allows our network to process recordings of arbitrary length italic xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinkresultsi the proposed method is evaluated on multiple public and inhouse benchmarks containing irregular and noisy recordings of the respiratory auscultation process for the identification of auscultation events including inhalation crackles and rhonchi moreover we provide an endtoend model interpretation pipeline italic xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinkconclusioni our analysis of different feature fusion strategies shows that the proposed feature concatenation method leads to better suppression of noninformative features which drastically reduces the classifier overhead resulting in a robust lightweight network italic xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlinksignificancei lung sound event detection is a primary diagnostic step for numerous respiratory diseases the proposed method provides a costeffective and efficient alternative to exhaustive manual segmentation and provides more accurate segmentation than existing methods the endtoend model interpretability helps to build the required trust in the system for use in clinical settings",Other
Automatic Classification of Normal–Abnormal Heart Sounds Using Convolution Neural Network and Long-Short Term Memory,https://doi.org/10.3390/electronics11081246,"the phonocardiogram pcg is an important analysis method for the diagnosis of cardiovascular disease which is usually performed by experienced medical experts due to the high ratio of patients to doctors there is a pressing need for a realtime automated phonocardiogram classification system for the diagnosis of cardiovascular disease this paper proposes a deep neuralnetwork structure based on a onedimensional convolutional neural network 1dcnn and a long shortterm memory network lstm which can directly classify unsegmented pcg to identify abnormal signal the pcg data were filtered and put into the model for analysis a total of 3099 pieces of heartsound recordings were used while another 100 patients heartsound data collected by our group and diagnosed by doctors were used to test and verify the model results show that the cnnlstm model provided a good overall balanced accuracy of 086 001 with a sensitivity of 087 002 and specificity of 089 002 the f1score was 091 001 and the receiveroperating characteristic roc plot produced an area under the curve auc value of 092 001 the sensitivity specificity and accuracy of the 100 patients data were 083 002 080 002 and 085 003 respectively the proposed model does not require feature engineering and heartsound segmentation which possesses reliable performance in classification of abnormal pcg and is fast and suitable for realtime diagnosis application",Other
Deep learning-based computer-aided heart sound analysis in children with left-to-right shunt congenital heart disease,https://doi.org/10.1016/j.ijcard.2021.12.012,"the purpose of this study was to explore a new algorithm model capable of leverage deep learning to screen and diagnose specific types of lefttoright shunt congenital heart disease chd in childrenusing deep learning screening models were constructed to identify 884 heart sound recordings from children with lefttoright shunt chd the most suitable model for each type was summarized and compared with expert auscultation an exploratory analysis was conducted to assess whether there were correlations between heart sounds and left ventricular ejection fraction lvef pulmonary artery pressure and malformation sizethe residual convolution recurrent neural network rcrnet classification model had higher accuracy than other models with respect to atrial septal defect asd ventricular septum defect vsd patent ductus arteriosus pda and combined chd and the best auscultation sites were determined to be the 4th 5th 2nd and 3rd auscultation areas respectively the diagnostic results of this model were better than those derived from expert auscultation with sensitivity values of 09321000 specificity values of 09440997 precision values of 08880997 and accuracy values of 09400994 absolute pearson correlation coefficient values between heart sounds of the four types of chd and lvef right ventricular systolic pressure rvsp and malformation size were all less than 03the rcrnet model can preliminarily determine types of lefttoright shunt chd and improve diagnostic efficiency which may provide a new choice algorithmic chd screening in children",Other
A Benchmark of State-of-the-Art Sound Event Detection Systems Evaluated on Synthetic Soundscapes,https://doi.org/10.1109/ICASSP43922.2022.9747577,"this paper proposes a benchmark of submissions to detection and classification acoustic scene and events 2021 challenge dcase task 4 representing a sampling of the stateoftheart in sound event detection task the submissions are evaluated according to the two polyphonic sound detection score scenarios proposed for the dcase 2021 challenge task 4 which allow to make an analysis on whether submissions are designed to perform finegrained temporal segmentation coarsegrained temporal segmentation or have been designed to be polyvalent on the scenarios proposedwe study the solutions proposed by participants to analyze their robustness to varying level target to nontarget signaltonoise ratio and to temporal localization of target sound events a last experiment is proposed in order to study the impact of nontarget events on systems outputs results show that systems adapted to provide coarse segmentation outputs are more robust to different target to nontarget signaltonoise ratio and with the help of specific data augmentation methods they are more robust to time localization of the original event results of the last experiment display that systems tend to spuriously predict short events when nontarget events are present this is particularly true for systems that are tailored to have a fine segmentation",Other
Cough Sound based COVID-19 Detection with Stacked Ensemble Model,https://doi.org/10.1109/ICSSIT53264.2022.9716373,"covid19 pandeamic has affected people all over the world covid19 may manifest with different severity in different people however it predominantly affects respiratory system symptoms may vary from sore throat and cough to shortness of breath and damaged lungs this work focusses on developing a smart system for early detection of covid19 based on cough sounds and machine learning algorithms such a system would be easily accessible and may provide initial screening for detection of covid19 moreover cough sounds may be recorded by the person on smartphone avoiding the need for visiting a hospital or testing facility and getting exposed to the disease during the pandeamic first the duration of cough sound is determined in the recorded audio signal using thresholding then statistical features are extracted for cough sound and normalized finally the performance of 10 different machine learning algorithms are compared for automatic detection of covid19 the proposed stacked ensemble of machine learning models yields the best performance with an accuracy of 7986 and area under region of convergence curve of 0797 for cough sounds of new patients",Other
COVID-19 Diagnosis from Crowdsourced Cough Sound Data,https://doi.org/10.3390/app12041795,"the highly contagious and rapidly mutating covid19 virus is affecting individuals worldwide a rapid and largescale method for covid19 testing is needed to prevent infection cough testing using ai has been shown to be potentially valuable in this paper we propose a covid19 diagnostic method based on an ai cough test we used only crowdsourced cough sound data to distinguish between the cough sound of covid19positive people and that of healthy people first we used the coughvid cough database to segment only the cough sound from the original cough data an effective audio feature set was then extracted from the segmented cough sounds a deep learning model was trained on the extracted feature set the covid19 diagnostic system constructed using this method had a sensitivity of 93 and a specificity of 94 and achieved better results than models trained by other existing methods",Other
Automated sleep apnea detection in snoring signal using long short-term memory neural networks,https://doi.org/10.1016/j.bspc.2021.103238,"obstructive sleep apnea hypopnea syndrome osahs is a high incidence disease with serious hazard and potential danger the polysomnographypsg has become the gold standard to diagnose osahs however the psg is limited for household use because of its operational complexity technical nature and high consumptioncurrently microphone as a noncontacting tensor is an alternative method and researchers devoted to analyze respiratory sound for detecting and evaluating osahs patients in this paper a classifier based on long shortterm memory lstm is proposed to identify the respiratory eventrelated snoring from simple snoring firstly we collected the sleep sound of 33 patients and 10 normal people from the hospital and 4780 abnormal snoring segments and 10740 normal snoring segments were recorded by mics then melfrequency cepstrum coefficientsmfcc mel filter banks fbanks shorttime energy and linear prediction coefficientlpc representing the different characteristics of snoring are extracted as characteristic features of snoringat last a multiinput model based on lstm is designed which can receive various audio features to synthesize information to identify snoring compared with single feature network processing the use of multiple feature coefficients can identify the features of snoring at a finegrained level in the experiment our method could classify respiratory event related snoring and normal snoring at accuracy 953 and the accuracy of the threecategory snore related to the severity of osahs can reach 816 the recognition results can be used for the auxiliary diagnosis of osahs",Other
Automatic Respiratory Sound Classification Via Multi-Branch Temporal Convolutional Network,https://doi.org/10.1109/ICASSP43922.2022.9746182,"automated classification of respiratory sounds has become an active research area in recent years while recent studies have utilised deep learning methods to aid with respiratory sound classification the performance is heavily influenced by the datasets available for respiratory sound classification tasks which tend to be smaller and imbalanced in this paper we propose to explore the effectiveness of a multibranch temporal convolutional network tcn architecture integrated with squeezeandexcitation network senet a system denoted herein as mbtcnse for respiratory sound classification to the best of the authors knowledge this is the first time that such a hybrid architecture has been employed for respiratory sounds classification experiments based on the icbhi challenge respiratory sound dataset demonstrate the effectiveness of our method",Other
Automatic pulmonary auscultation grading diagnosis of Coronavirus Disease 2019 in China with artificial intelligence algorithms: A cohort study,https://doi.org/10.1016/j.cmpb.2021.106500,"research on automatic auscultation diagnosis of covid19 has not yet been developed we therefore aimed to engineer a deep learning approach for the automated grading diagnosis of covid19 by pulmonary auscultation analysis172 confirmed cases of covid19 in tongji hospital were divided into moderate severe and critical group pulmonary auscultation were recorded in 610 sites per patient through 3m littmann stethoscope and the data were transferred to computer to construct the dataset convolutional neural network cnn were designed to generate classifications of the auscultation f1 score the area under the curve auc of the receiver operating characteristic curve sensitivity and specificity were quantified another 45 normal patients were served as control groupthere are about 5652 5946 and 7885 abnormal auscultation in the moderate severe and critical groups respectively the model showed promising performance with an averaged f1 scores 09938 95 ci 0992309952 auc roc score 09999 95 ci 0999810000 sensitivity 09938 95 ci 0991009965 and specificity 09979 95 ci 0997009988 in identifying the covid19 patients among normal moderate severe and critical group it is capable in identifying crackles wheezes phlegm sounds with an averaged f1 scores 09475 95 ci 0944009508 auc roc score 09762 95 ci 0984809865 sensitivity 09482 95 ci 0939309578 and specificity 09835 95 ci 0980609863our model is accurate and efficient in automatically diagnosing covid19 according to different categories laying a promising foundation for aienabled auscultation diagnosing systems for lung diseases in clinical applications",Other
A study of using cough sounds and deep neural networks for the early detection of Covid-19,https://doi.org/10.1016/j.bea.2022.100025,"the current clinical diagnosis of covid19 requires persontoperson contact needs variable time to produce results and is expensive it is even inaccessible to the general population in some developing countries due to insufficient healthcare facilities hence a lowcost quick and easily accessible solution for covid19 diagnosis is vital this paper presents a study that involves developing an algorithm for automated and noninvasive diagnosis of covid19 using cough sound samples and a deep neural network the cough sounds provide essential information about the behavior of glottis under different respiratory pathological conditions hence the characteristics of cough sounds can identify respiratory diseases like covid19 the proposed algorithm consists of three main steps a extraction of acoustic features from the cough sound samples b formation of a feature vector and c classification of the cough sound samples using a deep neural network the output from the proposed system provides a covid19 likelihood diagnosis in this work we consider three acoustic feature vectors namely a timedomain b frequencydomain and c mixeddomain ie a combination of features in both timedomain and frequencydomain the performance of the proposed algorithm is evaluated using cough sound samples collected from healthy and covid19 patients the results show that the proposed algorithm automatically detects covid19 cough sound samples with an overall accuracy of 892 975 and 938 using timedomain frequencydomain and mixeddomain feature vectors respectively the proposed algorithm coupled with its high accuracy demonstrates that it can be used for quick identification or early screening of covid19 we also compare our results with that of some stateoftheart works",Other
Design of Bird Sound Recognition Model Based on Lightweight,https://doi.org/10.1109/ACCESS.2022.3198104,"bird sounds recognition is of great significance in bird protection with appropriate sound classification research can automatically predict the quality of life in the area nowadays the deep learning model is used to classify bird sound data with high classification accuracy however the generalization ability of most existing bird sound recognition models is poor and the complicated algorithm is applied to extract bird sound features to address these problems a large data set containing 264 kinds of birds is constructed in this paper to enhance the generalization ability of the model and then a lightweight bird sound recognition model is proposed to build a lightweight feature extraction and recognition network with mobilenetv3 as the backbone by adjusting the depthwise separable convolution in the model the recognition ability of the model is improved a multiscale feature fusion structure is designed and the psa pyramid split attention psa module is added to the multiscale feature fusion structure to improve the adaptability of the network to scale extraction of spatial information and channel information to improve the refinement ability of the model towards the global information the channel attention mechanism and ordinary convolution are introduced into bneck module which makes the bneck module become the bnecks module the experimental results show that the accuracy of top1 and top5 of the model in identifying 264 kinds of birds on the selfbuilt data set is 9512 and 100 which are higher than that of mobilenetv1 mobilenetv2 mobilenetv3 respectively although the accuracy is lower than resnet50 the number of parameters and floatingpoint operations flops of the model is only 26m and 127m respectively the accuracy is only reduced by 225 while saving costs",Other
End-to-End Video Instance Segmentation with Transformers,https://doi.org/10.48550/arXiv.2011.14503 	,"video instance segmentation vis is the task that requires simultaneously classifying segmenting and tracking object instances of interest in video recent methods typically develop sophisticated pipelines to tackle this task here we propose a new video instance segmentation framework built upon transformers termed vistr which views the vis task as a direct endtoend parallel sequence decodingprediction problem given a video clip consisting of multiple image frames as input vistr outputs the sequence of masks for each instance in the video in order directly at the core is a new effective instance sequence matching and segmentation strategy which supervises and segments instances at the sequence level as a whole vistr frames the instance segmentation and tracking in the same perspective of similarity learning thus considerably simplifying the overall pipeline and is significantly different from existing approaches without bells and whistles vistr achieves the highest speed among all existing vis models and achieves the best result among methods using single model on the youtubevis dataset for the first time we demonstrate a much simpler and faster video instance segmentation framework built upon transformers achieving competitive accuracy we hope that vistr can motivate future research for more video understanding tasks",Other
Language-driven Semantic Segmentation,https://doi.org/10.48550/arXiv.2201.03546,"we present lseg a novel model for languagedriven semantic image segmentation lseg uses a text encoder to compute embeddings of descriptive input labels eg grass or building together with a transformerbased image encoder that computes dense perpixel embeddings of the input image the image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class the text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space eg cat and furry this allows lseg to generalize to previously unseen categories at test time without retraining or even requiring a single additional training sample we demonstrate that our approach achieves highly competitive zeroshot performance compared to existing zero and fewshot semantic segmentation methods and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided code and demo are available at httpsgithubcomislorglangseg",Other
Speech Emotion Recognition Using Deep Neural Network Considering Verbal and Nonverbal Speech Sounds,https://doi.org/10.1109/ICASSP.2019.8682283,"speech emotion recognition is becoming increasingly important for many applications in reallife communication nonverbal sounds within an utterance also play an important role for people to recognize emotion in current studies only few emotion recognition systems considered nonverbal sounds such as laughter cries or other emotion interjection which naturally exists in our daily conversation in this work both verbal and nonverbal sounds within an utterance were thus considered for emotion recognition of reallife conversations firstly an svmbased verbalnonverbal sound detector was developed a prosodic phrase pph autotagger was further employed to extract the verbalnonverbal segments for each segment the emotion and sound features were respectively extracted based on convolutional neural networks cnns and then concatenated to form a cnnbased generic feature vector finally a sequence of cnnbased feature vectors for an entire dialog turn was fed to an attentive long shortterm memory lstmbased sequencetosequence model to output an emotional sequence as recognition result experimental results on the recognition of seven emotional states in the nnime the nthuntua chinese interactive multimodal emotion corpus showed that the proposed method achieved a detection accuracy of 5200 outperforming the traditional methods",Other
Sound Event Detection and Time–Frequency Segmentation from Weakly Labelled Data,https://doi.org/10.1109/TASLP.2019.2895254,"sound event detection sed aims to detect when and recognize what sound events happen in an audio clip many supervised sed algorithms rely on strongly labelled data which contains the onset and offset annotations of sound events however many audio tagging datasets are weakly labelled that is only the presence of the sound events is known without knowing their onset and offset annotations in this paper we propose a timefrequency tf segmentation framework trained on weakly labelled data to tackle the sound event detection and separation problem in training a segmentation mapping is applied on a tf representation such as log mel spectrogram of an audio clip to obtain tf segmentation masks of sound events the tf segmentation masks can be used for separating the sound events from the background scenes in the timefrequency domain then a classification mapping is applied on the tf segmentation masks to estimate the presence probabilities of the sound events we model the segmentation mapping using a convolutional neural network and the classification mapping using a global weighted rank pooling gwrp in sed predicted onset and offset times can be obtained from the tf segmentation masks as a byproduct separated waveforms of sound events can be obtained from the tf segmentation masks we remixed the dcase 2018 task 1 acoustic scene data with the dcase 2018 task 2 sound events data when mixing under 0 db the proposed method achieved f1 scores of 0534 0398 and 0167 in audio tagging framewise sed and eventwise sed outperforming the fully connected deep neural network baseline of 0331 0237 and 0120 respectively in tf segmentation we achieved an f1 score of 0218 where previous methods were not able to do tf segmentation",Other
Classification of snoring sound based on a recurrent neural network,https://doi.org/10.1016/j.eswa.2019.01.020,"abstract snoring is a sleep disorder that may have adverse effects on an individuals health and social activities polysomnography is the most common way to diagnose snoring but involves considerable time and cost many recent studies have attempted to classify snoring and nonsnoring however since the length frequency and period of snoring episodes se differ according to the individual being measured it is very difficult to develop a general reference point to classify snoring therefore in order to classify different snoring patterns and noise for different individuals a learningbased snoring classification algorithm is essential to this end this study proposes a classification method based on a recurrent neural network rnn that can classify ses and nonsnoring episodes nses by learning the features of an individuals ses and nses measured in daily life based on the subjects sleep recordings using smartphone the method proposed in this study can be largely divided into segmentation feature extraction and classification the performance of this study was evaluated through statistical parameters despite the fact that the proposed rnnbased classifiers were trained using a relative small dataset they exhibited an extremely high accuracy of 989",Other
Context Prior for Scene Segmentation,https://doi.org/10.48550/arXiv.2004.01547 	,"recent works have widely explored the contextual dependencies to achieve more accurate segmentation results however most approaches rarely distinguish different types of contextual dependencies which may pollute the scene understanding in this work we directly supervise the feature aggregation to distinguish the intraclass and interclass context clearly specifically we develop a context prior with the supervision of the affinity loss given an input image and corresponding ground truth affinity loss constructs an ideal affinity map to supervise the learning of context prior the learned context prior extracts the pixels belonging to the same category while the reversed prior focuses on the pixels of different classes embedded into a conventional deep cnn the proposed context prior layer can selectively capture the intraclass and interclass contextual dependencies leading to robust feature representation to validate the effectiveness we design an effective context prior network cpnet extensive quantitative and qualitative evaluations demonstrate that the proposed model performs favorably against stateoftheart semantic segmentation approaches more specifically our algorithm achieves 463 miou on ade20k 539 miou on pascalcontext and 813 miou on cityscapes code is available at httpsgitiocontextprior",Other
Boundary IoU: Improving Object-Centric Image Segmentation Evaluation,https://doi.org/10.48550/arXiv.2103.16562,"we present boundary iou intersectionoverunion a new segmentation evaluation measure focused on boundary quality we perform an extensive analysis across different error types and object sizes and show that boundary iou is significantly more sensitive than the standard mask iou measure to boundary errors for large objects and does not overpenalize errors on smaller objects the new quality measure displays several desirable characteristics like symmetry wrt predictionground truth pairs and balanced responsiveness across scales which makes it more suitable for segmentation evaluation than other boundaryfocused measures like trimap iou and fmeasure based on boundary iou we update the standard evaluation protocols for instance and panoptic segmentation tasks by proposing the boundary ap average precision and boundary pq panoptic quality metrics respectively our experiments show that the new evaluation metrics track boundary quality improvements that are generally overlooked by current mask ioubased evaluation metrics we hope that the adoption of the new boundarysensitive evaluation metrics will lead to rapid progress in segmentation methods that improve boundary quality",Other
Plant Disease Detection and Classification by Deep Learning,https://doi.org/10.3390/plants8110468,"plant diseases affect the growth of their respective species therefore their early identification is very important many machine learning ml models have been employed for the detection and classification of plant diseases but after the advancements in a subset of ml that is deep learning dl this area of research appears to have great potential in terms of increased accuracy many developedmodified dl architectures are implemented along with several visualization techniques to detect and classify the symptoms of plant diseases moreover several performance metrics are used for the evaluation of these architecturestechniques this review provides a comprehensive explanation of dl models used to visualize various plant diseases in addition some research gaps are identified from which to obtain greater transparency for detecting diseases in plants even before their symptoms appear clearly",Other
Arabic text classification using deep learning models,https://doi.org/10.1016/j.ipm.2019.102121,"abstract text classification or categorization is the process of automatically tagging a textual document with most relevant labels or categories when the number of labels is restricted to one the task becomes singlelabel text categorization however the multilabel version is challenging for arabic language both tasks especially the latter one become more challenging in the absence of large and free arabic rich and rational datasets therefore we introduce new rich and unbiased datasets for both the singlelabel sanad as well as the multilabel nadia arabic text categorization tasks both corpora are made freely available to the research community on arabic computational linguistics further we present an extensive comparison of several deep learning dl models for arabic text categorization in order to evaluate the effectiveness of such models on sanad and nadia a unique characteristic of our proposed work when compared to existing ones is that it does not require a preprocessing phase and fully based on deep learning models besides we studied the impact of utilizing word2vec embedding models to improve the performance of the classification tasks our experimental results showed solid performance of all models on sanad corpus with a minimum accuracy of 9118 achieved by convolutionalgru and top performance of 9694 achieved by attentiongru as for nadia attentiongru achieved the highest overall accuracy of 8868 for a maximum subsets of 10 categories on masrawy dataset",Other
LightAMC: Lightweight Automatic Modulation Classification via Deep Learning and Compressive Sensing,https://doi.org/10.1109/TVT.2020.2971001,"automatic modulation classification amc is an promising technology for noncooperative communication systems in both military and civilian scenarios recently deep learning dl based amc methods have been proposed with outstanding performances however both high computing cost and large model sizes are the biggest hinders for deployment of the conventional dl based methods particularly in the application of internetofthings iot networks and unmanned aerial vehicle uavaided systems in this correspondence a novel dl based lightweight amc lightamc method is proposed with smaller model sizes and faster computational speed we first introduce a scaling factor for each neuron in convolutional neural network cnn and enforce scaling factors sparsity via compressive sensing it can give an assist to screen out redundant neurons and then these neurons are pruned experimental results show that the proposed lightamc method can effectively reduce model sizes and accelerate computation with the slight performance loss",Other
Image classification with deep learning in the presence of noisy labels: A survey,https://doi.org/10.1016/j.knosys.2021.106771,"image classification systems recently made a giant leap with the advancement of deep neural networks however these systems require an excessive amount of labeled data to be adequately trained gathering a correctly annotated dataset is not always feasible due to several factors such as the expensiveness of the labeling process or difficulty of correctly classifying data even for the experts because of these practical challenges label noise is a common problem in realworld datasets and numerous methods to train deep neural networks with label noise are proposed in the literature although deep neural networks are known to be relatively robust to label noise their tendency to overfit data makes them vulnerable to memorizing even random noise therefore it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently even though an extensive survey of machine learning techniques under label noise exists the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels this paper aims to present these algorithms while categorizing them into one of the two subgroups noise model based and noise model free methods algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels differently methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses regularizers or other learning paradigms",Other
Plant leaf disease classification using EfficientNet deep learning model,https://doi.org/10.1016/j.ecoinf.2020.101182,"most plant diseases show visible symptoms and the technique which is accepted today is that an experienced plant pathologist diagnoses the disease through optical observation of infected plant leaves the fact that the disease diagnosis process is slow to perform manually and another fact that the success of the diagnosis is proportional to the pathologists capabilities makes this problem an excellent application area for computeraided diagnostic systems instead of classical machine learning methods in which manual feature extraction should be flawless to achieve successful results there is a need for a model that does not need preprocessing and can perform a successful classification in this study efficientnet deep learning architecture was proposed in plant leaf disease classification and the performance of this model was compared with other stateoftheart deep learning models the plantvillage dataset was used to train models all the models were trained with original and augmented datasets having 55448 and 61486 images respectively efficientnet architecture and other deep learning models were trained using transfer learning approach in the transfer learning all layers of the models were set to be trainable the results obtained in the test dataset showed that b5 and b4 models of efficientnet architecture achieved the highest values compared to other deep learning models in original and augmented datasets with 9991 and 9997 respectively for accuracy and 9842 and 9939 respectively for precision efficientnet architecture was proposed for plant leaf disease classification the plantvillage dataset containing 55448 images with 39 classes was used proposed model was compared with other stateoftheart deep learning models all models were trained using transfer learning technique efficientnet b5 and b4 models were superior to other models in terms of accuracy",Other
Deep learning for electroencephalogram (EEG) classification tasks: a review,https://doi.org/10.1088/1741-2552/ab0ab5,"electroencephalography eeg analysis has been an important tool in neuroscience with applications in neuroscience neural engineering eg braincomputer interfaces bcis and even commercial applications many of the analytical tools used in eeg studies have used machine learning to uncover relevant information for neural classification and neuroimaging recently the availability of large eeg data sets and advances in machine learning have both led to the deployment of deep learning architectures especially in the analysis of eeg signals and in understanding the information it may contain for brain functionality the robust automatic classification of these signals is an important step towards making the use of eeg more practical in many applications and less reliant on trained professionals towards this goal a systematic review of the literature on deep learning applications to eeg classification was performed to address the following critical questions 1 which eeg classification tasks have been explored with deep learning 2 what input formulations have been used for training the deep networks 3 are there specific deep learning network structures suitable for specific types of tasksa systematic literature review of eeg classification using deep learning was performed on web of science and pubmed databases resulting in 90 identified studies those studies were analyzed based on type of task eeg preprocessing methods input type and deep learning architecturefor eeg classification tasks convolutional neural networks recurrent neural networks deep belief networks outperform stacked autoencoders and multilayer perceptron neural networks in classification accuracy the tasks that used deep learning fell into five general groups emotion recognition motor imagery mental workload seizure detection event related potential detection and sleep scoring for each type of task we describe the specific input formulation major characteristics and end classifier recommendations found through this reviewthis review summarizes the current practices and performance outcomes in the use of deep learning for eeg classification practical suggestions on the selection of many hyperparameters are provided in the hope that they will promote or guide the deployment of deep learning to eeg datasets in future research",Other
Potato Leaf Disease Classification Using Deep Learning Approach,https://doi.org/10.1109/IES50839.2020.9231784,"potato is one of the staple foods that widely consumed becoming the 4th staple food consumed throughout the world also the world demand for potato is increasing significantly primarily due to the world pandemic coronavirus however potato diseases are the leading cause of the decline in the quality and quantity of the harvest inappropriate classification and late detection of the diseases type will drastically worsen the plant conditions fortunately several diseases in potato plants can be identified based on leaf conditions therefore in this paper we present a system to classify the four types of diseases in potato plants based on leaf conditions by utilising deep learning using the vgg16 and vgg19 convolutional neural network architecture model to obtain an accurate classification system this experiment has achieved an average accuracy of 91 which indicates the feasibility of the deep neural network approach",Other
Temperate fish detection and classification: a deep learning based approach,https://doi.org/10.1007/s10489-020-02154-9,"abstract a wide range of applications in marine ecology extensively uses underwater cameras still to efficiently process the vast amount of data generated we need to develop tools that can automatically detect and recognize species captured on film classifying fish species from videos and images in natural environments can be challenging because of noise and variation in illumination and the surrounding habitat in this paper we propose a twostep deep learning approach for the detection and classification of temperate fishes without prefiltering the first step is to detect each single fish in an image independent of species and sex for this purpose we employ the you only look once yolo object detection technique in the second step we adopt a convolutional neural network cnn with the squeezeandexcitation se architecture for classifying each fish in the image without prefiltering we apply transfer learning to overcome the limited training samples of temperate fishes and to improve the accuracy of the classification this is done by training the object detection model with imagenet and the fish classifier via a public dataset fish4knowledge whereupon both the object detection and classifier are updated with temperate fishes of interest the weights obtained from pretraining are applied to posttraining as a priori our solution achieves the stateoftheart accuracy of 9927 using the pretraining model the accuracies using the posttraining model are also high 8368 and 8774 with and without image augmentation respectively this strongly indicates that the solution is viable with a more extensive dataset",Other
Deep learning for time series classification: a review,https://doi.org/10.1007/s10618-019-00619-1,"time series classification tsc is an important and challenging problem in data mining with the increase of time series data availability hundreds of tsc algorithms have been proposed among these methods only a few have considered deep neural networks dnns to perform this task this is surprising as deep learning has seen very successful applications in the last years dnns have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as residual and convolutional neural networks apart from images sequential data such as text and audio can also be processed with dnns to reach stateoftheart performance for document classification and speech recognition in this article we study the current stateoftheart performance of deep learning algorithms for tsc by presenting an empirical study of the most recent dnn architectures for tsc we give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of dnns for tsc we also provide an open source deep learning framework to the tsc community where we implemented each of the compared approaches and evaluated them on a univariate tsc benchmark the ucruea archive and 12 multivariate time series datasets by training 8730 deep learning models on 97 time series datasets we propose the most exhaustive study of dnns for tsc to date",Other
Plant Disease Classification: A Comparative Evaluation of Convolutional Neural Networks and Deep Learning Optimizers,https://doi.org/10.3390/plants9101319,"recently plant disease classification has been done by various stateoftheart deep learning dl architectures on the publicly availableauthor generated datasets this research proposed the deep learningbased comparative evaluation for the classification of plant disease in two steps firstly the best convolutional neural network cnn was obtained by conducting a comparative analysis among wellknown cnn architectures along with modified and cascadedhybrid versions of some of the dl models proposed in the recent researches secondly the performance of the bestobtained model was attempted to improve by training through various deep learning optimizers the comparison between various cnns was based on performance metrics such as validation accuracyloss f1score and the required number of epochs all the selected dl architectures were trained in the plantvillage dataset which contains 26 different diseases belonging to 14 respective plant species keras with tensorflow backend was used to train deep learning architectures it is concluded that the xception architecture trained with the adam optimizer attained the highest validation accuracy and f1score of 9981 and 09978 respectively which is comparatively better than the previous approaches and it proves the novelty of the work therefore the method proposed in this research can be applied to other agricultural applications for transparent detection and classification purposes",Other
CropDeep: The Crop Vision Dataset for Deep-Learning-Based Classification and Detection in Precision Agriculture,https://doi.org/10.3390/s19051058,"intelligence has been considered as the major challenge in promoting economic potential and production efficiency of precision agriculture in order to apply advanced deeplearning technology to complete various agricultural tasks in online and offline ways a large number of crop vision datasets with domainspecific annotation are urgently needed to encourage further progress in challenging realistic agricultural conditions we present the cropdeep species classification and detection dataset consisting of 31147 images with over 49000 annotated instances from 31 different classes in contrast to existing vision datasets images were collected with different cameras and equipment in greenhouses captured in a wide variety of situations it features visually similar species and periodic changes with more representative annotations which have supported a stronger benchmark for deeplearningbased classification and detection to further verify the application prospect we provide extensive baseline experiments using stateoftheart deeplearning classification and detection models results show that current deeplearningbased methods achieve well performance in classification accuracy over 99 while current deeplearning methods achieve only 92 detection accuracy illustrating the difficulty of the dataset and improvement room of stateoftheart deeplearning models when applied to crops production and management specifically we suggest that the yolov3 network has good potential application in agricultural detection tasks",Other
EEG Classification of Motor Imagery Using a Novel Deep Learning Framework,https://doi.org/10.3390/s19030551,"successful applications of braincomputer interface bci approaches to motor imagery mi are still limited in this paper we propose a classification framework for mi electroencephalogram eeg signals that combines a convolutional neural network cnn architecture with a variational autoencoder vae for classification the decoder of the vae generates a gaussian distribution so it can be used to fit the gaussian distribution of eeg signals a new representation of input was developed by combining the time frequency and channel information from the eeg signal and the cnnvae method was designed and optimized accordingly for this form of input in this network the classification of the extracted cnn features is performed via the deep network vae our framework with an average kappa value of 0564 outperforms the best classification method in the literature for bci competition iv dataset 2b with a 3 improvement furthermore using our own dataset the cnnvae framework also yields the best performance for both threeelectrode and fiveelectrode eegs and achieves the best average kappa values 0568 and 0603 respectively our results show that the proposed cnnvae method raises performance to the current state of the art",Other
A Deep Learning Model for Automated Sleep Stages Classification Using PSG Signals,https://doi.org/10.3390/ijerph16040599,"sleep disorder is a symptom of many neurological diseases that may significantly affect the quality of daily life traditional methods are timeconsuming and involve the manual scoring of polysomnogram psg signals obtained in a laboratory environment however the automated monitoring of sleep stages can help detect neurological disorders accurately as well in this study a flexible deep learning model is proposed using raw psg signals a onedimensional convolutional neural network 1dcnn is developed using electroencephalogram eeg and electrooculogram eog signals for the classification of sleep stages the performance of the system is evaluated using two public databases sleepedf and sleepedfx the developed model yielded the highest accuracies of 9806 9464 9236 9122 and 9100 for two to six sleep classes respectively using the sleepedf database further the proposed model obtained the highest accuracies of 9762 9434 9233 9098 and 8954 respectively for the same two to six sleep classes using the sleepedfx dataset the developed deep learning model is ready for clinical usage and can be tested with big psg data",Other
Leaf Segmentation and Classification with a Complicated Background Using Deep Learning,https://doi.org/10.3390/agronomy10111721,"the segmentation and classification of leaves in plant images are a great challenge especially when several leaves are overlapping in images with a complicated background in this paper the segmentation and classification of leaf images with a complicated background using deep learning are studied first more than 2500 leaf images with a complicated background are collected and artificially labeled with target pixels and background pixels twothousand of them are fed into a mask regionbased convolutional neural network mask rcnn to train a model for leaf segmentation then a training set that contains more than 1500 training images of 15 species is fed into a very deep convolutional network with 16 layers vgg16 to train a model for leaf classification the best hyperparameters for these methods are found by comparing a variety of parameter combinations the results show that the average misclassification error me of 80 test images using mask rcnn is 115 the average accuracy value for the leaf classification of 150 test images using vgg16 is up to 915 this indicates that these methods can be used to segment and classify the leaf image with a complicated background effectively it could provide a reference for the phenotype analysis and automatic classification of plants",Other
Non‐invasive setup for grape maturation classification using deep learning,https://doi.org/10.1002/jsfa.10824,"background the san francisco valley region from brazil is known worldwide for its fruit production and exportation especially grapes and wines the grapes have high quality not only due to the excellent morphological characteristics but also to the pleasant taste of their fruits such features are obtained because of the climatic conditions present in the region in addition to the favorable climate for grape cultivation harvesting at the right time interferes with fruit properties results this work aims to define grape maturation stage of syrah and cabernet sauvignon cultivars with the aid of deeplearning models the idea of working with these algorithms came from the fact that the techniques commonly used to find the ideal harvesting point are invasive expensive and take a long time to get their results in this work convolutional neural networks were used in an image classification system in which grape images were acquired preprocessed and classified based on their maturation stage images were acquired with varying illuminants that were considered as parameters of the classification models as well as the different postharvesting weeks the best models achieved maturation classification accuracy of 9341 and 7266 for syrah and cabernet sauvignon respectively conclusions it was possible to correctly classify wine grapes using computational intelligent algorithms with high accuracy regarding the harvesting time corroborating chemometric results 2020 society of chemical industry",Other